[
    {
        "title": "Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations",
        "answer": "alternately with one mini-batch of labeled data and INLINEFORM0 mini-batches of unlabeled data. Labeled data are input into the model to calculate the standard supervised loss for each mini-batch and the model weights are updated regularly. Meanwhile, each mini-batch of unlabeled data is selected randomly from the pool of all unlabeled data; the model computes the loss for CVT from the mini-batch of unlabeled data. This CVT loss is used to train auxiliary prediction modules, which see restricted views of the input, to match the output of the primary prediction module, which is the full model that sees the entire input.  The CVT loss is used to train auxiliary prediction modules, which see restricted views of the input, to match the output of the primary prediction module, which is the full model that sees the entire input.  The CVT loss is used to train auxiliary prediction modules, which see restricted views of the input, to match the output of the primary prediction module, which is the full model that sees the entire input.  The CVT loss is used to train auxiliary prediction modules, which see restricted views of the input, to match the output of the primary prediction module, which is the full model that sees the entire input.",
        "evidence": [
            "to (1) utilize the expressive power of neural network representations to smooth over superficial differences in the surface forms, (2) model pairwise and transitive relationships between tags, and (3) accurately generate tag sets that are unseen or rare in the training data. Experiments on four languages from the Universal Dependencies Treebank demonstrate superior tagging accuracies over existing cross-lingual approaches.",
            "Model Improvements",
            "CVT BIBREF20 is a semi-supervised learning technique whose goal is to improve the model representation using a combination of labeled and unlabeled data. During training, the model is trained alternately with one mini-batch of labeled data and INLINEFORM0 mini-batches of unlabeled data. Labeled data are input into the model to calculate the standard supervised loss for each mini-batch and the model weights are updated regularly. Meanwhile, each mini-batch of unlabeled data is selected randomly from the pool of all unlabeled data; the model computes the loss for CVT from the mini-batch of unlabeled data. This CVT loss is used to train auxiliary prediction modules, which see restricted views of the input, to match the output of the primary prediction module, which is the full model that",
            "gender biases in embeddings and to develop algorithms to reduce bias in some embedding. Hence, the authors show that embeddings can be used in applications without amplifying gender bias.",
            "What Does Zero-shot Transfer Model Learn? ::: Typology-manipulated Dataset",
            "Experiments ::: Label-partially-unseen evaluation ::: Discussion.",
            "settings using human experts. Furthermore, for insight-driven analyses, it can be more useful to focus on improving explanatory power than making small improvements in predictive performance.",
            "There are several recent studies applying domain adaptation methods to deep neural networks. However, few studies have focused on improving the fine tuning and dual outputs methods in the supervised setting. sun2015return have proposed an unsupervised domain adaptation method and apply it to the features from deep neural networks. Their idea is to minimize the domain shift by aligning the second-order statistics of source and target distributions. In our setting, it is not necessarily true that there is a correspondence between the source and target input distributions, and therefore we cannot expect their method to work well. wen2016multi have proposed a procedure to generate natural language for multiple domains of spoken dialogue systems. They improve the fine tuning method by",
            "topic datasets, adding that latent topics still benefits performance: even when we are discussing the same topic, we use different arguments and supporting evidence. Lastly, we get 4.8% improvement when adding comment information and it achieves comparable performance to UTCNN without topic information, which shows that comments also benefit performance. For platforms where user IDs are pixelated or otherwise hidden, adding comments to a text model still improves performance. In its integration of user, content, and comment information, the full UTCNN produces the highest f-scores on all Sup, Neu, and Uns stances among models that predict the Uns class, and the highest macro-average f-score overall. This shows its ability to balance a biased dataset and supports our claim that UTCNN",
            "passage, as shown in Table TABREF32 ; consequently the model representation learns new information from these new words effectively.",
            "human coding - data must be read and labelled correctly. This can require substantial resources. At the same time, the advantage is that validation of a supervised learning result is relatively straightforward as it requires comparing prediction results with actual outcomes. Furthermore, there is no need to label all text documents (or interview data from each respondent) prior to analyzing them. Rather, a sufficiently large set of documents can be labelled to train an algorithm and then used to classify the remaining documents. In unsupervised learning, the outcome variable is unknown. The exercise is, therefore, of a more exploratory nature. Here the purpose is to reveal patterns in the data that allow us to distinguish distinct groups whose differences are small, while variations",
            "was never seen in the training data, so it is replaced with $\\textit {unknown}$ ; however, it would be better to replace it with $\\textit {honcho\\_N/N}$ , which was seen in the training data. Finally, although using connected entities in Freebase as additional candidates during inference is helpful, it often over- or under-generates candidates. A more tailored, per-query search process could improve performance."
        ]
    },
    {
        "title": "Artificial Error Generation with Machine Translation and Syntactic Patterns",
        "answer": "English, Turkish, Chinese, Polish, Russian, Finnish, French, Spanish, Italian, German, Portuguese, Dutch, Swedish, Danish, Norwegian, Greek, Romanian, Hungarian, Czech, Slovak, Bulgarian, Croatian, Serbian, Slovenian, Macedonian, Albanian, Bosnian, Montenegrin, Ukrainian, Belarusian, Estonian, Latvian, Lithuanian, Georgian, Armenian, Azerbaijani, Kazakh, Kyrgyz, Tajik, Uzbek, Turkmen, Mongolian, Japanese, Korean, Vietnamese, Thai, Indonesian, Malay, Swahili, Zulu, Xhosa, Yoruba, Igbo, Shona, Amharic, Tigrinya, Somali, Oromo, Hausa, Yiddish, Hebrew, Arabic, Persian, Pashto, Dari, Kurdish, Turkish, Uzbek, Azerbaijani, Kazakh, Kyrgyz, Tajik, Turkmen, Mongolian, Japanese, Korean, Vietnamese, Thai, Indonesian, Malay, Swahili, Zulu, Xhosa, Yoruba, Igbo, Shona, Amharic, Tigrinya, Somali, Oromo, Hausa, Yiddish, Hebrew, Arabic, Persian, Pas",
        "evidence": [
            "Analysis of the convolutional language model",
            "remains unscathed. This effectively means that m-bert's subword vocabulary contains plenty of cmn-specific and yue-specific subwords which are exploited by the encoder when producing m-bert-based representations. Simultaneously, higher scores with m-bert (and xlm in Table TABREF46) are reported for resource-rich languages such as French, Spanish, and English, which are better represented in m-bert's training data. We also observe lower absolute scores (and a larger number of OOVs) for languages with very rich and productive morphological systems such as the two Slavic languages (Polish and Russian) and Finnish. Since Polish and Russian are known to have large Wikipedias and Common Crawl data BIBREF33 (e.g., their Wikipedias are in the top 10 largest Wikipedias worldwide), the problem with",
            "the relevant noise needed in training neural architectures in such low-resourced situations. The main contributions of this paper are:",
            "total of 229 languages. This test set omits 23 of the high resource languages that are written in unique scripts or for which language distance metrics could not be computed.",
            "In Turkish, people sometimes prefer to spell English characters for the corresponding Turkish characters (e.g. i for ı, c for ç) when writing in electronic format. To normalise such words, we used the Zemberek tool BIBREF15. All punctuation marks except “!\" and “?\" are removed, since they do not contribute much to the polarity of a document. We took into account emoticons, such as “:))\", and idioms, such as “kafayı yemek” (lose one's mind), since two or more words can express a sentiment together, irrespective of the individual words thereof. Since Turkish is an agglutinative language, we used the morphological parser and disambiguator tools BIBREF16, BIBREF17. We also performed negation handling and stop-word elimination. In negation handling, we append an underscore to the end of a word",
            "Translation experiments",
            "we would like to explore more sophistical structure of documents, like discourse tree, instead of rough topic segments. As for evaluation, we would like to elicit human judgments, for instance by inviting authors to rate the outputs from different systems, when applied to their own papers. More long term, we will study how extractive/abstractive techniques can be integrated; for instance, the output of an extractive system could be fed into an abstractive one, training the two jointly.",
            "Data Collection. To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. Paragraph Alignment. To further ensure the quality of the new dataset, the work of paragraph alignment is manually completed. After data cleaning and manual paragraph alignment, we obtained 35K aligned bilingual paragraphs. Clause Alignment. We applied our clause alignment algorithm on the 35K aligned bilingual",
            "Language Resource References\nlrec lrec2018",
            "Other languages, other ambiguities",
            "rest of this paper is structured as follows. Section SECREF2 discusses the related work that served as an inspiration for our analysis. Section SECREF3 comprises the proposed criteria and properties to characterize AV methods. Section SECREF4 describes the methodology, consisting of the used corpora, examined AV methods, selected performance measures and experiments. Finally, Section SECREF5 concludes the work and outlines future work.",
            "Because the language ID token is so beneficial to performance, it would be very interesting to find ways to extend a similar benefit to unseen languages. One possible way to do so is with tokens that identify something other than the language, such as typological features about the language's phonemic inventory. This could enable better sharing of resources among languages. Such typological knowledge is readily available in databases like Phoible and WALS for a wide variety of languages. It would be interesting to explore if any of these features is a good predictor of a language's orthographic rules. It would also be interesting to apply the artificial token approach to other problems besides multilingual g2p. One closely related application is monolingual English g2p. Some of the"
        ]
    },
    {
        "title": "Generating Word and Document Embeddings for Sentiment Analysis",
        "answer": "N_{t} / (N + N^{\\prime }) * w_{t} + N^{\\prime }_{t} / (N + N^{\\prime }) * (-w_{t}) + 1.  ( DISPLAY_FORM4 ) .  ( DISPLAY_FORM5 ) .  ( DISPLAY_FORM6 ) .  ( DISPLAY_FORM7 ) .  ( DISPLAY_FORM8 ) .  ( DISPLAY_FORM9 ) .  ( DISPLAY_FORM10 ) .  ( DISPLAY_FORM11 ) .  ( DISPLAY_FORM12 ) .  ( DISPLAY_FORM13 ) .  ( DISPLAY_FORM14 ) .  ( DISPLAY_FORM15 ) .  ( DISPLAY_FORM16 ) .  ( DISPLAY_FORM17 ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
        "evidence": [
            "definitions. We multiply each value in a row with the corresponding row word's raw supervised score, thereby having more meaningful clusters. Using the training data only, the supervised polarity score per word is calculated as in (DISPLAY_FORM4). Here, $ w_{t}$ denotes the sentiment score of word $t$, $N_{t}$ is the number of documents (reviews or tweets) in which $t$ occurs in the dataset of positive polarity, $N$ is the number of all the words in the corpus of positive polarity. $N^{\\prime }$ denotes the corpus of negative polarity. $N^{\\prime }_{t}$ and $N^{\\prime }$ denote similar values for the negative polarity corpus. We perform normalisation to prevent the imbalance problem and add a small number to both numerator and denominator for smoothing. As an alternative to multiplying",
            "Supervised models",
            "Word Error Rate results",
            "Supervised learning is frequently used to scale up analyses. For example, BIBREF42 wanted to analyze the motivations of Movember campaign participants. By developing a classifier based on a small set of annotations, they were able to expand the analysis to over 90k participants. The choice of supervised learning model is often guided by the task definition and the label types. For example, to identify stance towards rumors based on sequential annotations, an algorithm for learning from sequential BIBREF43 or time series data BIBREF44 could be used. The features (sometimes called variables or predictors) are used by the model to make the predictions. They may vary from content-based features such as single words, sequences of words, or information about their syntactic structure, to",
            "break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries, thesauri, WordNet) if required.  4. Annotators are kept anonymous, and are not able to communicate with each other during the annotation process. The selection criteria for the annotators required that all annotators must be native speakers of the target language. Preference to annotators with university education was given, but not required. Annotators were asked to complete a spreadsheet containing the translated pairs of words, as well as the part-of-speech, and a column to enter the score. The annotators did not have access to the original pairs in English. To ensure the quality of the collected ratings, we have employed an adjudication protocol similar to the one",
            "point of the ROC curve, provided that the given corpus is balanced. While inspecting the 250 characters long documents in more detail, we identified that they share similar vocabularies consisting of chat abbreviations such as “lol” (laughing out loud) or “k” (ok), smileys and specific obscene words. Therefore, we assume that the verification results of the examined methods are mainly caused by the similar vocabularies between the texts.",
            "algorithm and used the newly generated matrix by combining it with other subapproaches. The number of dimensions is chosen as 200 again according to the $U$ matrix. The details are given in Section 3.4. When using and evaluating this subapproach on the English corpora, we used the SentiWordNet lexicon BIBREF13. We have achieved better results for the dictionary-based algorithm when we employed the SVD reduction method compared to the use of clustering.",
            "Methodology ::: Supervised Contextual 4-scores",
            "parameters and fully connected layer, will be updated until they are mature to extract the sentence features. Subsequently, the same sets of parameters will be fine-tuned for supervised classification tasks. In sum, this pretraining is designed to produce good initial values for both model parameters and word embeddings. It is especially helpful for pretraining the embeddings of unknown words.",
            "word embeddings (PS-Embeddings) for classifying positive and negative citations. The sentence vectors formed a feature space, to which the examined citation sentence was mapped to. Those features were input into classifiers (support vector machines) for supervised classification. Using 10-cross-validation scheme, evaluation was conducted on a set of annotated citations. The results showed that word embeddings are effective on classifying positive and negative citations. However, hand-crafted features performed better for the overall classification.",
            "and transcript are available. With respect to the methods used for SBD, they mostly rely on statistical/neural machine translation BIBREF18 , BIBREF27 , language models BIBREF9 , BIBREF16 , BIBREF22 , BIBREF6 , conditional random fields BIBREF21 , BIBREF28 , BIBREF23 and deep neural networks BIBREF29 , BIBREF20 , BIBREF13 . Despite their differences in features and/or methodology, almost all previous cited research share a common element; the evaluation methodology. Metrics as Precision, Recall, F1-score, Classification Error Rate and Slot Error Rate (SER) are used to evaluate the proposed system against one reference. As discussed in Section SECREF1 , further NLP tasks rely on the result of SBD, meaning that is crucial to have a good segmentation. But comparing the output of a system",
            "meaning that for those tasks the semi-supervised models could not be used. Per task, the LSTM and feed-forward model's predictions were averaged over 10 prediction runs. Subsequently, the predictions of all individual models were combined into an average. Finally, models were removed from the ensemble in a stepwise manner if the removal increased the average score. This was done based on their original scores, i.e. starting out by trying to remove the worst individual model and working our way up to the best model. We only consider it an increase in score if the difference is larger than 0.002 (i.e. the difference between 0.716 and 0.718). If at some point the score does not increase and we are therefore unable to remove a model, the process is stopped and our best ensemble of models has"
        ]
    },
    {
        "title": "Using General Adversarial Networks for Marketing: A Case Study of Airbnb",
        "answer": "40,000 listings",
        "evidence": [
            "The data for the project was acquired from Airdna, a data processing service that collaborates with Airbnb to produce high-accuracy data summaries for listings in geographic regions of the United States. For the sake of simplicity, we focus our analysis on Airbnb listings from Manhattan, NY, during the time period of January 1, 2016, to January 1, 2017. The data provided to us contained information for roughly 40,000 Manhattan listings that were posted on Airbnb during this defined time period. For each listing, we were given information of the amenities of the listing (number of bathrooms, number of bedrooms …), the listing’s zip code, the host’s description of the listing, the price of the listing, and the occupancy rate of the listing. Airbnb defines a home's occupancy rate, as the",
            "WiSeBEWiSeBE",
            "How Many Expert Annotations?",
            "Data details and experimental setup",
            "BLEU by Sentence Length",
            "The encoder consists of an embedding layer, two stacks of transformer blocks (denoted as encoder transformer blocks and aggregation transformer blocks), and an attention layer. In the embedding layer, we aggregate both word- and character-level embeddings. Word embeddings are initialized by the 300-dimension fastText BIBREF20 vectors trained on Common Crawl (600B tokens), and are fixed during training. Character embeddings are initialized by 200-dimension random vectors. A convolutional layer with 96 kernels of size 5 is used to aggregate the sequence of characters. We use a max pooling layer on the character dimension, then a multi-layer perceptron (MLP) of size 96 is used to aggregate the concatenation of word- and character-level representations. A highway network BIBREF21 is used on",
            "minority of experts abstain to answer are filtered from the dataset. Privacy policies are ~3000 words long on average. The answers to the question asked by the users typically have ~100 words of evidence in the privacy policy document.",
            "Introduction ::: Sharing is caring",
            "The development of online peer-to-peer markets in the 1990s, galvanized by the launch of sites like eBay, fundamentally shifted the way buyers and sellers could connect [4]. These new markets not only leveraged technology to allow for faster transaction speeds, but in the process also exposed a variety of unprecedented market-designs [4]. Today, many of the most well-known peer-to-peer markets like Uber and Instacart use a centralized system that matches workers with assigned tasks via a series of complex algorithms [4]. Still, a number of other websites like Airbnb and eBay rely on sellers and buyers to organically find one-another in a decentralized fashion. In the case of these decentralized systems, sellers are asked to price and market their products in order to attract potential",
            "having the same size with the Japanese-Vietnamese parallel corpus. In the end, the data we have is double in size compared to the original one.",
            "Other languages, other ambiguities",
            "even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions. To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require background knowledge. We explicitly state the following items in the HIT instructions for question writing: No Yes-no questions should be asked. The question should have at least five words. Videos, images or inserted links should not be considered. No background knowledge should be required to answer the question. To help the workers better follow the instructions, we also include a representative example showing"
        ]
    },
    {
        "title": "Question Answering for Privacy Policies: Combining Computational and Legal Perspectives",
        "answer": "Yes",
        "evidence": [
            "Comparison with baseline models",
            "source and target side (in French). The average cross-entropy for this test set is 0.33, very close to 0, to be compared with an average cost of 58.52 when we process an actual source (in English). This means that the model has learned to copy words from source to target with no difficulty, even for sentences not seen in training. A follow-up question is whether training a copying task instead of a translation task limits the improvement: would the NMT learn better if the task was harder? To measure this, we introduce noise in the target sentences copied onto the source, following the procedure of BIBREF26 : it deletes random words and performs a small random permutation of the remaining words. Results (+ Source noise) show no difference for the French in-domain test sets, but bring the",
            "non-zero scores for other emotions. At this stage, we didn't perform any other entropy-based selection of posts, to be investigated in future work.",
            "mood from F1: 13 at the 1st percentile to F1: 33 at the 20th percentile. We observed peak F1-score performances at low percentiles for fatigue or loss of energy (5th percentile), disturbed sleep (10th percentile) as well as depressive symptoms and no evidence of depression (both 15th percentile) suggesting fewer features are needed to reach optimal performance. In contrast, peak F1-score performances occurred at moderate percentiles for evidence of depression (30th percentile) and depressed mood (55th percentile) suggesting that more features are needed to reach optimal performance. However, one notable difference between these two classes is the dramatic F1-score improvements for depressed mood i.e., 20 point increase from the 1st percentile to the 20th percentile compared to the more",
            "baselines, in contrast, illustrate that our task provides challenging test cases for the broader natural language processing community. MCScript forms the basis of a shared task organized at SemEval 2018. The dataset is available at http://www.sfb1102.uni-saarland.de/?page_id=2582.",
            "with the baseline (macro-F 0.47) and it was effective for detecting objective sentences (F-score 0.84) as well as separating X sentences from the rest (F-score 0.997), but it did not work well on distinguishing positive citations from the rest. For the overall classification (Table TABREF25 ), however, this method was not as good as hand-crafted features, such as n-grams and sentence structure features. I may conclude from this experiment that word2vec technique has the potential to capture sentiment information in the citations, but hand-crafted features have better performance.",
            "Besides the analysis on different reasoning types, we also look into the performance over questions with different first tokens in the development set, which provide us an automatic categorization of questions. According to the results in Table TABREF46 , the three neural baselines all perform the best on “Who” and “Where” questions, to which the answers are often named entities. Since the tweet contexts are short, there are only a small number of named entities to choose from, which could make the answer pattern easy to learn. On the other hand, the neural models fail to perform well on the “Why” questions, and the results of neural baselines are even worse than that of the matching baseline. We find that these questions generally have longer answer phrases than other types of questions,",
            "they combined these representations at the word level, whereas we combine them on the sentence level. Their mixed RNN model is also similar to our parse2seq baseline, although the mixed RNN decoder attended only to words. As the mixed RNN model outperformed the parallel RNN model, we do not attempt to compare our model to parallel RNN. These models are similar to ours in that they incorporate linearized parses into NMT; here, we utilize a multi-source framework.",
            "Three types of models were used in our system, a feed-forward neural network, an LSTM network and an SVM regressor. The neural nets were inspired by the work of Prayas BIBREF7 in the previous shared task. Different regression algorithms (e.g. AdaBoost, XGBoost) were also tried due to the success of SeerNet BIBREF8 , but our study was not able to reproduce their results for Spanish. For both the LSTM network and the feed-forward network, a parameter search was done for the number of layers, the number of nodes and dropout used. This was done for each subtask, i.e. different tasks can have a different number of layers. All models were implemented using Keras BIBREF9 . After the best parameter settings were found, the results of 10 system runs to produce our predictions were averaged (note",
            "We compare the proposed approach for translating natural language instructions into a navigation plan against alternative deep-learning models: [align=left,leftmargin=0em,labelsep=0.4em,font=] The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path. To test",
            "Furthermore, we test SMT models and various NMT models on the created dataset and provide a strong baseline for this task.",
            "course size and intervention ratio (hereafter, i.ratio), which is the ratio of intervened to non-intervened threads. UPA performs better than PPA and APA on low intervention courses (i.ratio INLINEFORM3 0.25) mainly because PPA and APA's performance drops steeply when i.ratio drops (see col.2 parenthesis and INLINEFORM4 of PPA and APA). While all the proposed models beat the baseline on every course except casebased-2. On medicalneuro-2 and compilers-4 which have the lowest i.ratio among the 12 courses none of the neural models better the reported baseline BIBREF7 (course level not scores not shown in this paper). The effect is pronounced in compilers-4 course where none of the neural models were able to predict any intervened threads. This is due to the inherent weakness of standard"
        ]
    },
    {
        "title": "Dynamic Compositional Neural Networks over Tree Structure",
        "answer": "Task 2: Online Testing of Task-oriented Dialogue, Mastering Training Games, and two typical tasks.  Task 1: aligning a set of instructions to a video of someone carrying out those instructions.  Task 3:  experimenting with Transformers ::: Ecosystem.  Task 4: Experiments on Additional Losses.  Task 5: popular encoders, at the same time also investigating other important questions such as performance of static versus contextualized word embeddings, or multilingual versus language-specific pretraining.  Task 6: Another purpose of the experiments is to outline the wide potential and applicability of the Multi-SimLex datasets for multilingual and cross-lingual representation learning evaluation.  Task 7: Experimental Results ::: Mastering Training Games.  Task 8: across the different compositional rules and formulate them.  Task 9: Experimental results on two typical tasks show the effectiveness of the proposed models.  Task 10: aligning a set of instructions to a video of someone carrying out those instructions.  Task 11:  experimenting with Transformers ::: Ecosystem.  Task 12: Experiments on Additional Losses.  Task 13: popular encoders, at the same time also investigating other important questions such as",
        "evidence": [
            "Translation experiments",
            "Task Definition",
            "aligning a set of instructions to a video of someone carrying out those instructions. In their task, they use the video transcript to represent the video, which they later augment with a visual cue detector on food entities. Their task focuses on procedure-based cooking videos, and contrary to our task is primarily a text alignment task. In our task we aim to answer questions-using the transcripts-on instructional-style videos, in which the answer can involve steps not mentioned in the question.",
            "Experiments\nAs explained earlier, the shared task is set up at the user level where the age, dialect, and gender of each user are the required predictions. In our experiments, we first model the task at the tweet level and then port these predictions at the user level. For our core modelling, we fine-tune BERT on the shared task data. We also introduce an additional in-house dataset labeled with dialect and gender tags to the task as we will explain below. As a baseline, we use a small gated recurrent units (GRU) model. We now introduce our tweet-level models.",
            "Experiment\nIn this section, we investigate the empirical performances of our proposed architectures on three experiments.",
            "Experimenting with Transformers ::: Ecosystem",
            "Task 2: Online Testing of Task-oriented Dialogue",
            "Experiments on Additional Losses ::: Which additional loss works better?",
            "popular encoders, at the same time also investigating other important questions such as performance of static versus contextualized word embeddings, or multilingual versus language-specific pretraining. Another purpose of the experiments is to outline the wide potential and applicability of the Multi-SimLex datasets for multilingual and cross-lingual representation learning evaluation.",
            "Data details and experimental setup",
            "Experimental Results ::: Mastering Training Games",
            "across the different compositional rules and formulate them. Experimental results on two typical tasks show the effectiveness of the proposed models."
        ]
    },
    {
        "title": "Automatic Discourse Segmentation: an evaluation in French",
        "answer": "Overall accuracy after filtering and accuracy given that the segment is in the top 10 videos.",
        "evidence": [
            "In this section we will compare the results of the different segmentation systems through automatic evaluations. First of all, the human segmentation, from the subcorpus $D$ composed of common documents. The results are presented in the table tab:humains. The first row shows the performance of the $I$ segments, taking the experts as a reference, while the second presents the process in the opposite direction. We have found that segmentation by experts and naive produces two subcorpus $E$ and $N$ with very similar characteristics. This surprised us, as we expected a more important difference between them. In any case, we deduced that, at least in this corpus, it is not necessary to be an expert in linguistics to discursively segment the documents. As far as system evaluations are",
            "Sentence alignment quality",
            "the minimal cosine distance distance to the query. Metrics. To evaluate our pipeline approach we use overall accuracy after filtering and accuracy given that the segment is in the top 10 videos. While the first metric is similar to SECREF17, the second can indicate if initially searching on the video space can be used to improve our selection:",
            "Description of segmentation strategies ::: Segmentation with explicit use of a marker and POS labels",
            "An evaluation of how good a method identifies relevant words in text documents can be performed qualitatively, e.g. at the document level, by inspecting the heatmap visualization of a document, or by reviewing the list of the most (or of the least) relevant words per document. A similar analysis can also be conducted at the dataset level, e.g. by compiling the list of the most relevant words for one category across all documents. The latter allows one to identify words that are representatives for a document category, and eventually to detect potential dataset biases or classifier specific drawbacks. However, in order to quantitatively compare algorithms such as LRP and SA regarding the identification of relevant words, we need an objective measure of the quality of the explanations",
            "For the Thai sentence segmentation task, our model is superior to all the baselines on both Thai sentence segmentation datasets, as shown in Table TABREF45 . On the Orchid dataset, the supervised model that includes both local and distant representation was adopted for comparison to the baseline model. Our model improves the F1 score achieved by CRF-ngram, which is the state-of-the-art model for Thai sentence segmentation in Orchid, from 91.9% (row (d)) to 92.5% (row (g)). Meanwhile, in the UGWC dataset, our CVT model (row (h)) achieves an F1 score of 88.9%, which is higher than the F1 score of both the baselines (CRF-ngram and Bi-LSTM-CRF (rows d and e, respectively)). Thus, our model is now the state-of-the-art model for Thai sentence segmentation on both the Orchid and UGWC datasets.",
            "Results and Evaluation",
            "In this section we outline the evaluation plan to verify the effectiveness of our learning approaches. To evaluate the news suggestion problem we are faced with two challenges. What comprises the ground truth for such a task ? How do we construct training and test splits given that entity pages consists of text added at different points in time ? Consider the ground truth challenge. Evaluating if an arbitrary news article should be included in Wikipedia is both subjective and difficult for a human if she is not an expert. An invasive approach, which was proposed by Barzilay and Sauper BIBREF8 , adds content directly to Wikipedia and expects the editors or other users to redact irrelevant content over a period of time. The limitations of such an evaluation technique is that content added",
            "Baselines ::: Baseline2: Segment retrieval",
            "pre-segmented using the Stanford Core NLP sentence splitter BIBREF69 embedded in the DKPro Core framework BIBREF70 . Annotators were asked to stick to the sentence level by default and label entire pre-segmented sentences. They should switch to annotations on the token level only if (a) a particular sentence contained more than one argument component, or (b) if the automatic sentence segmentation was wrong. Given the “noise” in user-generated Web data (wrong or missing punctuation, casing, etc.), this was often the case. Annotators were also asked to rephrase (summarize) each annotated argument component into a simple statement when applicable, as shown in Figure FIGREF66 . This was used as a first sanity checking step, as each argument component is expected to be a coherent discourse",
            "Feature selection",
            "can input to a corresponding segment and encouraged them to input a diverse set of questions which the span can answer. Along with the questions, the workers were also required to provide a justification as to why they made their questions. We manually checked this justification to filter out the questions with poor quality by removing those questions which were unrelated to the video. One initial challenge worth mentioning is that at first some workers input questions they had about the video and not questions which the video could answer. This was solved by providing them with an unrelated example. The second part of the question collection framework consisted of a paraphrasing task. In this task we presented workers with the questions generated by the first task and asked them to write"
        ]
    },
    {
        "title": "Multimodal Named Entity Recognition for Short Social Media Posts",
        "answer": "76 tutorial videos pertaining to an image editing software. 6,195 non-factoid QA pairs.  4963 test documents.  50 words.  10 times.  0.0141.  100 Reddit users.  200 aggregated postings.  569,037 sentence pairs.  393,000 pairs of sentences.  121,512 visual questions.  12 languages.  2,031-3,480 pairs.  0.0141.  100 Reddit users.  200 aggregated postings.  569,037 sentence pairs.  393,000 pairs of sentences.  121,512 visual questions.  12 languages.  2,031-3,480 pairs.  0.0141.  100 Reddit users.  200 aggregated postings.  569,037 sentence pairs.  393,000 pairs of sentences.  121,512 visual questions.  12 languages.  2,031-3,480 pairs.  0.0141.  100 Reddit users.  200 aggregated postings.  569,037 sentence pairs.  393,000 pairs of sentences.",
        "evidence": [
            "SnapCaptions Dataset",
            "BIBREF4, language understanding BIBREF5, BIBREF6, BIBREF7, machine translation BIBREF8, and zero-short language generation BIBREF9 up to co-reference resolution BIBREF10 and commonsense inference BIBREF11. While this approach has shown impressive improvements on benchmarks and evaluation metrics, the exponential increase in the size of the pretraining datasets as well as the model sizes BIBREF5, BIBREF12 has made it both difficult and costly for researchers and practitioners with limited computational resources to benefit from these models. For instance, RoBERTa BIBREF5 was trained on 160 GB of text using 1024 32GB V100. On Amazon-Web-Services cloud computing (AWS), such a pretraining would cost approximately 100K USD. Contrary to this trend, the booming research in Machine Learning in",
            "having the same size with the Japanese-Vietnamese parallel corpus. In the end, the data we have is double in size compared to the original one.",
            "relevant visual context to augment textual information, and 2) a generic modality-attention module which learns to attenuate irrelevant modalities while amplifying the most informative ones to extract contexts from, adaptive to each sample and token. The proposed MNER model with modality attention significantly outperforms the state-of-the-art text-only NER models by successfully leveraging provided visual contexts, opening up potential applications of MNER on myriads of social media platforms.",
            "Our dataset consists of 76 tutorial videos pertaining to an image editing software. All of the videos include spoken instructions which are transcribed and manually segmented into multiple segments. Specifically, we asked the annotators to manually divide each video into multiple segments such that each of the segments can serve as an answer to any question. For example, Fig. FIGREF1 shows example segments marked in red (each which are a complete unit as an answer span). Each sentence is associated with the starting and ending time-stamps, which can be used to access the relevant visual information. The dataset contains 6,195 non-factoid QA pairs, where the answers are the segments that were manually annotated. Fig. FIGREF5 shows an example of the annotations. video_id can be used to",
            "a length greater or equal to 100 tokens (after prepocessing), this amounts to 4963 test documents, from which we delete up to 50 words. For deleting a word we simply set the corresponding word embedding to zero in the CNN input. Moreover, in order to assess the pertinence of the LRP decomposition method as opposed to alternative relevance models, we additionally perform word deletions according to SA word relevances, as well as random deletion. In the latter case we sample a random sequence of 50 words per document, and delete the corresponding words successively from each document. We repeat the random sampling 10 times, and report the average results (the standard deviation of the accuracy is less than 0.0141 in all our experiments). We additionally perform a biased random deletion,",
            "They essentially split a sentence into multiple sub-sentences by a sliding window, then determine the sentence label by using the dominant label across all sub-sentences. The underlying assumption is that the sub-sentence with that granularity is potentially good enough to represent the whole sentence. However, it is hard to find the granularity of a “good sub-sentence” that works well across sentences. This motivates us to implement variable-size filters in a convolution layer in order to extract features of multigranular phrases. Breakthroughs of deep learning in NLP are also based on learning distributed word representations – also called “word embeddings” – by neural language models BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 . Word embeddings are derived by projecting words",
            "1996 to August 1997. (ii) Huang. huang2012improving incorporated global context to deal with challenges raised by words with multiple meanings; size: 100,232 word embeddings; training corpus: April 2010 snapshot of Wikipedia. (iii) GloVe. Size: 1,193,514 word embeddings; training corpus: a Twitter corpus of 2B tweets with 27B tokens. (iv) SENNA. Size: 130,000 word embeddings; training corpus: Wikipedia. Note that we use their 50-dimensional embeddings. (v) Word2Vec. It has no 50-dimensional embeddings available online. We use released code to train skip-gram on English Gigaword Corpus BIBREF26 with setup: window size 5, negative sampling, sampling rate $10^{-3}$ , threads 12. It is worth emphasizing that above embeddings sets are derived on different corpora with different algorithms.",
            "set given a 40/60% ratio. In contrast to the corpus INLINEFORM2 , we only performed slight preprocessing. Essentially, we removed user names, time-stamps, URLs, multiple blanks as well as annotations that were not part of the original conversations from all chat lines. Moreover, we did not normalize words (for example, shorten words such as “nooooo” to “no”) as we believe that these represent important style markers. Furthermore, we did not remove newlines between the chat lines, as the positions of specific words might play an important role regarding the individual's writing style. As a third corpus, we compiled INLINEFORM0 , which is a collection of 200 aggregated postings crawled from the Reddit platform. Overall, the postings were written by 100 Reddit users and stem from a variety",
            "We evaluate the RQE methods (i.e. deep learning model and logistic regression classifier) using two datasets of sentence pairs (SNLI and multiNLI), and three datasets of question pairs (Quora, Clinical-QE, and SemEval-cQA). The Stanford Natural Language Inference corpus (SNLI) BIBREF13 contains 569,037 sentence pairs written by humans based on image captioning. The training set of the MultiNLI corpus BIBREF14 consists of 393,000 pairs of sentences from five genres of written and spoken English (e.g. Travel, Government). Two other \"matched\" and \"mismatched\" sets are also available for development (20,000 pairs). Both SNLI and multiNLI consider three types of relationships between sentences: entailment, neutral and contradiction. We converted the contradiction and neutral labels to the same",
            "on each classifier's predictions for the 121,512 visual questions in the test dataset. We first show performance of the baseline and our two prediction systems using precision-recall curves. The goals are to achieve a high precision, to minimize wasting crowd effort when their efforts will be redundant, and a high recall, to avoid missing out on collecting the diversity of accepted answers from a crowd. We also report the average precision (AP), which indicates the area under a precision-recall curve. AP values range from 0 to 1 with better-performing prediction systems having larger values. Figure FIGREF8 a shows precision-recall curves for all prediction systems. Both our proposed classification systems outperform the VQA Algorithm BIBREF2 baseline; e.g., Ours - RF yields a 12",
            "sample, and 2) relying on a substantially larger English dataset as a source for the cross-lingual datasets: 1,888 pairs in this work versus 500 pairs in the work of Camacho:2017semeval. As a result, each of our cross-lingual datasets contains a substantially larger number of concept pairs, as shown in Table TABREF30. The cross-lingual Multi-Simlex datasets are constructed automatically, leveraging word pair translations and annotations collected in all 12 languages. This yields a total of 66 cross-lingual datasets, one for each possible combination of languages. Table TABREF30 provides the final number of concept pairs, which lie between 2,031 and 3,480 pairs for each cross-lingual dataset, whereas Table TABREF29 shows some sample pairs with their corresponding similarity scores. The"
        ]
    },
    {
        "title": "Neural Collective Entity Linking",
        "answer": "98.61% Accuracy.",
        "evidence": [
            "the entire text. We further introduce an attention scheme to improve the robustness of NCEL to data noise and train the model on Wikipedia hyperlinks to avoid overfitting and domain bias. In experiments, we evaluate NCEL on five publicly available datasets to verify the linking performance as well as generalization ability. We also conduct an extensive analysis of time complexity, the impact of key modules, and qualitative results, which demonstrate the effectiveness and efficiency of our proposed method.",
            "Stage 0 - Naive approach",
            "Results and Analysis",
            "NLP Toolkits",
            "even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions. To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require background knowledge. We explicitly state the following items in the HIT instructions for question writing: No Yes-no questions should be asked. The question should have at least five words. Videos, images or inserted links should not be considered. No background knowledge should be required to answer the question. To help the workers better follow the instructions, we also include a representative example showing",
            "the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
            "Experiments ::: User-Level Models",
            "of 98.60% on Clinical-RQE. We also performed a 10-fold cross-validation on the full Clinical-QE data of 8,588 question pairs, which gave 98.61% Accuracy. In the second experiment, we used these datasets for training only and compared their performance on our test set of 850 consumer health questions. Table TABREF29 presents the results of this experiment. Logistic Regression trained on the clinical-RQE data outperformed DL models trained on all datasets, with 73.18% Accuracy. To validate further the performance of the LR method, we evaluated it on question similarity detection. A typical approach to this task is to use an IR method to find similar question candidates, then a more sophisticated method to select and re-rank the similar questions. We followed a similar approach for this",
            "data generation procedures. Our findings confirm that back-translation is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement.",
            "given his trolling attempt, but also the recipients' perception of the troll's intention and subsequently their reaction to the trolling attempt. This categorization gives rise to very interesting problems in pragmatics that involve the computational modeling of intentions, perceived intentions, and reactions to perceived intentions. Second, we create a new annotated resource for computational modeling of trolling. Each instance in this resource corresponds to a suspected trolling attempt taken from a Reddit conversation, it's surrounding context, and its immediate responses and will be manually coded with information such as the troll's intention and the recipients' reactions using our proposed categorization of trolling. Finally, we identify the instances that are difficult to classify",
            "is to find the best assignment: INLINEFORM2  where INLINEFORM0 is the output variable of candidates, and INLINEFORM1 is a probability function as follows: INLINEFORM2  where INLINEFORM0 is the score function parameters by INLINEFORM1 . NCEL learns the mapping INLINEFORM2 through a neural network including three main modules: encoder, sub-graph convolution network (sub-GCN) and decoder. Next, we introduce them in turn. Encoder The function of this module is to integrate different features by a multi-layer perceptron (MLP): INLINEFORM0  where INLINEFORM0 is the hidden states of the current mention, INLINEFORM1 and INLINEFORM2 are trainable parameters and bias. We use ReLu as the non-linear activation INLINEFORM3 . Sub-Graph Convolution Network Similar to GCN, this module learns to abstract",
            "Acknowledgments\nThis research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada. We would like to thank Google Colab for providing support in terms of computational resources."
        ]
    },
    {
        "title": "Distant supervision for emotion detection using Facebook reactions",
        "answer": "Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook, Facebook,",
        "evidence": [
            "Selecting Facebook pages",
            "2.1m predicate instances involving 142k entity pairs and 184k entities. After removing infrequently-seen predicates (seen fewer than 6 times), there were 25k categories and 4.2k relations. We also used the test set created by Krishnamurthy and Mitchell, which contains 220 queries generated in the same fashion as the training data from a separate section of ClueWeb. However, as they did not release a development set with their data, we used this set as a development set. For a final evaluation, we generated another, similar test set from a different held out section of ClueWeb, in the same fashion as done by Krishnamurthy and Mitchell. This final test set contains 307 queries.",
            "Weibo, RenRen, and Chinese microblogs",
            "Research questions",
            "individuals claim credit for propaganda posts in the name of the regime. The emails contain social media posts and account names. BIBREF33 used the leaked posts as training data for a classification algorithm that subsequently helped them to identify more propaganda posts. In conjunction with a follow-up survey experiment they found that most content constitutes cheerleading for the regime rather than, for example, critique of foreign governments. In the next section we discuss an application of natural language processing in international development research.",
            "the assessors noticed that the performance of the QA systems was better on questions about diseases than on questions about drugs, which suggests a need for extending our medical QA collection with more information about drugs and associated question types. We also looked closely at the private websites used by the LiveQA-Med annotators to provide some of the reference answers for the test questions. For instance, the ConsumerLab website was useful to answer a question about the ingredients of a Drug (COENZYME Q10). Similarly, the eHealthMe website was used to answer a test question asking about interactions between two drugs (Phentermine and Dicyclomine) when no information was found in DailyMed. eHealthMe provides healthcare big data analysis and private research and studies including",
            "Reddit is popular website that allows registered users (without identity verification) to participate in fora grouped by topic or interest. Participation consists of posting stories that can be seen by other users, voting stories and comments, and comments in the story's comment section, in the form of a forum. The forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index",
            "Given the question “When is the deadline of AAAI?”, as a human, one might try searching “AAAI” on a search engine, follow the link to the official AAAI website, then search for keywords “deadline” or “due date” on the website to jump to a specific paragraph. Humans have a deep understanding of questions because of their significant background knowledge. As a result, the keywords they use to search are not limited to what appears in the question. Inspired by this observation, we study 3 query types for the Ctrl+F $<$query$>$ command. One token from the question: the setting with smallest action space. Because iMRC deals with Ctrl+F commands by exact string matching, there is no guarantee that all sentences are accessible from question tokens only. One token from the union of the question",
            "have an answer within the privacy policy, 33.1% were identified as being privacy-related questions but not within the scope of a privacy policy (e.g., 'has Viber had any privacy breaches in the past?') and 16.9% of questions were identified as completely out-of-scope (e.g., `'will the app consume much space?'). In the questions identified as relevant, 32% were ill-formed questions that were phrased by the user in a manner considered vague or ambiguous. Of the questions that were both relevant as well as `well-formed', 95.7% of the questions were not answered by the policy in question but it was reasonable to expect that a privacy policy would contain an answer. The remaining 4.3% were described as reasonable questions, but of a nature generally not discussed in privacy policies. This",
            "Dictionary-based approaches",
            "We have explored the potential of using Facebook reactions in a distant supervised setting to perform emotion classification. The evaluation on standard benchmarks shows that models trained as such, especially when enhanced with continuous vector representations, can achieve competitive results without relying on any handcrafted resource. An interesting aspect of our approach is the view to domain adaptation via the selection of Facebook pages to be used as training data. We believe that this approach has a lot of potential, and we see the following directions for improvement. Feature-wise, we want to train emotion-aware embeddings, in the vein of work by tang:14, and iacobacci2015sensembed. Retrofitting FB-embeddings trained on a larger corpus might also be successful, but would rely on",
            "Results on FBFans Dataset"
        ]
    },
    {
        "title": "MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge",
        "answer": "Amazon Mechanical Turk (MTurk) and Reddit.  Weibo, RenRen, and Chinese microblogs. Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi-Source.  Reddit.  Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi-Source.  Reddit.  Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi-Source.  Reddit.  Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi-Source.  Reddit.  Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi-Source.  Reddit.  Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi-Source.  Reddit.  Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi-Source.  Reddit.  Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi-Source.  Reddit.  Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi-Source.  Reddit.  Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi-Source.  Reddit.  Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi-Source.  Reddit.  Cloud Platform. Routing To Experts or Crowd. Online persuasion. Multi",
        "evidence": [
            "Algorithms Used",
            "Assembling training corpora of annotated natural language examples in specialized domains such as biomedicine poses considerable challenges. Experts with the requisite domain knowledge to perform high-quality annotation tend to be expensive, while lay annotators may not have the necessary knowledge to provide high-quality annotations. A practical approach for collecting a sufficiently large corpus would be to use crowdsourcing platforms like Amazon Mechanical Turk (MTurk). However, crowd workers in general are likely to provide noisy annotations BIBREF0 , BIBREF1 , BIBREF2 , an issue exacerbated by the technical nature of specialized content. Some of this noise may reflect worker quality and can be modeled BIBREF0 , BIBREF1 , BIBREF3 , BIBREF4 , but for some instances lay people may",
            "Weibo, RenRen, and Chinese microblogs",
            "system limited to estimating system uncertainty rather than crowd disagreement. Finally, we demonstrated how to employ the prediction system to accelerate the collection of diverse answers from a crowd by typically at least 20% over today's status quo of fixed redundancy allocation.",
            "Reddit is popular website that allows registered users (without identity verification) to participate in fora grouped by topic or interest. Participation consists of posting stories that can be seen by other users, voting stories and comments, and comments in the story's comment section, in the form of a forum. The forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index",
            "The Taskmaster Corpus ::: Two-person, spoken dataset\nIn order to replicate a two-participant, automated digital assistant experience, we built a WOz platform that pairs agents playing the digital assistant with crowdsourced workers playing the user in task-based conversational scenarios. An example dialog from this dataset is given in Figure FIGREF5.",
            "Cloud Platform",
            "Routing To Experts or Crowd",
            "assigned a rating of 10. We assumed it was a typo or misunderstanding regarding the words. To address this problem, such an annotation should be removed before calculating the true similarity. All the datasets except for RW simply calculated the average of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator.",
            "We report results of a comparison of the accuracy of crowdworkers and seven NaturalLanguage Processing (NLP) toolkits in solving two important NLP tasks, named-entity recognition (NER) and entity-level sentiment(ELS) analysis. We here focus on a challenging dataset, 1,000 political tweets that were collected during the U.S. presidential primary election in February 2016. Each tweet refers to at least one of four presidential candidates,i.e., four named entities. The groundtruth, established by experts in political communication, has entity-level sentiment information for each candidate mentioned in the tweet. We tested several commercial and open-source tools. Our experiments show that, for our dataset of political tweets, the most accurate NER system, Google Cloud NL, performed almost on",
            "Online persuasion",
            "Multi-Source"
        ]
    },
    {
        "title": "Stochastic Answer Networks for SQuAD 2.0",
        "answer": "A feed-forward neural network with two hidden layers. The first layer has 100 neurons with ReLU activation, and the second layer has 50 neurons with sigmoid activation. The output layer has one neuron with sigmoid activation. The input to the span detector is a vector of length 300, representing the sentence embedding. The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly with the classifier.  The span detector is trained jointly",
        "evidence": [
            "Stance detection",
            "at the considering time. Those components as a whole constitute a large trainable neural architecture called the famous attention-based encoder-decoder framework. This framework becomes popular in many sequence-to-sequence tasks. In the field of machine translation, using the attention-based encoder-decoder framework is referred to as Neural Machine Translation approach. First presented in BIBREF0 , the encoder and decoder in NMT are recurrent-based, which each hidden unit in those components is a recurrent unit like Long Short-term Memory (LSTM) BIBREF5 or Gated Recurrent Unit(GRU) BIBREF6 . Later, the encoder or decoder can also be a convolutional architecture, as in BIBREF7 . Recently, BIBREF8 introduces transformer architecture, in which both the encoder and decoder are a special",
            "Analysis. To better understand our model, we analyze the accuracy of the classifier in our joint model. We obtain 75.3 classification accuracy on the development with the threshold 0.5. By increasing value of INLINEFORM0 in Eq EQREF9 , the classification accuracy reached to 76.8 ( INLINEFORM1 ), however the final results of our model only have a small improvement (+0.2 in terms of F1 score). It shows that it is important to make balance between these two components: the span detector and unanswerable classifier.",
            "We now describe the architecture of our model MVCNN, illustrated in Figure 1 . Multichannel Input. The input of MVCNN includes multichannel feature maps of a considered sentence, each is a matrix initialized by a different embedding version. Let $s$ be sentence length, $d$ dimension of word embeddings and $c$ the total number of different embedding versions (i.e., channels). Hence, the whole initialized input is a three-dimensional array of size $c\\times d\\times s$ . Figure 1 depicts a sentence with $s=12$ words. Each word is initialized by $c=5$ embeddings, each coming from a different channel. In implementation, sentences in a mini-batch will be padded to the same length, and unknown words for corresponding channel are randomly initialized or can acquire good initialization from the",
            "Transductive String Kernels",
            "Effect of model structure",
            "BIBREF3 and hate-speech detection BIBREF4 over social media, plus statute law retrieval and entailment for Japanese BIBREF5. The remainder of this paper is organized as follows: Section SECREF2 describes our proposed architecture together with the Portuguese modules for its computational processing. Section SECREF3 discusses different design options and Section SECREF4 provides our conclusions together with some pointers for future work.",
            "Beam-search decoder",
            "the first and second layer we apply an exponential linear unit (ELU) as a non-linear activation function. Our reasoning for doing so is based on findings by Dash et al. that the experimental accuracy of ELUs over rectified linear units (RLU) tends to be somewhat higher for generative tasks [3]. Then, to scale the generator’s output to be in the range 0-1, we apply a sigmoid non-linearity between the second and the third layer of the model. The discriminator similarly used a feed-forward structure with three layers of depth. The input to the discriminator comes from two sources: real data fed directly into the discriminator and the data generated by the generator. This input is then piped into the first hidden layer. As before, an ELU transformation is then applied between the first and",
            "the $t^\\mathrm {th}$ word in the document. The convolution/detection layer produces a new representation composed of $F$ sequences indexed by $j$ , where each element of the sequence is computed as: $",
            "for different tasks (the span detector and binary classifier). Our model is pretty simple and intuitive, yet efficient. Without relying on the large pre-trained language models (ELMo) BIBREF7 , the proposed model achieves competitive results to the state-of-the-art on Stanford Question Answering Dataset (SQuAD) 2.0. The contribution of this work is summarized as follows. First, we propose a simple yet efficient model for MRC that handles unanswerable questions and is optimized jointly. Second, our model achieves competitive results on SQuAD v2.0.",
            "Error Detection Model"
        ]
    },
    {
        "title": "Winograd Schemas and Machine Translation",
        "answer": "Bengali, English, French, German, Hebrew, Italian, Japanese, Luxembourgish, Russian, Spanish, Yiddish, Arabic, Italian, Croatian, German, Hebrew, Russian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English, German, Italian, Russian, Hebrew, Croatian, English,",
        "evidence": [
            "Translation experiments",
            "Though Bengali is the seventh most spoken language in terms of number of native speakers BIBREF23, there is no standard corpus of questions available BIBREF0. We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to. The set of question categories is known as question taxonomy BIBREF0. We have used two layer taxonomy which was proposed by Xin Li, Dan Roth BIBREF24. This two layer taxonomy is made up of two classes which are Coarse Class and Finer Class. There are six coarse classes such as Numeric, Location, Entity, Description, Human and Abbreviation and fifty finer classes such as city, state, mountain, distance, count, definition,",
            "In many cases, the identification of the referent of the prounoun in a Winograd schema is critical for finding the correct translation of that pronoun in a different language. Therefore, Winograd schemas can be used as a very difficult challenge for the depth of understanding achieved by a machine translation program. The third person plural pronoun `they' has no gender in English and most other languages (unlike the third person singular pronouns `he', `she', and `it'). However Romance languages such as French, Italian, and Spanish, and Semitic languages such as Hebrew and Arabic distinguish between the masculine and the feminine third-person plural pronouns, at least in some grammatical cases. For instance in French, the masculine pronoun is `ils'; the feminine pronoun is `elles'. In",
            "interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design.",
            "Because these language ID tokens are so useful, it would be good if they could be effectively estimated for unseen languages. ostling2017continuous found that the language vectors their models learned correlated well to genetic relationships, so it would be interesting to see if the embeddings our source encoder learned for the language ID tokens showed anything similar. In a few cases they do (the languages closest to German in the vector space are Luxembourgish, Bavarian, and Yiddish, all close relatives). However, for the most part the structure of these vectors is not interpretable. Therefore, it would be difficult to estimate the embedding for an unseen language, or to “borrow” the language ID token of a similar language. A more promising way forward is to find a model that uses an",
            "questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment to many users, we maintain a consistent set of video segments and extend the possibility to generate a diverse set of question for the same segment.",
            "total of 229 languages. This test set omits 23 of the high resource languages that are written in unique scripts or for which language distance metrics could not be computed.",
            "At the time of writing, the authors have been mostly aware of FastBert, FARM, flair BIBREF22, BIBREF33, AllenNLP BIBREF21 and PyText but there are likely more interesting developments to be found, from research and internal projects to production packages.",
            "break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries, thesauri, WordNet) if required.  4. Annotators are kept anonymous, and are not able to communicate with each other during the annotation process. The selection criteria for the annotators required that all annotators must be native speakers of the target language. Preference to annotators with university education was given, but not required. Annotators were asked to complete a spreadsheet containing the translated pairs of words, as well as the part-of-speech, and a column to enter the score. The annotators did not have access to the original pairs in English. To ensure the quality of the collected ratings, we have employed an adjudication protocol similar to the one",
            "However, most of these datasets include English words only. There has been no Japanese dataset for the word-similarity task. Apart from English, WordSim353 and SimLex-999 Hill:2015:CL have been translated and rescored in other languages: German, Italian and Russian Leviant:2015:arXiv. SimLex-999 has also been translated and rescored in Hebrew and Croatian Mrksic:2017:TACL. SimLex-999 explicitly targets at similarity rather than relatedness and includes adjective, noun and verb pairs. However, this dataset contains only frequent words. In addition, the distributed representation of words is generally learned using only word-level information. Consequently, the distributed representation for low-frequency words and unknown words cannot be learned well with conventional models. However,",
            "Language Resource References\nlrec lrec2018",
            "Given the question “When is the deadline of AAAI?”, as a human, one might try searching “AAAI” on a search engine, follow the link to the official AAAI website, then search for keywords “deadline” or “due date” on the website to jump to a specific paragraph. Humans have a deep understanding of questions because of their significant background knowledge. As a result, the keywords they use to search are not limited to what appears in the question. Inspired by this observation, we study 3 query types for the Ctrl+F $<$query$>$ command. One token from the question: the setting with smallest action space. Because iMRC deals with Ctrl+F commands by exact string matching, there is no guarantee that all sentences are accessible from question tokens only. One token from the union of the question"
        ]
    },
    {
        "title": "How we do things with words: Analyzing text as social and cultural data",
        "answer": "Language-specific issues, conceptual validity, and subtleties of meaning and interpretation.  cultural and social situatedness, contested concepts, and operationalization and analysis.  challenges of examining thick social and cultural questions.  subtleties of meaning and interpretation, conceptual validity, and contested concepts.  subtleties of meaning and interpretation, conceptual validity, and language-specific issues.  language-specific issues, conceptual validity, and subtleties of meaning and interpretation.  subtleties of meaning and interpretation, language-specific issues, and conceptual validity.  language-specific issues, subtleties of meaning and interpretation, and conceptual validity.  conceptual validity, subtleties of meaning and interpretation, and language-specific issues.  subtleties of meaning and interpretation, language-specific issues, and conceptual validity.  language-specific issues, conceptual validity, and subtleties of meaning and interpretation.  subtleties of meaning and interpretation, language-specific issues, and conceptual validity.  conceptual validity, language-specific issues, and subtleties of meaning and interpretation.  subtleties of meaning and interpretation, language-specific issues, and conceptual validity.  language-specific issues, subtleties of meaning and interpretation, and conceptual validity.  subtleties of meaning and interpretation, language-specific",
        "evidence": [
            "each other. Computational approaches are valuable precisely because they help us identify patterns that would not otherwise be discernible. Yet these approaches are not a panacea. Examining thick social and cultural questions using computational text analysis carries significant challenges. For one, texts are culturally and socially situated. They reflect the ideas, values and beliefs of both their authors and their target audiences, and such subtleties of meaning and interpretation are difficult to incorporate in computational approaches. For another, many of the social and cultural concepts we seek to examine are highly contested — hate speech is just one such example. Choices regarding how to operationalize and analyze these concepts can raise serious concerns about conceptual validity",
            "Language-specific issues",
            "results in NLI and QA. Prior work in attention has been mostly in similarity based approaches while our work focuses on non-matching sequences.",
            "Results and Discussion ::: What makes Questions Unanswerable?",
            "9, web 6); toponyms (not presented here) and toponym descriptors (web 7); fairy tale characters (fastText 6); parts of speech and morphological forms (cases and numbers of nouns and adjectives, tenses of verbs); capitalization (in fact, first positions in the sentences) and punctuation issues (e. g., non-breaking spaces); Wikipedia authors and words from Wikipedia discussion pages (fastText 5); other different semantic categories. We also made an attempt to describe obtained components automatically in terms of common contexts of common morphological and semantic tags using MyStem tagger and semantic markup from Russian National Corpus. Unfortunately, these descriptions are not as good as desired and thus they are not presented here.",
            "Topic models (e.g., LDA BIBREF47 ) are usually unsupervised and therefore less biased towards human-defined categories. They are especially suited for insight-driven analysis, because they are constrained in ways that make their output interpretable. Although there is no guarantee that a “topic” will correspond to a recognizable theme or event or discourse, they often do so in ways that other methods do not. Their easy applicability without supervision and ready interpretability make topic models good for exploration. Topic models are less successful for many performance-driven applications. Raw word features are almost always better than topics for search and document classification. LSTMs and other neural network models are better as language models. Continuous word embeddings have more",
            "The goal of this study was to select documents suitable for a fine-grained analysis of arguments. In a preliminary study on annotating argumentation using a small sample (50 random documents) of forum posts and comments from the raw corpus, we found that many documents convey no argumentation at all, even in discussions about controversies. We observed that such contributions do not intend to persuade; these documents typically contain story-sharing, personal worries, user interaction (asking questions, expressing agreement), off-topic comments, and others. Such characteristics are typical to on-line discussions in general, but they have not been examined with respect to argumentation or persuasion. Indeed, we observed that there are (1) documents that are completely unrelated and (2)",
            "As mentioned in the previous section, we decided to base the question collection on script scenarios rather than specific texts. As a starting point for our data collection, we use scenarios from three script data collections BIBREF3 , BIBREF11 , BIBREF12 . Together, these resources contain more than 200 scenarios. To make sure that scenarios have different complexity and content, we selected 80 of them and came up with 20 new scenarios. Together with the 10 scenarios from InScript, we end up with a total of 110 scenarios. For the collection of texts, we followed modiinscript, where workers were asked to write a story about a given activity “as if explaining it to a child”. This results in elaborate and explicit texts that are centered around a single scenario. Consequently, the texts are",
            "we randomly sample three negative categories. Then each article and its positive/negative categories act as training pairs for the binary classifier. We notice “Wikipedia-based” training indeed contributes a lot for the topic detection task; however, its performances on emotion and situation detection problems are far from satisfactory. We believe this is mainly because the Yahoo-based topic categorization task is much closer to the Wikipedia-based topic categorization task; emotion and situation categorizations, however, are relatively further. Our entailment models, pretrained on MNLI/FEVER/RTE respectively, perform more robust on the three $\\textsc {0shot-tc}$ aspects (except for the RTE on emotion). Recall that they are not trained on any text classification data, and never know the",
            "A common obstacle to analyzing textual data in many fields, including the international development sector, is the plethora of languages that practitioners and researchers need to consider – each with subtle but meaningful differences. Fortunately, significant commercial interest in being able to translate large quantities of text inexpensively has led to major advances in recent years driven by Microsoft, Google, and Yandex with the introduction of neural machine translation BIBREF3 . They provide services that are free of charge that can be easily integrated into standard programming languages like Python and R. Open source neural machine translation systems are also being made available BIBREF4 . In a recent application, BIBREF5 estimate the policy preferences of Swiss legislators",
            "aligning a set of instructions to a video of someone carrying out those instructions. In their task, they use the video transcript to represent the video, which they later augment with a visual cue detector on food entities. Their task focuses on procedure-based cooking videos, and contrary to our task is primarily a text alignment task. In our task we aim to answer questions-using the transcripts-on instructional-style videos, in which the answer can involve steps not mentioned in the question.",
            "how trained AV models behave, if the lengths of the questioned documents are getting shorter and shorter. To our best knowledge, this question has not been addressed in previous authorship verification studies."
        ]
    },
    {
        "title": "Quasi-Recurrent Neural Networks",
        "answer": "mean-pooling, sum-pooling, max-pooling.",
        "evidence": [
            "Pooling Layer",
            "In the case there dataset was not split in training and gold (Arabic, German, Portuguese, Russian, and Swedish) then a cross-validation (10 folds) technique is used to test the classifier. The performance of the classifier is presented using different metrics depending the competition. SemEval uses the average of score INLINEFORM0 of positive and negative labels, TASS uses the accuracy and SENTIPOLC uses a custom metric (see BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 ).",
            "of these functions must be calculated for each timestep in sequence, their simplicity and parallelism along feature dimensions means that, in practice, evaluating them over even long sequences requires a negligible amount of computation time. A single QRNN layer thus performs an input-dependent pooling, followed by a gated linear combination of convolutional features. As with convolutional neural networks, two or more QRNN layers should be stacked to create a model with the capacity to approximate more complex functions.",
            "embeddings are transformed, these transformed embeddings are fed to the aggregator INLINEFORM0 to output an embedding INLINEFORM1 for the target entity INLINEFORM2 , i.e., DISPLAYFORM0  By definition, an aggregator INLINEFORM0 essentially takes as input a collection of vectors INLINEFORM1 ( INLINEFORM2 ) and maps them to a single vector. With this observation, the following two types of functions seem to be natural choices for neighborhood aggregators, and have been adopted previously: Pooling Functions. A typical pooling function is mean-pooling, which is defined by INLINEFORM0 . Besides mean-pooling, other previously adopted choices include sum- and max-pooling BIBREF9 . Due to their simple forms, pooling functions are permutation-invariant, but consider the neighbors equally. It is",
            "Summarization Performance",
            "the sum pooling over sentence embeddings to represent the document cluster. Therefore, the embedding similarity between a sentence and the document cluster could only measure the sentence saliency. To include the query information, we supplement the common hand-crafted feature TF-IDF cosine similarity to the query. This query-dependent feature, together with the embedding similarity, are used in sentence ranking. ISOLATION removes the attention mechanism, and mixtures hand-crafted and automatically learned features. All these methods adopt the same sentence selection process illustrated in Section \"Sentence Selection\" for a fair comparison.",
            "Standard and baseline methods",
            "Feature selection",
            "pooling while our convolution layer has variable-size filters. kimEMNLP2014 also studied multichannel representation and variable-size filters. Differently, their multichannel relies on a single version of pretrained embeddings (i.e., pretrained Word2Vec embeddings) with two copies: one is kept stable and the other one is fine-tuned by backpropagation. We develop this insight by incorporating diverse embedding versions. Additionally, their idea of variable-size filters is further developed. le2014distributed initialized the representation of a sentence as a parameter vector, treating it as a global feature and combining this vector with the representations of context words to do word prediction. Finally, this fine-tuned vector is used as representation of this sentence. Apparently, this",
            "Mix-Source Approach",
            "Data Collection ::: Analysis",
            "in short, we follow Pauls and Cranmer ( BIBREF8 ) in performing 5-nearest neighbor clustering on the matrix of state speech similarities to find candidates for affinity communities and then assign ties above thresholds. These text-based clusters and their vote-based clusters are then used as two layers of a multilayer network, and the Multilayer Extraction algorithm is used to detect affinity blocs across the two layers. This process is illustrated in [fig:plots]Figure 2, which displays (a) the vote and text similarity matrices, (b) the single layer vote and speech clusters, and (c) the multiplex affinity blocs located across both layers."
        ]
    },
    {
        "title": "Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction",
        "answer": "No",
        "evidence": [
            "Answer sentence selection",
            "Let's consider the following notations: $\\bullet $ An entity $e_i$ is a set of $J_i$ unordered records $\\lbrace r_{i,1}, ..., r_{i,j}, ..., r_{i,J_i}\\rbrace $; where record $r_{i,j}$ is defined as a pair of key $k_{i,j}$ and value $v_{i,j}$. We outline that $J_i$ might differ between entities. $\\bullet $ A data-structure $s$ is an unordered set of $I$ entities $e_i$. We thus denote $s \\lbrace e_1, ..., e_i, ..., e_I\\rbrace $. $\\bullet $ For each data-structure, a textual description $y$ is associated. We refer to the first $t$ words of a description $y$ as $y_{1:t}$. Thus, the full sequence of words can be noted as $y = y_{1:T}$. $\\bullet $ The dataset $\\mathcal {D}$ is a collection of $N$ aligned (data-structure, description) pairs $(s,y)$. For instance, Figure FIGREF1 illustrates a",
            "Expert annotations of Random and Difficult Instances",
            "exploit a language modeling objective (that is, relying only on explicit textual knowledge) can clearly make headway in a commonsense reasoning task, and we can further improve upon these approaches with a sequence-to-sequence model. This leaves us with two possible explanations: despite careful controls, the WinoGrande challenge still contains incidental biases that these more sophisticated models can exploit, or that we are genuinely making at least some progress in commonsense reasoning. The latter, in particular, challenges the notion that commonsense knowledge is (mostly) tacit. Perhaps it is the case that in a humongous corpus of natural language text, someone really has written about trying to stuff a tuba in a backpack?",
            "Disambiguation. This task aims to segment a formal written text into well formed sentences based on the existent punctuation marks BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 . In this context a sentence is defined (for English) by the Cambridge Dictionary as: “a group of words, usually containing a verb, that expresses a thought in the form of a statement, question, instruction, or exclamation and starts with a capital letter when written”. PMD carries certain complications, some given the ambiguity of punctuation marks within a sentence. A period can denote an acronym, an abbreviation, the end of the sentence or a combination of them as in the following example: The U.S. president, Mr. Donald Trump, is meeting with the F.B.I. director Christopher A. Wray next Thursday at 8p.m. However its",
            "Implications",
            "The definition of Recognizing Question Entailment (RQE) can have a significant impact on QA results. In related work, the meaning associated with Natural Language Inference (NLI) varies among different tasks and events. For instance, Recognizing Textual Entailment (RTE) was addressed by the PASCAL challenge BIBREF12 , where the entailment relation has been assessed manually by human judges who selected relevant sentences \"entailing\" a set of hypotheses from a list of documents returned by different Information Retrieval (IR) methods. In another definition, the Stanford Natural Language Inference corpus SNLI BIBREF13 , used three classification labels for the relations between two sentences: entailment, neutral and contradiction. For the entailment label, the annotators who built the",
            "Inference Without Parsed Sentences",
            "1 , where IE means a simple incremental encoding framework that ablated the knowledge context vector from $\\textbf {c}_{\\textbf {l}}$ in Eq. ( 17 ). Our models have lower perplexity and higher BLEU scores than the baselines. IE and IE+MSA have remarkably lower perplexity than other models. As for BLEU, IE+MSA(CA) obtained the highest BLEU-1 and BLEU-2 scores. This indicates that multi-source attention leads to generate story endings that have more overlaps with the reference endings.",
            "Benchmark the dataset ::: Situation detection",
            "even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions. To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require background knowledge. We explicitly state the following items in the HIT instructions for question writing: No Yes-no questions should be asked. The question should have at least five words. Videos, images or inserted links should not be considered. No background knowledge should be required to answer the question. To help the workers better follow the instructions, we also include a representative example showing",
            "Exp II: Transferability of Shared Sentence Representation"
        ]
    },
    {
        "title": "TWEETQA: A Social Media Focused Question Answering Dataset",
        "answer": "By checking if the tweets are from verified journalists or if they contain links to news articles.",
        "evidence": [
            "out most of the short tweets. For the tweets collected from CNN, INLINEFORM0 of them were filtered via semantic role labeling. For tweets from NBC, INLINEFORM1 of the tweets were filtered. We then use Amazon Mechanical Turk to collect question-answer pairs for the filtered tweets. For each Human Intelligence Task (HIT), we ask the worker to read three tweets and write two question-answer pairs for each tweet. To ensure the quality, we require the workers to be located in major English speaking countries (i.e. Canada, US, and UK) and have an acceptance rate larger than INLINEFORM0 . Since we use tweets as context, lots of important information are contained in hashtags or even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into",
            "and sentiment features, suggesting that age or gender (“mid-semester exams have me restless”), polarity and subjective terms (“lack of sleep is killing me”), and emoticons (“wide awake :(”) could be important for both identifying and correctly classifying a subset of these tweets.",
            "Twitter users who believe in climate change, we highlighted primarily politicians or news sources (i.e., verifiably affirming or denying climate change); these tweets rarely make spelling errors or use sarcasm. Due to this skew, the model yields a high rate of false negatives. It is likely that we could lessen the gap between validation and test accuracies by finding more “real\" Twitter users who are climate change believers, e.g. by using the methodology found in BIBREF4 .",
            "a particular advantage of using the pre-trained embeddings instead of the one trained with our corpora. Furthermore, we verified the statistical significance of our experiment by using Wilcoxon Rank-Sum Test, obtaining that there have been not significant difference between the various model considered while testing. The information extracted from the schedule also present several limitations. In fact, the hypothesis that a tweet is referring to a track broadcasted is not always verified. Even if it is common that radios listeners do comments about tracks played, or give suggestion to the radio host about what they would like to listen, it is also true that they might refer to a Contributor or Musical Work unrelated to the radio schedule.",
            "data from archived web pages, we need to consider what pages were captured, which are likely missing, and why. We must often repurpose born-digital data (e.g., Twitter was not designed to measure public opinion), but data biases may lead to spurious results and limit justification for generalization. In particular, data collected via black box APIs designed for commercial, not research, purposes are likely to introduce biases into the inferences we draw, and the closed nature of these APIs means we rarely know what biases are introduced, let alone how severely they might impact our research BIBREF10 . These, however, are not new problems. Historians, for example, have always understood that their sources were produced within particular contexts and for particular purposes, which are not",
            "of two methods: on one side, we use the entities extracted from the radio schedule for generating candidates entities in the user-generated tweets, thanks to a matching algorithm based on time proximity and string similarity. On the other side, we create a statistical model capable of detecting entities directly from the UGC, aimed to model the informal language of the raw texts. In Figure 1, an overview of the system proposed is presented.",
            "To create a model for predicting sentiments of event-related tweets, we divide the first data batch of influential tweets into training and validation datasets with a 90%/10% split. The training set contains 49.2% positive samples, and the validation set contains 49.0% positive samples. We form our test set by manually labeling a subset of 500 tweets from the the event-related tweets (randomly chosen across all five natural disasters), of which 50.0% are positive samples.",
            "The proliferation of social media has made it possible to collect user data in unprecedented ways. These data can come in the form of usage and behavior (e.g., who likes what on Facebook), network (e.g., who follows a given user on Instagram), and content (e.g., what people post to Twitter). Availability of such data have made it possible to make discoveries about individuals and communities, mobilizing social and psychological research and employing natural language processing methods. In this work, we focus on predicting social media user age, dialect, and gender based on posted language. More specifically, we use the total of 100 tweets from each manually-labeled user to predict each of these attributes. Our dataset comes from the Arabic author profiling and deception detection shared",
            "Experiments ::: Tweet-Level Models ::: Data Augmentation.",
            "a $24K$ corpus of tweets containing hate speech keywords and labelled the corpus as hate speech, offensive language, or neither by using crowd-sourcing and extracted different features such as $n$-grams, some tweet-level metadata such as the number of hashtags, mentions, retweets, and URLs, Part Of Speech (POS) tagging, etc. Their experiments on different multi-class classifiers showed that the Logistic Regression with L2 regularization performs the best at this task. Malmasi et al. BIBREF15 proposed an ensemble-based system that uses some linear SVM classifiers in parallel to distinguish hate speech from general profanity in social media. As one of the first attempts in neural network models, Djuric et al. BIBREF16 proposed a two-step method including a continuous bag of words model to",
            "After creating several embeddings as mentioned above, we create document (review or tweet) vectors. For each document, we sum all the vectors of words occurring in that document and take their average. In addition to it, we extract three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores, from each review. These polarity scores of words are computed as in (DISPLAY_FORM4). For example, if a review consists of five words, it would have five polarity scores and we utilise only three of these sentiment scores as mentioned. Lastly, we concatenate these three scores to the averaged word vector per review. That is, each review is represented by the average word vector of its constituent word embeddings and three supervised scores. We then feed these inputs into",
            "break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries, thesauri, WordNet) if required.  4. Annotators are kept anonymous, and are not able to communicate with each other during the annotation process. The selection criteria for the annotators required that all annotators must be native speakers of the target language. Preference to annotators with university education was given, but not required. Annotators were asked to complete a spreadsheet containing the translated pairs of words, as well as the part-of-speech, and a column to enter the score. The annotators did not have access to the original pairs in English. To ensure the quality of the collected ratings, we have employed an adjudication protocol similar to the one"
        ]
    },
    {
        "title": "Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation",
        "answer": "Affective Text, Fairy Tales, ISEAR, CreateDebate, CNN/Daily Mail, Multi-SimLex, Turkish movie reviews, Turkish Twitter corpus.",
        "evidence": [
            "Models Used in the Evaluation",
            "We also conduct human evaluation on the generated summaries. Similar to the previous work BIBREF3, BIBREF20, we randomly select 100 samples from the test set of CNN/Daily Mail dataset and ask 3 human testers to measure relevance and readability of each summary. Relevance is based on how much salient information does the summary contain, and readability is based on how fluent and grammatical the summary is. Given an article, different people may have different understandings of the main content of the article, the ideal situation is that more than one reference is paired with the articles. However, most of summarization datasets contain the pairs of article with a single reference summary due to the cost of annotating multi-references. Since we use the reference summaries as target",
            "Datasets",
            "the macro-average f-score as the evaluation metric for the overall performance because (1) the experimental dataset is severely imbalanced, which is common for contentious issues; and (2) for stance classification, content in minor-class posts is usually more important for further applications. For the CreateDebate dataset, accuracy was adopted as the evaluation metric to compare the results with related work BIBREF7 , BIBREF9 , BIBREF12 .",
            "Experiments ::: Label-partially-unseen evaluation ::: Baselines.",
            "popular encoders, at the same time also investigating other important questions such as performance of static versus contextualized word embeddings, or multilingual versus language-specific pretraining. Another purpose of the experiments is to outline the wide potential and applicability of the Multi-SimLex datasets for multilingual and cross-lingual representation learning evaluation.",
            "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7 . A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our",
            "We utilised two datasets for both Turkish and English to evaluate our methods. For Turkish, as the first dataset, we utilised the movie reviews which are collected from a popular website. The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment. These polarity scores are between the values 0.5 and 5, at intervals of 0.5. We consider a review to be negative it the score is equal to or lower than 2.5. On the other hand, if it is equal to or higher than 4, it is assumed to be positive. We have randomly selected 7,020 negative and 7,020 positive reviews and processed only them. The second Turkish dataset is the Twitter corpus which is formed of tweets about Turkish",
            "Results on CreateDebate Dataset",
            "Experiments\nIn this section, we introduce our evaluations on a standard dataset.",
            "possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth. Besides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human experts. Furthermore, for insight-driven analyses, it can be more useful to focus on improving",
            "In this section we outline the evaluation plan to verify the effectiveness of our learning approaches. To evaluate the news suggestion problem we are faced with two challenges. What comprises the ground truth for such a task ? How do we construct training and test splits given that entity pages consists of text added at different points in time ? Consider the ground truth challenge. Evaluating if an arbitrary news article should be included in Wikipedia is both subjective and difficult for a human if she is not an expert. An invasive approach, which was proposed by Barzilay and Sauper BIBREF8 , adds content directly to Wikipedia and expects the editors or other users to redact irrelevant content over a period of time. The limitations of such an evaluation technique is that content added"
        ]
    },
    {
        "title": "Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance",
        "answer": "TDC, Annodis, Taskmaster Corpus, Theses and Dissertations Catalog, TIMIT, Switchboard, Fisher, smac-1, EDU, Glozz, bot-generated corpora, user-generated corpora, Turkish media corpora, English corpora.",
        "evidence": [
            "Corpora",
            "Insight-driven computational analysis of text is becoming increasingly common. It not only helps us see more broadly, it helps us see subtle patterns more clearly and allows us to explore radical new questions about culture and society. In this article we have consolidated our experiences, as scholars from very different disciplines, in analyzing text as social and cultural data and described how the research process often unfolds. Each of the steps in the process is time-consuming and labor-intensive. Each presents challenges. And especially when working across disciplines, the research often involves a fair amount of discussion—even negotiation—about what means of operationalization and approaches to analysis are appropriate and feasible. And yet, with a bit of perseverance and mutual",
            "such as Apple, Amazon or Google vocal assistants now reach high performance level partly due to the amount of data possessed by these companies BIBREF9. Surprisingly, as data is said to be “the new oil\", few data sets are available for ASR systems. The best known are corpora like TIMIT BIBREF10, Switchboard BIBREF11 or Fisher BIBREF12 which date back to the early 1990s. The scarceness of available corpora is justified by the fact that gathering and annotating audio data is costly both in terms of money and time. Telephone conversations and broadcast recordings have been the primary source of spontaneous speech used. Out of all the 130 audio resources proposed by LDC to train automatic speech recognition systems in English, approximately 14% of them are based on broadcast news and",
            "point of the ROC curve, provided that the given corpus is balanced. While inspecting the 250 characters long documents in more detail, we identified that they share similar vocabularies consisting of chat abbreviations such as “lol” (laughing out loud) or “k” (ok), smileys and specific obscene words. Therefore, we assume that the verification results of the examined methods are mainly caused by the similar vocabularies between the texts.",
            "Annotation studies and corpus creation",
            "In Brazil, the governmental body responsible for overseeing and coordinating post-graduate programs, CAPES, keeps records of all theses and dissertations presented in the country. Information regarding such documents can be accessed online in the Theses and Dissertations Catalog (TDC), which contains abstracts in Portuguese and English, and additional metadata. Thus, this database can be a potential source of parallel corpora for the Portuguese and English languages. In this article, we present the development of a parallel corpus from TDC, which is made available by CAPES under the open data initiative. Approximately 240,000 documents were collected and aligned using the Hunalign tool. We demonstrate the capability of our developed corpus by training Statistical Machine Translation (SMT)",
            "in the corpus happen after one post. To highlight the performance of APA we show an example from smac-1 in Figure FIGREF22 with nine posts which was predicted correctly as intervened by APA but not by other models. Threads shows students confused over a missing figure in a homework. The instructor finally shows up, though late, to resolve the confusion.   RQ3. Do models trained with different context lengths perform better than when trained on a single context length?  We find that context length has a regularising effect on the model's performance at test time. This is not surprising since models trained with threads of single context length will not generalise to infer different context lengths. Row 4 of Table TABREF17 shows a steep performance drop in training by classifier with all",
            "Analysis of the convolutional language model",
            "raports, 266 000 words). The corpora were noted using Glozz. Annodis aims at building an annotated corpus. The proposed annotations are on two levels of analysis, that is, two perspectives: Ascendant: part of EDU are used in the construction of more complex structures, through the relations of discourse; Descending: approaches the text in its entirety and relies on the various shallow indices to identify high-level discursive structures (macro structures). Two types of persons annotated Annodis: linguistic experts and students. The first group constituted a $E$ subcorpus called “specialist” and the second group resulted in a $N$ subcorpus called “naive”. These rhetorically annotated subcorps were used as references in our experiments. (c.f. §SECREF6).",
            "The Taskmaster Corpus ::: Overview",
            "The performances of the NER experiments are reported separately for three different parts of the system proposed. Table 6 presents the comparison of the various methods while performing NER on the bot-generated corpora and the user-generated corpora. Results shown that, in the first case, in the training set the F1 score is always greater than 97%, with a maximum of 99.65%. With both test sets performances decrease, varying between 94-97%. In the case of UGC, comparing the F1 score we can observe how performances significantly decrease. It can be considered a natural consequence of the complex nature of the users' informal language in comparison to the structured message created by the bot. In Table 7, results of the schedule matching are reported. We can observe how the quality of the",
            "results show that we outperform the approaches which do not take into account the sentimental information. We also had better performances than other studies carried out on sentiment analysis in Turkish media. We also evaluated our novel embedding approaches on two English corpora of different genres. We outperformed the baseline approaches for this language as well. The source code and datasets are publicly available. The paper is organised as follows. In Section 2, we present the existing works on sentiment classification. In Section 3, we describe the methods proposed in this work. The experimental results are shown and the main contributions of our proposed approach are discussed in Section 4. In Section 5, we conclude the paper."
        ]
    },
    {
        "title": "Knowledge Based Machine Reading Comprehension",
        "answer": "Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure 3.    Figure",
        "evidence": [
            "Question Answering Model",
            "It would be computationally intractable to consider all Freebase entities as answers to queries, and so we use a simple candidate entity generation technique to consider only a small set of likely entities for a given query. We first find all entities in the query, and consider as candidates any entity that has either been seen at training time with a query entity or is directly connected to a query entity in Freebase. This candidate entity generation is common practice for recent question answering models over Freebase BIBREF8 , though, for the reasons stated above, it has not been used previously in open vocabulary semantic parsing models.",
            "Researches on question classification, question taxonomies and QA system have been undertaken in recent years. There are two types of approaches for question classification according to Banerjee et al in BIBREF13 - by rules and by machine learning approach. Rule based approaches use some hard coded grammar rules to map the question to an appropriate answer type BIBREF14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in BIBREF16 and BIBREF0. Many classifiers have been used in machine learning for QC such as Support Vector Machine (SVM) BIBREF16 BIBREF17, Support Vector Machines and Maximum Entropy Model BIBREF18, Naive Bayes (NB), Kernel Naive Bayes (KNB), Decision Tree (DT) and Rule Induction (RI) BIBREF13. In BIBREF0, they claimed to",
            "Paraphrase Generation Using Grammars",
            "hidden vectors from both directions is used as the question vector $v_{q}$ . The dot product is used to measure the semantic relevance between the b b question and two types of evidence.  $$f_{qa}(q, a) = v_{q}^{T}v_{p} + v_q^{T}v_{c}$$   (Eq. 10)",
            "are incomplete or open-ended (e.g., `the south west wind blows across nigeria between'). 3% of questions have an unresolvable coreference (e.g., `how do i get to Warsaw Missouri from here'). 4% of questions are vague, and a further 7% have unknown sources of error. 2% still contain false presuppositions (e.g., `what is the only fruit that does not have seeds?') and the remaining 42% do not have an answer within the document. This reinforces our belief that though they have been understudied in past work, any question answering system interacting with real users should expect to receive such unanticipated and unanswerable questions.",
            "at game step $t$ to generate head and tail pointers for answering the question; otherwise, the information gathering procedure continues. In this section, we describe the high-level model structure and training strategies of QA-DQN. We refer readers to BIBREF18 for detailed information. We will release datasets and code in the near future.",
            "questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment to many users, we maintain a consistent set of video segments and extend the possibility to generate a diverse set of question for the same segment.",
            "Open-ended survey questions\nOpen-ended questions are a rich source of information that should be leveraged to inform decision-making. We could be interested in several aspects of such a text document. One useful approach would be to find common, recurring topics across multiple respondents. This is an unsupervised learning task because we do not know what the topics are. Such models are known as topic models. They summarize multiple text documents into a number of common, semantic topics. BIBREF8 use a structural topic model (STM) that allows for the evaluation of the effects of structural covariates on topical structure, with the aim of analyzing several survey experiments and open-ended questions in the American National Election Study.",
            "What is the Model Learning?",
            "Candidate Generation",
            "Our question generation model takes the path between an “anchor” and the candidate answer, and outputs a question. We adopt the sequence to sequence architecture as our basic question generation model due to its effectiveness in natural language generation tasks. The hierarchical encoder and the hierarchical attention mechanism are introduced to take into account the structure of the input facts. Facts from external KBs are conventionally integrated into the model using the same way as described in the matching model. As illustrated in Figure 3 , the question generation model contains an encoder and a decoder. We use a hierarchical encoder consisting of two layers to model the meaning of each element (subject, predicate or object) and the relationship between them. Since each element"
        ]
    },
    {
        "title": "Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health",
        "answer": "CARD-660, SimVerb-3500, Ancient-Modern Chinese Dataset, TutorialVQA Dataset, SnapCaptions Dataset, Wikipedia entity pages.",
        "evidence": [
            "Datasets Used for the RQE Study",
            "What Does Zero-shot Transfer Model Learn? ::: Code-switching Dataset",
            "Acknowledgments\nThe authors would like to thank COMPETE 2020, PORTUGAL 2020 Program, the European Union, and ALENTEJO 2020 for supporting this research as part of Agatha Project SI & IDT number 18022 (Intelligent analysis system of open of sources information for surveillance/crime control). The authors would also like to thank LISP - Laboratory of Informatics, Systems and Parallelism.",
            "game. We followed the data partitions introduced with the dataset and used a train/validation/test sets of respectively $3,398$/727/728 (data-structure, description) pairs.",
            "Data Analysis",
            "correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample.  3) Source: CARD-660 BIBREF78. 67 word pairs are taken from this dataset focused on rare word similarity, applying the same selection criteria a) to e) employed for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus BIBREF94. CARD-660 contains some words that are very rare (logboat), domain-specific (erythroleukemia) and slang (2mrw), which might be difficult to translate and annotate across a wide array of languages. Hence, we opt for retaining only the concept pairs above the threshold of top 250K most frequent Wikipedia concepts, as above.  4) Source: SimVerb-3500 BIBREF15 Since both CARD-660 and",
            "Ancient-Modern Chinese Dataset",
            "We created a new dataset for the problem of following navigation instructions under the behavioral navigation framework of BIBREF5 . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation. As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants: While the dataset was",
            "TutorialVQA Dataset ::: Basis",
            "SnapCaptions Dataset",
            "The datasets we use for our experimental evaluation are directly extracted from the Wikipedia entity pages and their revision history. The generated data represents one of the contributions of our paper. The datasets are the following: Entity Classes. We focus on a manually predetermined set of entity classes for which we expect to have news coverage. The number of analyzed entity classes is 27, including INLINEFORM0 entities with at least one news reference. The entity classes were selected from the DBpedia class ontology. Figure FIGREF42 shows the number of entities per class for the years (2009-2014). News Articles. We extract all news references from the collected Wikipedia entity pages. The extracted news references are associated with the sections in which they appear. In total",
            "Data details and experimental setup"
        ]
    },
    {
        "title": "A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media",
        "answer": "BackTrans, Unpaired, CrossAlign, CPTG, DualRL",
        "evidence": [
            "Proposed Approach",
            "We compare our model with the following state-of-art generative models: BackTrans BIBREF7 : In BIBREF7 , authors propose a model using machine translation in order to preserve the meaning of the sentence while reducing stylistic properties. Unpaired BIBREF10 : In BIBREF10 , researchers implement a method to remove emotional words and add desired sentiment controlled by reinforcement learning. CrossAlign BIBREF6 : In BIBREF6 , authors leverage refined alignment of latent representations to perform style transfer and a cross-aligned auto-encoder is implemented. CPTG BIBREF24 : An interpolated reconstruction loss is introduced in BIBREF24 and a discriminator is implemented to control attributes in this work. DualRL BIBREF8 : In BIBREF8 , researchers use two reinforcement rewards",
            "Results ::: Comparison w.r.t. baselines.",
            "perspectives and approaches, which differ considerably from each other in conceptualization, scope, and theoretical refinement.”",
            "Combining IR and RQE Methods",
            "Acknowledgments\nWe would like to thank Jiateng Xie, Julian Salazar and Faisal Ladhak for the helpful comments and discussions.",
            "Hybrid models",
            "results in NLI and QA. Prior work in attention has been mostly in similarity based approaches while our work focuses on non-matching sequences.",
            "We compare our approach in eight different languages, three of them have important international contests, namely, SemEval (English), TASS (Spanish), and SENTIPOLC (Italian). Within the competitions our approach reaches from medium to high positions in the rankings; whereas in the remaining languages our approach outperforms the reported results.",
            "Models Used in the Evaluation",
            "Experiments on Additional Losses ::: Which additional loss works better?",
            "approach the similarities and differences that multiple references could provide into a sentence boundary evaluation protocol."
        ]
    },
    {
        "title": "External Lexical Information for Multilingual Part-of-Speech Tagging",
        "answer": "Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish, Indonesian, Germanic, Romance, Slavic, Indo-Iranian, English. 16 languages. 8 non-Indo-European languages. 8 Indo-European languages. 4 major Indo-European sub-families. 1 non-Indo-European language. 1 Indo-European language. 1 language. 1 Indo-European language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language. 1 language.",
        "evidence": [
            "Translation experiments",
            "Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. Although this language list contains only one non-Indo-European (Indonesian), four major Indo-European sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically and syntactically fairly diverse.",
            "interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design.",
            "are all neighbors. In order to quantify exactly the effect of language affinity on the similarity scores, we run correlation analyses between these and language features. In particular, we extract feature vectors from URIEL BIBREF99, a massively multilingual typological database that collects and normalizes information compiled by grammarians and field linguists about the world's languages. In particular, we focus on information about geography (the areas where the language speakers are concentrated), family (the phylogenetic tree each language belongs to), and typology (including syntax, phonological inventory, and phonology). Moreover, we consider typological representations of languages that are not manually crafted by experts, but rather learned from texts. BIBREF100 proposed to",
            "more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "At the time of writing, the authors have been mostly aware of FastBert, FARM, flair BIBREF22, BIBREF33, AllenNLP BIBREF21 and PyText but there are likely more interesting developments to be found, from research and internal projects to production packages.",
            "Evaluation\nWe evaluate our open-vocabulary semantic parser on a fill-in-the-blank natural language query task. Each test example is a natural language phrase containing at least two Freebase entities, one of which is held out. The system must propose a ranked list of Freebase entities to fill in the blank left by the held out entity, and the predicted entities are then judged manually for correctness. We compare our proposed models, which combine distributional and formal elements, with a purely distributional baseline from prior work. All of the data and code used in these experiments is available at http://github.com/allenai/open_vocab_semparse.",
            "Language Resource References\nlrec lrec2018",
            "Global Features",
            "Other languages, other ambiguities",
            "via bootstrapping with the intent-based dialogue application we have built, and the hierarchy we defined will allow us to eliminate costly annotation efforts in the future, especially for the word-level slots and intent keywords. Once enough domain-specific multi-modal data is collected, our future work is to explore training end-to-end dialogue agents for our in-cabin use-cases. We are planning to exploit other modalities for improved understanding of the in-cabin dialogue as well.",
            "Experiments ::: Linguistic Probes\nWe further analyze whether awareness of shallow syntax carries over to other linguistic tasks, via probes from BIBREF1. Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases. Unlike §SECREF11, there is minimal downstream task architecture, bringing into focus the transferability of cwrs, as opposed to task-specific adaptation."
        ]
    },
    {
        "title": "Disunited Nations? A Multiplex Network Approach to Detecting Preference Affinity Blocs using Texts and Votes",
        "answer": "CNN/Daily Mail BIBREF19, BIBREF1. TutorialVQA Dataset. SnapCaptions Dataset. Datasets. game. RQE Study. Emotion detection. UnifyEmotion. AQUAINT. ROUGE F1. METEOR. Twitter. CNN. NBC. Freebase/ProBase/Reverb/NELL. RQE. 287,226 training pairs. 13,368 validation pairs. 11,490 test pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 287,226 training pairs. 13,368 validation pairs. 11,490 test pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728",
        "evidence": [
            "Datasets",
            "Three types of models were used in our system, a feed-forward neural network, an LSTM network and an SVM regressor. The neural nets were inspired by the work of Prayas BIBREF7 in the previous shared task. Different regression algorithms (e.g. AdaBoost, XGBoost) were also tried due to the success of SeerNet BIBREF8 , but our study was not able to reproduce their results for Spanish. For both the LSTM network and the feed-forward network, a parameter search was done for the number of layers, the number of nodes and dropout used. This was done for each subtask, i.e. different tasks can have a different number of layers. All models were implemented using Keras BIBREF9 . After the best parameter settings were found, the results of 10 system runs to produce our predictions were averaged (note",
            "Benchmark the dataset ::: Emotion detection ::: UnifyEmotion.",
            "and AQUAINT. Our proposed method performs consistently well on all datasets that demonstrates the good generalization ability.",
            "GAN setups",
            "Experiments ::: Preliminaries ::: Dataset and Metrics.\nWe conduct our model on the large-scale dataset CNN/Daily Mail BIBREF19, BIBREF1, which is widely used in the task of abstractive document summarization with multi-sentences summaries. We use the scripts provided by BIBREF11 to obtain the non-anonymized version of the dataset without preprocessing to replace named entities. The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics.",
            "TutorialVQA Dataset ::: Overview",
            "One major challenge of building a QA dataset on tweets is the sparsity of informative tweets. Many users write tweets to express their feelings or emotions about their personal lives. These tweets are generally uninformative and also very difficult to ask questions about. Given the linguistic variance of tweets, it is generally hard to directly distinguish those tweets from informative ones. In terms of this, rather than starting from Twitter API Search, we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles. In order to get enough data, we first extract the URLs of all section pages (e.g. World, Politics, Money, Tech) from the snapshot of each home page and then crawl all articles with tweets",
            "dataset is designed so that successfully answering the questions requires understanding and the knowledge involved in a document. Each instance is composed of a question, a set of triplets derived from a document, and the answer. We implement a framework consisting of both a question answering model and a question generation model, both of which take the knowledge extracted from the document as well as relevant facts from an external knowledge base such as Freebase/ProBase/Reverb/NELL. The question answering model gives each candidate answer a score by measuring the semantic relevance between representation and the candidate answer representation in vector space. The question generation model provides each candidate answer with a score by measuring semantic relevance between the question",
            "Datasets Used for the RQE Study",
            "game. We followed the data partitions introduced with the dataset and used a train/validation/test sets of respectively $3,398$/727/728 (data-structure, description) pairs.",
            "Results: SnapCaptions Dataset"
        ]
    },
    {
        "title": "When to reply? Context Sensitive Models to Predict Instructor Interventions in MOOC Forums",
        "answer": "thread sequence and infer states that triggered intervention. Vocabulary dependent latent post categories. Context that triggers instructor intervention. Latent topics. Latent states. Latent post categories. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that triggered intervention. Latent states that triggered intervention. Latent post categories that",
        "evidence": [
            "in this line of research, they have not investigated the role of structure and sequence in the threaded discussion in predicting instructor interventions. BIBREF0 proposed probabilistic graphical models to model structure and sequence. They inferred vocabulary dependent latent post categories to model the thread sequence and infer states that triggered intervention. Their model, however, requires a hyperparameter for the number of latent states. It is likely that their empirically reported setting will not generalise due to their weak evaluation BIBREF7 . In this paper, we propose models to infer the context that triggers instructor intervention that does not require context lengths to be set apriori. All our proposed models generalise over modelling assumptions made by BIBREF0 . For the",
            "Interactive Dialog Context LM",
            "that with enough training data, content-based models can perform well; at the same time, the lack of user information results in too few clues for minor-class posts to either predict their stance directly or link them to other users and posts for improved performance. The 17.5% improvement when adding user information suggests that user information is especially useful when the dataset is highly imbalanced. All models that consider user information predict the minority class successfully. UCTNN without topic information works well but achieves lower performance than the full UTCNN model. The 4.9% performance gain brought by LDA shows that although it is satisfactory for single topic datasets, adding that latent topics still benefits performance: even when we are discussing the same topic,",
            "Instructor Intervention in MOOC forums",
            "Methodology ::: Supervised Contextual 4-scores",
            "In neural network based language models, the dialog context can be represented as a dense continuous vector. This context vector can be produced in a number of ways. One simple approach is to use bag of word embeddings. However, bag of word embedding context representation does not take word order into consideration. An alternative approach is to use an RNN to read the preceding text. The last hidden state of the RNN encoder can be seen as the representation of the text and be used as the context vector for the next turn. To generate document level context representation, one may cascade all sentences in a document by removing the sentence boundaries. The last RNN hidden state of the previous utterance serves as the initial RNN state of the next utterance. As in BIBREF11 , we refer to",
            "Do models generalize explicit supervision, or just memorize it? ::: Setup",
            "over all positions. DISPLAYFORM0   where the task-specific query vector INLINEFORM0 is a learned parameter. The final task-specific representation INLINEFORM1 is summarized by DISPLAYFORM0  At last, a task-specific fully connected layer followed by a softmax non-linear layer processes the task-specific context INLINEFORM0 and predicts the probability distribution over classes.",
            "allow us to examine the following key questions related to lay and expert annotations in specialized domains: Can we predict item difficulty? We define a training instance as difficult if a lay annotator or an automated model disagree on its labeling. We show that difficulty can be predicted, and that it is distinct from inter-annotator agreement. Further, such predictions can be used during training to improve information extraction models. Are there systematic differences between expert and lay annotations? We observe decidedly lower agreement between lay workers as compared to domain experts. Lay annotations have high precision but low recall with respect to expert annotations in the new data that we collected. More generally, we expect lay annotations to be lower quality, which may",
            "Further, introspection of attention help us better understand what aspects of a discussion post propagate through the discussion thread that prompts instructor intervention.",
            "give us a hint of the responder's interpretation, we cannot be sure of his interpretation without also considering the context. One way to improve interpretation is to exploit the response strategy, but the response strategy in our model is predicted independently of interpretation. So one solution could be similar to the one proposed above for the disclosure task problem: jointly learning classifiers that predict both variables simultaneously. Another possibility is to use the temporal sequence of response comments and make use of older response interpretation as input features for later comments. This could be useful since commenters seem to influence each other as they read through the conversation. Errors on Response Strategy (B) prediction: In some cases there is a blurry line",
            "vs. 0.959). Constructing affinity blocs based on texts and votes in tandem thus leads to a more substantively intuitive model, as well as increased predictive performance. Although this is a relatively large gain in predictive accuracy, these substantively small quantities confirm that the prediction of violent conflict onset remains an enduring challenge for scholars of IR. Nonetheless, the ability to exploit revealed preference information in speeches and votes in tandem appears to promise fruitful potential gains in terms of methodological capability and theoretical soundness."
        ]
    },
    {
        "title": "A Hierarchical Model for Data-to-Text Generation",
        "answer": "6–7 BLEU.",
        "evidence": [
            "Standard and baseline methods",
            "for the INLINEFORM7 -th visual question. Given no extra human budget, total diversity comes from the second term which indicates the diversity captured when only INLINEFORM8 answers are collected for every visual question. Given a maximum available extra human budget ( INLINEFORM9 ), total diversity comes from the first term which indicates the diversity captured when INLINEFORM10 answers are collected for every visual question. Given a partial extra human budget ( INLINEFORM11 ), the aim is to have perfect predictions such that the minimum number of answers ( INLINEFORM12 ) are allocated only for visual questions with one true answer so that all diverse answers are safely captured. We measure diversity per visual question as the number of all true answers collected per visual question (",
            "LR+IR method outperformed the best system in all measures, with 80.57% Accuracy and 77.47% MAP (official system ranking measure in SemEval-cQA). On the cQA-2017 test data, the LR+IR method obtained 44.66% MAP and outperformed the cQA-2017 best system in Accuracy with 67.27%.",
            "AV approach involves hyperparameters that have been entirely fixed such that there is no further possibility to improve its performance from outside (without deviating from the definitions in the publication of the method), the method is considered to be non-optimizable. Non-optimizable AV methods are preferable in forensic settings as, here, the existence of a training/validation corpus is not always self-evident. Among the proposed AV approaches in the respective literature, we identified only a small fraction BIBREF21 , BIBREF2 , BIBREF30 that fall into this category.",
            "improvement – showing the model both correct and incorrect variations of the same sentences likely assists in learning a discriminative model.",
            "Proposed Approach",
            "We compare the proposed approach for translating natural language instructions into a navigation plan against alternative deep-learning models: [align=left,leftmargin=0em,labelsep=0.4em,font=] The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path. To test",
            "semantically more meaningful semantic extraction than the baseline methods. In the next section we will confirm these observations quantitatively.",
            "1.17), the baseline 2 mean score is slightly higher than the baseline 1's: 2.86 (the stdev is 1.27), while SimplerVoice evaluation score is the highest one: 4.82 (the stdev is 0.35) which means the most effective approach to provide users with products' usage. Additionally, a paired-samples t-test was conducted to compare the MOS scores of users' responses among all products using baseline 1 and SimplerVoice system. There was a significant difference in the scores for baseline 1 (Mean = 2.57, Stdev = 1.17) and SimplerVoice (Mean = 4.82, Stdev = 0.35); t= -8.18224, p =1.19747e-07. These results show that there is a statistically significant difference in the MOS means between baseline 1 and SimplerVoice and that SimplerVoice performs more effectively than baseline 1 over different types of",
            "The results of the answerability baselines are presented in Table TABREF31, and on answer sentence selection in Table TABREF32. We observe that bert exhibits the best performance on a binary answerability identification task. However, most baselines considerably exceed the performance of a majority-class baseline. This suggests considerable information in the question, indicating it's possible answerability within this domain. Table.TABREF32 describes the performance of our baselines on the answer sentence selection task. The No-answer (NA) baseline performs at 28 F1, providing a lower bound on performance at this task. We observe that our best-performing baseline, Bert + Unanswerable achieves an F1 of 39.8. This suggest that bert is capable of making some progress towards answering",
            "Quantitative Evaluation",
            "Table TABREF18 shows the effect of adding different methods to the baseline NMT system, on the ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words). Our \"mainstream improvements\" add around 6–7 BLEU in both data conditions. In the ultra-low data condition, reducing the BPE vocabulary size is very effective (+4.9 BLEU). Reducing the batch size to 1000 token results in a BLEU gain of 0.3, and the lexical model yields an additional +0.6 BLEU. However, aggressive (word) dropout (+3.4 BLEU) and tuning other hyperparameters (+0.7 BLEU) has a stronger effect than the lexical model, and adding the lexical model (9) on top of the optimized configuration (8) does not improve performance. Together, the adaptations to the ultra-low data setting"
        ]
    },
    {
        "title": "Fully Convolutional Speech Recognition",
        "answer": "state-of-the-art performance on Wall Street Journal.",
        "evidence": [
            "State-of-the-art",
            "model in sentiment classification. In this work, I are aiming at evaluating word embeddings for sentiment analysis of citations. The research questions are:",
            "Results on TAC2010 and WW",
            "Conclusion\nWe introduced the first fully convolutional pipeline for speech recognition, that can directly process the raw waveform and shows state-of-the art performance on Wall Street Journal and on Librispeech among end-to-end systems. This first attempt at exploiting convolutional language models in speech recognition shows significant improvement over a 4-gram language model on both datasets. Replacing mel-filterbanks by a learnable front-end gives additional gains in performance, that appear to be more prevalent on noisy data. This suggests learning the front-end is a promising avenue for speech recognition with challenging recording conditions.",
            "Summarization Performance",
            "out most of the short tweets. For the tweets collected from CNN, INLINEFORM0 of them were filtered via semantic role labeling. For tweets from NBC, INLINEFORM1 of the tweets were filtered. We then use Amazon Mechanical Turk to collect question-answer pairs for the filtered tweets. For each Human Intelligence Task (HIT), we ask the worker to read three tweets and write two question-answer pairs for each tweet. To ensure the quality, we require the workers to be located in major English speaking countries (i.e. Canada, US, and UK) and have an acceptance rate larger than INLINEFORM0 . Since we use tweets as context, lots of important information are contained in hashtags or even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into",
            "of pages. Based on results of the attempted combinations as well as on the distribution of emotions in the development dataset (Figure FIGREF9 ), we selected a best model (B-M), namely the combined set of Time, The Guardian and Disney, which yields the highest results on development data. Time and The Guardian perform well on most emotions but Disney helps to boost the performance for the Joy class.",
            "page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\% for the \\emph{article-section placement}. Finally, we compare our approach against competitive baselines and show significant improvements.",
            "the machine comprehension tasks, the library feature evaluations on SQuAD1.1 (BIBREF17) and SQuAD2.0 (BIBREF25). Others currently-supported benchmarks include SWAG (BIBREF26), RACE (BIBREF27) and ARC (BIBREF28).",
            "Acknowledgements\nWe would like to thank Valerio Basile, Julio Villena-Roman, and Preslav Nakov for kindly give us access to the gold-standards of SENTIPOLC'14, TASS'15 and SemEval 2015 & 2016, respectively. The authors also thank Elio Villaseñor for the helpful discussions in early stages of this research.",
            "scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014.",
            "Benchmark the dataset ::: Situation detection"
        ]
    },
    {
        "title": "Massively Multilingual Neural Grapheme-to-Phoneme Conversion",
        "answer": "Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News Articles, Entity Classes, News",
        "evidence": [
            "Datasets Used for the RQE Study",
            "the answers to the similar questions will be correct answers for the new question. Different machine learning and deep learning approaches were tested in the scope of SemEval 2016 BIBREF3 and 2017 BIBREF4 task 3B. The best performing system in 2017 achieved a MAP of 47.22% using supervised Logistic Regression that combined different unsupervised similarity measures such as Cosine and Soft-Cosine BIBREF37 . The second best system achieved 46.93% MAP with a learning-to-rank method using Logistic Regression and a rich set of features including lexical and semantic features as well as embeddings generated by different neural networks (siamese, Bi-LSTM, GRU and CNNs) BIBREF38 . In the scope of this challenge, a dataset was collected from Qatar Living forum for training. We refer to this",
            "game. We followed the data partitions introduced with the dataset and used a train/validation/test sets of respectively $3,398$/727/728 (data-structure, description) pairs.",
            "Feature Sets",
            "Results and Analysis",
            "and AQUAINT. Our proposed method performs consistently well on all datasets that demonstrates the good generalization ability.",
            "Ancient-Modern Chinese Dataset",
            "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7 . A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our",
            "Dataset Analysis ::: Baseline Experiments: Response Generation",
            "SnapCaptions Dataset",
            "Acknowledgements\nWe would like to thank the H2020 project AI4EU (825619) which partially supports Laure Soulier and Patrick Gallinari.",
            "The datasets we use for our experimental evaluation are directly extracted from the Wikipedia entity pages and their revision history. The generated data represents one of the contributions of our paper. The datasets are the following: Entity Classes. We focus on a manually predetermined set of entity classes for which we expect to have news coverage. The number of analyzed entity classes is 27, including INLINEFORM0 entities with at least one news reference. The entity classes were selected from the DBpedia class ontology. Figure FIGREF42 shows the number of entities per class for the years (2009-2014). News Articles. We extract all news references from the collected Wikipedia entity pages. The extracted news references are associated with the sections in which they appear. In total"
        ]
    },
    {
        "title": "A Neural Approach to Irony Generation",
        "answer": "annotators who are proficient in English.",
        "evidence": [
            "(G2) and harmonic mean (H2) of the sentiment accuracy and the BLEU score. As for the irony accuracy, we only report it in human evaluation results because it is more accurate for the human to evaluate the quality of irony as it is very complicated. We first sample 50 non-ironic input sentences and their corresponding output sentences of different models. Then, we ask four annotators who are proficient in English to evaluate the qualities of the generated sentences of different models. They are required to rank the output sentences of our model and baselines from the best to the worst in terms of irony accuracy (Irony), Sentiment preservation (Senti) and content preservation (Content). The best output is ranked with 1 and the worst output is ranked with 6. That means that the smaller our",
            "best relevance score). In another observation, the assessors reported that many of the returned answers had a correct question type but a wrong focus, which indicates that including a focus recognition module to filter such wrong answers can improve further the QA performance in terms of precision. Another aspect that was reported is the repetition of the same (or similar) answer from different websites, which could be addressed by improving answer selection with inter-answer comparisons and removal of near-duplicates. Also, half of the LiveQA test questions are about Drugs, when only two of our resources are specialized in Drugs, among 12 sub-collections overall. Accordingly, the assessors noticed that the performance of the QA systems was better on questions about diseases than on",
            "counter-claims (with and without evidence) and verification inquiries previously annotated by Procter.et.al.2013. They used unigrams, punctuations, and POS as features in three classifiers. Park.Cardie.2014 classified propositions into three classes (unverifiable, verifiable non-experimental, and verifiable experimental) and ignored non-argumentative texts. Using multi-class SVM and a wide range of features (n-grams, POS, sentiment clue words, tense, person) they achieved Macro INLINEFORM0 0.69. Peldszus.2014 experimented with a rather complex labeling schema of argument segments, but their data were artificially created for their task and manually cleaned, such as removing segments that did not meet the criteria or non-argumentative segments. In the first step of their two-phase",
            "when the reliability is measured based on the output of different annotators, no coder is taken as the standard and the likelihood of coders reaching agreement by chance (rather than because they are “correct\") is factored into the resulting statistic. Comparing against a “gold standard” suggests that the threshold for human inter- and intra-coder reliability should be particularly high. Accuracy, as well as other measures such as precision, recall and F-score, are sometimes presented as a measure of validity, but if we do not have a genuinely objective determination of what something is supposed measure—as is often the case in text analysis—then accuracy is perhaps a better indication of reliability than of validity. In that case, validity needs to be assessed based on other techniques",
            "properties, we conduct three experiments using 12 existing AV approaches, including the current state of the art. The examined methods were trained, optimized and evaluated on three self-compiled corpora, where each corpus focuses on a different aspect of applicability. Our results indicate that part of the methods are able to cope with very challenging verification cases such as 250 characters long informal chat conversations (72.7% accuracy) or cases in which two scientific documents were written at different times with an average difference of 15.6 years (>75% accuracy). However, we also identified that all involved methods are prone to cross-topic verification cases.",
            "(IR and IR+RQE). Assessor A evaluated 2,000 answers from both methods. Table TABREF103 presents the inter-annotator agreement (IAA) through F1 score computed by considering one of the assessors as reference. In the first evaluation, we computed the True Positives (TP) and False Positives (FP) over all ratings and the Precision and F1 score. As there are no negative labels (only true or false positives for each category), Recall is 100%. We also computed a partial IAA by grouping the \"Correct and Complete Answer\" and \"Correct but Incomplete\" ratings (as Correct), and the \"Incorrect but Related\" and \"Incorrect\" ratings (as Incorrect). The average agreement on distinguishing the Correct and Incorrect answers is 94.33% F1 score. Therefore, we used the evaluations performed by assessor A for",
            "that were as similar as possible to the treatment group, but were not banned from Reddit. The goal is to estimate the counterfactual scenario: in this case, what would have happened had the site not taken action against these specific forums? An ideal control would make it possible to distinguish the effect of the treatment — closing the forums — from other idiosyncratic properties of texts that were treated. We also look for categories of documents that might not be useful. We might remove documents that are meta-discourse, like introductions and notes, or documents that are in a language that is not the primary language of the collection, or duplicates when we are working with archived web pages. However, we need to carefully consider the potential consequences of information we remove.",
            "self-reported events of individuals describing being part of bullying events, but we believe that the real impact of computational trolling research is not on analyzing retrospective incidents, but on analyzing real-time conversations. chen2012detecting use lexical and semantic features to determine sentence offensiveness levels to identify cyberbullying, offensive or abusive comments on Youtube. On Youtube as well, dinakar2012common identified sensitive topics for cyberbullying. dadvar2014experts used expert systems to classify between bullying and no bullying in posts. van2015detection predict fine-grained categories for cyberbullying, distinguishing between insults and threats and identified user roles in the exchanges. Finally, hardaker2010trolling argues that trolling cannot be",
            "evaluated a variation in which all embeddings from the same user were forced to be identical (this is the UTCNN shared user embedding setting in Table TABREF22 ). This setting yielded only a 2.5% improvement over the model without comments, which is not statistically significant. However, when separating authors/likers and commenters embeddings (i.e., the UTCNN full model), we achieved much greater improvements (4.8%). We attribute this result to the tendency of users to use different wording for different roles (for instance author vs commenter). This is observed when the user, acting as an author, attempts to support her argument against nuclear power by using improvements in solar power; when acting as a commenter, though, she interacts with post contents by criticizing past",
            "or asking about third party collection/use of data), identify different sources of evidence for questions that ask if a practice is performed or not (4%), have differing interpretations of policy content (3%), identify a partial answer to a question in the privacy policy (2%) (for example, when the user asks `who is allowed to use the app' a majority of our annotators decline to answer, but the remaining annotators highlight partial evidence in the privacy policy which states that children under the age of 13 are not allowed to use the app), and other legitimate sources of disagreement (6%) which include personal subjective views of the annotators (for example, when the user asks `is my DNA information used in any way other than what is specified', some experts consider the boilerplate",
            "as on-topic persuasive. We will refer to this corpus as gold data persuasive. We examined all disagreements between annotators and discovered some typical problems, such as implicitness or topic relevance. First, the authors often express their stance towards the topic implicitly, so it must be inferred by the reader. To do so, certain common-ground knowledge is required. However, such knowledge heavily depends on many aspects, such as the reader's familiarity with the topic or her cultural background, as well as the context of the source website or the discussion forum thread. This also applies for sarcasm and irony. Second, the decision whether a particular topic is persuasive was always made with respect to the controversial topic under examination. Some authors shift the focus to a",
            "sentence INLINEFORM1 , our sentiment reward is defined as: DISPLAYFORM0  To encourage our model to focus on both the irony accuracy and the sentiment preservation, we apply the harmonic mean of irony reward and sentiment reward: DISPLAYFORM0"
        ]
    },
    {
        "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer",
        "answer": "RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers.",
        "evidence": [
            "Model architecture",
            "a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost. By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on WMT English-French NewsTest2014, while decoding at 100 words/sec on single-threaded CPU. We believe this is the best published accuracy/speed trade-off of an NMT system.",
            "UTCNN Model Description",
            "models into an end-to-end cloud platform that provides both an easy-to-use annotation interface as well as an inference interface that allows users to upload documents and inspect model outputs.",
            "Models Used in the Evaluation",
            "Pretraining with Shallow Syntactic Annotations ::: Pretraining Model Architecture",
            "QA Model 2: KVMemNet",
            "What is the Model Learning?",
            "special tokens (most pretrained Transformers models have a maximum sequence length they can handle, defined during their pretraining step). Tokenizers can be instantiated from existing configurations available through Transformers originating from the pretrained models or created more generally by the user from user-specifications. Model - All models follow the same hierarchy of abstraction: a base class implements the model's computation graph from encoding (projection on the embedding matrix) through the series of self-attention layers and up to the last layer hidden states. The base class is specific to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of",
            "Sequence-to-Sequence Model",
            "Hybrid models",
            "network architecture and continuously models both dynamics through time. In MARN, view-specific dynamics within each modality are modeled using a Long-short Term Hybrid Memory (LSTHM) assigned to that modality. The hybrid memory allows each modality's LSTHM to store important cross-view dynamics related to that modality. Cross-view dynamics are discovered at each recurrence time-step using a specific neural component called the Multi-attention Block (MAB). The MAB is capable of simultaneously finding multiple cross-view dynamics in each recurrence timestep. The MARN resembles the mechanism of our brains for understanding communication, where different regions independently process and understand different modalities BIBREF2 , BIBREF3 – our LSTHM – and are connected together using neural"
        ]
    },
    {
        "title": "From Textual Information Sources to Linked Data in the Agatha Project",
        "answer": "overall accuracy after filtering and accuracy given that the segment is in the top 10 videos.",
        "evidence": [
            "the minimal cosine distance distance to the query. Metrics. To evaluate our pipeline approach we use overall accuracy after filtering and accuracy given that the segment is in the top 10 videos. While the first metric is similar to SECREF17, the second can indicate if initially searching on the video space can be used to improve our selection:",
            "Evaluation Plan",
            "Mix-Source Approach",
            "Moreover, with SimplerVoice, we test if our system could provide users with the proper visual components to help them understand the products' usage based on the proposed techniques, and measure the usefulness of SimplerVoice's generated description. We evaluated the effectiveness & interpretability of 3 above approaches by conducting a controlled user study with 15 subjects who were Vietnamese native and did not speak/comprehend English. A dataset of random 20 U.S. products including products' title, UPC code, and product package images were chosen to be displayed in the user study. Note that the 15 participated subjects had not used the 20 products before and were also not familiar with the packaged products including the chosen 20 products; hence, they were \"illiterate\" in terms of",
            "Methodology ::: Gender bias evaluation procedure of an ASR system performance ::: Evaluation",
            "Wikipedia, explicitly addressing long-tail and trunk entities. Afterwards, based on the constructed template our method determines the best fit for the news article with one of the sections. We evaluate the proposed approach on a news corpus consisting of 351,982 articles crawled from the news external references in Wikipedia from 73,734 entity pages. Given the Wikipedia snapshot at a given year (in our case [2009-2014]), we suggest news articles that might be cited in the coming years. The existing news references in the entity pages along with their reference date act as our ground-truth to evaluate our approach. In summary, we make the following contributions.",
            "comparison of our approach (when INLINEFORM0 is optimized using SVM) against the best performing baseline S2. With the increase in the number of training instances for the ASP task the performance is a monotonically non-decreasing function. For the year 2009, we optimize the learning objective of INLINEFORM1 with around 8% of the total instances, and evaluate on the rest. The performance on average is around P=0.66 across all classes. Even though for many classes the performance is already stable (as we will see in the next section), for some classes we improve further. If we take into account the years between 2010 and 2012, we have an increase of INLINEFORM2 P=0.17, with around 70% of instances used for training and the remainder for evaluation. For the remaining years the total",
            "Experiments\nThis section describes our evaluation of the proposed approach for interpreting navigation commands in natural language. We provide both quantitative and qualitative results.",
            "Comparison with baseline models",
            "Evaluation of the first retrieved answer",
            "Results of RQE Approaches",
            "As pointed out by the reviewers the success of the multichannel approach is likely due to a combination of several quite different effects. First, there is the effect of the embedding learning algorithm. These algorithms differ in many aspects, including in sensitivity to word order (e.g., SENNA: yes, word2vec: no), in objective function and in their treatment of ambiguity (explicitly modeled only by huang2012improving. Second, there is the effect of the corpus. We would expect the size and genre of the corpus to have a big effect even though we did not analyze this effect in this paper. Third, complementarity of word embeddings is likely to be more useful for some tasks than for others. Sentiment is a good application for complementary word embeddings because solving this task requires"
        ]
    },
    {
        "title": "Sparse and Constrained Attention for Neural Machine Translation",
        "answer": "English, French, Chinese, Japanese, Estonian, Finnish, Russian, Spanish, Italian, German, Turkish, Farsi, Kiswahili, Welsh, Vietnamese, Polish, Russian, English-French, Chinese-Japanese, English-French, English-Spanish, English-Italian, English-German, English-Turkish, English-Farsi, English-Kiswahili, English-Welsh, English-Vietnamese, English-Polish, English-Russian.",
        "evidence": [
            "Across all languages, 145 human annotators were asked to score all 1,888 pairs (in their given language). We finally collect at least ten valid annotations for each word pair in each language. All annotators were required to abide by the following instructions:  1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity.  2. Each annotator must score the entire set of 1,888 pairs in the dataset. The pairs must not be shared between different annotators.  3. Annotators are able to break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries,",
            "Analysis of the convolutional language model",
            "such as sentence length ratio in two languages and align mode probability. However, these methods are designed for other bilingual language pairs that are written in different language characters (e.g. English-French, Chinese-Japanese). The ancient-modern Chinese has some characteristics that are quite different from other language pairs. For example, ancient and modern Chinese are both written in Chinese characters, but ancient Chinese is highly concise and its syntactical structure is different from modern Chinese. The traditional methods do not take these characteristics into account. In this paper, we propose an effective ancient-modern Chinese text alignment method at the level of clause based on the characteristics of these two languages. The proposed method combines both",
            "10K-20K segment of the bars represents the proportion of words in the dataset that occur in the list of most frequent words between the frequency rank of 10,000 and 20,000 in that language; likewise with other intervals. Frequency lists for the presented languages are derived from Wikipedia and Common Crawl corpora. While many concept pairs are direct or approximate translations of English pairs, we can see that the frequency distribution does vary across different languages, and is also related to inherent language properties. For instance, in Finnish and Russian, while we use infinitive forms of all verbs, conjugated verb inflections are often more frequent in raw corpora than the corresponding infinitive forms. The variance can also be partially explained by the difference in",
            "play an important role as we observe very low scores when pairing cmn with morphologically rich languages such fin, est, pol, and rus. Similar to prior work of Vulic:2019we and doval2019onthe, given the fact that unsupervised vecmap is the most robust unsupervised CLWE method at present BIBREF158, our results again question the usefulness of fully unsupervised approaches for a large number of languages, and call for further developments in the area of unsupervised and weakly supervised cross-lingual representation learning. The scores of m-bert and xlm-100 lead to similar conclusions as in the monolingual settings. Reasonable correlation scores are achieved only for a small subset of resource-rich language pairs (e.g., eng, fra, spa, cmn) which dominate the multilingual m-bert training.",
            "only be directly translated in Estonian to puit, and are not distinguishable. Therefore, the translators approximated the translation for timber to the compound noun puitmaterjal (literally: wood material) in order to produce a valid pair in the target language. In some cases, a direct transliteration from English is used. For example, the pair: physician and doctor both translate to the same word in Estonian (arst); the less formal word doktor is used as a translation of doctor to generate a valid pair. We measure the quality of the translated pairs by using a random sample set of 100 pairs (from the 1,888 pairs) to be translated by an independent translator for each target language. The sample is proportionally stratified according to the part-of-speech categories. The independent",
            "As mentioned earlier, one of our lattice types is based on bi-layered PCFGs introduced here. In their traditional use, the latent states in L-PCFGs aim to capture syntactic information. We introduce here the use of an L-PCFG with two layers of latent states: one layer is intended to capture the usual syntactic information, and the other aims to capture semantic and topical information by using a large set of states with specific feature functions. To create the bi-layered L-PCFG, we again use the spectral algorithm of narayan-15 to estimate a grammar $G_{\\mathrm {par}}$ from the Paralex corpus. We use the word alignment of paraphrase question pairs in Paralex to map inside and outside trees of each nonterminals in the treebank to bag of word features. The number of latent states we use is",
            "We defined cross-language features as a set of features that could be applied in most similar languages, not only related language families such as Germanic languages (English, German, etc.), Romance languages (Spanish, Italian, etc.), among others; but also similar surface features such as punctuation, diacritics, symbol duplication, case sensitivity, etc. Later, the combination of these features will be explored to find the best configuration for a given classifier. Generally, Twitter messages are full of slang, misspelling, typographical and grammatical errors among others; in order to tackle these aspects we consider different parameters to study this effect. The following points are the parameters to be considered as spelling features. Punctuation (del-punc) considers the use of",
            "a range of language understanding tasks in NLP. Examples include dialog state tracking BIBREF16, BIBREF17, spoken language understanding BIBREF18, BIBREF19, text simplification BIBREF20, BIBREF21, BIBREF22, dictionary and thesaurus construction BIBREF23, BIBREF24. Despite the proven usefulness of semantic similarity datasets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian BIBREF25, whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish BIBREF26, 500 in Farsi BIBREF27, or 300 in Finnish BIBREF28) and coverage (e.g., all datasets which originated from the original English",
            "that the scores range from 0 to 6: the higher the score, the more similar the participants found the concepts in the pair. The examples from Table TABREF23 show evidence of both the stability of average similarity scores across languages (unlikely – friendly, book – literature, and vanish – disappear), as well as language-specific differences (care – caution). Some differences in similarity scores seem to group languages into clusters. For example, the word pair regular – average has an average similarity score of 4.0 and 4.1 in English and Spanish, respectively, whereas in Kiswahili and Welsh the average similarity score of this pair is 0.5 and 0.8. We analyze this phenomenon in more detail in §SECREF25. There are also examples for each of the four languages having a notably higher or",
            "utilized. We attempt to leverage high resource data by treating g2p as a multisource neural machine translation (NMT) problem. The source sequences for our system are words in the standard orthography in any language. The target sequences are the corresponding representation in the International Phonetic Alphabet (IPA). Our results show that the parameters learned by the shared encoder–decoder are able to exploit the orthographic and phonemic similarities between the various languages in our data.",
            "We collected Japanese-Vietnamese parallel data from TED talks extracted from WIT3's corpus BIBREF15 . After removing blank and duplicate lines we obtained 106758 pairs of sentences. The validation set used in all experiments is dev2010 and the test set is tst2010. The data augmentation methods has been applied only for the Japanese INLINEFORM0 Vietnamese direction. For Back Translation, we use Vietnamese monolingual data from VNESEcorpus of DongDu which includes 349578 sentences. We shuffle the lines of VNESEcorpus corpus and take out the first 106758 sentences (the same as the number of sentence pairs in the original parallel corpus). For Mix-Source, instead of using a subsampled monolingual corpus, we use the Vietnamese part of the Japanese-Vietnamese parallel corpus in order to learn"
        ]
    },
    {
        "title": "Dialog Context Language Modeling with Recurrent Neural Networks",
        "answer": "128 turns.",
        "evidence": [
            "slower turn taking cadence, prevents the conversation from becoming fully fledged, overly complex human discourse. This creates an idealized spoken environment, revealing how users would openly and candidly express themselves with an automated assistant that provided superior natural language understanding. Perhaps the most relevant work to consider here is the recently released MultiWOZ dataset BIBREF13, since it is similar in size, content and collection methodologies. MultiWOZ has roughly 10,000 dialogs which feature several domains and topics. The dialogs are annotated with both dialog states and dialog acts. MultiWOZ is an entirely written corpus and uses crowdsourced workers for both assistant and user roles. In contrast, Taskmaster-1 has roughly 13,000 dialogs spanning six domains",
            "Conversation Excerpts",
            "Abstract\nIn this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network (RNN) based contextual language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3% on perplexity. The proposed models also demonstrate advantageous performance over other competitive contextual language models.",
            "typed input to create a full text version of the dialog. Finally, these conversations are checked for transcription errors and typos and then annotated, as described in Section SECREF48.",
            "Related work ::: Human-machine vs. human-human dialog",
            "Discourse Segmenter Overall Description",
            "news, whereas ETAPE and REPERE consists also of debates and entertainment shows, spontaneous speech introducing more difficulty in its recognition. Our training set contains 27,085 speech utterances produced by 2,506 speakers, accounting for approximately 100 hours of speech. Our evaluation set contains 74,064 speech utterances produced by 1,268 speakers for a total of 70 hours of speech. Training data by show, medium and speech type is summarized in Table and evaluation data in Table . Evaluation data has a higher variety of shows with both prepared (P) and spontaneous (S) speech type (accented speech from African radio broadcast is also included in the evaluation set).",
            "failed) or cases in which the user could not hear the agent or vice-versa. In addition, weekly meetings were held with the agents to answer questions and gather feedback on their experiences. Agents typically work four hours per day with dialog types changing every hour. Crowdsourced workers playing the user are accessed using Amazon Mechanical Turk. Payment for a completed dialog session lasting roughly five to seven minutes was typically in the range of $\\$1.00$ to $\\$1.30$. Problematic users are detected either by the agent involved in the specific dialog or by post-session assessment and removed from future requests.",
            "in F1). On IMDB62, as expected with short text inputs (mean=349 words/review), the discourse features in general do not add further contribution. Even the best model CNN2-DE brings only marginal improvement, confirming our findings from varying the chunk size on novel-9, where discourse features did not help at this input size. Equipped with discourse features, SVM2-PV performs slightly better than SVM2 on novel-50 (by 0.4 with GR, 0.9 with RST features). On IMDB62, the same pattern persists for the SVMs: discourse features do not make noticeable improvements (by 0.0 and 0.5 with GR and RST respectively).",
            "of dialog participants and presented a semi-supervised model for their classification. Their social act types were inspired by research in psychology and organizational behavior and were motivated by work in dialog understanding. They annotated a corpus in three languages using in-house annotators and achieved INLINEFORM0 in the range from 0.13 to 0.53. Georgila.et.al.2011 focused on cross-cultural aspects of persuasion or argumentation dialogs. They developed a novel annotation scheme stemming from different literature sources on negotiation and argumentation as well as from their original analysis of the phenomena. The annotation scheme is claimed to cover three dimensions of an utterance, namely speech act, topic, and response or reference to a previous utterance. They annotated 21",
            "tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task-completed dialogue. Guidance ability for out of scope input: There are two scores 0, 1, which represent able to guide or unable to guide. For the number of dialogue turns, we have a penalty rule that for a dialogue task, if the system cannot return the result (or accomplish the task) in 30 turns, the dialogue task is end by force. Meanwhile, if a system cannot accomplish a task in less than 30 dialogue turns, the number of dialogue turns is set to 30.",
            "Similarly, we use another replay buffer to store question answering transitions (observation string when interaction stops, question string, ground-truth answer). Because both iSQuAD and iNewsQA are converted from datasets that provide ground-truth answer positions, we can leverage this information and train the question answerer with supervised learning. Specifically, we only push question answering transitions when the ground-truth answer is in the observation string. For each transition, we convert the ground-truth answer head- and tail-positions from the SQuAD and NewsQA datasets to positions in the current observation string. After every 5 game steps, we randomly sample a mini-batch of 64 transitions from the replay buffer and train the question answerer using the Negative"
        ]
    },
    {
        "title": "Shallow Syntax in Deep Water",
        "answer": "mSynC, cwrs, L-PCFG, spectral algorithm, latent states, learned linear weighting, bag of word features.",
        "evidence": [
            "Experiments ::: Downstream Task Transfer ::: Phrase-structure parsing",
            "workers to write the full conversation themselves based on scenarios outlined for each task, thereby playing roles of both the user and assistant. We refer to this written dialog type as “self-dialogs\". In a departure from traditional annotation techniques BIBREF10, BIBREF8, BIBREF13, dialogs are labeled with simple API calls and arguments. This technique is much easier for annotators to learn and simpler to apply. As such it is more cost effective and, in addition, the same model can be used for multiple service providers. Taskmaster-1 has richer and more diverse language than the current popular benchmark in task-oriented dialog, MultiWOZ BIBREF13. Table TABREF2 shows that Taskmaster-1 has more unique words and is more difficult for language models to fit. We also find that Taskmaster-1",
            "at capturing domain-specific features as compared to word2vec, which generally captures general semantic and syntactic characteristics, but not the sentimental ones.",
            "parsing, where (gold) chunks align with syntactic phrases, indicating that task-relevant signal learned from exposure to shallow syntax is already learned by ELMo. On sentiment classification, chunk features are slightly harmful on average (but variance is high); mSynC again performs similarly to ELMo-transformer. Overall, the performance differences across all tasks are small enough to infer that shallow syntax is not particularly helpful when using cwrs.",
            "Converting Freebase queries to features",
            "Morphological analysis (hajivc1998tagging, oflazer1994tagging, inter alia) is the task of predicting fine-grained annotations about the syntactic properties of tokens in a language such as part-of-speech, case, or tense. For instance, in Figure FIGREF2 , the given Portuguese sentence is labeled with the respective morphological tags such as Gender and its label value Masculine. The accuracy of morphological analyzers is paramount, because their results are often a first step in the NLP pipeline for tasks such as translation BIBREF1 , BIBREF2 and parsing BIBREF3 , and errors in the upstream analysis may cascade to the downstream tasks. One difficulty, however, in creating these taggers is that only a limited amount of annotated data is available for a majority of the world's languages to",
            "of AttSum is consistent with the way how humans read when having a particular query in their minds. Naturally, they pay more attention to the sentences that meet the query need. It is noted that, unlike most previous summarization systems, our model is totally data-driven, i.e., all the features are learned automatically. We verify AttSum on the widely-used DUC 2005 $\\sim $ 2007 query-focused summarization benchmark datasets. AttSum outperforms widely-used summarization systems which rely on rich hand-crafted features. We also conduct qualitative analysis for those sentences with large relevance scores to the query. The result reveals that AttSum indeed focuses on highly query relevant content. The contributions of our work are as follows:",
            "Language Dependent Features",
            "features and thereby improve sentence classification performance. The letters “M” and “V” in the name “MVCNN” of our architecture denote the multichannel and variable-size convolution filters, respectively. “Multichannel” employs language from computer vision where a color image has red, green and blue channels. Here, a channel is a description by an embedding version. For many sentence classification tasks, only relatively small training sets are available. MVCNN has a large number of parameters, so that overfitting is a danger when they are trained on small training sets. We address this problem by pretraining MVCNN on unlabeled data. These pretrained weights can then be fine-tuned for the specific classification task. In sum, we attribute the success of MVCNN to: (i) designing",
            "which are based on PropBank semantic role labels. Motivation: Exploit the semantics of Capturing the semantics of the sentences. Binary features from Stanford Coreference Chain Resolver BIBREF118 , e.g., presence of the sentence in a chain, transition type (i.e., nominal–pronominal), distance to previous/next sentences in the chain, or number of inter-sentence coreference links. Motivation: Presence of coreference chains indicates links outside the sentence and thus may be informative, for example, for classifying whether the sentence is a part of a larger argument component. Results of a PTDB-style discourse parser BIBREF119 , namely the type of discourse relation (explicit, implicit), presence of discourse connectives, and attributions. Motivation: It has been claimed that discourse",
            "As mentioned earlier, one of our lattice types is based on bi-layered PCFGs introduced here. In their traditional use, the latent states in L-PCFGs aim to capture syntactic information. We introduce here the use of an L-PCFG with two layers of latent states: one layer is intended to capture the usual syntactic information, and the other aims to capture semantic and topical information by using a large set of states with specific feature functions. To create the bi-layered L-PCFG, we again use the spectral algorithm of narayan-15 to estimate a grammar $G_{\\mathrm {par}}$ from the Paralex corpus. We use the word alignment of paraphrase question pairs in Paralex to map inside and outside trees of each nonterminals in the treebank to bag of word features. The number of latent states we use is",
            "a sampled softmax BIBREF18. For downstream tasks, we use a learned linear weighting of all layers in the encoders to obtain a task-specific mSynC, following BIBREF2."
        ]
    },
    {
        "title": "TTTTTackling WinoGrande Schemas",
        "answer": "Current state of the art.",
        "evidence": [
            "Current state of the art",
            "scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014.",
            "Background",
            "Original Toulmin's model",
            "even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions. To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require background knowledge. We explicitly state the following items in the HIT instructions for question writing: No Yes-no questions should be asked. The question should have at least five words. Videos, images or inserted links should not be considered. No background knowledge should be required to answer the question. To help the workers better follow the instructions, we also include a representative example showing",
            "Discussion, conclusions and future work",
            "Properties of back-translated data",
            "Acknowledgements\nWe would like to thank Valerio Basile, Julio Villena-Roman, and Preslav Nakov for kindly give us access to the gold-standards of SENTIPOLC'14, TASS'15 and SemEval 2015 & 2016, respectively. The authors also thank Elio Villaseñor for the helpful discussions in early stages of this research.",
            "results in NLI and QA. Prior work in attention has been mostly in similarity based approaches while our work focuses on non-matching sequences.",
            "Traditional Methodologies",
            "We compare our model with the following state-of-art generative models: BackTrans BIBREF7 : In BIBREF7 , authors propose a model using machine translation in order to preserve the meaning of the sentence while reducing stylistic properties. Unpaired BIBREF10 : In BIBREF10 , researchers implement a method to remove emotional words and add desired sentiment controlled by reinforcement learning. CrossAlign BIBREF6 : In BIBREF6 , authors leverage refined alignment of latent representations to perform style transfer and a cross-aligned auto-encoder is implemented. CPTG BIBREF24 : An interpolated reconstruction loss is introduced in BIBREF24 and a discriminator is implemented to control attributes in this work. DualRL BIBREF8 : In BIBREF8 , researchers use two reinforcement rewards",
            "LSTM Gating Signals"
        ]
    },
    {
        "title": "A Question-Entailment Approach to Question Answering",
        "answer": "Random Forest, Deep Learning, Recurrent Neural Networks, Convolutional Neural Networks, Long Short-Term Memory, Gated Recurrent Unit, Quasi-Recurrent Neural Networks.  Stochastic Gradient Descent.  Latent Dirichlet Allocation.  Natural Language Inference.  Information Retrieval.  Statistical Natural Language Processing.  Language Modeling.  Generative Statistical Model.  Information Theory.  Statistical Model.  Deep Learning Approach.  Semantic Space Model.  Latent Semantic Analysis.  Word Embeddings.  Attention Mechanism.  Skip-Connections.  Fully Connected Layer.  Rectified Linear Unit.  ReLU Activation.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU.  ReLU",
        "evidence": [
            "Datasets Used for the RQE Study",
            "in the respective literature. In 2013, for example, Veenman and Li BIBREF2 presented an AV method based on compression, which has its roots in the field of information theory. In 2015, Bagnall BIBREF3 introduced the first deep learning approach that makes use of language modeling, an important key concept in statistical natural language processing. In 2017, Castañeda and Calvo BIBREF4 proposed an AV method that applies a semantic space model through Latent Dirichlet Allocation, a generative statistical model used in information retrieval and computational linguistics. Despite the increasing number of AV approaches, a closer look at the respective studies reveals that only minor attention is paid to their underlying characteristics such as reliability and robustness. These, however, must",
            "a better-trained model. In near future, we will incorporate all the above techniques to get comparable performance with the state-of-the-art hybrid DNN/RNN-HMM systems.",
            "The definition of Recognizing Question Entailment (RQE) can have a significant impact on QA results. In related work, the meaning associated with Natural Language Inference (NLI) varies among different tasks and events. For instance, Recognizing Textual Entailment (RTE) was addressed by the PASCAL challenge BIBREF12 , where the entailment relation has been assessed manually by human judges who selected relevant sentences \"entailing\" a set of hypotheses from a list of documents returned by different Information Retrieval (IR) methods. In another definition, the Stanford Natural Language Inference corpus SNLI BIBREF13 , used three classification labels for the relations between two sentences: entailment, neutral and contradiction. For the entailment label, the annotators who built the",
            "IR-based system with 0.827 average score. It also achieved a higher score than the best results achieved in the medical challenge at LiveQA'17. Evaluating the RQE system alone is not relevant, as applying RQE on the full collection for each user question is not feasible for a real-time system because of the extended execution time.",
            "Following the recent progress in deep learning, researchers and practitioners of machine learning are recognizing the importance of understanding and interpreting what goes on inside these black box models. Recurrent neural networks have recently revolutionized speech recognition and translation, and these powerful models could be very useful in other applications involving sequential data. However, adoption has been slow in applications such as health care, where practitioners are reluctant to let an opaque expert system make crucial decisions. If we can make the inner workings of RNNs more interpretable, more applications can benefit from their power. There are several aspects of what makes a model or algorithm understandable to humans. One aspect is model complexity or parsimony.",
            "(dis)agreement label for a given visual question. This model consists of an ensemble of decision tree classifiers. We train the system to learn the unique weighted combinations of the aforementioned 2,497 features that each decision tree applies to make a prediction. At test time, given a novel visual question, the trained system converts a 2,497 feature descriptor of the visual question into a final prediction that reflects the majority vote prediction from the ensemble of decision trees. The system returns the final prediction along with a probability indicating the system's confidence in that prediction. We employ the Matlab implementation of random forests, using 25 trees and the default parameters. We next adapt a VQA deep learning architecture BIBREF24 to learn the predictive",
            "Deep Learning Model",
            "a deep network, with multiple stacked layers, the same update equations are used in each layer, with the state, cell, and gate vectors replaced by layer-specific vectors INLINEFORM0 and so on for layer INLINEFORM1 . For all but the first layer, the input INLINEFORM2 is replaced by the hidden state vector from the previous layer INLINEFORM3 . For the fully connected layers, we use rectified linear unit (ReLU) BIBREF29 activation, except for the final layer which depends on the form of supervision and loss used in training.",
            "Exploring alternatives to traditional RNNs for sequence tasks is a major area of current research. Quasi-recurrent neural networks are related to several such recently described models, especially the strongly-typed recurrent neural networks (T-RNN) introduced by BIBREF12 . While the motivation and constraints described in that work are different, BIBREF12 's concepts of “learnware” and “firmware” parallel our discussion of convolution-like and pooling-like subcomponents. As the use of a fully connected layer for recurrent connections violates the constraint of “strong typing”, all strongly-typed RNN architectures (including the T-RNN, T-GRU, and T-LSTM) are also quasi-recurrent. However, some QRNN models (including those with attention or skip-connections) are not “strongly typed”. In",
            "collecting questions is an additional challenge. Different difficulties in building a QA System are mentioned in the literature BIBREF2 BIBREF3. The first work on a machine learning based approach towards Bengali question classification is presented in BIBREF0 that employ the Stochastic Gradient Descent (SGD).",
            "Reading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries. In recent years, many large-scale RC datasets in English BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11. The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance. Nonetheless, RC on languages other than English has been limited due to the absence of sufficient training data. Although some efforts have been made to create RC datasets for Chinese BIBREF13, BIBREF14 and Korean BIBREF15, it is not feasible to collect RC datasets for every language since"
        ]
    },
    {
        "title": "Argumentation Mining in User-Generated Web Discourse",
        "answer": "lexical and semantic challenges of domain-specific data that often include advanced argumentation and complex contextual information.  sparseness of relevant information sources.  lack of similarities between users' searches. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in conversational styles. differences in convers",
        "evidence": [
            "Topics and registers",
            "Domain shift is a fundamental problem in machine learning, that has attracted a lot of attention in the natural language processing and vision communities BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . To understand and address this problem, generated by the lack of labeled data in a target domain, researchers have studied the behavior of machine learning methods in cross-domain settings BIBREF2 , BIBREF11 , BIBREF10 and came up with various domain adaptation techniques BIBREF12 , BIBREF5 , BIBREF6 , BIBREF9 . In cross-domain classification, a classifier is trained on data from a source domain and tested on data from a (different) target domain. The accuracy of machine learning methods is usually lower in the cross-domain",
            "at capturing domain-specific features as compared to word2vec, which generally captures general semantic and syntactic characteristics, but not the sentimental ones.",
            "can input to a corresponding segment and encouraged them to input a diverse set of questions which the span can answer. Along with the questions, the workers were also required to provide a justification as to why they made their questions. We manually checked this justification to filter out the questions with poor quality by removing those questions which were unrelated to the video. One initial challenge worth mentioning is that at first some workers input questions they had about the video and not questions which the video could answer. This was solved by providing them with an unrelated example. The second part of the question collection framework consisted of a paraphrasing task. In this task we presented workers with the questions generated by the first task and asked them to write",
            "Many speech processing tasks – such as automatic speech recognition or spoken term detection – hinge on associating segments of speech signals with word labels. In most systems developed for such tasks, words are broken down into sub-word units such as phones, and models are built for the individual units. An alternative, which has been considered by some researchers, is to consider each entire word segment as a single unit, without assigning parts of it to sub-word units. One motivation for the use of whole-word approaches is that they avoid the need for sub-word models. This is helpful since, despite decades of work on sub-word modeling BIBREF0 , BIBREF1 , it still poses significant challenges. For example, speech processing systems are still hampered by differences in conversational",
            "cited in entity pages comes from the first paragraphs. However, challenging task such as the canonicalization and chronological ordering of facts, still remain.",
            "Exp III: Introducing Sequence Labeling as Auxiliary Task",
            "With the availability of rich data on users' locations, profiles and search history, personalization has become the leading trend in large-scale information retrieval. However, efficiency through personalization is not yet the most suitable model when tackling domain-specific searches. This is due to several factors, such as the lexical and semantic challenges of domain-specific data that often include advanced argumentation and complex contextual information, the higher sparseness of relevant information sources, and the more pronounced lack of similarities between users' searches. A recent study on expert search strategies among healthcare information professionals BIBREF0 showed that, for a given search task, they spend an average of 60 minutes per collection or database, 3 minutes to",
            "this dataset to shed light on challenges to question answerability, with domain-general implications for any question answering system. The PrivacyQA corpus offers a challenging corpus for question answering, with genuine real-world utility.",
            "in which supervision is provided by heterogeneous labelers with varying levels of expertise BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF14 . The related question of whether effort is better spent collecting additional annotations for already labeled (but potentially noisily so) examples or novel instances has also been addressed BIBREF18 . What distinguishes the work here is our focus on providing an operational definition of instance difficulty, showing that this can be predicted, and then using this to inform task routing.",
            "several machine learning algorithms to perform entity recognition combining task-specific and corpus-based features. We also show how to improve recognition results by jointly considering formal and user-generated content",
            "in the data. Understanding how women under-representation in broadcast data can lead to bias in ASR performances is the key to prevent re-implementing and reinforcing discrimination already existing in our societies. This is in line with the concept of “Fairness by Design\" proposed by BIBREF31. Gender, race, religion, nationality are all characteristics that we deem unfair to classify on, and these ethical standpoints needs to be taken into account in systems' design. Characteristics that are not considered as relevant in a given task can be encapsulated in data nonetheless, and lead to bias performance. Being aware of the demographic skews our data set might contain is a first step to track the life cycle of a training data set and a necessary step to control the tools we develop."
        ]
    },
    {
        "title": "Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model",
        "answer": "English",
        "evidence": [
            "What Does Zero-shot Transfer Model Learn? ::: Unseen Language Dataset",
            "Model Training",
            "is applied. These settings were determined through experiments on the development set. The classifier network is trained with a cross entropy loss and optimized using stochastic gradient descent (SGD) with Nesterov momentum BIBREF33 . The learning rate is initialized at 0.1 and is reduced by a factor of 10 according to the following heuristic: If 99% of the current epoch's average batch loss is greater than the running average of batch losses over the last 3 epochs, this is considered a plateau; if there are 3 consecutive plateau epochs, then the learning rate is reduced. Training stops when reducing the learning rate no longer improves dev set AP. Then, the model from the epoch corresponding to the the best dev set AP is chosen. Several other optimizers—Adagrad BIBREF34 , Adadelta",
            "Do models generalize explicit supervision, or just memorize it? ::: Setup",
            "not giving negative examples on verbs that appear in the test set; (2) at a construction level, by not giving negative examples about a particular construction, e.g., verbs after a subject RC. We observe no huge score drops by both. This suggests that our learning signals at a lexical level (negative words) strengthen the abstract syntactic knowledge about the target constructions, and also that the models can generalize the knowledge acquired by negative examples to similar constructions for which negative examples are not explicitly given. The result also implies that negative examples do not have to be complete and can be noisy, which will be appealing from an engineering perspective.",
            "( INLINEFORM2 and INLINEFORM3 ) to represent the hidden vector sequence given the question INLINEFORM4 and the document INLINEFORM5 in the target (source) domain respectively. The domain discriminator INLINEFORM0 focuses on identifying the domain of the vector sequence is from given INLINEFORM1 or INLINEFORM2 , where the objective is to minimize INLINEFORM3 . DISPLAYFORM0  Given a training example from the target domain ( INLINEFORM0 ), INLINEFORM1 learns to assign a lower score to INLINEFORM2 and INLINEFORM3 in that example, that is, to minimize INLINEFORM4 and INLINEFORM5 . On the other hand, given a training example from the source domain ( INLINEFORM6 ), INLINEFORM7 learns to assign a larger value to INLINEFORM8 and INLINEFORM9 . Furthermore, we update the parameters of the embedding",
            "Our Definition-Wild challenges the system design – how to develop a $\\textsc {0shot-tc}$ system, without accessing any task-specific labeled data, to deal with labels from diverse aspects? In this work, we propose to treat $\\textsc {0shot-tc}$ as a textual entailment problem. This is to imitate how humans decide the truth value of labels from any aspects. Usually, humans understand the problem described by the aspect and the meaning of the label candidates. Then humans mentally construct a hypothesis by filling a label candidate, e.g., “sports”, into the aspect-defined problem “the text is about $\\underline{?}$”, and ask ourselves if this hypothesis is true, given the text. We treat $\\textsc {0shot-tc}$ as a textual entailment problem so that our model can gain knowledge from entailment",
            "Similarly, we use another replay buffer to store question answering transitions (observation string when interaction stops, question string, ground-truth answer). Because both iSQuAD and iNewsQA are converted from datasets that provide ground-truth answer positions, we can leverage this information and train the question answerer with supervised learning. Specifically, we only push question answering transitions when the ground-truth answer is in the observation string. For each transition, we convert the ground-truth answer head- and tail-positions from the SQuAD and NewsQA datasets to positions in the current observation string. After every 5 game steps, we randomly sample a mini-batch of 64 transitions from the replay buffer and train the question answerer using the Negative",
            "Table TABREF6 shows the result of different models trained on either Chinese or English and tested on Chinese. In row (f), multi-BERT is fine-tuned on English but tested on Chinese, which achieves competitive performance compared with QANet trained on Chinese. We also find that multi-BERT trained on English has relatively lower EM compared with the model with comparable F1 scores. This shows that the model learned with zero-shot can roughly identify the answer spans in context but less accurate. In row (c), we fine-tuned a BERT model pre-trained on English monolingual corpus (English BERT) on Chinese RC training data directly by appending fastText-initialized Chinese word embeddings to the original word embeddings of English-BERT. Its F1 score is even lower than that of zero-shot",
            "scenario. The largest degradation is for natural, where the model is prevented from learning from informative source sentences, which leads to a decrease of 0.4 to over 1.0 BLEU. We assume from these experiments that BT impacts most of all the decoder, and learning to encode a pseudo-source, be it a copy or an actual back-translation, only marginally helps to significantly improve the quality. Finally, in the fwdtrans-nmt setup, freezing the decoder does not seem to harm learning with a natural source.",
            "All the models follow the same philosophy of abstraction enabling a unified API in the library. Configuration - A configuration class instance (usually inheriting from a base class `PretrainedConfig`) stores the model and tokenizer parameters (such as the vocabulary size, the hidden dimensions, dropout rate, etc.). This configuration object can be saved and loaded for reproducibility or simply modified for architecture search. The configuration defines the architecture of the model but also architecture optimizations like the heads to prune. Configurations are agnostic to the deep learning framework used. Tokenizers - A Tokenizer class (inheriting from a base class `PreTrainedTokenizer`) is available for each model. This class stores the vocabulary token-to-index map for the corresponding",
            "Deep Learning Model"
        ]
    },
    {
        "title": "Automated News Suggestions for Populating Wikipedia Entity Pages",
        "answer": "salience feature, relative authority feature, news domain authority, entity embeddings, string based similarities, compatibility similarities.",
        "evidence": [
            "of the individual feature groups that contribute to the superior performance in comparison to the baselines. Relative entity frequency from the salience feature, models the entity salience as an exponentially decaying function based on the positional index of the paragraph where the entity appears. The performance of INLINEFORM0 with relative entity frequency from the salience feature group is close to that of all the features combined. The authority and novelty features account to a further improvement in terms of precision, by adding roughly a 7%-10% increase. However, if both feature groups are considered separately, they significantly outperform the baseline B1.",
            "Representing Words and Documents",
            "and each entity and entity pair has an associated feature vector $\\psi $ . The truth probability of a category instance $c(e)$ or relation instance $r(e_1, e_2)$ is thus given by: $",
            "\\textsc {Palladio})$ . Here, the $\\textit {N/N}$ predicate represents a generic noun modifier relation; however, this relation is too vague for the predicate model to accurately learn its denotation. A similar problem occurs with prepositions and possessives, e.g., it is similarly hard to learn the denotation of the predicate $\\textit {of}$ . Our system improves the analysis of noun-mediated relations by simply including the noun in the predicate name. In the architect example above, our system produces the relation $\\textit {architect\\_N/N}$ . It does this by concatenating all intervening noun modifiers between two entity mentions and including them in the predicate name; for example, “Illinois attorney general Lisa Madigan” produces the predicate $\\textit {attorney\\_general\\_N/N}$ . We",
            "This prompts us to consider the saliency of attention. The concept of saliency was first introduced in vision for visualizing the spatial support on an image for a particular object class BIBREF16 . In NLP, saliency has been used to study the importance of words toward a final decision BIBREF0 . We propose to examine the saliency of attention. Specifically, given a premise-hypothesis pair and the model's decision $y$ , we consider the similarity between a pair of premise and hypothesis hidden states $e_{ij}$ as a variable. The score of the decision $S(y)$ is thus a function of $e_{ij}$ for all $i$ and $j$ . The saliency of $e_{ij}$ is then defined to be $|\\frac{\\partial S(y)}{\\partial {e_{ij}}}|$ . The second row of Fig. 1 presents the attention saliency map for the three examples",
            "several machine learning algorithms to perform entity recognition combining task-specific and corpus-based features. We also show how to improve recognition results by jointly considering formal and user-generated content",
            "systems often perform the two tasks in isolation. Usually, they design query-dependent features (e.g., query word overlap) to learn the relevance ranking, and query-independent features (e.g., term frequency) to learn the saliency ranking. Then, the two types of features are combined to train an overall ranking model. Note that the only supervision available is the reference summaries. Humans write summaries with the trade-off between relevance and saliency. Some salient content may not appear in reference summaries if it fails to respond to the query. Likewise, the content relevant to the query but not representative of documents will be excluded either. As a result, in an isolated model, weights for neither query-dependent nor query-independent features could be learned well from",
            "propose an approach that constructs short summaries for the long tail. The summaries are called `gems' and the size of a `gem' can be user defined. They focus on generating summaries that are novel and diverse. However, they do not consider any structure of entities, which is present in Wikipedia. In contrast to BIBREF8 and BIBREF12 , we actually focus on suggesting entire documents to Wikipedia entity pages. These are authoritative documents (news), which are highly relevant for the entity, novel for the entity and in which the entity is salient. Whereas relevance in Sauper and Barzilay is implicitly computed by web page ranking we solve that problem by looking at relative authority and salience of an entity, using the news article and entity page only. As Sauper and Barzilay concentrate",
            "Properties of Attentions",
            "This model tracks how the grammatical relations of salient entities (e.g., subj, obj, etc.) change between pairs of sentences in a document, thus capturing a form of discourse coherence. The grid is summarized into a vector of transition probabilities. However, because the model only records the transition between two consecutive sentences at a time, the coherence is local. BIBREF2 (henceforth F15) further extends the entity-grid model by replacing grammatical relations with discourse relations from Rhetorical Structure Theory BIBREF7 . Their study uses a linear-kernel SVM to perform pairwise author classifications, where a non-discourse model captures lexical and syntactic features. They find that adding the entity-grid with grammatical relations enhances the non-discourse model by",
            "any news articles for entities with very high a priori authority (such as Clinton) due to the strict inequality constraint, we can relax the constraint such that the authority of co-occurring entities is above a certain threshold. News Domain Authority. The news domain authority addresses two main aspects. Firstly, if bundled together with the relative authority feature, we can ensure that dependent on the entity authority, we suggest news from authoritative sources, hence ensuring the quality of suggested articles. The second aspect is in a news streaming scenario where multiple news domains report the same event — ideally only articles coming from authoritative sources would fulfill the conditions for the news suggestion task. The news domain authority is computed based on the number of",
            "string based similarities, compatibility similarities and the embeddings of contexts as well as the entity as the local feature vectors."
        ]
    },
    {
        "title": "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
        "answer": "English, Portuguese, Chinese, Japanese, Korean, French.",
        "evidence": [
            "Data",
            "trained on a massively multilingual corpus. A language embedding vector is concatenated to the input at each time step. The language embeddings their system learned correlate closely to the genetic relationships between languages. However, neither of these models was applied to g2p.",
            "Language Resource References\nlrec lrec2018",
            "Results and Analysis",
            "The TDC datasets are available in the CAPES open data website divided by years, from 2013 to 2016 in CSV and XLSX formats. We downloaded all CSV files from the respective website and loaded them into an SQL database for better manipulation. The database was then filtered to remove documents without both Portuguese and English abstracts, and additional metadata selected. After the initial filtering, the resulting documents were processed for language checking to make sure that there was no misplacing of English abstracts in the Portuguese field, or the other way around, removing the documents that presented such inconsistency. We also performed a case folding to lower case letters, since the TDC datasets present all fields with uppercase letters. In addition, we also removed",
            "Methodology ::: Data presentation",
            "English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set. Detailed guidelines for both phases are available online at: https://multisimlex.com.",
            "Emotion datasets",
            "Properties of back-translated data",
            "included in the corpus are in English, and were collected before April 1, 2018, predating many companies' GDPR-focused BIBREF41 updates. We leave it to future studies BIBREF42 to look at the impact of the GDPR (e.g., to what extent GDPR requirements contribute to making it possible to provide users with more informative answers, and to what extent their disclosures continue to omit issues that matter to users).",
            "Labels and metadata",
            "when testing on Korean. Translating the English training data into Chinese, Japanese and Korean still improve the performance on Korean. We also found that when translated into the same language, the English training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not."
        ]
    },
    {
        "title": "Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation",
        "answer": "Vietnamese part of the Japanese-Vietnamese parallel corpus.",
        "evidence": [
            "subsampled monolingual corpus, we use the Vietnamese part of the Japanese-Vietnamese parallel corpus in order to learn the multilingual information in the same domain. Our datasets are listed in Table TABREF14 .",
            "Emotion datasets",
            "We use the large-scale Yahoo dataset released by DBLPZhangZL15. Yahoo has 10 classes: {“Society & Culture”, “Science & Mathematics”, “Health”, “Education & Reference”, “Computers & Internet”, “Sports”, “Business & Finance”, “Entertainment & Music”, “Family & Relationships”, “Politics & Government”}, with original split: 1.4M/60k in train/test (all labels are balanced distributed). We reorganize the dataset by first fixing the dev and test sets as follows: for dev, all 10 labels are included, with 6k labeled instances for each; For test, all 10 labels are included, with 10k instances for each. Then training sets are created on remaining instances as follows. For label-partially-unseen, we create two versions of Yahoo train for $\\textsc {0shot-tc}$: Train-v0: 5 classes: {“Society &",
            "Japanese Tokenization",
            "Abstract\nNeural machine translation (NMT) systems have recently obtained state-of-the art in many machine translation systems between popular language pairs because of the availability of data. For low-resourced language pairs, there are few researches in this field due to the lack of bilingual data. In this paper, we attempt to build the first NMT systems for a low-resourced language pairs:Japanese-Vietnamese. We have also shown significant improvements when combining advanced methods to reduce the adverse impacts of data sparsity and improve the quality of NMT systems. In addition, we proposed a variant of Byte-Pair Encoding algorithm to perform effective word segmentation for Vietnamese texts and alleviate the rare-word problem that persists in NMT systems.",
            "BLEU score, we speculate that it is because the Vietnamese INLINEFORM3 Japanese system is not good enough to produce reasonable synthetic data. In the meanwhile, combining Back Translation and Mix-Source brings a considerable improvements of 0.6 BLEU points compared to not using them.",
            "trained on a massively multilingual corpus. A language embedding vector is concatenated to the input at each time step. The language embeddings their system learned correlate closely to the genetic relationships between languages. However, neither of these models was applied to g2p.",
            "TutorialVQA Dataset ::: Basis",
            "Construction of a Japanese Word Similarity Dataset",
            "when testing on Korean. Translating the English training data into Chinese, Japanese and Korean still improve the performance on Korean. We also found that when translated into the same language, the English training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not.",
            "by translating an English dataset to Japanese. We hope that a Japanese database will facilitate research in Japanese distributed representations.",
            "Ancient-Modern Chinese Dataset"
        ]
    },
    {
        "title": "Ancient-Modern Chinese Translation with a Large Training Dataset",
        "answer": "APDA shared task organizers.",
        "evidence": [
            "Ancient-Modern Chinese Dataset",
            "on to spurious features of the dataset. This is particularly true for datasets that are not well-balanced, and for annotations that are noisy. In our introductory example on hate speech in Reddit BIBREF0 , the annotations are automatically derived from the forum in which each post appears, and indeed, many of the posts in the forums (subreddits) that were banned by Reddit would be perceived by many as hate speech. But even in banned subreddits, not all of the content is hate speech (e.g., some of the top features were self-referential like the name of the subreddit) but a classifier would learn a high weight for these features. Even when expert annotations are available on the level of individual posts, spurious features may remain. BIBREF45 produced expert annotations of hate speech on",
            "having the same size with the Japanese-Vietnamese parallel corpus. In the end, the data we have is double in size compared to the original one.",
            "From gender representation in data to gender bias in AI ::: On the importance of data",
            "show great advantages over the statistic machine translation (SMT) systems. However, NMT approach has not been widely applied to the ancient-modern Chinese translation task. One of the main reasons is the limited high-quality parallel data resource. The most popular method of acquiring translation examples is bilingual text alignment BIBREF5 . This kind of method can be classified into two types: lexical-based and statistical-based. The lexical-based approaches BIBREF6 , BIBREF7 focus on lexical information, which utilize the bilingual dictionary BIBREF8 , BIBREF9 or lexical features. Meanwhile, the statistical-based approaches BIBREF10 , BIBREF11 rely on statistical information, such as sentence length ratio in two languages and align mode probability. However, these methods are designed",
            "Weibo, RenRen, and Chinese microblogs",
            "What Does Zero-shot Transfer Model Learn? ::: Unseen Language Dataset",
            "For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the",
            "Cross-Lingual Multi-SimLex Datasets",
            "Approach ::: Datasets",
            "SnapCaptions Dataset",
            "Chinese character usually corresponds to one or more modern Chinese words. In the first part, we carry out Chinese Word segmentation to the modern Chinese clause INLINEFORM1 . Then we match the ancient characters and modern words in the order from left to right. In further matching, the words that have been matched will be deleted from the original clauses. However, some ancient characters do not appear in its corresponding modern Chinese words. An ancient Chinese dictionary is employed to address this issue. We preprocess the ancient Chinese dictionary and remove the stop words. In this dictionary matching step, we retrieve the dictionary definition of each unmatched ancient character and use it to match the remaining modern Chinese words. To reduce the impact of universal word matching,"
        ]
    },
    {
        "title": "UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish",
        "answer": "cross-validation (10 folds) technique is used to test the classifier.",
        "evidence": [
            "Semi-supervised Learning",
            "is applied. These settings were determined through experiments on the development set. The classifier network is trained with a cross entropy loss and optimized using stochastic gradient descent (SGD) with Nesterov momentum BIBREF33 . The learning rate is initialized at 0.1 and is reduced by a factor of 10 according to the following heuristic: If 99% of the current epoch's average batch loss is greater than the running average of batch losses over the last 3 epochs, this is considered a plateau; if there are 3 consecutive plateau epochs, then the learning rate is reduced. Training stops when reducing the learning rate no longer improves dev set AP. Then, the model from the epoch corresponding to the the best dev set AP is chosen. Several other optimizers—Adagrad BIBREF34 , Adadelta",
            "systems often perform the two tasks in isolation. Usually, they design query-dependent features (e.g., query word overlap) to learn the relevance ranking, and query-independent features (e.g., term frequency) to learn the saliency ranking. Then, the two types of features are combined to train an overall ranking model. Note that the only supervision available is the reference summaries. Humans write summaries with the trade-off between relevance and saliency. Some salient content may not appear in reference summaries if it fails to respond to the query. Likewise, the content relevant to the query but not representative of documents will be excluded either. As a result, in an isolated model, weights for neither query-dependent nor query-independent features could be learned well from",
            "We propose a simple domain adaptation method for neural networks in a supervised setting. Supervised domain adaptation is a way of improving the generalization performance on the target domain by using the source domain dataset, assuming that both of the datasets are labeled. Recently, recurrent neural networks have been shown to be successful on a variety of NLP tasks such as caption generation; however, the existing domain adaptation techniques are limited to (1) tune the model parameters by the target dataset after the training by the source dataset, or (2) design the network to have dual output, one for the source domain and the other for the target domain. Reformulating the idea of the domain adaptation technique proposed by Daume (2007), we propose a simple domain adaptation method,",
            "findings BIBREF1 . A complementary approach to unsupervised and supervised learning is the use of crowdsourced human coders. BIBREF2 show that crowdsourcing text analysis is a way to achieve reliable and replicable results quickly and inexpensively through the CrowdFlower platform. This approach can work well in supporting a supervised approach where outcomes need to be labelled. For example, BIBREF2 use this technique to produce party positions on the economic left-right and liberal-conservative dimensions from party manifestos. Online coders receive small specific tasks to reduce individual biases. Their individual responses are then aggregated to create an overall measure of party positions.",
            "trained on a massively multilingual corpus. A language embedding vector is concatenated to the input at each time step. The language embeddings their system learned correlate closely to the genetic relationships between languages. However, neither of these models was applied to g2p.",
            "What is the Model Learning?",
            "In the case there dataset was not split in training and gold (Arabic, German, Portuguese, Russian, and Swedish) then a cross-validation (10 folds) technique is used to test the classifier. The performance of the classifier is presented using different metrics depending the competition. SemEval uses the average of score INLINEFORM0 of positive and negative labels, TASS uses the accuracy and SENTIPOLC uses a custom metric (see BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 ).",
            "Do models generalize explicit supervision, or just memorize it? ::: Setup",
            "are not available. Even though we focused on only using parallel data, our results are also relevant for work on using auxiliary data to improve low-resource MT. Supervised systems serve as an important baseline to judge the effectiveness of semisupervised or unsupervised approaches, and the quality of supervised systems trained on little data can directly impact semi-supervised workflows, for instance for the back-translation of monolingual data.",
            "and $b_r$ are learnable parameters. $r_t$ represents how much degree of the current attention should be updated. Small value of $r_{ti}$ indicates that the content of $i$-th position is not much relevant to current decoding state $s_t$, and the attention on $i$-th position should be weakened to avoid confusing the model. The attention distribution is updated as follows (the symbol $\\odot $ means element-wise product):",
            "tweets corpus. Words that were clearly not indicators of emotion were removed. The rest was annotated per emotion or removed if it was unclear to which emotion the word belonged. This allowed us to create silver datasets per emotion, assigning tweets to an emotion if an annotated emotion-word occurred in the tweet. Our semi-supervised approach is quite straightforward: first a model is trained on the training set and then this model is used to predict the labels of the silver data. This silver data is then simply added to our training set, after which the model is retrained. However, an extra step is applied to ensure that the silver data is of reasonable quality. Instead of training a single model initially, ten different models were trained which predict the labels of the silver"
        ]
    },
    {
        "title": "Conflict as an Inverse of Attention in Sequence Relationship",
        "answer": "Conflict model, Task-oriented dialogue, Online testing of task-oriented dialogue, Predicting conflict onset, Quantifying task difficulty.",
        "evidence": [
            "Conflict model",
            "context determining the grammaticality of the target word (No/*Most) does not precede it. Rather, the grammaticality is determined by the following context. As we discuss in Section method, this property makes it difficult to apply training with negative examples for NPIs for most of the methods studied in this work. All examples above (UNKREF1–UNKREF11) are actual test sentences, and we can see that since they are synthetic some may sound somewhat unnatural. The main argument behind using this dataset is that even not very natural, they are still strictly grammatical, and an LM equipped with robust syntactic abilities should be able to handle them as human would do.",
            "We proposed a new problem of predicting whether different people would answer with the same response to the same visual question. Towards motivating the practical implications for this problem, we analyzed nearly half a million visual questions and demonstrated there is nearly a 50/50 split between visual questions that lead to answer agreement versus disagreement. We observed that crowd disagreement arose for various types of answers (yes/no, counting, other) for many different reasons. We next proposed a system that automatically predicts whether a visual question will lead to a single versus multiple answers from a crowd. Our method outperforms a strong existing VQA system limited to estimating system uncertainty rather than crowd disagreement. Finally, we demonstrated how to employ",
            "Task 2: Online Testing of Task-oriented Dialogue",
            "to resolve conflicts. Voss2006 analyzed experts' protocols dealing with problem-solving. The model has also been used in research on computer-supported collaborative learning. Erduran2004 adapt Toulmin's model for coding classroom argumentative discourse among teachers and students. Stegmann2011 builds on a simplified Toulmin's model for scripted construction of argument in computer-supported collaborative learning. Garcia-Mila2013 coded utterances into categories from Toulmin's model in persuasion and consensus-reaching among students. Weinberger.Fischer.2006 analyze asynchronous discussion boards in which learners engage in an argumentative discourse with the goal to acquire knowledge. For coding the argument dimension, they created a set of argumentative moves based on Toulmin's model.",
            "METHODS",
            "The self-dialog technique renders quality data and avoids some of the challenges seen with the two-person approach. To begin, since the same person is writing both sides of the conversation, we never see misunderstandings that lead to frustration as is sometimes experienced between interlocutors in the two-person approach. In addition, all the self-dialogs follow a reasonable path even when the user is constructing conversations that include understanding errors or other types of dialog glitches such as when a particular choice is not available. As it turns out, crowdsourced workers are quite effective at recreating various types of interactions, both error-free and those containing various forms of linguistic repair. The sample dialog in Figure FIGREF44 shows the result of a self-dialog",
            "predictors of conflict onset. Here, we assess the goodness-of-fit of our models, the out-of-sample predictive accuracy of the models, and robustness checks of our results at different tie similarity thresholds. To assess the in-sample goodness-of-fit (GOF) of exponential random graph models, it is common to simulate network statistics which were not specified in the model to see how well the fitted model can simulate statistics on the outcome network. In our case, the outcome network is the conflict onset network, and we perform 50 simulations over each of the 30 time steps for 1,500 total simulations for each of the four models. The in-sample areas under the ROC and PR curves for the three models of primary concern are presented in Figure FIGREF36 and Figure FIGREF37 , respectively.",
            "In the case there dataset was not split in training and gold (Arabic, German, Portuguese, Russian, and Swedish) then a cross-validation (10 folds) technique is used to test the classifier. The performance of the classifier is presented using different metrics depending the competition. SemEval uses the average of score INLINEFORM0 of positive and negative labels, TASS uses the accuracy and SENTIPOLC uses a custom metric (see BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 ).",
            "As a main field of interest in the current study, we chose controversies in education. One distinguishing feature of educational topics is their breadth of sub-topics and points of view, as they attract researchers, practitioners, parents, students, or policy-makers. We assume that this diversity leads to the linguistic variability of the education topics and thus represents a challenge for NLP. In a cooperation with researchers from the German Institute for International Educational Research we identified the following current controversial topics in education in English-speaking countries: (1) homeschooling, (2) public versus private schools, (3) redshirting — intentionally delaying the entry of an age-eligible child into kindergarten, allowing their child more time to mature",
            "repel each other and finally empirically show how this method in conjunction with attention can boost the overall performance.",
            "Quantifying Task Difficulty"
        ]
    },
    {
        "title": "A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification",
        "answer": "CNNs, BoW/SVM classifier.",
        "evidence": [
            "Dictionary-based approaches",
            "LR+IR method outperformed the best system in all measures, with 80.57% Accuracy and 77.47% MAP (official system ranking measure in SemEval-cQA). On the cQA-2017 test data, the LR+IR method obtained 44.66% MAP and outperformed the cQA-2017 best system in Accuracy with 67.27%.",
            "MARN Model",
            "outperform their models that rely on morphosyntactic resources (lexicons or morphological analysers). In this paper, we report on larger comparison, carried out in a larger multilingual setting and comparing different tagging models. Using different 16 datasets, we compare the performances of two feature-based models enriched with external lexicons and of two LSTM-based models enriched with word vector representations. A secondary goal of our work is to compare the relative improvements linked to the use of external lexical information in the two feature-based models, which use different models (MEMM vs. CRF) and feature sets. More specifically, our starting point is the MElt system BIBREF12 , an MEMM tagging system. We first briefly describe this system and the way we adapted it by",
            "model to classify a question into a restricted set of 10 question types and crawled \"relevant\" online web pages to find the answers. However, the results were lower than those achieved by the systems relying on finding similar answered questions. These results support the relevance of similar question matching for the end-to-end QA task as a new way of approaching QA instead of the classical QA approaches based on Question Analysis and Answer Retrieval.",
            "Results ::: Comparison w.r.t. baselines.",
            "BIBREF13 presented the Unmasking approach which, according to the authors, represents a unary AV method. However, if we take a closer look at the learning process of Unmasking, we can see that it is based on a binary SVM classifier that consumes feature vectors (derived from “degradation curves”) labeled as Y (“same author”) or N (“different author”). Unmasking, therefore, cannot be considered to be unary as the decision is not solely based on the documents within INLINEFORM0 , in analogy to the CNG approach of Jankowska et al. BIBREF24 discussed above. It should be highlighted again that the aforementioned three approaches are binary-intrinsic since their decision criteria INLINEFORM1 or INLINEFORM2 was determined on a set of problems labeled in a binary manner (Y and N) while after",
            "perspectives and approaches, which differ considerably from each other in conceptualization, scope, and theoretical refinement.”",
            "Principal components of different models",
            "not familiar with the packaged products including the chosen 20 products; hence, they were \"illiterate\" in terms of comprehending English and in terms of having used any of the products although they might be literate in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\" (MOS) BIBREF23 , BIBREF24 to measure the effectiveness: how similar a response were comparing to the correct product's usage. The MOS score range is 1-5 (1-Bad, 2-Poor, 3-Fair,",
            "We compare our approach in eight different languages, three of them have important international contests, namely, SemEval (English), TASS (Spanish), and SENTIPOLC (Italian). Within the competitions our approach reaches from medium to high positions in the rankings; whereas in the remaining languages our approach outperforms the reported results.",
            "This section summarizes our experimental results. We first describe the dataset, experimental setup, training procedure and classification accuracy of our ML models. We will consider four ML models: three CNNs with different filter sizes and a BoW/SVM classifier. Then, we demonstrate that LRP can be used to identify relevant words in text documents. We compare heatmaps for the best performing CNN model and the BoW/SVM classifier, and report the most representative words for three exemplary document categories. These results demonstrate qualitatively that the CNN model produces better explanations than the BoW/SVM classifier. After that we move to the evaluation of the document summary vectors, where we show that a 2D PCA projection of the document vectors computed from the LRP scores"
        ]
    },
    {
        "title": "Extractive Summarization of Long Documents by Combining Global and Local Context",
        "answer": "global context: whole document, local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.  local context: specific part of the document.",
        "evidence": [
            "And the improvement is even greater when we only consider long documents. Rather surprisingly, this is not the case for the global context. Adding a representation of the whole document (i.e. global context) never significantly improves performance. In essence, it seems that all the benefits of our model come exclusively from modeling the local context, even for the longest documents. Further investigation of this finding is left as future work.",
            "Local Features",
            "perspectives and approaches, which differ considerably from each other in conceptualization, scope, and theoretical refinement.”",
            "External State Interactive Dialog Context LM",
            "string based similarities, compatibility similarities and the embeddings of contexts as well as the entity as the local feature vectors.",
            "correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample.  3) Source: CARD-660 BIBREF78. 67 word pairs are taken from this dataset focused on rare word similarity, applying the same selection criteria a) to e) employed for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus BIBREF94. CARD-660 contains some words that are very rare (logboat), domain-specific (erythroleukemia) and slang (2mrw), which might be difficult to translate and annotate across a wide array of languages. Hence, we opt for retaining only the concept pairs above the threshold of top 250K most frequent Wikipedia concepts, as above.  4) Source: SimVerb-3500 BIBREF15 Since both CARD-660 and",
            "Proposed model ::: Local Variance Loss",
            "Preliminaries and Framework",
            "Do models generalize explicit supervision, or just memorize it? ::: Setup",
            "We applied multi-source attention HLSTM where commonsense knowledge is encoded by graph attention. HLSTM+Contextual Attention(CA): Contextual attention is applied to represent commonsense knowledge.",
            "Related Work and Discussion",
            "Through building context clues and using implicit knowledge, the model is able to produce reasonable story endings. context clues implied in the post and make the inference based on it. Automatic and manual evaluation shows that our model can generate more reasonable story endings than state-of-the-art baselines."
        ]
    },
    {
        "title": "Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets",
        "answer": "Accuracy, F1, c@1, AUC, AUC@1, WER, WER 100, PER, ROUGE-1, ROUGE-2, ROUGE-L, NDCG, EM.",
        "evidence": [
            "Evaluation Metrics",
            "possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth. Besides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human experts. Furthermore, for insight-driven analyses, it can be more useful to focus on improving",
            "Performance Measures\nAccording to our extensive literature research, numerous measures (e. g., Accuracy, F INLINEFORM0 , c@1, AUC, AUC@1, INLINEFORM1 or EER) have been used so far to assess the performance of AV methods. In regard to our experiments, we decided to use c@1 and AUC for several reasons. First, Accuracy, F INLINEFORM2 and INLINEFORM3 are not applicable in cases where AV methods leave verification problems unanswered, which concerns some of our examined AV approaches. Second, using AUC alone is meaningless for non-optimizable AV methods, as explained in Section UID17 . Third, both have been used in the PAN-AV competitions BIBREF5 , BIBREF12 . Note that we also list the confusion matrix outcomes.",
            "We use manual evaluation for task 2. For each system and each complete user intent, the initial sentence, which is used to start the dialogue, is the same. The tester then begin to converse to each system. A dialogue is finished if the system successfully returns the information which the user inquires or the number of dialogue turns is larger than 30 for a single task. For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied,",
            "scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014.",
            "Experiments ::: Label-partially-unseen evaluation ::: Discussion.",
            "Train and Testing Evaluation Setup",
            "We use the following three evaluation metrics: Phoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences. Word Error Rate (WER) is the percentage of words in which the predicted phoneme sequence does not exactly match the gold standard phoneme sequence. Word Error Rate 100 (WER 100) is the percentage of words in the test set for which the correct guess is not in the first 100 guesses of the system. In system evaluations, WER, WER 100, and PER numbers presented for multiple languages are averaged, weighting each language equally BIBREF13 . It would be interesting to compute error metrics that incorporate phoneme similarity, such as those proposed by",
            "qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.",
            "Evaluation Metrics\nFollowing previous work BIBREF2 , BIBREF14 , BIBREF11 , we use the standard F1 scores of ROUGE-1, ROUGE-2 and ROUGE-L BIBREF16 to evaluate the selected templates and generated summaries, where the official ROUGE script is applied. We employ the normalized discounted cumulative gain (NDCG) BIBREF20 from information retrieval to evaluate the Fast Rerank module.\n\n\nResults and Analysis\nIn this section, we report our experimental results with thorough analysis and discussions.",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\", “iol\"). In this plan, “R-1\",“C-1\", “C-0\", and “O-3\" are symbols for locations (nodes) in the graph. We compare the performance of translation approaches based on four metrics: [align=left,leftmargin=0em,labelsep=0.4em,font=] As in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0. The harmonic average of the precision and recall over all the test set",
            "Cross-Lingual Evaluation ::: Models in Comparison"
        ]
    },
    {
        "title": "Creative GANs for generating poems, lyrics, and metaphors",
        "answer": "SNLI, multiNLI, Quora, Clinical-QE, SemEval-cQA, Rotowire, Affective Text, Fairy Tales, ISEAR, SnapCaptions, TutorialVQA.",
        "evidence": [
            "Datasets",
            "We evaluate the RQE methods (i.e. deep learning model and logistic regression classifier) using two datasets of sentence pairs (SNLI and multiNLI), and three datasets of question pairs (Quora, Clinical-QE, and SemEval-cQA). The Stanford Natural Language Inference corpus (SNLI) BIBREF13 contains 569,037 sentence pairs written by humans based on image captioning. The training set of the MultiNLI corpus BIBREF14 consists of 393,000 pairs of sentences from five genres of written and spoken English (e.g. Travel, Government). Two other \"matched\" and \"mismatched\" sets are also available for development (20,000 pairs). Both SNLI and multiNLI consider three types of relationships between sentences: entailment, neutral and contradiction. We converted the contradiction and neutral labels to the same",
            "Models Used in the Evaluation",
            "Experimental setup ::: The Rotowire dataset",
            "For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the",
            "acknowledging the flaws in our data and drawing cautious and reasonable conclusions from them. In all cases, we should report the choices we have made when creating or re-using any dataset.",
            "Dictionary-based approaches",
            "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7 . A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our",
            "GAN setups",
            "Datasets Used for the RQE Study",
            "Results: SnapCaptions Dataset",
            "TutorialVQA Dataset ::: Overview"
        ]
    },
    {
        "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text",
        "answer": "Baseline LSTM-LM, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline",
        "evidence": [
            "Baseline",
            "Besides the analysis on different reasoning types, we also look into the performance over questions with different first tokens in the development set, which provide us an automatic categorization of questions. According to the results in Table TABREF46 , the three neural baselines all perform the best on “Who” and “Where” questions, to which the answers are often named entities. Since the tweet contexts are short, there are only a small number of named entities to choose from, which could make the answer pattern easy to learn. On the other hand, the neural models fail to perform well on the “Why” questions, and the results of neural baselines are even worse than that of the matching baseline. We find that these questions generally have longer answer phrases than other types of questions,",
            "Algorithms Used",
            "semantically more meaningful semantic extraction than the baseline methods. In the next section we will confirm these observations quantitatively.",
            "Preliminaries and Framework",
            "Target Task and Setup ::: Language models ::: Baseline LSTM-LM",
            "originally used in the two systems, respectively.  Baseline: Similar to our model, but without local context and global context, i.e. the input to MLP is the sentence representation only. Lead: Given a length limit of INLINEFORM0 words for the summary, Lead will return the first INLINEFORM1 words of the source document. Oracle: uses the Gold Standard extractive labels, generated based on ROUGE (Sec. 4.2).",
            "Baselines ::: Baseline3: Pipeline Segment retrieval",
            "Mainstream Improvements\nWe consider the hyperparameters used by BIBREF3 to be our baseline. This baseline does not make use of various advances in NMT architectures and training tricks. In contrast to the baseline, we use a BiDeep RNN architecture BIBREF25 , label smoothing BIBREF26 , dropout BIBREF27 , word dropout BIBREF28 , layer normalization BIBREF29 and tied embeddings BIBREF30 .",
            "TutorialVQA Dataset ::: Basis",
            "Experiments\nTo show the challenge of TweetQA for existing approaches, we consider four representative methods as baselines. For data processing, we first remove the URLs in the tweets and then tokenize the QA pairs and tweets using NLTK. This process is consistent for all baselines.",
            "The baseline and the models are evaluated on a corpus of 12 MOOC discussion forums. We train on 80% of the training data and report evaluation results on the held-out 20% of test data. We report INLINEFORM0 scores on the positive class (interventions), in line with prior work. We also argue that recall of the positive class is more important than precision, since it is costlier for instructors to miss intervening on a thread than spending irrelevant time intervening on a less critical threads due to false positives. Model hyperpameter settings. All proposed and baseline neural models are trained using Adam optimizer with a learning rate of 0.001. We used cross-entropy as loss function. Importantly we updated the model parameters during training after each instance as in vanilla stochastic"
        ]
    },
    {
        "title": "Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity",
        "answer": "manually annotated, using IOB tags.",
        "evidence": [
            "Data Collection and Annotation",
            "In May 2018, we crawled Twitter using the Python library Tweepy, creating two datasets on which Contributor and Musical Work entities have been manually annotated, using IOB tags. The first set contains user-generated tweets related to the BBC Radio 3 channel. It represents the source of user-generated content on which we aim to predict the named entities. We create it filtering the messages containing hashtags related to BBC Radio 3, such as #BBCRadio3 or #BBCR3. We obtain a set of 2,225 unique user-generated tweets. The second set consists of the messages automatically generated by the BBC Radio 3 Music Bot. This set contains 5,093 automatically generated tweets, thanks to which we have recreated the schedule. In Table 4, the amount of tokens and relative entities annotated are reported",
            "English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set. Detailed guidelines for both phases are available online at: https://multisimlex.com.",
            "annotations are used, this difference is less than 1%. This trend in performance may be explained by differences in annotation quality: the randomly sampled set was more consistently annotated by both experts and crowd because the difficult set is harder. However, in both cases expert annotations are better, with a bigger difference between the expert and crowd models on the difficult set. The last row is the model trained on all 5k abstracts with crowd annotations. Its F1 score is lower than either expert model trained on only 20% of data, suggesting that expert annotations should be collected whenever possible. Again the crowd model on complete data has higher precision than expert models but its recall is much lower.",
            "same focus-type pair. Each question was indexed with additional annotations for the question focus, its synonyms and the question type synonyms.",
            "pre-segmented using the Stanford Core NLP sentence splitter BIBREF69 embedded in the DKPro Core framework BIBREF70 . Annotators were asked to stick to the sentence level by default and label entire pre-segmented sentences. They should switch to annotations on the token level only if (a) a particular sentence contained more than one argument component, or (b) if the automatic sentence segmentation was wrong. Given the “noise” in user-generated Web data (wrong or missing punctuation, casing, etc.), this was often the case. Annotators were also asked to rephrase (summarize) each annotated argument component into a simple statement when applicable, as shown in Figure FIGREF66 . This was used as a first sanity checking step, as each argument component is expected to be a coherent discourse",
            "Datasets Used for the RQE Study",
            "The TDC datasets are available in the CAPES open data website divided by years, from 2013 to 2016 in CSV and XLSX formats. We downloaded all CSV files from the respective website and loaded them into an SQL database for better manipulation. The database was then filtered to remove documents without both Portuguese and English abstracts, and additional metadata selected. After the initial filtering, the resulting documents were processed for language checking to make sure that there was no misplacing of English abstracts in the Portuguese field, or the other way around, removing the documents that presented such inconsistency. We also performed a case folding to lower case letters, since the TDC datasets present all fields with uppercase letters. In addition, we also removed",
            "The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC). These captions are collected exclusively from snaps submitted to public and crowd-sourced stories (aka Snapchat Live Stories or Our Stories). Examples of such public crowd-sourced stories are “New York Story” or “Thanksgiving Story”, which comprise snaps that are aggregated for various public events, venues, etc. All snaps were posted between year 2016 and 2017, and do not contain raw images or other associated information (only textual captions and obfuscated visual descriptor features extracted from the pre-trained InceptionNet are available). We split the dataset into train",
            "versa. Therefore, we manually validated the complete dataset in a final step. We asked annotators to read all texts, questions, and answers, and to mark for each question whether the correct and incorrect answers were appropriate. If an answer was inappropriate or contained any errors, they selected a different answer from the set of collected candidates. For approximately 11.5% of the questions, at least one answer was replaced. 135 questions (approx. 1%) were excluded from the dataset because no appropriate correct or incorrect answer could be found.",
            "How Many Expert Annotations?",
            "Dataset"
        ]
    },
    {
        "title": "Multi-attention Recurrent Network for Human Communication Comprehension",
        "answer": "Long-short Term Hybrid Memory is a hybrid memory model that combines the strengths of LSTMs and other neural network models, whereas LSTMs are a type of recurrent neural network that are better suited for language modeling tasks.",
        "evidence": [
            "Long-short Term Hybrid Memory",
            "LSTMs and other neural network models are better as language models. Continuous word embeddings have more expressive power to represent fine-grained semantic similarities between words. A topic model provides a different perspective on a collection. It creates a set of probability distributions over the vocabulary of the collection, which, when combined together in different proportions, best match the content of the collection. We can sort the words in each of these distributions in descending order by probability, take some arbitrary number of most-probable words, and get a sense of what (if anything) the topic is “about”. Each of the text segments also has its own distribution over the topics, and we can sort these segments by their probability within a given topic to get a sense of",
            "However, some QRNN models (including those with attention or skip-connections) are not “strongly typed”. In particular, a T-RNN differs from a QRNN as described in this paper with filter size 1 and f-pooling only in the absence of an activation function on INLINEFORM0 . Similarly, T-GRUs and T-LSTMs differ from QRNNs with filter size 2 and fo- or ifo-pooling respectively in that they lack INLINEFORM1 on INLINEFORM2 and use INLINEFORM3 rather than sigmoid on INLINEFORM4 . The QRNN is also related to work in hybrid convolutional–recurrent models. BIBREF31 apply CNNs at the word level to generate INLINEFORM0 -gram features used by an LSTM for text classification. BIBREF32 also tackle text classification by applying convolutions at the character level, with a stride to reduce sequence length,",
            "the HMM is not performing well, resulting in an LSTM with a smaller number of hidden state dimensions that could be interpreted individually (Figures 3 , 3 ).",
            "Limitations of LSTM-LMs ::: Setup",
            "$M$8 is the input gate, $M$9 is the forget gate, and $m \\in M$0 is the output gate. $m \\in M$1 is the proposed update to the hybrid memory $m \\in M$2 at time $m \\in M$3 . $m \\in M$4 is the time distributed output of each modality. The neural cross-view dynamics code $z_{t}$ is the output of the Multi-attention Block at the previous time-step and is discussed in detail in next subsection. This neural cross-view dynamics code $z_{t}$ is passed to each of the individual LSTHMs and is the hybrid factor, allowing each individual LSTHM to carry cross-view dynamics that it finds related to its modality. The set of weights $W^m_*$ , $U^m_*$ and $V^m_*$ respectively map the input of LSTHM $x^m_t$ , output of LSTHM $h^m_t$ , and neural cross-view dynamics code $z_{t}$ to each LSTHM memory space",
            "Experimental Configurations\nWe evaluate the effectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification. We compare our LAN with two baseline aggregators, MEAN and LSTM, as described in the Encoder section. MEAN is used on behalf of pooling functions since it leads to the best performance in BIBREF9 ijcai2017-250. LSTM is used due to its large expressive capability BIBREF26 .",
            "similar general trends, the backward LSTM can sometimes focus on different parts of the sentence (e.g., see Fig. 11 of Appendix), suggesting the forward and backward readings provide complementary understanding of the sentence.",
            "a better-trained model. In near future, we will incorporate all the above techniques to get comparable performance with the state-of-the-art hybrid DNN/RNN-HMM systems.",
            "meaning that for those tasks the semi-supervised models could not be used. Per task, the LSTM and feed-forward model's predictions were averaged over 10 prediction runs. Subsequently, the predictions of all individual models were combined into an average. Finally, models were removed from the ensemble in a stepwise manner if the removal increased the average score. This was done based on their original scores, i.e. starting out by trying to remove the worst individual model and working our way up to the best model. We only consider it an increase in score if the difference is larger than 0.002 (i.e. the difference between 0.716 and 0.718). If at some point the score does not increase and we are therefore unable to remove a model, the process is stopped and our best ensemble of models has",
            "LSTM-Minus",
            "Using our cleaned data set, we now built a recurrent neural network (RNN) with long short-term memory gates (LSTM). Our RNN/LSTM is trained to predict, given a description, whether a home corresponds to a high/medium/low popularity listing. The architecture of the RNN/LSTM employs Tensorflow’s Dynamic RNN package. Each sentence input is first fed into an embedding layer, where the input’s text is converted to a GloVe vector. These GloVe vectors are learned via a global word-word co-occurrence matrix using our corpus of Airbnb listing descriptions [8]. At each time step, the GloVe vectors are then fed into an LSTM layer. For each layer, the model forward propagates the output of the LSTM layer to the next time-step’s LSTM layer via a rectified linear unit (RLU) activation function. Each"
        ]
    },
    {
        "title": "Construction of a Japanese Word Similarity Dataset",
        "answer": "CreateDebate Dataset",
        "evidence": [
            "Data Statistics",
            "From gender representation in data to gender bias in AI ::: On the importance of data",
            "Results on CreateDebate Dataset",
            "In-domain and out-of-domain data",
            "Data Collection ::: Analysis ::: Categories of Questions",
            "Data Collection and Annotation",
            "Approach ::: Datasets",
            "Data details and experimental setup",
            "Research questions",
            "Our Dataset",
            "Results and Analysis",
            "Datasets Used for the RQE Study"
        ]
    },
    {
        "title": "Assessing the Applicability of Authorship Verification Methods",
        "answer": "A corpus involving a specific challenge.",
        "evidence": [
            "Corpus and Annotation",
            "such as Apple, Amazon or Google vocal assistants now reach high performance level partly due to the amount of data possessed by these companies BIBREF9. Surprisingly, as data is said to be “the new oil\", few data sets are available for ASR systems. The best known are corpora like TIMIT BIBREF10, Switchboard BIBREF11 or Fisher BIBREF12 which date back to the early 1990s. The scarceness of available corpora is justified by the fact that gathering and annotating audio data is costly both in terms of money and time. Telephone conversations and broadcast recordings have been the primary source of spontaneous speech used. Out of all the 130 audio resources proposed by LDC to train automatic speech recognition systems in English, approximately 14% of them are based on broadcast news and",
            "minor attention is paid to their underlying characteristics such as reliability and robustness. These, however, must be taken into account before AV methods can be applied in real forensic settings. The objective of this paper is to fill this gap and to propose important properties and criteria that are not only intended to characterize AV methods, but also allow their assessment in a more systematic manner. By this, we hope to contribute to the further development of this young research field. Based on the proposed properties, we investigate the applicability of 12 existing AV approaches on three self-compiled corpora, where each corpus involves a specific challenge. The rest of this paper is structured as follows. Section SECREF2 discusses the related work that served as an inspiration",
            "The Taskmaster Corpus ::: Self-dialogs (one-person written dataset)\nWhile the two-person approach to data collection creates a realistic scenario for robust, spoken dialog data collection, this technique is time consuming, complex and expensive, requiring considerable technical implementation as well as administrative procedures to train and manage agents and crowdsourced workers. In order to extend the Taskmaster dataset at minimal cost, we use an alternative self-dialog approach in which crowdsourced workers write the full dialogs themselves (i.e. interpreting the roles of both user and assistant).",
            "Also, the task is not exactly question answering, but identification of document passages containing the answer. TriviaQA BIBREF21 is a corpus that contains automatically collected question-answer pairs from 14 trivia and quiz-league websites, together with web-crawled evidence documents from Wikipedia and Bing. While a majority of questions require world knowledge for finding the correct answer, it is mostly factual knowledge.",
            "such as Part-of-Speech Tagging, Syntax Parsing or Chunking, there requires a step to concatenate those subwords to make a word since in those tasks, word is necessarily considered as the smallest unit to be processed. This step is normally referred to as word segmentation. There are various word segmentation methods; The best ones are using machine learning approaches to learn from a labeled corpus. It makes the tasks hard and expensive to be applied in other domains. Furthermore, the translation unit in Machine Translation does not need to be a word but can be a subword or sequence of subwords if it have its own meaning. With this observation in mind, if we consider a subword, i.e. the sequence of characters between two white spaces, as a byte and a sentence as a sequence of bytes, we",
            "subsampled monolingual corpus, we use the Vietnamese part of the Japanese-Vietnamese parallel corpus in order to learn the multilingual information in the same domain. Our datasets are listed in Table TABREF14 .",
            "We create a new corpus which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation mining field to date. We choose several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming. A novel aspect of the corpus is its coverage of different registers of user-generated Web content, such as comments to articles, discussion forum posts, blog posts, as well as professional newswire articles. Since the data come from a variety of sources and no assumptions about its actual content with respect to argumentation can be drawn, we conduct two extensive annotation studies. In the first study, we tackle the problem of relatively high “noise” in the retrieved data. In particular, not all of the documents",
            "ft embeddings are not available. On the other hand, m-bert has perfect coverage. A general comparison between CC+Wiki and Wiki ft vectors, however, supports the intuition that larger corpora (such as CC+Wiki) yield higher correlations. Another finding is that a single massively multilingual model such as m-bert cannot produce semantically rich word-level representations. Whether this actually happens because the training objective is different—or because the need to represent 100+ languages reduces its language-specific capacity—is investigated further below. The overall results also clearly indicate that (i) there are differences in performance across different monolingual Multi-SimLex datasets, and (ii) unsupervised post-processing is universally useful, and can lead to huge",
            "In addition to using the three approaches independently, we also combined all the matrices generated in the previous approaches. That is, we concatenate the reduced forms (SVD - U) of corpus-based, dictionary-based, and the whole of 4-score vectors of each word, horizontally. Accordingly, each corpus word is represented by a 404-dimensional vector, since corpus-based and dictionary-based vector components are each composed of 200 dimensions, whereas the 4-score vector component is formed by four values. The main intuition behind the ensemble method is that some approaches compensate for what the others may lack. For example, the corpus-based approach captures the domain-specific, semantic, and syntactic characteristics. On the other hand, the 4-scores method captures supervised features,",
            "will be generated (using probabilities from the background corpus) and attempt to create them by sampling from the collection of applicable patterns. This process is repeated until all the required errors have been generated or the sentence is exhausted. During generation, we try to balance the distribution of error types as well as keeping the same proportion of incorrect and correct sentences as in the background corpus BIBREF10 . The required POS tags were generated with RASP BIBREF11 , using the CLAWS2 tagset.",
            "Answer Collection System"
        ]
    },
    {
        "title": "Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach",
        "answer": "PGN, FastAbs, DCA, CNN/Daily Mail, UTCNN, SMT, NMT, BIBREF0, BIBREF7, Toulmin's model.",
        "evidence": [
            "Baseline Models",
            "As shown in Table TABREF13 (the performance of other models is collected from their papers), our model exceeds the PGN baseline by 3.85, 2.1 and 3.37 in terms of R-1, R-2 and R-L respectively and receives over 3.23 point boost on METEOR. FastAbs BIBREF3 regards ROUGE scores as reward signals with reinforcement learning, which brings a great performance gain. DCA BIBREF4 proposes deep communicating agents with reinforcement setting and achieves the best results on CNN/Daily Mail. Although our experimental results have not outperformed the state-of-the-art models, our model has a much simpler structure with fewer parameters. Besides, these simple methods do yield a boost in performance compared with PGN baseline and may be applied on other models with attention mechanism. We further",
            "All the models follow the same philosophy of abstraction enabling a unified API in the library. Configuration - A configuration class instance (usually inheriting from a base class `PretrainedConfig`) stores the model and tokenizer parameters (such as the vocabulary size, the hidden dimensions, dropout rate, etc.). This configuration object can be saved and loaded for reproducibility or simply modified for architecture search. The configuration defines the architecture of the model but also architecture optimizations like the heads to prune. Configurations are agnostic to the deep learning framework used. Tokenizers - A Tokenizer class (inheriting from a base class `PreTrainedTokenizer`) is available for each model. This class stores the vocabulary token-to-index map for the corresponding",
            "What is the Model Learning?",
            "UTCNN Model Description",
            "for prediction models built by training a support vector machine using 5-fold, stratified cross-validation with all feature groups as a baseline for each class. We observed high performance for no evidence of depression and fatigue or loss of energy and moderate performance for all remaining classes.",
            "Furthermore, we test SMT models and various NMT models on the created dataset and provide a strong baseline for this task.",
            "lengths to be set apriori. All our proposed models generalise over modelling assumptions made by BIBREF0 . For the purpose of comparison against a state-of-the-art and competing baselines we choose BIBREF7 since BIBREF0 's system and data are not available for replication.",
            "the matching baseline. We find that these questions generally have longer answer phrases than other types of questions, with the average answer length being 3.74 compared to 2.13 for any other types. Also, since all the answers are written by humans instead of just spans from the context, these abstractive answers can make it even harder for current models to handle. We also observe that when people write “Why” questions, they tend to copy word spans from the tweet, potentially making the task easier for the matching baseline.",
            "Inductive Embedding Models",
            "TutorialVQA Dataset ::: Basis",
            "Original Toulmin's model"
        ]
    },
    {
        "title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU",
        "answer": "LSTM Gating Signals",
        "evidence": [
            "Baseline Agent ::: Model Structure ::: Encoder",
            "BIBREF2 was able to reach CPU decoding speeds of 100 words/sec for a deep model, but used 44 CPU cores to do so. There has been recent work in speeding up decoding by reducing the search space BIBREF3 , but little in computational improvements. In this work, we consider a production scenario which requires low-latency, high-throughput NMT decoding. We focus on CPU-based decoders, since GPU/FPGA/ASIC-based decoders require specialized hardware deployment and logistical constraints such as batch processing. Efficient CPU decoders can also be used for on-device mobile translation. We focus on single-threaded decoding and single-sentence processing, since multiple threads can be used to reduce latency but not total throughput. We approach this problem from two angles: In Section \"Decoder",
            "Decoder",
            "is better). As they are trained from much smaller amounts of data than current systems, these baselines are not quite competitive to today's best system, but still represent serious baselines for these datasets. Given our setups, fine-tuning with in-domain natural data improves BLEU by almost 4 points for both translation directions on in-domain tests; it also improves, albeit by a smaller margin, the BLEU score of the out-of-domain tests.",
            "Stage 1 - Retraining decoder only",
            "LSTM Gating Signals",
            "Standard and baseline methods",
            "input is then piped into the first hidden layer. As before, an ELU transformation is then applied between the first and second layer, as well as between the second and third hidden layers. Finally, a sigmoid activation is used on the output of the last hidden layer. This sigmoid activation is important since the output of our discriminator is a binary boolean that indicates whether the discriminator believes the input to have been real data or data produced by the generator. This discriminator is thus trained to minimize the binary cross-entropy loss of its prediction (whether the data was real or fake) and the real ground-truth of each data point. The general framework defined above was inspired largely by the open-source code of Nag Dev, and was built using Pytorch [7]. One key",
            "Experimental Setup ::: Baselines\nWe compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base: A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue BIBREF0; VAE-CNN: A variational autoencoder model with a LSTM encoder and a dilated CNN decoder BIBREF7; vMF-VAE: A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution BIBREF5.",
            "Beam-search decoder",
            ", a soft alignment is learned and a source context INLINEFORM9 at time INLINEFORM10 is calculated prior to the prediction of the decoder. Similar to the encoder, the recurrent decoder recursively generates one target word INLINEFORM0 to form a translated target sentence INLINEFORM1 in the end. At time INLINEFORM2 , it takes the previous hidden state of the decoder INLINEFORM3 , the previous embedded word representation INLINEFORM4 and the time-specific context vector INLINEFORM5 as inputs to calculate the current hidden state INLINEFORM6 : INLINEFORM7  Again, INLINEFORM0 is the recurrent activation function of the decoder and INLINEFORM1 is the shared word embedding matrix of the target sentences. Given the parallel corpus consisting of INLINEFORM0 training examples INLINEFORM1 , the",
            "We compare the proposed approach for translating natural language instructions into a navigation plan against alternative deep-learning models: [align=left,leftmargin=0em,labelsep=0.4em,font=] The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path. To test"
        ]
    },
    {
        "title": "A Stable Variational Autoencoder for Text Modelling",
        "answer": "BLEU score, sentiment accuracy, sentiment delta, and geometric mean.",
        "evidence": [
            "Sentence alignment quality",
            "Using GANs, we can train generative models in a two-player game setting between a discriminator and a generator, where the discriminator (a binary classifier) learns to distinguish between real and fake data samples and the generator tries to fool the discriminator by generating authentic and high quality output BIBREF16. GANs have shown to be successful in image generation tasks BIBREF17 and recently, some progress has been observed in text generation BIBREF13, BIBREF12, BIBREF15. Our generator is a language model trained using backpropagation through time BIBREF18. During the pre-training phase we optimize for MLE and during the GAN training phase, we optimize on the creativity reward from the discriminator. The discriminator's encoder has the same architecture as the generator encoder",
            "An evaluation of how good a method identifies relevant words in text documents can be performed qualitatively, e.g. at the document level, by inspecting the heatmap visualization of a document, or by reviewing the list of the most (or of the least) relevant words per document. A similar analysis can also be conducted at the dataset level, e.g. by compiling the list of the most relevant words for one category across all documents. The latter allows one to identify words that are representatives for a document category, and eventually to detect potential dataset biases or classifier specific drawbacks. However, in order to quantitatively compare algorithms such as LRP and SA regarding the identification of relevant words, we need an objective measure of the quality of the explanations",
            "Experiments ::: How do the generated hypotheses influence",
            "on the extracted information: $\\bullet $ Relation Generation (RG) estimates how well the system is able to generate text containing factual (i.e., correct) records. We measure the precision and absolute number (denoted respectively RG-P% and RG-#) of unique relations $r$ extracted from $\\hat{y}_{1:T}$ that also appear in $s$. $\\bullet $ Content Selection (CS) measures how well the generated document matches the gold document in terms of mentioned records. We measure the precision and recall (denoted respectively CS-P% and CS-R%) of unique relations $r$ extracted from $\\hat{y}_{1:T}$ that are also extracted from $y_{1:T}$. $\\bullet $ Content Ordering (CO) analyzes how well the system orders the records discussed in the description. We measure the normalized Damerau-Levenshtein distance",
            "with the improvement of template quality (larger ranges lead to better chances for good templates). Even with randomly-selected templates, our model still works with stable performance, demonstrating its robustness.",
            "typed input to create a full text version of the dialog. Finally, these conversations are checked for transcription errors and typos and then annotated, as described in Section SECREF48.",
            "reading of the texts alongside those outputs tells us something about the models that can be used for more effective model building. Applying this circular, iterative process to 450 18th-century novels written in three languages, Piper was able to uncover a new form of “conversional novel” that was not previously captured in “literary history's received critical categories” BIBREF48 . Along similar lines, we can subject both the machine-generated output and the human annotations to another round of content validation. That is, take a stratified random sample, selecting observations from the full range of scores, and ask: Do these make sense in light of the systematized concept? If not, what seems to be missing? Or is something extraneous being captured? This is primarily a qualitative",
            "Experiment\nTo make a comprehensive evaluation, we assess our model on five text classification tasks and a semantic matching task.",
            "The performances of the NER experiments are reported separately for three different parts of the system proposed. Table 6 presents the comparison of the various methods while performing NER on the bot-generated corpora and the user-generated corpora. Results shown that, in the first case, in the training set the F1 score is always greater than 97%, with a maximum of 99.65%. With both test sets performances decrease, varying between 94-97%. In the case of UGC, comparing the F1 score we can observe how performances significantly decrease. It can be considered a natural consequence of the complex nature of the users' informal language in comparison to the structured message created by the bot. In Table 7, results of the schedule matching are reported. We can observe how the quality of the",
            "In order to evaluate sentiment preservation, we use the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. We call the value as sentiment delta (senti delta). Besides, we report the sentiment accuracy (Senti ACC) which measures whether the output sentence has the same sentiment polarity as the input sentence based on our standardized sentiment classifiers. The BLEU score BIBREF25 between the input sentences and the output sentences is calculated to evaluate the content preservation performance. In order to evaluate the overall performance of different models, we also report the geometric mean (G2) and harmonic mean (H2) of the sentiment accuracy and the BLEU score. As for the irony accuracy, we only report it",
            "Usually, summaries generated in this way have a better performance on fluency and grammar, but they may contain much redundancy and lack in coherence across sentences. In contrast, abstractive methods attempt to mimic what humans do by first extracting content from the source document and then produce new sentences that aggregate and organize the extracted information. Since the sentences are generated from scratch they tend to have a relatively worse performance on fluency and grammar. Furthermore, while abstractive summaries are typically less redundant, they may end up including misleading or even utterly false statements, because the methods to extract and aggregate information form the source document are still rather noisy. In this work, we focus on extracting informative sentences"
        ]
    },
    {
        "title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances",
        "answer": "architecture",
        "evidence": [
            "We would like to investigate effectiveness the proposed joint model. To do so, the same shared layer/architecture is employed in the following variants of the proposed model: The results in terms of EM and F1 is summarized in Table TABREF20 . We observe that Joint SAN outperforms the SAN baseline with a large margin, e.g., 67.89 vs 69.27 (+1.38) and 70.68 vs 72.20 (+1.52) in terms of EM and F1 scores respectively, so it demonstrates the effectiveness of the joint optimization. By incorporating the output information of classifier into Joint SAN, it obtains a slight improvement, e.g., 72.2 vs 72.66 (+0.46) in terms of F1 score. By analyzing the results, we found that in most cases when our model extract an NULL string answer, the classifier also predicts it as an unanswerable question with",
            "Conflict model",
            "What is the Model Learning?",
            "Principal components of different models",
            "model the shared structure between words. Other directions for future work include additional forms of supervision and training, as well as application to downstream tasks.",
            "UTCNN Model Description",
            "Modeling multimodal human communication has been studied previously. Past approaches can be categorized as follows: Non-temporal Models: Studies have focused on simplifying the temporal aspect of cross-view dynamics BIBREF5 , BIBREF6 , BIBREF7 in order to model co-occurrences of information across the modalities. In these models, each modality is summarized in a representation by collapsing the time dimension, such as averaging the modality information through time BIBREF8 . While these models are successful in understanding co-occurrences, the lack of temporal modeling is a major flaw as these models cannot deal with multiple contradictory evidences, eg. if a smile and frown happen together in an utterance. Furthermore, these approaches cannot accurately model long sequences since the",
            "Exp II: Transferability of Shared Sentence Representation",
            "information can be shared across different domains. Untying the embedding encoder L1 and the output layer L4 leads to the least degradation in comparison to row (a).",
            "The model\nWe create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11) . The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like.",
            "Analysis of the convolutional language model",
            "Original Toulmin's model"
        ]
    },
    {
        "title": "Is there Gender bias and stereotype in Portuguese Word Embeddings?",
        "answer": "GloVe vectors, IMV word embeddings, IWV word embeddings.  Latent semantic analysis (LSA) word embeddings, Latent Dirichlet allocation (LDA) word embeddings, cross-lingual word embeddings (CLWEs) BIBREF146.  Latent semantic analysis (LSA) word embeddings, Latent Dirichlet allocation (LDA) word embeddings, cross-lingual word embeddings (CLWEs) BIBREF146.  Latent semantic analysis (LSA) word embeddings, Latent Dirichlet allocation (LDA) word embeddings, cross-lingual word embeddings (CLWEs) BIBREF146.  Latent semantic analysis (LSA) word embeddings, Latent Dirichlet allocation (LDA) word embeddings, cross-lingual word embeddings (CLWEs) BIBREF146.  Latent semantic analysis (LSA) word embeddings, Latent Dirichlet allocation (LDA) word embeddings, cross-lingual word embeddings (CLWEs) BIBREF146.  Latent semantic analysis (LSA) word embeddings, Latent Dirichlet allocation (LDA) word embeddings, cross-lingual word embeddings (CLWEs) B",
        "evidence": [
            "Word embeddings",
            "of the problem. This is an important way to solve different types of problems that permeate computer science and other areas. One of the main uses of ML is in text processing, where the analysis of the content the entry point for various learning algorithms. However, the use of this content can represent the insertion of different types of bias in training and may vary with the context worked. This work aims to analyze and remove gender stereotypes from word embedding in Portuguese, analogous to what was done in BIBREF3 for the English language. Hence, we propose to employ a public word2vec model pre-trained to analyze gender bias in the Portuguese language, quantifying biases present in the model so that it is possible to reduce the spreading of sexism of such models. There is also a",
            "The encoder consists of an embedding layer, two stacks of transformer blocks (denoted as encoder transformer blocks and aggregation transformer blocks), and an attention layer. In the embedding layer, we aggregate both word- and character-level embeddings. Word embeddings are initialized by the 300-dimension fastText BIBREF20 vectors trained on Common Crawl (600B tokens), and are fixed during training. Character embeddings are initialized by 200-dimension random vectors. A convolutional layer with 96 kernels of size 5 is used to aggregate the sequence of characters. We use a max pooling layer on the character dimension, then a multi-layer perceptron (MLP) of size 96 is used to aggregate the concatenation of word- and character-level representations. A highway network BIBREF21 is used on",
            "also reflects onto word embeddings (WEs), representations of words as low-dimensional vectors that have become indispensable for a wide range of NLP applications BIBREF51, BIBREF52, BIBREF53. In particular, it involves both static WEs learned from co-occurrence patterns BIBREF32, BIBREF54, BIBREF31 and contextualized WEs learned from modeling word sequences BIBREF55, BIBREF29. As a result, in the induced representations, geometrical closeness (measured e.g. through cosine distance) conflates genuine similarity with broad relatedness. For instance, the vectors for antonyms such as sober and drunk, by definition dissimilar, might be neighbors in the semantic space under the distributional hypothesis. BIBREF36, BIBREF56, and BIBREF53 demonstrated that different choices of hyper-parameters in",
            "word embeddings (PS-Embeddings) for classifying positive and negative citations. The sentence vectors formed a feature space, to which the examined citation sentence was mapped to. Those features were input into classifiers (support vector machines) for supervised classification. Using 10-cross-validation scheme, evaluation was conducted on a set of annotated citations. The results showed that word embeddings are effective on classifying positive and negative citations. However, hand-crafted features performed better for the overall classification.",
            "each row by dividing entries in the row by the maximum score in it. Secondly, we perform the principal component analysis (PCA) method to reduce the dimensionality. It captures latent meanings and takes into account high-order cooccurrence removing noise. The attribute (column) number of the matrix is reduced to 200. We then compute cosine similarity between each row pair $w_i$ and $w_j$ as in (DISPLAY_FORM3) to find out how similar two word vectors (rows) are. Thirdly, all the values in the matrix are subtracted from 1 to create a dissimilarity matrix. Then, we feed the matrix as input into the fuzzy c-means clustering algorithm. We chose the number of clusters as 200, as it is considered a standard for word embeddings in the literature. After clustering, the dimension i for a",
            "summarization across different domains, i.e. news, personal stories, meetings, and medical articles, as well as across different neural architectures, in order to better understand the general pros and cons of different design choices. They find that non auto-regressive sentence extraction performs as well or better than auto-regressive extraction in all domains, where by auto-regressive sentence extraction they mean using previous predictions to inform future predictions. Furthermore, they find that the Average Word Embedding sentence encoder works at least as well as encoders based on CNN and RNN. In light of these findings, our model is not auto-regressive and uses the Average Word Embedding encoder.",
            "In the literature, the main consensus is that the use of dense word embeddings outperform the sparse embeddings in many tasks. Latent semantic analysis (LSA) used to be the most popular method in generating word embeddings before the invention of the word2vec and other word vector algorithms which are mostly created by shallow neural network models. Although many studies have been employed on generating word vectors including both semantic and sentimental components, generating and analysing the effects of different types of embeddings on different tasks is an emerging field for Turkish. Latent Dirichlet allocation (LDA) is used in BIBREF2 to extract mixture of latent topics. However, it focusses on finding the latent topics of a document, not the word meanings themselves. In BIBREF3, LSA",
            "popular encoders, at the same time also investigating other important questions such as performance of static versus contextualized word embeddings, or multilingual versus language-specific pretraining. Another purpose of the experiments is to outline the wide potential and applicability of the Multi-SimLex datasets for multilingual and cross-lingual representation learning evaluation.",
            "recurrent neural network language models [1]. For future work, we may also consider experimenting with different forms of word embeddings, such as improved word vectors (IWV) which were suggested by Rezaeinia et al [10]. These IMV word embeddings are trained in a similar fashion to GloVe vectors, but also encode additional information about each word, like the word’s part of speech. For our model, we may consider encoding similar information in order for the generator to more easily learn common semantic patterns inherent to the marketing data being analyzed.",
            "(e.g., synonymy or antonymy). The goal is to “attract” synonyms closer to each other in the transformed vector space as they reflect true semantic similarity, and “repel” antonyms further apart. In the second step, the linguistic constraints are translated from English to the target language via a shared cross-lingual word vector space. To this end, following Ponti:2019emnlp we rely on cross-lingual word embeddings (CLWEs) BIBREF146 available online, which are based on Wiki ft vectors. Following that, a constraint refinement step is applied in the target language which aims to eliminate the noise inserted during the translation process. This is done by training a relation classification tool: it is trained again on the English linguistic constraints and then used on the translated target",
            "We next briefly describe the most closely related prior work. Maas et al. BIBREF9 and Bengio and Heigold BIBREF10 used acoustic word embeddings, based on convolutional neural networks (CNNs), to generate scores for word segments in automatic speech recognition. Maas et al. trained CNNs to predict (continuous-valued) embeddings of the word labels, and used the resulting embeddings to define feature functions in a segmental conditional random field BIBREF17 rescoring system. Bengio and Heigold also developed CNN-based embeddings for lattice rescoring, but with a contrastive loss to separate embeddings of a given word from embeddings of other words. Levin et al. BIBREF11 developed unsupervised embeddings based on representing each word as a vector of DTW distances to a collection of"
        ]
    },
    {
        "title": "An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models",
        "answer": "BERT, multilingual bert, xlm, LSTM, RNN, NMT, NLI, SimLex-999, WordSim353, Mult-SimLex, MultiNLI, MLQA, XQuAD, TyDiQA, PAWS-X, BERT, multilingual bert, xlm, LSTM, RNN, NMT, NLI, SimLex-999, WordSim353, Mult-SimLex, MultiNLI, MLQA, XQuAD, TyDiQA, PAWS-X, BERT, multilingual bert, xlm, LSTM, RNN, NMT, NLI, SimLex-999, WordSim353, Mult-SimLex, MultiNLI, MLQA, XQuAD, TyDiQA, PAWS-X, BERT, multilingual bert, xlm, LSTM, RNN, NMT, NLI, SimLex-999, WordSim353, Mult-SimLex, MultiNLI, MLQA, XQuAD, TyDiQA, PAWS-X, BERT, multilingual bert, xlm, LSTM, RNN, NMT, NLI, SimLex-999, WordSim353, Mult-SimLex, MultiNLI, ML",
        "evidence": [
            "Language Modeling",
            "Neural machine translation (NMT) emerged in the last few years as a very successful paradigm BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . While NMT is generally more fluent than previous statistical systems, adequacy is still a major concern BIBREF4 : common mistakes include dropping source words and repeating words in the generated translation. Previous work has attempted to mitigate this problem in various ways. BIBREF5 incorporate coverage and length penalties during beam search—a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam. Other approaches involve architectural changes: providing coverage vectors to track the attention history BIBREF6 , BIBREF7 , using gating architectures and adaptive attention to control the amount",
            "popular encoders, at the same time also investigating other important questions such as performance of static versus contextualized word embeddings, or multilingual versus language-specific pretraining. Another purpose of the experiments is to outline the wide potential and applicability of the Multi-SimLex datasets for multilingual and cross-lingual representation learning evaluation.",
            "Abstract\nDeep learning models have achieved remarkable success in natural language inference (NLI) tasks. While these models are widely explored, they are hard to interpret and it is often unclear how and why they actually work. In this paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for NLI. In particular, we propose to interpret the intermediate layers of NLI models by visualizing the saliency of attention and LSTM gating signals. We present several examples for which our methods are able to reveal interesting insights and identify the critical information contributing to the model decisions.",
            "What Does Zero-shot Transfer Model Learn? ::: Unseen Language Dataset",
            "In neural network based language models, the dialog context can be represented as a dense continuous vector. This context vector can be produced in a number of ways. One simple approach is to use bag of word embeddings. However, bag of word embedding context representation does not take word order into consideration. An alternative approach is to use an RNN to read the preceding text. The last hidden state of the RNN encoder can be seen as the representation of the text and be used as the context vector for the next turn. To generate document level context representation, one may cascade all sentences in a document by removing the sentence boundaries. The last RNN hidden state of the previous utterance serves as the initial RNN state of the next utterance. As in BIBREF11 , we refer to",
            "more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given model to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related",
            "What is the Model Learning?",
            "However, most of these datasets include English words only. There has been no Japanese dataset for the word-similarity task. Apart from English, WordSim353 and SimLex-999 Hill:2015:CL have been translated and rescored in other languages: German, Italian and Russian Leviant:2015:arXiv. SimLex-999 has also been translated and rescored in Hebrew and Croatian Mrksic:2017:TACL. SimLex-999 explicitly targets at similarity rather than relatedness and includes adjective, noun and verb pairs. However, this dataset contains only frequent words. In addition, the distributed representation of words is generally learned using only word-level information. Consequently, the distributed representation for low-frequency words and unknown words cannot be learned well with conventional models. However,",
            "language models such as multilingual bert BIBREF29 or xlm BIBREF30 are typically probed on XNLI test data BIBREF84 for cross-lingual natural language inference. XNLI was created by translating examples from the English MultiNLI dataset, and projecting its sentence labels BIBREF85. Other recent multilingual datasets target the task of question answering based on reading comprehension: i) MLQA BIBREF86 includes 7 languages ii) XQuAD BIBREF87 10 languages; iii) TyDiQA BIBREF88 9 widely spoken typologically diverse languages. While MLQA and XQuAD result from the translation from an English dataset, TyDiQA was built independently in each language. Another multilingual dataset, PAWS-X BIBREF89, focused on the paraphrase identification task and was created translating the original English PAWS",
            "and repeated reading, widely seen in successful models for other NLP tasks. As such, we expect our methods to be potentially useful for other natural understanding tasks as well."
        ]
    },
    {
        "title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset",
        "answer": "Taskmaster-1, medical, open-domain, SQUAD, GLUE, SuperGLUE",
        "evidence": [
            "This section presents experiments conducted on the annotated corpora introduced in section SECREF4 . We put the main focus on identifying argument components in the discourse. To comply with the machine learning terminology, in this section we will use the term domain as an equivalent to a topic (remember that our dataset includes six different topics; see section SECREF38 ). We evaluate three different scenarios. First, we report ten-fold cross validation over a random ordering of the entire data set. Second, we deal with in-domain ten-fold cross validation for each of the six domains. Third, in order to evaluate the domain portability of our approach, we train the system on five domains and test on the remaining one for all six domains (which we report as cross-domain validation).",
            "Dataset",
            "In-domain and out-of-domain data",
            "What Does Zero-shot Transfer Model Learn? ::: Code-switching Dataset",
            "The TDC datasets are available in the CAPES open data website divided by years, from 2013 to 2016 in CSV and XLSX formats. We downloaded all CSV files from the respective website and loaded them into an SQL database for better manipulation. The database was then filtered to remove documents without both Portuguese and English abstracts, and additional metadata selected. After the initial filtering, the resulting documents were processed for language checking to make sure that there was no misplacing of English abstracts in the Portuguese field, or the other way around, removing the documents that presented such inconsistency. We also performed a case folding to lower case letters, since the TDC datasets present all fields with uppercase letters. In addition, we also removed",
            "A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken \"Wizard of Oz\" (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is \"self-dialog\" in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and",
            "searches as secondary answer sources. Each medical question was decomposed into several subquestions. To extract the answer from the selected text passage, a bi-directional attention model trained on the SQUAD dataset was used. Deep neural network models have been pushing the limits of performance achieved in QA related tasks using large training datasets. The results obtained by CMU-OAQA and PRNA showed that large open-domain datasets were beneficial for the medical domain. However, the best system (CMU-OAQA) relying on the same training data obtained a score of 1.139 on the LiveQA open-domain task. While this gap in performance can be explained in part by the discrepancies between the medical test questions and the open-domain questions, it also highlights the need for larger medical",
            "The accuracy is We split the ground-truth dataset to train/dev/test into the ratio of 6/2/2. The resulting size is 3,718 (train), 1,238 (dev) and 1,239 qa pairs (test).",
            "The datasets in GLUE are: CoLA (BIBREF54), Stanford Sentiment Treebank (SST) (BIBREF53), Microsoft Research Paragraph Corpus (MRPC) BIBREF44, Semantic Textual Similarity Benchmark (STS) BIBREF38, Quora Question Pairs (QQP) BIBREF46, Multi-Genre NLI (MNLI) BIBREF55, Question NLI (QNLI) BIBREF17, Recognizing Textual Entailment (RTE) BIBREF42, BIBREF39, BIBREF45, BIBREF40 and Winograd NLI (WNLI) BIBREF48. The datasets in SuperGLUE are: Boolean Questions (BoolQ) BIBREF41, CommitmentBank (CB) BIBREF43, Choice of Plausible Alternatives (COPA) BIBREF51, Multi-Sentence Reading Comprehension (MultiRC) BIBREF47, Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) BIBREF56, Word-in-Context (WiC) BIBREF49, Winograd Schema Challenge (WSC) BIBREF52, Diverse Natural Language Inference",
            "Domain adaptation is a machine learning paradigm that aims at improving the generalization performance of a new (target) domain by using a dataset from the original (source) domain. Suppose that, as the source domain dataset, we have a captioning corpus, consisting of images of daily lives and each image has captions. Suppose also that we would like to generate captions for exotic cuisine, which are rare in the corpus. It is usually very costly to make a new corpus for the target domain, i.e., taking and captioning those images. The research question here is how we can leverage the source domain dataset to improve the performance on the target domain. As described by Daumé daume:07, there are mainly two settings of domain adaptation: fully supervised and semi-supervised. Our focus is the",
            "Datasets Used for the RQE Study",
            "tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}. For dialects, the data are labeled with 15 classes, from the set {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia, Sudan, Tunisia, UAE, Yemen}. The gender task involves binary labels from the set {male, female}."
        ]
    },
    {
        "title": "Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge",
        "answer": "Freebase",
        "evidence": [
            "The Behavioral Graph: A Knowledge Base For Navigation",
            "sense of whether some things have been left out of the coding rules and whether anything is superfluous, misleading, or confusing. Adjustments are made and the process is repeated, often with another researcher involved. The final annotations can be collected using a crowdsourcing platform, a smaller number of highly-trained annotators, or a group of experts. Which type of annotator to use should be informed by the complexity and specificity of the concept. For more complex concepts, highly-trained or expert annotators tend to produce more reliable results. However, complex concepts can sometimes be broken down into micro-tasks that can be performed independently in parallel by crowdsourced annotators. Concepts from highly specialized domains may require expert annotators. In all cases,",
            "Traditional semantic parsers map language onto compositional, executable queries in a fixed schema. This mapping allows them to effectively leverage the information contained in large, formal knowledge bases (KBs, e.g., Freebase) to answer questions, but it is also fundamentally limiting---these semantic parsers can only assign meaning to language that falls within the KB's manually-produced schema. Recently proposed methods for open vocabulary semantic parsing overcome this limitation by learning execution models for arbitrary language, essentially using a text corpus as a kind of knowledge base. However, all prior approaches to open vocabulary semantic parsing replace a formal KB with textual information, making no use of the KB in their models. We show how to combine the disparate",
            "GAN setups",
            "Also, the task is not exactly question answering, but identification of document passages containing the answer. TriviaQA BIBREF21 is a corpus that contains automatically collected question-answer pairs from 14 trivia and quiz-league websites, together with web-crawled evidence documents from Wikipedia and Bing. While a majority of questions require world knowledge for finding the correct answer, it is mostly factual knowledge.",
            "Datasets Used for the RQE Study",
            "their memory cell by exploiting reset, update, and new gates to decide how to utilize and update the memory of previous information. In this way, we can represent the contextual information within each topic segment for all the sentences in that segment.",
            "Incorporating External Knowledge",
            "was never seen in the training data, so it is replaced with $\\textit {unknown}$ ; however, it would be better to replace it with $\\textit {honcho\\_N/N}$ , which was seen in the training data. Finally, although using connected entities in Freebase as additional candidates during inference is helpful, it often over- or under-generates candidates. A more tailored, per-query search process could improve performance.",
            "In the computer science field, an ontology can be defined has: a formal specification of a conceptualization; shared vocabulary and taxonomy which models a domain with the definition of objects and/or concepts and their properties and relations; the representation of entities, ideas, and events, along with their properties and relations, according to a system of categories. A knowledge base is one kind of repository typically used to store answers to questions or solutions to problems enabling rapid search, retrieval, and reuse, either by an ontology or directly by those requesting support. For a more detailed description of ontologies and knowledge bases, see for instance BIBREF17. For designing the ontology adequate for our goals, we referred to the Simple Event Model (SEM) BIBREF18 as",
            "even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions. To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require background knowledge. We explicitly state the following items in the HIT instructions for question writing: No Yes-no questions should be asked. The question should have at least five words. Videos, images or inserted links should not be considered. No background knowledge should be required to answer the question. To help the workers better follow the instructions, we also include a representative example showing",
            "the graph. The LBP and PageRank/random walk based methods achieve similar high time complexity of INLINEFORM4 mainly because of the inference on the entire graph."
        ]
    },
    {
        "title": "SimplerVoice: A Key Message&Visual Description Generator System for Illiteracy",
        "answer": "S-V-O model.",
        "evidence": [
            "SMT error generation system. During training, error-corrected sentences in the training data are treated as the source, and the original sentences written by language learners as the target. Pialign BIBREF6 is used to create a phrase translation table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table, as proposed by Felice:2014-CoNLL. Decoding is performed using Moses BIBREF7 and the language model used during decoding is built from the original erroneous sentences in the learner corpus. The IRSTLM Toolkit BIBREF8 is used for building a 5-gram language model with modified Kneser-Ney smoothing BIBREF9 .",
            "Hidden Markov models",
            "Through building context clues and using implicit knowledge, the model is able to produce reasonable story endings. context clues implied in the post and make the inference based on it. Automatic and manual evaluation shows that our model can generate more reasonable story endings than state-of-the-art baselines.",
            "Methodology ::: Generating Document Vectors",
            "Fortunately, we believe that the introduction of unsupervised generative language models presents a way in which to tackle this particular shortcoming of peer-to-peer markets. In 2014, Ian Goodfellow et. al proposed the general adversarial network (GAN) [5]. The group showcased how this generative model could learn to artificially replicate data patterns to an unprecedented realistic degree [5]. Since then, these models have shown tremendous potential in their ability to generate photo-realistic images and coherent text samples [5]. The framework that GANs use for generating new data points employs an end-to-end neural network comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being",
            "how trained AV models behave, if the lengths of the questioned documents are getting shorter and shorter. To our best knowledge, this question has not been addressed in previous authorship verification studies.",
            "What is the Model Learning?",
            "The intuition of this query representation is that the generated key message should be able to describe the action of a person using, or interacting with the target object. Moreover, the simple \"S-V-O\" model has been proposed to use in other studies BIBREF11 , BIBREF12 , BIBREF13 since it is able to provide adequate semantics meaning. The detail of generating the S-V-O query is provided in Section SECREF3 . Once the query is constructed, SimplerVoice converts the query text into visual forms. There is a variety of visual formats to provide users: photos, icons, pictographs, etc. These visual components can be obtained by different means, such as: using search engine, mapping query/ontology to a database of images. However, the key point is to choose the optimal display for illiteracy",
            "of two methods: on one side, we use the entities extracted from the radio schedule for generating candidates entities in the user-generated tweets, thanks to a matching algorithm based on time proximity and string similarity. On the other side, we create a statistical model capable of detecting entities directly from the UGC, aimed to model the informal language of the raw texts. In Figure 1, an overview of the system proposed is presented.",
            "vector INLINEFORM0 from INLINEFORM1 to INLINEFORM2 with a fully-connected (FC) layer. The resulting L vectors are output to the next layer as columns of a context matrix INLINEFORM3 . Decoder layer: After the FC layer, the model predicts likelihoods over the sequence of behaviors that correspond to the input instructions with a GRU network. Without loss of generality, consider the INLINEFORM0 -th recurrent cell in the GRU network. This cell takes two inputs: a hidden state vector INLINEFORM1 from the prior cell, and a one-hot embedding of the previous behavior INLINEFORM2 that was predicted by the model. Based on these inputs, the GRU cell outputs a new hidden state INLINEFORM3 to compute likelihoods for the next behavior. These likelihoods are estimated by combining the output state",
            "Principal components of different models",
            "LSTM Gating Signals"
        ]
    },
    {
        "title": "A Parallel Corpus of Theses and Dissertations Abstracts",
        "answer": "SMT, NMT, GRU.",
        "evidence": [
            "RNN-based NMT model",
            "source and target side (in French). The average cross-entropy for this test set is 0.33, very close to 0, to be compared with an average cost of 58.52 when we process an actual source (in English). This means that the model has learned to copy words from source to target with no difficulty, even for sentences not seen in training. A follow-up question is whether training a copying task instead of a translation task limits the improvement: would the NMT learn better if the task was harder? To measure this, we introduce noise in the target sentences copied onto the source, following the procedure of BIBREF26 : it deletes random words and performs a small random permutation of the remaining words. Results (+ Source noise) show no difference for the French in-domain test sets, but bring the",
            "EXPERIMENTS",
            "Speech-to-Text Experiments for AMIE: Training and Testing Models on ASR Outputs",
            "Furthermore, we test SMT models and various NMT models on the created dataset and provide a strong baseline for this task.",
            "More broadly, this line of research still accepts the premise that NMT models are data-inefficient and require large amounts of auxiliary data to train. In this work, we want to re-visit this point, and will focus on techniques to make more efficient use of small amounts of parallel training data. Low-resource NMT without auxiliary data has received less attention; work in this direction includes BIBREF23 , BIBREF24 .",
            "we took a reversed approach. First, for each video, we manually identified the sentence spans that can serve as answers. These candidates are of various granularity and may overlap. The segments are also complete in that they encompass the beginning and end of a task. In total, we identified 408 segments from the 76 videos. Second we asked AMT workers to provide question annotations for the videos. Our AMT experiment consisted of two parts. In the first part, we presented the workers with the video content of a segment. For each segment, we asked workers to generate questions that can be answered by the presented segment. We did not limit the number of questions a worker can input to a corresponding segment and encouraged them to input a diverse set of questions which the span can answer.",
            "Models for Comparison",
            "In this section, we will describe the general architecture of NMT as a kind of sequence-to-sequence modeling framework. In this kind of sequence-to-sequence modeling framework, often there is an encoder trying to encode context information from the input sequence and a decoder to generate one item of the output sequence at a time based on the context of both input and output sequences. Besides, an additional component, named attention, exists in between, deciding which parts of the input sequence the decoder should pay attention in order to choose which to output next. In other words, this attention component calculates the context relevant to the decision of the decoder at the considering time. Those components as a whole constitute a large trainable neural architecture called the famous",
            "Three types of models were used in our system, a feed-forward neural network, an LSTM network and an SVM regressor. The neural nets were inspired by the work of Prayas BIBREF7 in the previous shared task. Different regression algorithms (e.g. AdaBoost, XGBoost) were also tried due to the success of SeerNet BIBREF8 , but our study was not able to reproduce their results for Spanish. For both the LSTM network and the feed-forward network, a parameter search was done for the number of layers, the number of nodes and dropout used. This was done for each subtask, i.e. different tasks can have a different number of layers. All models were implemented using Keras BIBREF9 . After the best parameter settings were found, the results of 10 system runs to produce our predictions were averaged (note",
            "Experiments ::: Tweet-Level Models ::: Baseline GRU.",
            "Experiments\nOur experiments revolve around the following questions: Q1: As we consider three factors for clause alignment, do all these factors help? How does our method compare with previous methods? Q2: How does the NMT and SMT models perform on this new dataset we build?"
        ]
    },
    {
        "title": "Data Innovation for International Development: An overview of natural language processing for qualitative data analysis",
        "answer": "NLP and machine learning.",
        "evidence": [
            "Practitioners in the development sector have long recognized the potential of qualitative data to inform programming and gain a better understanding of values, behaviors and attitudes of people and communities affected by their efforts. Some organizations mainly rely on interview or focus group data, some also consider policy documents and reports, and others have started tapping into social media data. Regardless of where the data comes from, analyzing it in a systematic way to inform quick decision-making poses challenges, in terms of high costs, time, or expertise required. The application of natural language processing (NLP) and machine learning (ML) can make feasible the speedy analysis of qualitative data on a large scale. We start with a brief description of the main approaches to",
            "and translate those sub-components (“Wohnung” and “reinigung”) and combine their translations to generate the unseen words (“house cleaning”). This is achieved by segmenting words into subword units using segmentation techniques in the preprocessing phase prior to translation. There are several segmentation methods; Some are the complicate ones which require linguistic resources or human-crafted rules. Thus, they are not language-independent and expensive to obtain for low-resourced languages. Byte-Pair Encoding, otherwise, is a simple but robust technique to do subword segmentation. Since it is an unsupervised and fast technique, it has great effects when applied to build NMT systems for morphologically rich languages. BPE is originally proposed in BIBREF9 as a data compression technique",
            "Analysis of the convolutional language model",
            "Classical QA systems face two main challenges related to question analysis and answer extraction. Several QA approaches were proposed in the literature for the open domain BIBREF16 , BIBREF17 and the medical domain BIBREF18 , BIBREF19 , BIBREF20 . A variety of methods were developed for question analysis, focus (topic) recognition and question type identification BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 . Similarly, many different approaches tackled document or passage retrieval and answer selection and (re)ranking BIBREF25 , BIBREF26 , BIBREF27 . An alternative approach consists in finding similar questions or FAQs that are already answered BIBREF28 , BIBREF29 . One of the earliest question answering systems based on finding similar questions and re-using the existing answers was FAQ",
            "Qualitative Evaluation",
            "we would like to explore more sophistical structure of documents, like discourse tree, instead of rough topic segments. As for evaluation, we would like to elicit human judgments, for instance by inviting authors to rate the outputs from different systems, when applied to their own papers. More long term, we will study how extractive/abstractive techniques can be integrated; for instance, the output of an extractive system could be fed into an abstractive one, training the two jointly.",
            "Experiments\nTo show the challenge of TweetQA for existing approaches, we consider four representative methods as baselines. For data processing, we first remove the URLs in the tweets and then tokenize the QA pairs and tweets using NLTK. This process is consistent for all baselines.",
            "References\nE. Davis, “Qualitative Spatial Reasoning in Interpreting Text and Narrative” Spatial Cognition and Computation, 13:4, 2013, 264-294. H. Levesque, E. Davis, and L. Morgenstern, “The Winograd Schema Challenge,” KR 2012. L. Morgenstern, E. Davis, and C. Ortiz, “Report on the Winograd Schema Challenge, 2016”, in preparation. T. Winograd, “Procedures as a Representation for Data in a Computer Program for Understanding Natural Language,\" Ph.D. thesis, Department of Mathematics, MIT, August 1970. Published as MIT AITR-235, January 1971. T. Winograd, Understanding Natural Language, Academic Press, 1972.",
            "Entity extraction is one of the most major NLP components. Most NLP tools (e.g., NLTK, Stanford CoreNLP, etc.), including commercial services (e.g., Google Cloud API, Alchemy API, etc.), provide entity extraction functions to recognize named entities (e.g., PERSON, LOCATION, ORGANIZATION, etc.) from texts. Some studies have defined fine-grained entity types and developed extraction methods BIBREF0 based on these types. However, these methods cannot comprehensively cover domain-specific entities. For instance, a real estate search engine needs housing equipment names to index these terms for providing fine-grained search conditions. There is a significant demand for constructing user-specific entity dictionaries, such as the case of cuisine and ingredient names for restaurant services. A",
            "would be to include additional knowledge from anthologies along with a sentiment or polarity. One could modify NELL BIBREF12 to broaden the understanding of entities in the comments. Non-cursing aggressions and insults This is a challenging problem, since the majority of abusive and insulting comments rely on profanity and swearing. The problem arises with subtler aggressions and insults that are equally or even more annoying, such as “Troll? How cute.” and “settle down drama queen”. The classifier has a more difficult task of determining that these are indeed aggressions or insults. This error also decreases the recall of trolling intention. A solution would be to exploit all the comments made by the suspected troll in the entire conversation in order to increase the chances of finding",
            "in the respective literature. In 2013, for example, Veenman and Li BIBREF2 presented an AV method based on compression, which has its roots in the field of information theory. In 2015, Bagnall BIBREF3 introduced the first deep learning approach that makes use of language modeling, an important key concept in statistical natural language processing. In 2017, Castañeda and Calvo BIBREF4 proposed an AV method that applies a semantic space model through Latent Dirichlet Allocation, a generative statistical model used in information retrieval and computational linguistics. Despite the increasing number of AV approaches, a closer look at the respective studies reveals that only minor attention is paid to their underlying characteristics such as reliability and robustness. These, however, must",
            "NLP Toolkits"
        ]
    },
    {
        "title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages",
        "answer": "SemEval, TASS, SENTIPOLC, UniSent, CARD-660, SimVerb-3500, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660, CARD-660,",
        "evidence": [
            "Comparison to Other Datasets",
            "count, contrary expression and dependency relations. Jochim et al. tried to improve the result by using unigram and bigram features BIBREF1 . BIBREF2 used word level features, contextual polarity features, and sentence structure based features to detect sentiment citations. Although they generated good results using the combination of features, it required a lot of engineering work and big amount of annotated data to obtain the features. Further more, capturing accurate features relies on other NLP techniques, such as part-of-speech tagging (POS) and sentence parsing. Therefore, it is necessary to explore other techniques that are free from hand-crafted features. With the development of neural networks and deep learning, it is possible to learn the representations of concepts from",
            "source models. Moreover, these first components have almost zero alignment shifts. Other principal components have very similar singular values, and thus they cannot be determined uniquely with high confidence. Normalized interpretibility measures for different components (calculated for 50 top/bottom words) for the source and the rotated models are shown in Fig. 4.",
            "mentioned above, which provides emotion-similarity for each token. Note that differently from the previous two types of embeddings, the retrofitted ones do rely on handcrafted information in the form of a lexical resource.",
            "We compare our approach in eight different languages, three of them have important international contests, namely, SemEval (English), TASS (Spanish), and SENTIPOLC (Italian). Within the competitions our approach reaches from medium to high positions in the rankings; whereas in the remaining languages our approach outperforms the reported results.",
            "average of tweet sentiment for the subset of tweets by users who tweeted both before and after the event (hence minimizing awareness bias). We use Student's t-test to calculate the significance of mean sentiment differences pre- and post-event (see Section SECREF4 ). Note that we perform these mean comparisons on all event-related data, since the low number of geo-tagged samples would produce an underpowered study.",
            "Sentiment Classification",
            "In this paper, we introduce UniSent a universal sentiment lexica for 1000 languages created using an English sentiment lexicon and a massively parallel corpus in the Bible domain. To the best of our knowledge, UniSent is the largest sentiment resource to date in terms of number of covered languages, including many low resource languages. To create UniSent, we propose Adapted Sentiment Pivot, a novel method that combines annotation projection, vocabulary expansion, and unsupervised domain adaptation. We evaluate the quality of UniSent for Macedonian, Czech, German, Spanish, and French and show that its quality is comparable to manually or semi-manually created sentiment resources. With the publication of this paper, we release UniSent lexica as well as Adapted Sentiment Pivot related",
            "data. On the other hand, both multi-source systems perform reasonably well without parsed data, although the BLEU scores are worse than multi-source with parsed data.",
            "correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample.  3) Source: CARD-660 BIBREF78. 67 word pairs are taken from this dataset focused on rare word similarity, applying the same selection criteria a) to e) employed for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus BIBREF94. CARD-660 contains some words that are very rare (logboat), domain-specific (erythroleukemia) and slang (2mrw), which might be difficult to translate and annotate across a wide array of languages. Hence, we opt for retaining only the concept pairs above the threshold of top 250K most frequent Wikipedia concepts, as above.  4) Source: SimVerb-3500 BIBREF15 Since both CARD-660 and",
            "approach the similarities and differences that multiple references could provide into a sentence boundary evaluation protocol.",
            "Multi-Source"
        ]
    },
    {
        "title": "TutorialVQA: Question Answering Dataset for Tutorial Videos",
        "answer": "video transcripts.",
        "evidence": [
            "Multi-Source",
            "Experiments on Triplet Classification\nTriplet classification aims at classifying a fact triplet INLINEFORM0 as true or false. In the dataset of BIBREF9 ijcai2017-250, triplets in the validation and testing sets are labeled as true or false, while triplets in the training set are all true ones. To tackle this task, we preset a threshold INLINEFORM0 for each relation r. If INLINEFORM1 , the triplet is classified as positive, otherwise it is negative. We determine the optimal INLINEFORM2 by maximizing classification accuracy on the validation set.",
            "To boost performance, the SVM, LSTM, and feed-forward models were combined into an ensemble. For both the LSTM and feed-forward approach, three different models were trained. The first model was trained on the training data (regular), the second model was trained on both the training and translated training data (translated) and the third one was trained on both the training data and the semi-supervised data (silver). Due to the nature of the SVM algorithm, semi-supervised learning does not help, so only the regular and translated model were trained in this case. This results in 8 different models per subtask. Note that for the valence tasks no silver training data was obtained, meaning that for those tasks the semi-supervised models could not be used. Per task, the LSTM and feed-forward",
            "Acknowledgments\nWe would like to thank Jiateng Xie, Julian Salazar and Faisal Ladhak for the helpful comments and discussions.",
            "whose head entity and tail entity are noun or verb, meanwhile both occurring in SCT. Moreover, we retained at most 10 triples if there are too many. The average number of triples for each query word is 3.4.",
            "Models enriched with external lexical information",
            "We use the following cheap ways to generate pseudo-source texts: copy: in this setting, the source side is a mere copy of the target-side data. Since the source vocabulary of the NMT is fixed, copying the target sentences can cause the occurrence of OOVs. To avoid this situation, BIBREF24 decompose the target words into source-side units to make the copy look like source sentences. Each OOV found in the copy is split into smaller units until all the resulting chunks are in the source vocabulary. copy-marked: another way to integrate copies without having to deal with OOVs is to augment the source vocabulary with a copy of the target vocabulary. In this setup, BIBREF25 ensure that both vocabularies never overlap by marking the target word copies with a special language identifier.",
            "How Many Expert Annotations?",
            "GLUE and SuperGLUE",
            "Theoretical background",
            "Effect of Cross-View Training (CVT)",
            "tutorial videos pertaining to an image editing program. We introduce a dataset, TutorialVQA, consisting of about 6,000manually collected triples of (video, question, answer span). We also provide experimental results with several baselines algorithms using the video transcripts. The results indicate that the task is challenging and call for the investigation of new algorithms."
        ]
    },
    {
        "title": "Transductive Learning with String Kernels for Cross-Domain Text Classification",
        "answer": "SVM, Maximum Entropy Model, Naive Bayes (NB), Kernel Naive Bayes (KNB), Decision Tree (DT), Rule Induction (RI), Multi Layer Perceptron (MLP), Limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS), Adam optimizer.",
        "evidence": [
            "Algorithms Used",
            "What is the Model Learning?",
            "Researches on question classification, question taxonomies and QA system have been undertaken in recent years. There are two types of approaches for question classification according to Banerjee et al in BIBREF13 - by rules and by machine learning approach. Rule based approaches use some hard coded grammar rules to map the question to an appropriate answer type BIBREF14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in BIBREF16 and BIBREF0. Many classifiers have been used in machine learning for QC such as Support Vector Machine (SVM) BIBREF16 BIBREF17, Support Vector Machines and Maximum Entropy Model BIBREF18, Naive Bayes (NB), Kernel Naive Bayes (KNB), Decision Tree (DT) and Rule Induction (RI) BIBREF13. In BIBREF0, they claimed to",
            "developments in transfer learning for NLP both accessible and easy to use while maintaining high standards of software engineering and machine learning engineering.",
            "Implementation of the System ::: Classification Algorithms ::: Multi Layer Perceptron (MLP)\nMLP contains three layers - an input layer, an output layer and some hidden layers. Input layer receives the signal, the output layer gives a decision or prediction about the input and the computation of the MLP is conducted in the hidden layers. In our system, we use 100 layers. For weight optimization, we use Limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS) optimization algorithm.",
            "Limitations of LSTM-LMs",
            "For the implementation of our neural network, we used pytorch-pretrained-bert library containing the pre-trained BERT model, text tokenizer, and pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a free research tool with a Tesla K80 GPU and 12G RAM. Based on our experiments, we trained our classifier with a batch size of 32 for 3 epochs. The dropout probability is set to 0.1 for all layers. Adam optimizer is used with a learning rate of 2e-5. As an input, we tokenized each tweet with the BERT tokenizer. It contains invalid characters removal, punctuation splitting, and lowercasing the words. Based on the original BERT BIBREF11, we split words to subword units using WordPiece tokenization. As tweets are short texts, we set the maximum",
            "settings using human experts. Furthermore, for insight-driven analyses, it can be more useful to focus on improving explanatory power than making small improvements in predictive performance.",
            "several machine learning algorithms to perform entity recognition combining task-specific and corpus-based features. We also show how to improve recognition results by jointly considering formal and user-generated content",
            "gender biases in embeddings and to develop algorithms to reduce bias in some embedding. Hence, the authors show that embeddings can be used in applications without amplifying gender bias.",
            "Multilingual Neural NLP",
            "Recently, the transformative potential of machine learning (ML) has propelled ML into the forefront of mainstream media. In Brazil, the use of such technique has been widely diffused gaining more space. Thus, it is used to search for patterns, regularities or even concepts expressed in data sets BIBREF0 , and can be applied as a form of aid in several areas of everyday life. Among the different definitions, ML can be seen as the ability to improve performance in accomplishing a task through the experience BIBREF1 . Thus, BIBREF2 presents this as a method of inferences of functions or hypotheses capable of solving a problem algorithmically from data representing instances of the problem. This is an important way to solve different types of problems that permeate computer science and other"
        ]
    },
    {
        "title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches",
        "answer": "CNN/Daily Mail BIBREF19, BIBREF1. TutorialVQA Dataset. SnapCaptions Dataset. Datasets. game. RQE Study. Emotion detection. UnifyEmotion. AQUAINT. ROUGE F1. METEOR. Twitter. CNN. NBC. Freebase/ProBase/Reverb/NELL. RQE. 287,226 training pairs. 13,368 validation pairs. 11,490 test pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 287,226 training pairs. 13,368 validation pairs. 11,490 test pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728 pairs. 3,398/727/728",
        "evidence": [
            "Datasets",
            "Three types of models were used in our system, a feed-forward neural network, an LSTM network and an SVM regressor. The neural nets were inspired by the work of Prayas BIBREF7 in the previous shared task. Different regression algorithms (e.g. AdaBoost, XGBoost) were also tried due to the success of SeerNet BIBREF8 , but our study was not able to reproduce their results for Spanish. For both the LSTM network and the feed-forward network, a parameter search was done for the number of layers, the number of nodes and dropout used. This was done for each subtask, i.e. different tasks can have a different number of layers. All models were implemented using Keras BIBREF9 . After the best parameter settings were found, the results of 10 system runs to produce our predictions were averaged (note",
            "Benchmark the dataset ::: Emotion detection ::: UnifyEmotion.",
            "and AQUAINT. Our proposed method performs consistently well on all datasets that demonstrates the good generalization ability.",
            "GAN setups",
            "Experiments ::: Preliminaries ::: Dataset and Metrics.\nWe conduct our model on the large-scale dataset CNN/Daily Mail BIBREF19, BIBREF1, which is widely used in the task of abstractive document summarization with multi-sentences summaries. We use the scripts provided by BIBREF11 to obtain the non-anonymized version of the dataset without preprocessing to replace named entities. The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics.",
            "TutorialVQA Dataset ::: Overview",
            "One major challenge of building a QA dataset on tweets is the sparsity of informative tweets. Many users write tweets to express their feelings or emotions about their personal lives. These tweets are generally uninformative and also very difficult to ask questions about. Given the linguistic variance of tweets, it is generally hard to directly distinguish those tweets from informative ones. In terms of this, rather than starting from Twitter API Search, we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles. In order to get enough data, we first extract the URLs of all section pages (e.g. World, Politics, Money, Tech) from the snapshot of each home page and then crawl all articles with tweets",
            "dataset is designed so that successfully answering the questions requires understanding and the knowledge involved in a document. Each instance is composed of a question, a set of triplets derived from a document, and the answer. We implement a framework consisting of both a question answering model and a question generation model, both of which take the knowledge extracted from the document as well as relevant facts from an external knowledge base such as Freebase/ProBase/Reverb/NELL. The question answering model gives each candidate answer a score by measuring the semantic relevance between representation and the candidate answer representation in vector space. The question generation model provides each candidate answer with a score by measuring semantic relevance between the question",
            "Datasets Used for the RQE Study",
            "game. We followed the data partitions introduced with the dataset and used a train/validation/test sets of respectively $3,398$/727/728 (data-structure, description) pairs.",
            "Results: SnapCaptions Dataset"
        ]
    },
    {
        "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
        "answer": "100",
        "evidence": [
            "What is the Model Learning?",
            "Bi-Layered L-PCFGs",
            "systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size influences. The block “tricks” (28–29) shows the system performance when no mutual-learning or no pretraining is used. The block “layers” (30–33) demonstrates how the system performs when it has different numbers of convolution layers. From the “layers” block, we can see that our system performs best with two layers of convolution in Standard Sentiment Treebank and Subjectivity Classification tasks (row 31),",
            "How Many Expert Annotations?",
            "more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "Implementation of the System ::: Classification Algorithms ::: Multi Layer Perceptron (MLP)\nMLP contains three layers - an input layer, an output layer and some hidden layers. Input layer receives the signal, the output layer gives a decision or prediction about the input and the computation of the MLP is conducted in the hidden layers. In our system, we use 100 layers. For weight optimization, we use Limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS) optimization algorithm.",
            "Principal components of different models",
            "Sequence-to-Sequence Model",
            "Convolutional Language Model\nThe convolutional language model (LM) is the GCNN-14B from BIBREF0 , which achieved competitive results on several language modeling benchmarks. The network contains 14 convolutional residual blocks BIBREF21 with a growing number of channels, and uses gated linear units as activation function. The language model is used to score candidate transcriptions in addition to the acoustic model in the beam search decoder described in the next section. Compared to n-gram LMs, convolutional LMs allow for much larger context sizes. Our detailed experiments study the effect of context size on the final speech recognition performance.",
            "the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
            "attention. The attention function outputs are concatenated and projected again before giving the final output. Multi-head attention allows the model to attend to multiple features at different positions. The encoder is composed of a stack of INLINEFORM0 identical layers. Each layer has two sub-layers: multi-head self-attention mechanism and position-wise fully connected feed-forward network. Similarly, the decoder is also composed of a stack of INLINEFORM1 identical layers. In addition to the two sub-layers in each encoder layer, the decoder contains a third sub-layer which performs multi-head attention over the output of the encoder stack (see more details in BIBREF4 ).",
            "Experiments ::: User-Level Models"
        ]
    },
    {
        "title": "\"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach",
        "answer": "the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.",
        "evidence": [
            "on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.",
            "Collectively, the success of large pretrained neural models, both encoder-only BERT-like architectures as well as encoder–decoder architectures like T5, raise interesting questions for the pursuit of commonsense reasoning. Researchers have discovered that previous models perform well on benchmark datasets because they pick up on incidental biases in the dataset that have nothing to do with the task; in contrast, the WinoGrande dataset has devoted considerable effort to reducing such biases, which may allow models to (inadvertently) “cheat” (for example, using simple statistical associations). While it is certainly true that datasets over-estimate the commonsense reasoning capabilities of modern models BIBREF1, there are alternative and complementary explanations as well: It has been a",
            "and as a result their better performance may not be due to better modeling of cross-view dynamics, but rather due to more parameters. However we performed extensive grid search on the number of parameters in MARN with one attention. Increasing the number of parameters further (by increasing dense layers, LSTHM cellsizes etc.) did not improve performance. This indicates that the better performance of MARN with multiple attentions is not due to the higher number of parameters but rather due to better modeling of cross-view dynamics. RQ4: Different tasks and datasets require different number of attentions. This is highly dependent on each dataset's nature and the underlying interconnections between modalities.",
            "Fortunately, we believe that the introduction of unsupervised generative language models presents a way in which to tackle this particular shortcoming of peer-to-peer markets. In 2014, Ian Goodfellow et. al proposed the general adversarial network (GAN) [5]. The group showcased how this generative model could learn to artificially replicate data patterns to an unprecedented realistic degree [5]. Since then, these models have shown tremendous potential in their ability to generate photo-realistic images and coherent text samples [5]. The framework that GANs use for generating new data points employs an end-to-end neural network comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being",
            "Following the recent progress in deep learning, researchers and practitioners of machine learning are recognizing the importance of understanding and interpreting what goes on inside these black box models. Recurrent neural networks have recently revolutionized speech recognition and translation, and these powerful models could be very useful in other applications involving sequential data. However, adoption has been slow in applications such as health care, where practitioners are reluctant to let an opaque expert system make crucial decisions. If we can make the inner workings of RNNs more interpretable, more applications can benefit from their power. There are several aspects of what makes a model or algorithm understandable to humans. One aspect is model complexity or parsimony.",
            "When confronting the different CNN models, we observe that the CNN2 and CNN3 models, as opposed to CNN1, produce a steeper decrease of the classification performance when deleting the most relevant words from the initially correctly classified documents, both when considering LRP as well as SA as relevance model, as shown by the first deletion experiment. This indicates that the networks with greater filter sizes are more sensitive to single word deletions, most presumably because during these deletions the meaning of the surrounding words becomes less obvious to the classifier. This also provides some weak evidence that, while CNN2 and CNN3 behave similarly (which suggests that a convolutional filter size of two is already enough for the considered classification problem), the learned",
            "the relevant noise needed in training neural architectures in such low-resourced situations. The main contributions of this paper are:",
            "Explanation of individual classification decisions in terms of input variables has been studied for a variety of machine learning classifiers such as additive classifiers BIBREF18 , kernel-based classifiers BIBREF19 or hierarchical networks BIBREF11 . Model-agnostic methods for explanations relying on random sampling have also been proposed BIBREF20 , BIBREF21 , BIBREF22 . Despite their generality, the latter however incur an additional computational cost due to the need to process the whole sample to provide a single explanation. Other methods are more specific to deep convolutional neural networks used in computer vision: the authors of BIBREF8 proposed a network propagation technique based on deconvolutions to reconstruct input image patterns that are linked to a particular feature map",
            "the question in words as well as part-of-speech tags for the question (SVM-BOW + LEN + POS). This results in vectors of 200, 201 and 228 dimensions respectively, which are provided to an SVM with a linear kernel. CNN: We utilize a CNN neural encoder for answerability prediction. We use GloVe word embeddings BIBREF50, and a filter size of 5 with 64 filters to encode questions. BERT: BERT BIBREF51 is a bidirectional transformer-based language-model BIBREF52. We fine-tune BERT-base on our binary answerability identification task with a learning rate of 2e-5 for 3 epochs, with a maximum sequence length of 128.",
            "CNN and USE representations. Combining the RNN and USE further improves results. We hypothesize that this is due to complementary sentence information encoded in universal representations. For all models, correlations for Intervention and Outcomes are higher than for Population, which is expected given the difficulty distributions in Figure 1 . In these, the sentences are more uniformly distributed, with a fair number of difficult and easier sentences. By contrast, in Population there are a greater number of easy sentences and considerably fewer difficult sentences, which makes the difficulty ranking task particularly challenging.",
            "We observe our Random Forest classifier outperforms our deep learning classifier; e.g., Ours: RF yields a three percentage point improvement with respect to AP while consistently yielding improved precision-recall values over Ours: LSTM-CNN (Figure FIGREF8 a). In general, deep learning systems hold promise to replace handcrafted features to pick out the discriminative features. Our baselines highlight a possible value in developing a different deep learning architecture for the problem of learning answer disagreement than applied for predicting answers to visual questions. We show examples of prediction results where our top-performing RF classifier makes its most confident predictions (Figure FIGREF8 b). In these examples, the predictor expects human agreement for “what room... ?\" visual",
            "We propose new visualization and interpretation strategies for neural models to understand how and why they work. We demonstrate the effectiveness of the proposed strategies on a complex task (NLI). Our strategies are able to provide interesting insights not achievable by previous explanation techniques. Our future work will extend our study to consider other NLP tasks and models with the goal of producing useful insights for further improving these models. Model In this section we describe the ESIM model. We divide ESIM to three main parts: 1) input encoding, 2) attention, and 3) inference. Figure 4 demonstrates a high-level view of the ESIM framework. Let $u=[u_1, \\cdots , u_n]$ and $v=[v_1, \\cdots , v_m]$ be the given premise with length $n$ and hypothesis with length $m$ respectively,"
        ]
    },
    {
        "title": "Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks",
        "answer": "16 different text classification tasks.",
        "evidence": [
            "Translation experiments",
            "Task Definition",
            "can input to a corresponding segment and encouraged them to input a diverse set of questions which the span can answer. Along with the questions, the workers were also required to provide a justification as to why they made their questions. We manually checked this justification to filter out the questions with poor quality by removing those questions which were unrelated to the video. One initial challenge worth mentioning is that at first some workers input questions they had about the video and not questions which the video could answer. This was solved by providing them with an unrelated example. The second part of the question collection framework consisted of a paraphrasing task. In this task we presented workers with the questions generated by the first task and asked them to write",
            "most of the silver data is not used in the experiments. Note that since only the emotions were annotated, this method is only applicable to the EI tasks.",
            "The query vector of each task's attention could be either static parameters or generated dynamically. We conduct extensive experiments on 16 different text classification tasks, which demonstrate the benefits of our architecture.",
            "Task 2: Online Testing of Task-oriented Dialogue",
            "across the different compositional rules and formulate them. Experimental results on two typical tasks show the effectiveness of the proposed models.",
            "for example, the emoticons, politeness cues and polarity are not better disclosure predictors than the majority base. Also, we observe that only n-grams and GloVe features are the only group of features that contribute to more than a class type for the different tasks. Now, the “All Features” experiment shows how the interaction between feature sets perform than any of the other features groups in isolation. The accuracy metric for each trolling task is meant to provide an overall performance for all the classes within a particular task, and allow comparison between different experiments. In particular, we observe that GloVe vectors are the most powerful feature set, accuracy-wise, even better than the experiments with all features for all tasks except interpretation. The overall Total",
            "Experimenting with Transformers ::: Ecosystem",
            "Experiments on Additional Losses ::: Which additional loss works better?",
            "Experimental Results ::: Mastering Training Games",
            "Data details and experimental setup"
        ]
    },
    {
        "title": "Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation",
        "answer": "20.5%",
        "evidence": [
            "Comparison with baseline models",
            "on five year windows and then assessing predictions on the next year. The areas under the precision recall curves are then summed over the entire date range. The predictive capability of Model 3 outperforms their paper's baseline model with no preference networks but underperforms their date-adjusted model as measured by area under the precision recall curve (0.081 vs. 0.959). The multiplex model, however, outperforms their baseline paper model (with and without clusters), the date-adjusted model, and our textual extension model. The multiplex model exhibits a 20.5% increase in area under the precision recall curve compared to the original date-adjusted model (1.156 vs. 0.959). Constructing affinity blocs based on texts and votes in tandem thus leads to a more substantively intuitive",
            "all types of errors, our pattern-based method consistently outperformed the system by Felice2014a. The combination of the pattern-based method with the machine translation approach gave further substantial improvements and the best performance on all datasets.",
            "For the Thai sentence segmentation task, our model is superior to all the baselines on both Thai sentence segmentation datasets, as shown in Table TABREF45 . On the Orchid dataset, the supervised model that includes both local and distant representation was adopted for comparison to the baseline model. Our model improves the F1 score achieved by CRF-ngram, which is the state-of-the-art model for Thai sentence segmentation in Orchid, from 91.9% (row (d)) to 92.5% (row (g)). Meanwhile, in the UGWC dataset, our CVT model (row (h)) achieves an F1 score of 88.9%, which is higher than the F1 score of both the baselines (CRF-ngram and Bi-LSTM-CRF (rows d and e, respectively)). Thus, our model is now the state-of-the-art model for Thai sentence segmentation on both the Orchid and UGWC datasets.",
            "course size and intervention ratio (hereafter, i.ratio), which is the ratio of intervened to non-intervened threads. UPA performs better than PPA and APA on low intervention courses (i.ratio INLINEFORM3 0.25) mainly because PPA and APA's performance drops steeply when i.ratio drops (see col.2 parenthesis and INLINEFORM4 of PPA and APA). While all the proposed models beat the baseline on every course except casebased-2. On medicalneuro-2 and compilers-4 which have the lowest i.ratio among the 12 courses none of the neural models better the reported baseline BIBREF7 (course level not scores not shown in this paper). The effect is pronounced in compilers-4 course where none of the neural models were able to predict any intervened threads. This is due to the inherent weakness of standard",
            "the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
            "greater than INLINEFORM9 . Please note that we do not show the performance for year 2014 as we do not have data for 2015 for evaluation.",
            "BIBREF27 by adding a BoW/SVM baseline to the experiments and proposing a new criterion for assessing model explanatory power.",
            "We presented an example of generated story endings in Table 3 . Our model generates more natural and reasonable endings than the baselines. In this example, the baselines predicted wrong events in the ending. Baselines (Seq2Seq, HLSTM, and HLSTM+Copy) have predicted improper entities (cake), generated repetitive contents (her family), or copied wrong words (eat). The models equipped with incremental encoding or knowledge through MSA(GA/CA) perform better in this example. The ending by IE+MSA is more coherent in logic, and fluent in grammar. We can see that there may exist multiple reasonable endings for the same story context. In order to verify the ability of our model to utilize the context clues and implicit knowledge when planning the story plot, we visualized the attention weights of",
            "tuned three-layer model achieves much higher syntactic performance than their model (Section exp), being a stronger baseline to our extensions, which we introduce next.",
            "is better). As they are trained from much smaller amounts of data than current systems, these baselines are not quite competitive to today's best system, but still represent serious baselines for these datasets. Given our setups, fine-tuning with in-domain natural data improves BLEU by almost 4 points for both translation directions on in-domain tests; it also improves, albeit by a smaller margin, the BLEU score of the out-of-domain tests.",
            "for prediction models built by training a support vector machine using 5-fold, stratified cross-validation with all feature groups as a baseline for each class. We observed high performance for no evidence of depression and fatigue or loss of energy and moderate performance for all remaining classes."
        ]
    },
    {
        "title": "Sentiment Analysis of Citations Using Word2vec",
        "answer": "F1 score, accuracy, score, recall, perplexity, EM, precision, and recall.",
        "evidence": [
            "Evaluation Metrics",
            "random search. By using H+M, sample size can be set to 32 or 64, as rule of thumb, and even reach improvements in most cases (see § SECREF4 ). Nonetheless, this simplification and performance boosting comes along with possible higher optimization times. Finally, the performance of each configuration is obtained using a cross-validation technique on the training data, and the metrics are usually used in classification such as: accuracy, score INLINEFORM0 , and recall, among others.",
            "break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries, thesauri, WordNet) if required.  4. Annotators are kept anonymous, and are not able to communicate with each other during the annotation process. The selection criteria for the annotators required that all annotators must be native speakers of the target language. Preference to annotators with university education was given, but not required. Annotators were asked to complete a spreadsheet containing the translated pairs of words, as well as the part-of-speech, and a column to enter the score. The annotators did not have access to the original pairs in English. To ensure the quality of the collected ratings, we have employed an adjudication protocol similar to the one",
            "average, weighted by support (the number of true instances for each label). Weighted-F alters macro-F to account for label imbalance.",
            "Conclusions\nIn this paper we presented WiSeBE, a semi-automatic multi-reference sentence boundary evaluation protocol based on the necessity of having a more reliable way for evaluating the SBD task. We showed how INLINEFORM0 is an inclusive metric which not only evaluates the performance of a system against all references, but also takes into account the agreement between them. According to your point of view, this inclusivity is very important given the difficulties that are present when working with spoken language and the possible disagreements that a task like SBD could provoke.  INLINEFORM0 shows to be correlated with standard SBD metrics, however we want to measure its correlation with extrinsic evaluations techniques like automatic summarization and machine translation.",
            "regularization with an alpha of 0.01 to the loss function for model updating. Moreover, dropout is applied to the local representation vectors, recurrent representation vectors, between all bidirectional LSTMs and enclosed by the self-attention mechanism in the high-level module. During training, both the supervised and semi-supervised models are trained until the validation metrics stop improving; the metrics are (1) sentence boundary F1 score and (2) overall F1 score for Thai sentence segmentation and English punctuation restoration, respectively. CVT has three main parameters that impact model accuracy. The first is the drop rate of the masked language model, which determines the number of tokens that are dropped and used for learning auxiliary prediction modules as described in",
            "Methodology ::: Supervised Contextual 4-scores",
            "Dataset and Analysis Methodology",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\", “iol\"). In this plan, “R-1\",“C-1\", “C-0\", and “O-3\" are symbols for locations (nodes) in the graph. We compare the performance of translation approaches based on four metrics: [align=left,leftmargin=0em,labelsep=0.4em,font=] As in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0. The harmonic average of the precision and recall over all the test set",
            "Target Task and Setup\nThe most common evaluation metric of an LM is perplexity. Although neural LMs achieve impressive perplexity BIBREF9, it is an average score across all tokens and does not inform the models' behaviors on linguistically challenging structures, which are rare in the corpus. This is the main motivation to separately evaluate the models' syntactic robustness by a different task.",
            "Dictionary-based approaches",
            ". It would be interesting to compute error metrics that incorporate phoneme similarity, such as those proposed by hixon2011phonemic. PER weights all phoneme errors the same, even though some errors are more harmful than others: and are usually contrastive, whereas and almost never are. Such statistics would be especially interesting for evaluating a multilingual system, because different languages often map the same grapheme to phonemes that are only subtly different from each other. However, these statistics have not been widely reported for other g2p systems, so we omit them here."
        ]
    },
    {
        "title": "Neural Factor Graph Models for Cross-lingual Morphological Tagging",
        "answer": "English, Bengali, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish, Indonesian, German, Japanese, Hebrew, Croatian, Russian, French, Dutch, Arabic, Chinese, Hindi, Korean, Thai, Vietnamese, Turkish, Greek, Hungarian, Romanian, Czech, Slovak, Bulgarian, Estonian, Latvian, Lithuanian, Ukrainian, Belarusian, Azerbaijani, Georgian, Kazakh, Kyrgyz, Tajik, Uzbek, Mongolian, Albanian, Bosnian, Macedonian, Montenegrin, Serbian, Swahili, Amharic, Swazi, Zulu, Xhosa, Afrikaans, Catalan, Welsh, Icelandic, Maltese, Occitan, Pashto, Punjabi, Sindhi, Sinhala, Tamil, Telugu, Urdu, Welsh, Yiddish.  Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish. Although this language list contains only one non-Indo-European (Indonesian), four major Indo-European sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically and syntactically fairly",
        "evidence": [
            "Translation experiments",
            "Though Bengali is the seventh most spoken language in terms of number of native speakers BIBREF23, there is no standard corpus of questions available BIBREF0. We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to. The set of question categories is known as question taxonomy BIBREF0. We have used two layer taxonomy which was proposed by Xin Li, Dan Roth BIBREF24. This two layer taxonomy is made up of two classes which are Coarse Class and Finer Class. There are six coarse classes such as Numeric, Location, Entity, Description, Human and Abbreviation and fifty finer classes such as city, state, mountain, distance, count, definition,",
            "Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. Although this language list contains only one non-Indo-European (Indonesian), four major Indo-European sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically and syntactically fairly diverse.",
            "interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design.",
            "In this overview, our aim has been to demonstrate how new forms of data can be leveraged to complement the work of practitioners in international development. We have demonstrated that a wide variety of questions can be asked. Exploratory work can be performed to systematize large quantities of text. Additionally, we can learn about the sentiment that specific groups express towards specific topics. Networks can be uncovered, the spread of information or ideas can be traced, and influential actors identified. We can classify documents based on human coding of a subset of documents and establish which topics predict/correlate with predefined outcomes such as successful employment or completion of a program. While the application used here to illustrate the discussion focuses on texts in",
            "more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "However, most of these datasets include English words only. There has been no Japanese dataset for the word-similarity task. Apart from English, WordSim353 and SimLex-999 Hill:2015:CL have been translated and rescored in other languages: German, Italian and Russian Leviant:2015:arXiv. SimLex-999 has also been translated and rescored in Hebrew and Croatian Mrksic:2017:TACL. SimLex-999 explicitly targets at similarity rather than relatedness and includes adjective, noun and verb pairs. However, this dataset contains only frequent words. In addition, the distributed representation of words is generally learned using only word-level information. Consequently, the distributed representation for low-frequency words and unknown words cannot be learned well with conventional models. However,",
            "Analysis of the convolutional language model",
            "At the time of writing, the authors have been mostly aware of FastBert, FARM, flair BIBREF22, BIBREF33, AllenNLP BIBREF21 and PyText but there are likely more interesting developments to be found, from research and internal projects to production packages.",
            "break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries, thesauri, WordNet) if required.  4. Annotators are kept anonymous, and are not able to communicate with each other during the annotation process. The selection criteria for the annotators required that all annotators must be native speakers of the target language. Preference to annotators with university education was given, but not required. Annotators were asked to complete a spreadsheet containing the translated pairs of words, as well as the part-of-speech, and a column to enter the score. The annotators did not have access to the original pairs in English. To ensure the quality of the collected ratings, we have employed an adjudication protocol similar to the one",
            "German, Spanish, French, and Polish official versions of the debates with their translations performed using Google Translate. BIBREF6 find that features identified from texts are very similar between automatically translated documents and official manual translations. Furthermore, topic model estimates are also similar across languages when comparing machine and human translations of EU Parliament debates.",
            "via bootstrapping with the intent-based dialogue application we have built, and the hierarchy we defined will allow us to eliminate costly annotation efforts in the future, especially for the word-level slots and intent keywords. Once enough domain-specific multi-modal data is collected, our future work is to explore training end-to-end dialogue agents for our in-cabin use-cases. We are planning to exploit other modalities for improved understanding of the in-cabin dialogue as well."
        ]
    },
    {
        "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
        "answer": "BERT, RoBERTa, DistilBERT, ALBERT, XLNet, Longformer, FlanT5, T5, Electra, and BigBird.",
        "evidence": [
            "All the models follow the same philosophy of abstraction enabling a unified API in the library. Configuration - A configuration class instance (usually inheriting from a base class `PretrainedConfig`) stores the model and tokenizer parameters (such as the vocabulary size, the hidden dimensions, dropout rate, etc.). This configuration object can be saved and loaded for reproducibility or simply modified for architecture search. The configuration defines the architecture of the model but also architecture optimizations like the heads to prune. Configurations are agnostic to the deep learning framework used. Tokenizers - A Tokenizer class (inheriting from a base class `PreTrainedTokenizer`) is available for each model. This class stores the vocabulary token-to-index map for the corresponding",
            "more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "Neural models for NER have been recently proposed, producing state-of-the-art performance on standard NER tasks. For example, some of the end-to-end NER systems BIBREF4 , BIBREF2 , BIBREF3 , BIBREF0 , BIBREF1 use a recurrent neural network usually with a CRF BIBREF5 , BIBREF6 for sequence labeling, accompanied with feature extractors for words and characters (CNN, LSTMs, etc.), and achieve the state-of-the-art performance mostly without any use of gazetteers information. Note that most of these work aggregate textual contexts via concatenation of word embeddings and character embeddings. Recently, several work have addressed the NER task specifically on noisy short text segments such as Tweets, etc. BIBREF7 , BIBREF8 . They report performance gains from leveraging external sources of",
            "GAN setups",
            "a pretraining would cost approximately 100K USD. Contrary to this trend, the booming research in Machine Learning in general and Natural Language Processing in particular is arguably explained significantly by a strong focus on knowledge sharing and large-scale community efforts resulting in the development of standard libraries, an increased availability of published research code and strong incentives to share state-of-the-art pretrained models. The combination of these factors has lead researchers to reproduce previous results more easily, investigate current approaches and test hypotheses without having to redevelop them first, and focus their efforts on formulating and testing new hypotheses. To bring Transfer Learning methods and large-scale pretrained Transformers back into the",
            "To boost performance, the SVM, LSTM, and feed-forward models were combined into an ensemble. For both the LSTM and feed-forward approach, three different models were trained. The first model was trained on the training data (regular), the second model was trained on both the training and translated training data (translated) and the third one was trained on both the training data and the semi-supervised data (silver). Due to the nature of the SVM algorithm, semi-supervised learning does not help, so only the regular and translated model were trained in this case. This results in 8 different models per subtask. Note that for the valence tasks no silver training data was obtained, meaning that for those tasks the semi-supervised models could not be used. Per task, the LSTM and feed-forward",
            "none of the neural models were able to predict any intervened threads. This is due to the inherent weakness of standard neural models, which are unable to learn features well enough when faced with sparse data. The best performance of UPA indicates that the reply context of the instructor's post INLINEFORM0 correlates strongly with that of the previous post INLINEFORM1 . This is not surprising since normal conversations are typically structured that way.",
            "our knowledge, are the only publicly accessible commercial APIs for the task of entity-level sentiment analysis that is agnostic to the text domain. We also report results of TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, and Stanford NLP NER BIBREF21.",
            "a better-trained model. In near future, we will incorporate all the above techniques to get comparable performance with the state-of-the-art hybrid DNN/RNN-HMM systems.",
            "Supplemental Material ::: Hyperparameters ::: ELMo-transformer\nOur baseline pretraining model was a reimplementation of that given in BIBREF0. Hyperparameters were generally identical, but we trained on only 2 GPUs with (up to) 4,000 tokens per batch. This difference in batch size meant we used 6,000 warm up steps with the learning rate schedule of BIBREF16.",
            "Fortunately, we believe that the introduction of unsupervised generative language models presents a way in which to tackle this particular shortcoming of peer-to-peer markets. In 2014, Ian Goodfellow et. al proposed the general adversarial network (GAN) [5]. The group showcased how this generative model could learn to artificially replicate data patterns to an unprecedented realistic degree [5]. Since then, these models have shown tremendous potential in their ability to generate photo-realistic images and coherent text samples [5]. The framework that GANs use for generating new data points employs an end-to-end neural network comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being",
            "Collectively, the success of large pretrained neural models, both encoder-only BERT-like architectures as well as encoder–decoder architectures like T5, raise interesting questions for the pursuit of commonsense reasoning. Researchers have discovered that previous models perform well on benchmark datasets because they pick up on incidental biases in the dataset that have nothing to do with the task; in contrast, the WinoGrande dataset has devoted considerable effort to reducing such biases, which may allow models to (inadvertently) “cheat” (for example, using simple statistical associations). While it is certainly true that datasets over-estimate the commonsense reasoning capabilities of modern models BIBREF1, there are alternative and complementary explanations as well: It has been a"
        ]
    },
    {
        "title": "Multichannel Variable-Size Convolution for Sentence Classification",
        "answer": "It combines multichannel initialization and variable-size filters.",
        "evidence": [
            "Conclusion\nThis work presented MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization – diverse versions of pretrained word embeddings are used – and variable-size filters – features of multigranular phrases are extracted with variable-size convolution filters. We demonstrated that multichannel initialization and variable-size filters enhance system performance on sentiment classification and subjectivity classification tasks.",
            "Qualitative Comparison",
            "texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection with respect to the method of data acquisition. As for our data collection, full texts were not shown to workers as a basis for question formulation, but only the text's title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background knowledge. The NewsQA text collection differs from ours in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge required to answer the questions is mostly factual knowledge and script knowledge is only marginally relevant. Also, the task is not exactly question answering, but identification of document passages containing the answer.",
            "UTCNN Model Description",
            "Network (DCN) attention is applied to compute the bi-directional attention: INLINEFORM5 and INLINEFORM6 , where INLINEFORM7 denotes article-to-template attention and INLINEFORM8 is template-to-article attention. Finally, we obtain the template-aware article representation INLINEFORM9 : DISPLAYFORM0",
            "Effect of Cross-View Training (CVT)",
            "that a convolutional filter size of two is already enough for the considered classification problem), the learned filters in CNN2 and CNN3 do not only focus on isolated words but additionally consider bigrams or trigrams of words, as their results differ a lot from the CNN1 model in the first deletion experiment. In order to quantitatively evaluate and compare the ML models in combination with a relevance decomposition or explanation technique, we apply the evaluation method described in section \"Measuring Model Explanatory Power through Extrinsic Validation\" . That is, we compute the accuracy of an external classifier (here KNN) on the classification of document summary vectors (obtained with the ML model's predicted class). For these experiments we remove test documents which are empty",
            "having the same size with the Japanese-Vietnamese parallel corpus. In the end, the data we have is double in size compared to the original one.",
            "Intuitively, many aspects of the semantics of long sequences are context-invariant and can be computed in parallel (e.g., convolutionally), but some aspects require long-distance context and must be computed recurrently. Many existing neural network architectures either fail to take advantage of the contextual information or fail to take advantage of the parallelism. QRNNs exploit both parallelism and context, exhibiting advantages from both convolutional and recurrent neural networks. QRNNs have better predictive accuracy than LSTM-based models of equal hidden size, even though they use fewer parameters and run substantially faster. Our experiments show that the speed and accuracy advantages remain consistent across tasks and at both word and character levels. Extensions to both CNNs and",
            "Acknowledgements\nThis project was partially supported by Oculus research grant. We thank the reviewers for their valuable feedback.",
            "since the latter ignores word order. We incorporate each of the four KBs separately into PCNet and KVMemNet, and find that incorporating external KBs could bring improvements. From Figure 4 , we can see that the KVMemNet model attends to the key “(st. johns, is located)” for the question “Where is st johns mi located?”. Thus, the model has higher confidence in regarding value “in clinton county” as the answer.",
            "Competitor Methods\nRecNN BIBREF11 : Recursive neural network with standard compositional function. RNTN BIBREF23 : The RNTN is a recursive neural network with neural tensor layer, which can model strong interactions between two constituents. MV-RecNN BIBREF11 : The MV-RecNN is to represent every word and longer phrase in a parse tree as both a vector and a matrix in order to model rich compositionality. TreeLSTM BIBREF9 : Recursive neural network with Long Short-Term Memory unit."
        ]
    },
    {
        "title": "A Lightweight Front-end Tool for Interactive Entity Population",
        "answer": "JavaScript",
        "evidence": [
            "NLP Toolkits",
            "input is then piped into the first hidden layer. As before, an ELU transformation is then applied between the first and second layer, as well as between the second and third hidden layers. Finally, a sigmoid activation is used on the output of the last hidden layer. This sigmoid activation is important since the output of our discriminator is a binary boolean that indicates whether the discriminator believes the input to have been real data or data produced by the generator. This discriminator is thus trained to minimize the binary cross-entropy loss of its prediction (whether the data was real or fake) and the real ground-truth of each data point. The general framework defined above was inspired largely by the open-source code of Nag Dev, and was built using Pytorch [7]. One key",
            "Implementation\nLUWAK is implemented in pure JavaScript code, and it uses the LocalStorage of a web browser. A user does not have to install any packages except for a web browser. The only thing the user must do is to download the LUWAK software and put it in a local directory. We believe the cost of installing the tool will keep away a good amount of potential users. This philosophy follows our empirical feeling of the curse of installing required packages/libraries. Moreover, LUWAK does not need users to consider the usage and maintenance of these additional packages. That is why we are deeply committed to making LUWAK a pure client-side tool in the off-the-shelf style.",
            "Most lexical resources for sentiment analysis are in English. To still be able to benefit from these sources, the lexicons in the AffectiveTweets package were translated to Spanish, using the machine translation platform Apertium BIBREF5 . All lexicons from the AffectiveTweets package were translated, except for SentiStrength. Instead of translating this lexicon, the English version was replaced by the Spanish variant made available by BIBREF6 . For each subtask, the optimal combination of lexicons was determined. This was done by first calculating the benefits of adding each lexicon individually, after which only beneficial lexicons were added until the score did not increase anymore (e.g. after adding the best four lexicons the fifth one did not help anymore, so only four were added).",
            "How Many Expert Annotations?",
            "Data details and experimental setup",
            "Moreover, with SimplerVoice, we test if our system could provide users with the proper visual components to help them understand the products' usage based on the proposed techniques, and measure the usefulness of SimplerVoice's generated description. We evaluated the effectiveness & interpretability of 3 above approaches by conducting a controlled user study with 15 subjects who were Vietnamese native and did not speak/comprehend English. A dataset of random 20 U.S. products including products' title, UPC code, and product package images were chosen to be displayed in the user study. Note that the 15 participated subjects had not used the 20 products before and were also not familiar with the packaged products including the chosen 20 products; hence, they were \"illiterate\" in terms of",
            "even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions. To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require background knowledge. We explicitly state the following items in the HIT instructions for question writing: No Yes-no questions should be asked. The question should have at least five words. Videos, images or inserted links should not be considered. No background knowledge should be required to answer the question. To help the workers better follow the instructions, we also include a representative example showing",
            "In Turkish, people sometimes prefer to spell English characters for the corresponding Turkish characters (e.g. i for ı, c for ç) when writing in electronic format. To normalise such words, we used the Zemberek tool BIBREF15. All punctuation marks except “!\" and “?\" are removed, since they do not contribute much to the polarity of a document. We took into account emoticons, such as “:))\", and idioms, such as “kafayı yemek” (lose one's mind), since two or more words can express a sentiment together, irrespective of the individual words thereof. Since Turkish is an agglutinative language, we used the morphological parser and disambiguator tools BIBREF16, BIBREF17. We also performed negation handling and stop-word elimination. In negation handling, we append an underscore to the end of a word",
            "trained on a massively multilingual corpus. A language embedding vector is concatenated to the input at each time step. The language embeddings their system learned correlate closely to the genetic relationships between languages. However, neither of these models was applied to g2p.",
            "State-of-the-art",
            "Feature Contribution"
        ]
    },
    {
        "title": "Modeling Trolling in Social Media Conversations",
        "answer": "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs.",
        "evidence": [
            "Datasets and Experimental Setup",
            "having the same size with the Japanese-Vietnamese parallel corpus. In the end, the data we have is double in size compared to the original one.",
            "contains reports by approximately 3000 respondents from all over the world, for a total of 7665 sentences labelled with an emotion, making this the largest dataset out of the three we use.",
            "Data",
            "we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script to download the original tweets and detailed documentation on how we build our dataset. Also note that since we keep the original news article and news titles for each tweet, our dataset can also be used to explore more challenging generation tasks. Table TABREF19 shows the statistics of our current collection, and the frequency of",
            "Results: SnapCaptions Dataset",
            "Data\nThe next step involves deciding on the data sources, collecting and compiling the dataset, and inspecting its metadata.",
            "Ancient-Modern Chinese Dataset",
            "game. We followed the data partitions introduced with the dataset and used a train/validation/test sets of respectively $3,398$/727/728 (data-structure, description) pairs.",
            "TutorialVQA Dataset ::: Overview",
            "Data Collection ::: Analysis",
            "Our Dataset"
        ]
    },
    {
        "title": "Rotations and Interpretability of Word Embeddings: the Case of the Russian Language",
        "answer": "SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, SVD, S",
        "evidence": [
            "Interpretability measures",
            "When do experts disagree? We would like to analyze the reasons for potential disagreement on the annotation task, to ensure disagreements arise due to valid differences in opinion rather than lack of adequate specification in annotation guidelines. It is important to note that the annotators are experts rather than crowdworkers. Accordingly, their judgements can be considered valid, legally-informed opinions even when their perspectives differ. For the sake of this question we randomly sample 100 instances in the test data and analyze them for likely reasons for disagreements. We consider a disagreement to have occurred when more than one expert does not agree with the majority consensus. By disagreement we mean there is no overlap between the text identified as relevant by one expert and",
            "rest of this paper is structured as follows. Section SECREF2 discusses the related work that served as an inspiration for our analysis. Section SECREF3 comprises the proposed criteria and properties to characterize AV methods. Section SECREF4 describes the methodology, consisting of the used corpora, examined AV methods, selected performance measures and experiments. Finally, Section SECREF5 concludes the work and outlines future work.",
            "Cross-Lingual Evaluation ::: Models in Comparison",
            "possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth. Besides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human experts. Furthermore, for insight-driven analyses, it can be more useful to focus on improving",
            "(Q^T W^T W W^T W Q)_{1, 1} = \\left(q^T W^T W q\\right)^2\n$  is maximized when $q$ is the eigenvector of $W^T W$ with the largest singular value, i. e., the first right singular vector of $W$ BIBREF28 . Let's fix this vector and choose other vectors to be orthogonal to the selected ones and to maximize the interpretability. We arrive at $Q = V$ , where $V$ is the right orthogonal factor in SVD $W = U \\Sigma V^T$ .",
            "better identifies relevant words than sensitivity analysis. px (iii) A novel way of generating vector-based document representations is introduced and it is verified that these document vectors present semantic regularities within their original feature space akin word vector representations. px (iv) A measure for model explanatory power is proposed and it is shown that two ML models, a neural network and a BoW/SVM classifier, although presenting similar classification performance may largely differ in terms of explainability.  The work is organized as follows. In section \"Representing Words and Documents\" we describe the related work for explaining classifier decisions with respect to input space variables. In section \"Predicting Category with a Convolutional Neural Network\" we introduce",
            "Annotation study 2: Annotating micro-structure of arguments",
            "is not always possible for a given text. Therefore, we carried out answer collection in two steps. In the first step, we asked participants to assign a category to each text–question pair. We distinguish two categories of answerable questions: The category text-based was assigned to questions that can be answered from the text directly. If the answer could only be inferred by using commonsense knowledge, the category script-based was assigned. Making this distinction is interesting for evaluation purposes, since it enables us to estimate the number of commonsense inference questions. For questions that did not make sense at all given a text, unfitting was assigned. If a question made sense for a text, but it was impossible to find an answer, the label unknown was used. In a second step,",
            "source models. Moreover, these first components have almost zero alignment shifts. Other principal components have very similar singular values, and thus they cannot be determined uniquely with high confidence. Normalized interpretibility measures for different components (calculated for 50 top/bottom words) for the source and the rotated models are shown in Fig. 4.",
            "Experimental Setup ::: Answerability Identification Baselines",
            "to make their semantic context clear. In contrast, larger segments, like novels, have too much variation, making it difficult to train focused models. Finding a good segmentation sometimes means combining short documents and subdividing long documents. The word “document\" can therefore be misleading. But it is so ingrained in the common NLP lexicon that we use it anyway in this article. For insight-driven text analysis, it is often critical that high-level patterns can be communicated. Furthermore, interpretable models make it easier to find spurious features, to do error analysis, and to support interpretation of results. Some approaches are effective for prediction, but harder to interpret. The value we place on interpretability can therefore influence the approach we choose. There is"
        ]
    },
    {
        "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study",
        "answer": "overfitting, unresolvable coreference, false presuppositions, unanswerable questions, unanticipated questions, unknown sources of error, vague questions.  ###",
        "evidence": [
            "Appendix B: Error Analysis",
            "cited in entity pages comes from the first paragraphs. However, challenging task such as the canonicalization and chronological ordering of facts, still remain.",
            "How Many Expert Annotations?",
            "First, their hierarchical method has a large number of parameters and it is therefore slow to train and likely prone to overfitting. Secondly, it does not take the global context of the whole document into account, which may arguably be critical in extractive methods, when deciding on the salience of a sentence (or even a word). The extractive summarizer we present in this paper tries to address these limitations by adopting the parameter lean LSTM-minus method, and by explicitly modeling the global context.",
            "to extend their gratitude to Elias Wright, Gian Mascioli, Kiara Pillay, Harrison Kay, Eliel Talo, Alexander Fagella and N. Cameron Russell for providing their valuable expertise and insight to this effort. The authors are also grateful to Eduard Hovy, Lorrie Cranor, Florian Schaub, Joel Reidenberg, Aditya Potukuchi and Igor Shalyminov for helpful discussions related to this work, and to the three anonymous reviewers of this draft for their constructive feedback. Finally, the authors would like to thank all crowdworkers who consented to participate in this study.",
            "are incomplete or open-ended (e.g., `the south west wind blows across nigeria between'). 3% of questions have an unresolvable coreference (e.g., `how do i get to Warsaw Missouri from here'). 4% of questions are vague, and a further 7% have unknown sources of error. 2% still contain false presuppositions (e.g., `what is the only fruit that does not have seeds?') and the remaining 42% do not have an answer within the document. This reinforces our belief that though they have been understudied in past work, any question answering system interacting with real users should expect to receive such unanticipated and unanswerable questions.",
            "Quantifying Task Difficulty",
            "Final Remarks",
            "rest of this paper is structured as follows. Section SECREF2 discusses the related work that served as an inspiration for our analysis. Section SECREF3 comprises the proposed criteria and properties to characterize AV methods. Section SECREF4 describes the methodology, consisting of the used corpora, examined AV methods, selected performance measures and experiments. Finally, Section SECREF5 concludes the work and outlines future work.",
            "Introduction and related work",
            "Research questions",
            "acknowledging the flaws in our data and drawing cautious and reasonable conclusions from them. In all cases, we should report the choices we have made when creating or re-using any dataset."
        ]
    },
    {
        "title": "Attention Optimization for Abstractive Document Summarization",
        "answer": "F1-score, accuracy, MAP, Cosine, Soft-Cosine, EM, precision, recall.  macro-average f-score, accuracy, MAP, Cosine, Soft-Cosine, EM, precision, recall.  macro-average f-score, accuracy, MAP, Cosine, Soft-Cosine, EM, precision, recall.  macro-average f-score, accuracy, MAP, Cosine, Soft-Cosine, EM, precision, recall.  macro-average f-score, accuracy, MAP, Cosine, Soft-Cosine, EM, precision, recall.  macro-average f-score, accuracy, MAP, Cosine, Soft-Cosine, EM, precision, recall.  macro-average f-score, accuracy, MAP, Cosine, Soft-Cosine, EM, precision, recall.  macro-average f-score, accuracy, MAP, Cosine, Soft-Cosine, EM, precision, recall.  macro-average f-score, accuracy, MAP, Cosine, Soft-Cosine, EM, precision, recall.  macro-average f-score, accuracy, MAP, Cosine, Soft-Cosine, EM, precision, recall.  macro-average f-score, accuracy, MAP, Cosine, Soft-Cosine,",
        "evidence": [
            "Evaluation Metrics",
            "the answers to the similar questions will be correct answers for the new question. Different machine learning and deep learning approaches were tested in the scope of SemEval 2016 BIBREF3 and 2017 BIBREF4 task 3B. The best performing system in 2017 achieved a MAP of 47.22% using supervised Logistic Regression that combined different unsupervised similarity measures such as Cosine and Soft-Cosine BIBREF37 . The second best system achieved 46.93% MAP with a learning-to-rank method using Logistic Regression and a rich set of features including lexical and semantic features as well as embeddings generated by different neural networks (siamese, Bi-LSTM, GRU and CNNs) BIBREF38 . In the scope of this challenge, a dataset was collected from Qatar Living forum for training. We refer to this",
            "that besides the released data for training and developing, we also allow to collect external data for training and developing. To considering that, the task 1 is indeed includes two sub tasks. One is a closed evaluation, in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric.",
            "the macro-average f-score as the evaluation metric for the overall performance because (1) the experimental dataset is severely imbalanced, which is common for contentious issues; and (2) for stance classification, content in minor-class posts is usually more important for further applications. For the CreateDebate dataset, accuracy was adopted as the evaluation metric to compare the results with related work BIBREF7 , BIBREF9 , BIBREF12 .",
            "the SER for each of one the two available references and then compute their mean. They found this approach to be very strict because for those boundaries where no agreement between references existed, the system was going to be partially wrong even the fact that it has correctly predicted the boundary. Their second approach tried to moderate the number of unjust penalizations. For this case, a classification was considered incorrect only if it didn't match either of the two references. These two examples exemplify the real need and some straightforward solutions for multi-reference evaluation metrics. However, we think that it is possible to consider in a more inclusive approach the similarities and differences that multiple references could provide into a sentence boundary evaluation",
            "qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\", “iol\"). In this plan, “R-1\",“C-1\", “C-0\", and “O-3\" are symbols for locations (nodes) in the graph. We compare the performance of translation approaches based on four metrics: [align=left,leftmargin=0em,labelsep=0.4em,font=] As in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0. The harmonic average of the precision and recall over all the test set",
            "We focused evaluation over a small but diversified dataset composed by 10 YouTube videos in the English language in the news context. The selected videos cover different topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 minutes. To encourage the diversity of content format we included newscasts, interviews, reports and round tables. During the transcription phase we opted for a manual transcription process because we observed that using transcripts from an ASR system will difficult in a large degree the manual segmentation process. The number of words per transcript oscilate between 271 and 1,602 with a total number of 8,080. We gave clear instructions to three evaluators ( INLINEFORM0 ) of how segmentation was needed to be perform,",
            "Experiments ::: Label-partially-unseen evaluation ::: Baselines.",
            "scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014.",
            "discussing both the opportunities and complications that often arise. In the Reddit case, for example, hate speech is measured, however imperfectly, by the presence of particular words semi-automatically extracted from a machine learning algorithm. Operationalizations are never perfect translations, and are often refined over the course of an investigation, but they are crucial. We begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results. The research process sounds more or less linear this way, but each of these phases overlaps, and in some instances turns back upon itself. The analysis phase, for example, often feeds back into the original",
            "Target Task and Setup\nThe most common evaluation metric of an LM is perplexity. Although neural LMs achieve impressive perplexity BIBREF9, it is an average score across all tokens and does not inform the models' behaviors on linguistically challenging structures, which are rare in the corpus. This is the main motivation to separately evaluate the models' syntactic robustness by a different task."
        ]
    },
    {
        "title": "Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling",
        "answer": "seq2seq, CTC, attention, hybrid RNN/DNN-HMM, QRNN, CNN, RNN, parse2seq, multi-source system.  Language embedding vector, attentional sequence-to-sequence models, RNN (GRU/LSTM) layer, seq2seq Neural Parsing, hierarchical seq2seq Generation Model.  Two-step decoder, delayed copy mechanism, pointer network, dynamic entity representation.  paraphrasing model, question generation model, PCNet, KVMemNet.  seq2seq model, QRNN, CNN, RNN, parse2seq, multi-source system, attentional sequence-to-sequence models, RNN (GRU/LSTM) layer, seq2seq Neural Parsing, hierarchical seq2seq Generation Model.  Two-step decoder, delayed copy mechanism, pointer network, dynamic entity representation.  paraphrasing model, question generation model, PCNet, KVMemNet.  seq2seq model, QRNN, CNN, RNN, parse2seq, multi-source system, attentional sequence-to-sequence models, RNN (GRU/LSTM) layer, seq2seq Neural Parsing, hierarchical seq2seq Generation Model.  Two-step decoder, delayed copy mechanism, pointer network, dynamic entity representation",
        "evidence": [
            "The sequence-to-sequence (seq2seq) model proposed in BIBREF0 , BIBREF1 , BIBREF2 is a neural architecture for performing sequence classification and later adopted to perform speech recognition in BIBREF3 , BIBREF4 , BIBREF5 . The model allows to integrate the main blocks of ASR such as acoustic model, alignment model and language model into a single framework. The recent ASR advancements in connectionist temporal classification (CTC) BIBREF5 , BIBREF4 and attention BIBREF3 , BIBREF6 based approaches has created larger interest in speech community to use seq2seq models. To leverage performance gains from this model as similar or better to conventional hybrid RNN/DNN-HMM models requires a huge amount of data BIBREF7 . Intuitively, this is due to the wide-range role of the model in",
            "on the model architectures. The error generation methods can generate alternative versions of the same input text – the pattern-based method randomly samples the error locations, and the SMT system can provide an n-best list of alternative translations. Therefore, we also investigated the combination of multiple error-generated versions of the input files when training error detection models. Figure FIGREF6 shows the INLINEFORM0 score on the development set, as the training data is increased by using more translations from the n-best list of the SMT system. These results reveal that allowing the model to see multiple alternative versions of the same file gives a distinct improvement – showing the model both correct and incorrect variations of the same sentences likely assists in learning",
            "users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of the base class, adding a specific head on top of the base model hidden states. Examples of these heads are language modeling or sequence classification heads. These classes follow similar naming pattern: XXXForSequenceClassification or XXXForMaskedLM where XXX is the name of the model and can be used for adaptation (fine-tuning) or pre-training. All models are available both in PyTorch and TensorFlow (starting 2.0) and offer deep inter-operability between both frameworks. For instance, a model trained in one of frameworks can be saved on drive for the standard library serialization practice and then be reloaded from the saved files in the other framework seamlessly, making",
            "Taskmaster-1 has more unique words and is more difficult for language models to fit. We also find that Taskmaster-1 is more realistic than MultiWOZ. Specifically, the two-person dialogs in Taskmaster-1 involve more real-word entities than seen in MutliWOZ since we do not restrict conversations to a small knowledge base. Beyond the corpus and the methodologies used to create it, we present several baseline models including state-of-the-art neural seq2seq architectures together with perplexity and BLEU scores. We also provide qualitative human performance evaluations for these models and find that automatic evaluation metrics correlate well with human judgments. We will publicly release our corpus containing conversations, API call and argument annotations, and also the human judgments.",
            "accuracy advantages remain consistent across tasks and at both word and character levels. Extensions to both CNNs and RNNs are often directly applicable to the QRNN, while the model's hidden states are more interpretable than those of other recurrent architectures as its channels maintain their independence across timesteps. We believe that QRNNs can serve as a building block for long-sequence tasks that were previously impractical with traditional RNNs.",
            "deterioration in BLEU for longer sentences for all systems. In particular, although the parse2seq model outperformed the seq2seq model overall, it does worse than seq2seq for sentences containing more than 30 words. This indicates that parse2seq performance does indeed suffer due to its long sequences. On the other hand, the multi-source system outperforms the seq2seq baseline for all sentence lengths and does particularly well for sentences with over 50 words. This may be because the multi-source system has both sequential and parsed input, so it can rely more on sequential input for very long sentences.",
            "trained on a massively multilingual corpus. A language embedding vector is concatenated to the input at each time step. The language embeddings their system learned correlate closely to the genetic relationships between languages. However, neither of these models was applied to g2p.",
            "Attentional sequence-to-sequence models have become the new standard for machine translation, but one challenge of such models is a significant increase in training and decoding cost compared to phrase-based systems. Here, we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation (NMT), while achieving CPU decoding speed/throughput close to that of a phrasal decoder. We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of",
            "Hierarchical seq2seq Generation Model",
            "Seq2seq Neural Parsing\nUsing linearized parse trees within sequential frameworks was first done in the context of neural parsing. vinyals2015grammar parsed using an attentional seq2seq model; they used linearized, unlexicalized parse trees on the target side and sentences on the source side. In addition, as in this work, they used an external parser to create synthetic parsed training data, resulting in improved parsing performance. choe2016parsing adopted a similar strategy, using linearized parses in an RNN language modeling framework.",
            "in generated descriptions. For example, Puduppully et al. BIBREF12 propose a two-step decoder which first targets specific records and then use them as a plan for the actual text generation. Similarly, Li et al. BIBREF28 proposed a delayed copy mechanism where their decoder also acts in two steps: 1) using a classical LSTM decoder to generate delexicalized text and 2) using a pointer network BIBREF30 to replace placeholders by records from the input data. Closer to our work, very recent work BIBREF11, BIBREF19, BIBREF13 have proposed to take into account the data structure. More particularly, Puduppully et al. BIBREF13 follow entity-centric theories BIBREF31, BIBREF32 and propose a model based on dynamic entity representation at decoding time. It consists in conditioning the decoder on",
            "Table 4 shows the results of different question generation models. Our approach is abbreviated as QGNet, which stands for the use of a paraphrasing model plus our question generation model. We can see that QGNet performs better than Seq2Seq in terms of BLEU score because many important words of low frequency from the input are replicated to the target sequence. However, the improvement is not significant for the QA task. We also incorporate each of the four KBs into QGNet, and observe slight improvements on NELL and Reverb. Despite the overall accuracy of QGNet being lower than PCNet and KVMemNet, combining outcomes with them could generates 1.5% and 0.8% absolute gains, respectively. We show examples generated by our QG model in Figure 5 , in which the paths of two candidate answers are"
        ]
    },
    {
        "title": "Leveraging Discourse Information Effectively for Authorship Attribution",
        "answer": "T-BRNN-pre.",
        "evidence": [
            "State-of-the-art",
            "a pretraining would cost approximately 100K USD. Contrary to this trend, the booming research in Machine Learning in general and Natural Language Processing in particular is arguably explained significantly by a strong focus on knowledge sharing and large-scale community efforts resulting in the development of standard libraries, an increased availability of published research code and strong incentives to share state-of-the-art pretrained models. The combination of these factors has lead researchers to reproduce previous results more easily, investigate current approaches and test hypotheses without having to redevelop them first, and focus their efforts on formulating and testing new hypotheses. To bring Transfer Learning methods and large-scale pretrained Transformers back into the",
            "How Many Expert Annotations?",
            "mentioned above, which provides emotion-similarity for each token. Note that differently from the previous two types of embeddings, the retrofitted ones do rely on handcrafted information in the form of a lexical resource.",
            "Original Toulmin's model",
            "Thus, our model is now the state-of-the-art model for Thai sentence segmentation on both the Orchid and UGWC datasets. Our model outperforms all the sequence tagging models. T-BRNN-pre (row (c)) is the current state-of-the-art model, as shown in Table TABREF47 . The CVT model improves the overall F1 score from the 64.4% of T-BRNN-pre to 65.3% (row (h)), despite the fact that T-BRNN-pre integrates a pretrained word vector. Moreover, our model also achieves a 2-class F1 score 1.3% higher than that of Bi-LSTM-CRF (row (e)).",
            "Previous Work and Evaluation Data",
            "Stage 1 - Retraining decoder only",
            "even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions. To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require background knowledge. We explicitly state the following items in the HIT instructions for question writing: No Yes-no questions should be asked. The question should have at least five words. Videos, images or inserted links should not be considered. No background knowledge should be required to answer the question. To help the workers better follow the instructions, we also include a representative example showing",
            "Background",
            "NMT setups and performance",
            "classify. 2. Insert nonlinear layers: Here, the first architecture is upgraded and an architecture with a more robust classifier is provided in which instead of using a fully connected network without hidden layer, a fully connected network with two hidden layers in size 768 is used. The first two layers use the Leaky Relu activation function with negative slope = 0.01, but the final layer, as the first architecture, uses softmax activation function as shown in Figure FIGREF8. 3. Insert Bi-LSTM layer: Unlike previous architectures that only use [CLS] as the input for the classifier, in this architecture all outputs of the latest transformer encoder are used in such a way that they are given as inputs to a bidirectional recurrent neural network (Bi-LSTM) as shown in Figure FIGREF8. After"
        ]
    },
    {
        "title": "Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents",
        "answer": "Accuracy, F1 scores of ROUGE-1, ROUGE-2, ROUGE-L, NDCG.  Accuracy, F1 scores of ROUGE-1, ROUGE-2, ROUGE-L, NDCG.  Accuracy, F1 scores of ROUGE-1, ROUGE-2, ROUGE-L, NDCG.  Accuracy, F1 scores of ROUGE-1, ROUGE-2, ROUGE-L, NDCG.  Accuracy, F1 scores of ROUGE-1, ROUGE-2, ROUGE-L, NDCG.  Accuracy, F1 scores of ROUGE-1, ROUGE-2, ROUGE-L, NDCG.  Accuracy, F1 scores of ROUGE-1, ROUGE-2, ROUGE-L, NDCG.  Accuracy, F1 scores of ROUGE-1, ROUGE-2, ROUGE-L, NDCG.  Accuracy, F1 scores of ROUGE-1, ROUGE-2, ROUGE-L, NDCG.  Accuracy, F1 scores of ROUGE-1, ROUGE-2, ROUGE-L, NDCG.  Accuracy, F1 scores of",
        "evidence": [
            "Evaluation Metrics",
            "In the case there dataset was not split in training and gold (Arabic, German, Portuguese, Russian, and Swedish) then a cross-validation (10 folds) technique is used to test the classifier. The performance of the classifier is presented using different metrics depending the competition. SemEval uses the average of score INLINEFORM0 of positive and negative labels, TASS uses the accuracy and SENTIPOLC uses a custom metric (see BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 ).",
            "Results ::: Performance (WER) analysis on evaluation data ::: Speech type as a third entangled factor?",
            "In section \"Quality of Word Relevances and Model Explanatory Power\" we define objective criteria for evaluating word relevance scores, as well as for assessing model explanatory power. In section \"Results\" we introduce the dataset and experimental setup, and present the results. Finally, section \"Conclusion\" concludes our work.",
            "Performance Measures\nAccording to our extensive literature research, numerous measures (e. g., Accuracy, F INLINEFORM0 , c@1, AUC, AUC@1, INLINEFORM1 or EER) have been used so far to assess the performance of AV methods. In regard to our experiments, we decided to use c@1 and AUC for several reasons. First, Accuracy, F INLINEFORM2 and INLINEFORM3 are not applicable in cases where AV methods leave verification problems unanswered, which concerns some of our examined AV approaches. Second, using AUC alone is meaningless for non-optimizable AV methods, as explained in Section UID17 . Third, both have been used in the PAN-AV competitions BIBREF5 , BIBREF12 . Note that we also list the confusion matrix outcomes.",
            "Discussion of RQE Results",
            "Methodology ::: Data presentation",
            "We focused evaluation over a small but diversified dataset composed by 10 YouTube videos in the English language in the news context. The selected videos cover different topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 minutes. To encourage the diversity of content format we included newscasts, interviews, reports and round tables. During the transcription phase we opted for a manual transcription process because we observed that using transcripts from an ASR system will difficult in a large degree the manual segmentation process. The number of words per transcript oscilate between 271 and 1,602 with a total number of 8,080. We gave clear instructions to three evaluators ( INLINEFORM0 ) of how segmentation was needed to be perform,",
            "Summarization Performance",
            "qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.",
            "Evaluation Metrics\nFollowing previous work BIBREF2 , BIBREF14 , BIBREF11 , we use the standard F1 scores of ROUGE-1, ROUGE-2 and ROUGE-L BIBREF16 to evaluate the selected templates and generated summaries, where the official ROUGE script is applied. We employ the normalized discounted cumulative gain (NDCG) BIBREF20 from information retrieval to evaluate the Fast Rerank module.\n\n\nResults and Analysis\nIn this section, we report our experimental results with thorough analysis and discussions.",
            "included. On average, results were slightly better than those presented in the paper, although not statistically significantly. Moreover, the granularity of tag inventories in the lexicons is diverse, which makes it difficult to draw general conclusions about results based on full tags. This is why we only report results based on (coarse) PoS extracted from the original lexicons."
        ]
    },
    {
        "title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled Data",
        "answer": "Hurricane Katrina, Odisha Cyclone, and three others not specified.  (Note: The text does not specify the three other disasters.)  However, based on the context, it can be inferred that the three other disasters are the ones mentioned in the context of the ISEAR dataset, which are not specified in the provided snippet.)  Hurricane Katrina and Odisha Cyclone are mentioned in the context of the ISEAR dataset, but the other three disasters are not specified.  The text does not provide enough information to determine the other three disasters.  However, based on the context, it can be inferred that the three other disasters are the ones mentioned in the context of the ISEAR dataset, which are not specified in the provided snippet.)  Hurricane Katrina and Odisha Cyclone are mentioned in the context of the ISEAR dataset, but the other three disasters are not specified.  The text does not provide enough information to determine the other three disasters.  However, based on the context, it can be inferred that the three other disasters are the ones mentioned in the context of the ISEAR dataset, which are not specified in the provided snippet.)  Hurricane Katrina and Odisha Cyclone are mentioned in the context of the ISEAR dataset,",
        "evidence": [
            "While it is well-documented that climate change accepters and deniers have become increasingly polarized in the United States over time, there has been no large-scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. On the sub-population of Twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U.S. in 2018. We begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic-level results in line with prior research. We then apply RNNs to",
            "Previous Work and Evaluation Data",
            "In 2015, the United Nations Development Programme (UNDP) Regional Hub for Europe and CIS launched a Fragments of Impact Initiative (FoI) that helped to collect qualitative (micro-narratives) and quantitative data from multiple countries. Within a six-month period, around 10,000 interviews were conducted in multiple languages. These covered the perception of the local population in countries including Tajikistan, Yemen, Serbia, Kyrgyzstan and Moldova on peace and reconciliation, local and rural development, value chain, female entrepreneurship and empowerment, and youth unemployment issues. The micro-narratives were collected using SenseMaker(r), a commercial tool for collecting qualitative and quantitative data. The micro-narratives were individual responses to context-tailored questions.",
            "best relevance score). In another observation, the assessors reported that many of the returned answers had a correct question type but a wrong focus, which indicates that including a focus recognition module to filter such wrong answers can improve further the QA performance in terms of precision. Another aspect that was reported is the repetition of the same (or similar) answer from different websites, which could be addressed by improving answer selection with inter-answer comparisons and removal of near-duplicates. Also, half of the LiveQA test questions are about Drugs, when only two of our resources are specialized in Drugs, among 12 sub-collections overall. Accordingly, the assessors noticed that the performance of the QA systems was better on questions about diseases than on",
            "for generating five types of errors, and the system in Section SECREF3 is an extension of this model. While previous work focused on generating a specific subset of error types, we explored two holistic approaches to AEG and showed that they are able to significantly improve error detection performance.",
            "BIBREF9 analyze Islamic fatwas to determine whether Jihadist clerics write about different topics to those by non-Jihadists. Using an STM model, they uncover fifteen topics within the collection of religious writings. The model successfully identifies characteristic words in each topic that are common within the topic but occur infrequently in the fourteen other topics. The fifteen topics are labelled manually where the labels are human interpretations of the common underlying meaning of the characteristic words within each topic. Some of the topics deal with fighting, excommunication, prayer, or Ramadan. While the topics are relatively distinct, there is some overlap, i.e. the distance between them varies. This information can be leveraged to map out rhetorical network.",
            "text categorization BIBREF2 , entity disambiguation BIBREF3 , entity ranking BIBREF4 and distant supervision BIBREF5 , BIBREF6 . However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay. Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone, respectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the",
            "How Many Expert Annotations?",
            "The ISEAR (International Survey on Emotion Antecedents and Reactions BIBREF11 , BIBREF12 ) is a dataset created in the context of a psychology project of the 1990s, by collecting questionnaires answered by people with different cultural backgrounds. The main aim of this project was to gather insights in cross-cultural aspects of emotional reactions. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of seven major emotions (joy, fear, anger, sadness, disgust, shame and guilt). In each case, the questions covered the way they had appraised a given situation and how they reacted. The final dataset contains reports by approximately 3000 respondents from all over the world, for a total of 7665 sentences labelled",
            "also ensured that questions are linguistically simple, elaborate and explicit. Workers were asked to formulate questions about details of such a situation, i.e. independent of a concrete narrative. This resulted in questions, the answer to which is not literally mentioned in the text. To cover a broad range of question types, we asked participants to write 3 temporal questions (asking about time points and event order), 3 content questions (asking about persons or details in the scenario) and 3 reasoning questions (asking how or why something happened). They were also asked to formulate 6 free questions, which resulted in a total of 15 questions. Asking each worker for a high number of questions enforced that more creative questions were formulated, which go beyond obvious questions for a",
            "Acknowledgments\nThis work was funded by the Amazon Academic Research Awards program.",
            "the entire text. We further introduce an attention scheme to improve the robustness of NCEL to data noise and train the model on Wikipedia hyperlinks to avoid overfitting and domain bias. In experiments, we evaluate NCEL on five publicly available datasets to verify the linking performance as well as generalization ability. We also conduct an extensive analysis of time complexity, the impact of key modules, and qualitative results, which demonstrate the effectiveness and efficiency of our proposed method."
        ]
    },
    {
        "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding",
        "answer": "link prediction and triplet classification.",
        "evidence": [
            "Knowledge graphs (KGs) such as Freebase BIBREF0 , DBpedia BIBREF1 , and YAGO BIBREF2 play a critical role in various NLP tasks, including question answering BIBREF3 , information retrieval BIBREF4 , and personalized recommendation BIBREF5 . A typical KG consists of numerous facts about a predefined set of entities. Each fact is in the form of a triplet INLINEFORM0 (or INLINEFORM1 for short), where INLINEFORM2 and INLINEFORM3 are two entities and INLINEFORM4 is a relation the fact describes. Due to the discrete and incomplete natures of KGs, various KG embedding models are proposed to facilitate KG completion tasks, e.g., link prediction and triplet classification. After vectorizing entities and relations in a low-dimensional space, those models predict missing facts by manipulating the",
            "Experiments",
            "computation graphs and are tested to ensure they match the original author implementations' performances on various benchmarks.",
            "sense of whether some things have been left out of the coding rules and whether anything is superfluous, misleading, or confusing. Adjustments are made and the process is repeated, often with another researcher involved. The final annotations can be collected using a crowdsourcing platform, a smaller number of highly-trained annotators, or a group of experts. Which type of annotator to use should be informed by the complexity and specificity of the concept. For more complex concepts, highly-trained or expert annotators tend to produce more reliable results. However, complex concepts can sometimes be broken down into micro-tasks that can be performed independently in parallel by crowdsourced annotators. Concepts from highly specialized domains may require expert annotators. In all cases,",
            "aligning a set of instructions to a video of someone carrying out those instructions. In their task, they use the video transcript to represent the video, which they later augment with a visual cue detector on food entities. Their task focuses on procedure-based cooking videos, and contrary to our task is primarily a text alignment task. In our task we aim to answer questions-using the transcripts-on instructional-style videos, in which the answer can involve steps not mentioned in the question.",
            "aggregators. We also introduce a novel aggregator, namely, Logic Attention Network (LAN), which addresses the properties by aggregating neighbors with both rules- and network-based attention weights. By comparing with conventional aggregators on two knowledge graph completion tasks, we experimentally validate LAN's superiority in terms of the desired properties.",
            "has only 47K question-answer pairs when LiveQA participating systems relied on much larger collections, including the World Wide Web. Our system answered one subquestion at most when many LiveQA test questions had several subquestions. The latter observation, (b), makes the hybrid IR+RQE approach even more promising as it gives it a large potential for the improvement of answer completeness. The former observation, (a), provides another interesting insight: restricting the answer source to only reliable collections can actually improve the QA performance without losing coverage (i.e., our QA approach provided at least one answer to each test question and obtained the best relevance score). In another observation, the assessors reported that many of the returned answers had a correct",
            "domain-adaptation problem (See Section SECREF6 for further discussion on this). The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. Note that thankful was only available during specific time spans related to certain events, as Mother's Day in May 2016. For each page, we downloaded the latest 1000 posts, or the maximum available if there are fewer, from February 2016, retrieving the counts of reactions for each post. The output is a JSON file containing a list of dictionaries with a timestamp, the post and a reaction vector with frequency values, which",
            "Experiment\nTo make a comprehensive evaluation, we assess our model on five text classification tasks and a semantic matching task.",
            "Traditional semantic parsers map language onto compositional, executable queries in a fixed schema. This mapping allows them to effectively leverage the information contained in large, formal knowledge bases (KBs, e.g., Freebase) to answer questions, but it is also fundamentally limiting---these semantic parsers can only assign meaning to language that falls within the KB's manually-produced schema. Recently proposed methods for open vocabulary semantic parsing overcome this limitation by learning execution models for arbitrary language, essentially using a text corpus as a kind of knowledge base. However, all prior approaches to open vocabulary semantic parsing replace a formal KB with textual information, making no use of the KB in their models. We show how to combine the disparate",
            "by this loss reach near perfect level on most syntactic constructions for which we create negative examples, with only a slight increase of perplexity about 1.0 point. Past work conceptually similar to us is BIBREF5, which, while not directly exploiting negative examples, trains an LM with additional explicit supervision signals to the evaluation task. They hypothesize that LSTMs do have enough capacity to acquire robust syntactic abilities but the learning signals given by the raw text are weak, and show that multi-task learning with a binary classification task to predict the upcoming verb form (singular or plural) helps models aware of the target syntax (subject-verb agreement). Our experiments basically confirm and strengthen this argument, with even stronger learning signals from",
            "Experimenting with Transformers ::: Language understanding benchmarks"
        ]
    },
    {
        "title": "The First Evaluation of Chinese Human-Computer Dialogue Technology",
        "answer": "Accuracy, MAP, BLEU score, perplexity, task completion ratio, user satisfaction degree, EM, precision, recall, cosine distance, overall accuracy, accuracy given that the segment is in the top 10 videos.",
        "evidence": [
            "Evaluation Metrics",
            "then a more sophisticated method to select and re-rank the similar questions. We followed a similar approach for this evaluation by combining the LR method with the IR baseline provided in the context of SemEval-cQA. The hybrid method combines the score provided by the Logistic Regression model and the reciprocal rank from the IR baseline using a weight-based combination:  INLINEFORM0  The weight INLINEFORM0 was set empirically through several tests on the cQA-2016 development set ( INLINEFORM1 ). Table TABREF30 presents the results on the cQA-2016 and cQA-2017 test datasets. The hybrid method (LR+IR) provided the best results on both datasets. On the 2016 test data, the LR+IR method outperformed the best system in all measures, with 80.57% Accuracy and 77.47% MAP (official system ranking",
            "Machine translation evaluation\nTo evaluate the usefulness of our corpus for SMT purposes, we used it to train an automatic translator with Moses BIBREF8 . We also trained an NMT model using the OpenNMT system BIBREF9 , and used the Google Translate Toolkit to produce state-of-the-art comparison results. The produced translations were evaluated according to the BLEU score BIBREF10 .",
            "Target Task and Setup\nThe most common evaluation metric of an LM is perplexity. Although neural LMs achieve impressive perplexity BIBREF9, it is an average score across all tokens and does not inform the models' behaviors on linguistically challenging structures, which are rare in the corpus. This is the main motivation to separately evaluate the models' syntactic robustness by a different task.",
            "We use manual evaluation for task 2. For each system and each complete user intent, the initial sentence, which is used to start the dialogue, is the same. The tester then begin to converse to each system. A dialogue is finished if the system successfully returns the information which the user inquires or the number of dialogue turns is larger than 30 for a single task. For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied,",
            "Detection systems based on multi-reference (dis)agreement. We evaluate and compare the performance of different SBD systems over a set of Youtube transcripts using WiSeBE and standard metrics. This double evaluation gives an understanding of how WiSeBE is a more reliable metric for the SBD task.",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\", “iol\"). In this plan, “R-1\",“C-1\", “C-0\", and “O-3\" are symbols for locations (nodes) in the graph. We compare the performance of translation approaches based on four metrics: [align=left,leftmargin=0em,labelsep=0.4em,font=] As in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0. The harmonic average of the precision and recall over all the test set",
            "qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.",
            "Train and Testing Evaluation Setup",
            "achieve competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. For explanatory purposes, related work is discussed further and more in detail when we describe the benchmarks for evaluation (Section SECREF3 ) and when we compare our models to existing ones (Section SECREF5 ). We also explore and discuss how choosing different sets of Facebook pages as training data provides an intrinsic domain-adaptation method.",
            "Cross-Lingual Evaluation ::: Results and Discussion",
            "the minimal cosine distance distance to the query. Metrics. To evaluate our pipeline approach we use overall accuracy after filtering and accuracy given that the segment is in the top 10 videos. While the first metric is similar to SECREF17, the second can indicate if initially searching on the video space can be used to improve our selection:"
        ]
    },
    {
        "title": "BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization",
        "answer": "Through a Convolution Encoder Block, a Similarity Matrix and a Pooling Layer.",
        "evidence": [
            "build a model from multiple languages. The data is just combined together assuming the target languages are seen during the training. And, hence no special transfer learning techniques were used here to address the unseen languages during training. The main motivation and contribution behind this work is as follows:",
            "What is the Model Learning?",
            "the results for models trained with expert annotations. The model trained on random data has a slightly better F1 than that trained on the same amount of difficult data. The model trained on random data has higher precision but lower recall. Rows 3 and 4 list the results for models trained on the same data but with crowd annotation. Models trained with expert-annotated data are clearly superior to those trained with crowd labels with respect to F1, indicating that the experts produced higher quality annotations. For crowdsourced annotations, training the model with data sampled at i.i.d. random achieves 2% higher F1 than when difficult instances are used. When expert annotations are used, this difference is less than 1%. This trend in performance may be explained by differences in",
            "to gold summaries. Furthermore, when the N-Optimal templates are used, additional improvements can be observed as INLINEFORM0 grows. This trend is also confirmed by Figure FIGREF39 , in which the ROUGE scores increase before 30 and stabilize afterwards. These results suggest that the ranges given by Retrieve indeed help to find quality templates.",
            "Pattern Extraction",
            "special tokens (most pretrained Transformers models have a maximum sequence length they can handle, defined during their pretraining step). Tokenizers can be instantiated from existing configurations available through Transformers originating from the pretrained models or created more generally by the user from user-specifications. Model - All models follow the same hierarchy of abstraction: a base class implements the model's computation graph from encoding (projection on the embedding matrix) through the series of self-attention layers and up to the last layer hidden states. The base class is specific to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of",
            "Hyperparameters and Training",
            "TutorialVQA Dataset ::: Basis",
            "the embeddings not pre-trained. Table 8 reports the results obtained. We can observe how generally the system benefits from the use of the schedule information. Especially in the testing part, where the neural network recognizes with less accuracy, the explicit information contained in the schedule can be exploited for identifying the entities at which users are referring while listening to the radio and posting the tweets.",
            "on the article representations. Since natural language text is complicated and verbose in nature, and training data is insufficient in size to help the models distinguish important article information from noise, sequence-to-sequence models tend to deteriorate with the accumulation of word generation, e.g., they generate irrelevant and repeated words frequently BIBREF5 . Template-based summarization BIBREF6 is an effective approach to traditional abstractive summarization, in which a number of hard templates are manually created by domain experts, and key snippets are then extracted and populated into the templates to form the final summaries. The advantage of such approach is it can guarantee concise and coherent summaries in no need of any training data. However, it is unrealistic to",
            "how trained AV models behave, if the lengths of the questioned documents are getting shorter and shorter. To our best knowledge, this question has not been addressed in previous authorship verification studies.",
            "The above retrieval process is essentially based on superficial word matching and cannot measure the deep semantic relationship between two articles. Therefore, the Fast Rerank module is developed to identify a best template from the candidates based on their deep semantic relevance with the source article. We regard the candidate with highest relevance as the template. As illustrated in Figure FIGREF6 , this module consists of a Convolution Encoder Block, a Similarity Matrix and a Pooling Layer. Convolution Encoder Block. This block maps the input article and its candidate templates into high-level representations. The popular ways to this are either by using recurrent neural network (RNN) or a stack of convolutional neural network (CNN), while none of them are suitable for our problem."
        ]
    },
    {
        "title": "AttSum: Joint Learning of Focusing and Summarization with Neural Attention",
        "answer": "BackTrans, Unpaired, CrossAlign, CPTG, DualRL, GLUE, SuperGLUE, MElt, KVMemNet, DNN/RNN-HMM.",
        "evidence": [
            "Models for Comparison",
            "We compare our model with the following state-of-art generative models: BackTrans BIBREF7 : In BIBREF7 , authors propose a model using machine translation in order to preserve the meaning of the sentence while reducing stylistic properties. Unpaired BIBREF10 : In BIBREF10 , researchers implement a method to remove emotional words and add desired sentiment controlled by reinforcement learning. CrossAlign BIBREF6 : In BIBREF6 , authors leverage refined alignment of latent representations to perform style transfer and a cross-aligned auto-encoder is implemented. CPTG BIBREF24 : An interpolated reconstruction loss is introduced in BIBREF24 and a discriminator is implemented to control attributes in this work. DualRL BIBREF8 : In BIBREF8 , researchers use two reinforcement rewards",
            "GLUE and SuperGLUE",
            "in MElt the best performing feature set, described in Table TABREF1 . All models discussed in this paper are based on this feature set.",
            "What is the Model Learning?",
            "QA Model 2: KVMemNet",
            "GAN setups",
            "Results & Discussion",
            "Acknowledgements\nThis project was partially supported by Oculus research grant. We thank the reviewers for their valuable feedback.",
            "a better-trained model. In near future, we will incorporate all the above techniques to get comparable performance with the state-of-the-art hybrid DNN/RNN-HMM systems.",
            "Standard and baseline methods",
            "Effect of model structure"
        ]
    },
    {
        "title": "A Simple Approach to Multilingual Polarity Classification in Twitter",
        "answer": "English, Portuguese, Spanish, French, German, Italian, Dutch, Russian, Chinese, Japanese, Korean, Arabic, Hebrew, Hindi, Greek, Turkish, Vietnamese, Thai, Indonesian, Malaysian, Swahili, Polish, Romanian, Bulgarian, Czech, Slovak, Croatian, Serbian, Ukrainian, Estonian, Latvian, Lithuanian, Hungarian, Icelandic, Norwegian, Danish, Swedish, Finnish, Irish, Welsh, Scottish Gaelic, Breton, Basque, Catalan, Occitan, Corsican, Galician, Frisian, Azerbaijani, Georgian, Kazakh, Kyrgyz, Mongolian, Tajik, Uzbek, Belarusian, Bosnian, Albanian, Macedonian, Montenegrin, Slovenian, Kazakh, Tajik, Uzbek, Pashto, Kurdish, Persian, Azerbaijani, Georgian, Kazakh, Kyrgyz, Mongolian, Tajik, Uzbek, Belarusian, Bosnian, Albanian, Macedonian, Montenegrin, Slovenian, Kazakh, Tajik, Uzbek, Pashto, Kurdish, Persian, Azerbaijani, Georgian, Kazakh, Kyrgyz, Mongolian, Tajik, Uzbek, Belarusian, Bosnian, Albanian,",
        "evidence": [
            "Framework",
            "multilingual pretraining can damage performance even on resource-rich languages and language pairs. We observe a steep rise in performance when the multilingual model is trained on a much smaller set of languages (17 versus 100), and further improvements can be achieved by training a dedicated bilingual model. Finally, leveraging bilingual parallel data seems to offer additional slight gains, but a tiny difference between xlm-2 and xlm-2++ also suggests that this rich bilingual information is not used in the optimal way within the xlm architecture for semantic similarity. In summary, these results indicate that, in order to improve performance in cross-lingual transfer tasks, more work should be invested into 1) pretraining dedicated language pair-specific models, and 2) creative ways of",
            "model as a modified Toulmin's model. It contains five argument components, namely claim, premise, backing, rebuttal, and refutation. When annotating a document, any arbitrary token span can be labeled with an argument component; the components do not overlap. The spans are not known in advance and the annotator thus chooses the span and the component type at the same time. All components are optional (they do not have to be present in the argument) except the claim, which is either explicit or implicit (see above). If a token span is not labeled by any argument component, it is not considered as a part of the argument and is later denoted as none (this category is not assigned by the annotators). An example analysis of a forum post is shown in Figure FIGREF65 . Figure FIGREF66 then shows",
            "System Design\nIn this section, we describe the system design, and workflow of SimplerVoice (Figure FIGREF1 ). SimplerVoice has 4 main components: input retrieval, object2text, text2visual, and output display. Figure FIGREF1 provides the overall structure of SimplerVoice system.",
            "Working with Multiple Languages",
            "The framework for processing Portuguese texts is depicted in Fig. FIGREF2, which illustrates how relevant pieces of information are extracted from the text. Namely, input files (Portuguese texts) go through a series of modules: part-of-speech tagging, named entity recognition, dependency parsing, semantic role labeling, subject-verb-object triple extraction, and lexicon matching. The main goal of all the modules except lexicon matching is to identify events given in the text. These events are then used to populate an ontology. The lexicon matching, on the other hand, was created to link words that are found in the text source with the data available not only on Eurovoc BIBREF6 thesaurus but also on the EU's terminology database IATE BIBREF7 (see Section SECREF12 for details). Most of",
            "Principal components of different models",
            "The encoder consists of an embedding layer, two stacks of transformer blocks (denoted as encoder transformer blocks and aggregation transformer blocks), and an attention layer. In the embedding layer, we aggregate both word- and character-level embeddings. Word embeddings are initialized by the 300-dimension fastText BIBREF20 vectors trained on Common Crawl (600B tokens), and are fixed during training. Character embeddings are initialized by 200-dimension random vectors. A convolutional layer with 96 kernels of size 5 is used to aggregate the sequence of characters. We use a max pooling layer on the character dimension, then a multi-layer perceptron (MLP) of size 96 is used to aggregate the concatenation of word- and character-level representations. A highway network BIBREF21 is used on",
            "English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set. Detailed guidelines for both phases are available online at: https://multisimlex.com.",
            "framework establishes a more general model for how generative algorithms can be used to produce text samples for the purposes of marketing.",
            "to take advantage of the built ontology tree. In detail, SimplerVoice uses the joint verb set of 3 types of nouns: the input object, the parents, and the neighbors as the 3 noun types are always in the same context of ontology. Equation EQREF8 shows the word-sense disambiguation mechanism with INLINEFORM0 (Object) indicates the verb set of an object generated by heuristics, and n-grams model: DISPLAYFORM0  Low-informative verbs: In order to ensure the quality of generated verbs, SimplerVoice maintains a list of restricted verbs that need to be filtered out. There are a lot of general, and low-informative verbs generated by n-grams model such as \"be\", \"have\", \"use\", etc. as these verbs are highly used in daily sentences/conversation. The restricted verb list could help to ensure the right",
            "the speedy analysis of qualitative data on a large scale. We start with a brief description of the main approaches to NLP and how they can be augmented by human coding. We then move on to the issue of working with multiple languages and different document formats. We then provide an overview of the recent application of these techniques in a United Nations Development Program (UNDP) study."
        ]
    },
    {
        "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing",
        "answer": "Baseline LSTM-LM, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline3, Baseline, Baseline",
        "evidence": [
            "Baseline",
            "Besides the analysis on different reasoning types, we also look into the performance over questions with different first tokens in the development set, which provide us an automatic categorization of questions. According to the results in Table TABREF46 , the three neural baselines all perform the best on “Who” and “Where” questions, to which the answers are often named entities. Since the tweet contexts are short, there are only a small number of named entities to choose from, which could make the answer pattern easy to learn. On the other hand, the neural models fail to perform well on the “Why” questions, and the results of neural baselines are even worse than that of the matching baseline. We find that these questions generally have longer answer phrases than other types of questions,",
            "Algorithms Used",
            "semantically more meaningful semantic extraction than the baseline methods. In the next section we will confirm these observations quantitatively.",
            "Preliminaries and Framework",
            "Target Task and Setup ::: Language models ::: Baseline LSTM-LM",
            "originally used in the two systems, respectively.  Baseline: Similar to our model, but without local context and global context, i.e. the input to MLP is the sentence representation only. Lead: Given a length limit of INLINEFORM0 words for the summary, Lead will return the first INLINEFORM1 words of the source document. Oracle: uses the Gold Standard extractive labels, generated based on ROUGE (Sec. 4.2).",
            "Baselines ::: Baseline3: Pipeline Segment retrieval",
            "Mainstream Improvements\nWe consider the hyperparameters used by BIBREF3 to be our baseline. This baseline does not make use of various advances in NMT architectures and training tricks. In contrast to the baseline, we use a BiDeep RNN architecture BIBREF25 , label smoothing BIBREF26 , dropout BIBREF27 , word dropout BIBREF28 , layer normalization BIBREF29 and tied embeddings BIBREF30 .",
            "TutorialVQA Dataset ::: Basis",
            "Experiments\nTo show the challenge of TweetQA for existing approaches, we consider four representative methods as baselines. For data processing, we first remove the URLs in the tweets and then tokenize the QA pairs and tweets using NLTK. This process is consistent for all baselines.",
            "The baseline and the models are evaluated on a corpus of 12 MOOC discussion forums. We train on 80% of the training data and report evaluation results on the held-out 20% of test data. We report INLINEFORM0 scores on the positive class (interventions), in line with prior work. We also argue that recall of the positive class is more important than precision, since it is costlier for instructors to miss intervening on a thread than spending irrelevant time intervening on a less critical threads due to false positives. Model hyperpameter settings. All proposed and baseline neural models are trained using Adam optimizer with a learning rate of 0.001. We used cross-entropy as loss function. Importantly we updated the model parameters during training after each instance as in vanilla stochastic"
        ]
    },
    {
        "title": "Story Ending Generation with Incremental Encoding and Commonsense Knowledge",
        "answer": "Baseline3, Baseline, Lead, Oracle, SMT models, NMT models, Baseline LSTM-LM.",
        "evidence": [
            "Standard and baseline methods",
            "we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script to download the original tweets and detailed documentation on how we build our dataset. Also note that since we keep the original news article and news titles for each tweet, our dataset can also be used to explore more challenging generation tasks. Table TABREF19 shows the statistics of our current collection, and the frequency of",
            "Mainstream Improvements\nWe consider the hyperparameters used by BIBREF3 to be our baseline. This baseline does not make use of various advances in NMT architectures and training tricks. In contrast to the baseline, we use a BiDeep RNN architecture BIBREF25 , label smoothing BIBREF26 , dropout BIBREF27 , word dropout BIBREF28 , layer normalization BIBREF29 and tied embeddings BIBREF30 .",
            "Datasets Used for the RQE Study",
            "Baselines ::: Baseline3: Pipeline Segment retrieval",
            "originally used in the two systems, respectively.  Baseline: Similar to our model, but without local context and global context, i.e. the input to MLP is the sentence representation only. Lead: Given a length limit of INLINEFORM0 words for the summary, Lead will return the first INLINEFORM1 words of the source document. Oracle: uses the Gold Standard extractive labels, generated based on ROUGE (Sec. 4.2).",
            "Furthermore, we test SMT models and various NMT models on the created dataset and provide a strong baseline for this task.",
            "TutorialVQA Dataset ::: Basis",
            "is better). As they are trained from much smaller amounts of data than current systems, these baselines are not quite competitive to today's best system, but still represent serious baselines for these datasets. Given our setups, fine-tuning with in-domain natural data improves BLEU by almost 4 points for both translation directions on in-domain tests; it also improves, albeit by a smaller margin, the BLEU score of the out-of-domain tests.",
            "Experiments\nTo show the challenge of TweetQA for existing approaches, we consider four representative methods as baselines. For data processing, we first remove the URLs in the tweets and then tokenize the QA pairs and tweets using NLTK. This process is consistent for all baselines.",
            "Models for Comparison",
            "Target Task and Setup ::: Language models ::: Baseline LSTM-LM"
        ]
    },
    {
        "title": "Multi-Source Syntactic Neural Machine Translation",
        "answer": "3x",
        "evidence": [
            "Inference Without Parsed Sentences",
            "on one CPU core, using all of the speedups described in Section \"Decoder Speed Improvements\" . BIBREF12 has a similar computational cost to (S7), but we were not able to replicate those results in terms of accuracy. Although we are comparing an ensemble to a single model, we can see ensemble (E1) is over 3x faster to decode than the single model (S7). Additionally, we have found that model (S4) is roughly 3x faster to train than (S7) using the same GPU resources, so (E1) is also 1.5x faster to train than a single model (S7).",
            "shows the predictions of some particular questions by each of the classifiers. The input is a full question and the output is the class of the question.",
            "data. On the other hand, both multi-source systems perform reasonably well without parsed data, although the BLEU scores are worse than multi-source with parsed data.",
            "What Does Zero-shot Transfer Model Learn? ::: Unseen Language Dataset",
            "none of the neural models were able to predict any intervened threads. This is due to the inherent weakness of standard neural models, which are unable to learn features well enough when faced with sparse data. The best performance of UPA indicates that the reply context of the instructor's post INLINEFORM0 correlates strongly with that of the previous post INLINEFORM1 . This is not surprising since normal conversations are typically structured that way.",
            "Proposed model ::: Model Training\nThe model is firstly pre-trained to minimize the maximum-likelihood loss, which is widely used in sequence generation tasks. We define $y^* = \\lbrace y^*_1, \\cdots , y_T^*\\rbrace $ as the ground-truth output sequence for a given input sequence $x$, then the loss function is formulated as: After converging, the model is further optimized with local variance loss and global variance loss. The mix of loss functions is: where $\\lambda _1$ and $\\lambda _2$ are hyper-parameters. -0.13cm",
            "Our results demonstrate that NMT is in fact a suitable choice in low-data settings, and can outperform PBSMT with far less parallel training data than previously claimed. Recently, the main trend in low-resource MT research has been the better exploitation of monolingual and multilingual resources. Our results show that low-resource NMT is very sensitive to hyperparameters such as BPE vocabulary size, word dropout, and others, and by following a set of best practices, we can train competitive NMT systems without relying on auxiliary resources. This has practical relevance for languages where large amounts of monolingual data, or multilingual data involving related languages, are not available. Even though we focused on only using parallel data, our results are also relevant for work on",
            "is too fine-grained, as a single token does not convey enough information that could be encoded as features for a machine learner. However, as discussed in section UID73 , the annotations were performed on data pre-segmented to sentences and annotating tokens was necessary only when the sentence segmentation was wrong or one sentence contained multiple argument components. Our corpus consists of 3899 sentences, from which 2214 sentences (57%) contain no argument component. From the remaining ones, only 50 sentences (1%) have more than one argument component. Although in 19 cases (0.5%) the sentence contains a Claim-Premise pair which is an important distinction from the argumentation perspective, given the overall small number of such occurrences, we simplify the task by treating each",
            "Much recent work on semantic parsing has been evaluated using the WebQuestions dataset BIBREF3 . This dataset is not suitable for evaluating our model because it was filtered to only questions that are mappable to Freebase queries. In contrast, our focus is on language that is not directly mappable to Freebase. We thus use the dataset introduced by Krishnamurthy and Mitchell krishnamurthy-2015-semparse-open-vocabulary, which consists of the ClueWeb09 web corpus along with Google's FACC entity linking of that corpus to Freebase BIBREF9 . For training data, 3 million webpages from this corpus were processed with a CCG parser to produce logical forms BIBREF10 . This produced 2.1m predicate instances involving 142k entity pairs and 184k entities. After removing infrequently-seen predicates",
            "(an all-zero vector INLINEFORM1 or a random vector INLINEFORM2 is assigned for any unknown token INLINEFORM3 , thus INLINEFORM4 or INLINEFORM5 ). While this concatenation approach does not cause significant errors for well-formatted text, we observe that it induces performance degradation for our social media post datasets which contain a significant number of missing tokens. Similarly, naive merging of textual and visual information ( INLINEFORM0 ) yields suboptimal results as each modality is treated equally informative, whereas in our datasets some of the images may contain irrelevant contexts to textual modalities. Hence, ideally there needs a mechanism in which the model can effectively turn the switch on and off the modalities adaptive to each sample. To this end, we propose a",
            "Table II shows the accuracy and F1 score for different classifiers with and without eliminating stop words while extracting features. Figure FIGREF21 shows the average results of different classifiers in a bar chart with and without eliminating stop words from the questions. Overall, SGD has shown the best performance on our dataset as it introduces non-linearity and uses back-propagation for updating parameter weights using loss function calculated on training set into classification. K-NN has shown the weakest performance overall, as this algorithm has a bad reputation of not working well in high dimensional data BIBREF28. MLP and SVM have shown similar performance. MLP takes advantage of multiple hidden layers in order to take non-linearly separable samples in a linearly separable"
        ]
    },
    {
        "title": "Domain Adaptation for Neural Networks by Parameter Augmentation",
        "answer": "210",
        "evidence": [
            "can give a large benefit. We compared the domain adaptation methods by the percentage of correct answers. The source dataset is 40K samples from MS COCO and the target dataset is the TOEIC dataset. We split the TOEIC dataset to 400 samples for training and 210 samples for testing. The percentages of correct answers for each method are summarized in Table 8 . Since the questions have four choices, all methods should perform better than 25%. TgtOnly is close to the baseline because the model is trained with only 400 samples. As in the previous experiments, FineTune and Dual are better than All and Proposed is better than the other methods.",
            "Examples and Attention Visualization",
            "How Many Expert Annotations?",
            "While initial work BIBREF4, BIBREF10 has collected test examples from naturally occurring sentences, this approach suffers from the coverage issue, as syntactically challenging examples are relatively rare. We use the test set compiled by BIBREF0, which consists of synthetic examples (in English) created by a fixed vocabulary and a grammar. This approach allows us to collect varieties of sentences with complex structures. The test set is divided by a necessary syntactic ability. Many are about different patterns of subject-verb agreement, including local (UNKREF8) and non-local ones across a prepositional phrase or a subject/object RC, and coordinated verb phrases (UNKREF9). (UNKREF1) is an example of agreement across an object RC.  The senators smile/*smiles. The senators like to watch",
            "Cross-Domain Classification",
            "we contribute PrivacyQA, a corpus consisting of 1750 questions about the contents of privacy policies, paired with over 3500 expert annotations. The goal of this effort is to kickstart the development of question-answering methods for this domain, to address the (unrealistic) expectation that a large population should be reading many policies per day. In doing so, we identify several understudied challenges to our ability to answer these questions, with broad implications for systems seeking to serve users' information-seeking intent. By releasing this resource, we hope to provide an impetus to develop systems capable of language understanding in this increasingly important domain.",
            "it could be the case that the use of negative examples still does not fully alleviate this factor. Here, to understand the true limitation of the current LSTM architecture, we try to eliminate such other factors as much as possible under a controlled experiment.",
            "the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
            "sample examples of objects from the target class are available during the training phase.” BIBREF25  Note that in the context of authorship verification, target class refers to the known author INLINEFORM0 such that for a document INLINEFORM1 of an unknown author INLINEFORM2 the task is to verify whether INLINEFORM3 holds. One of the most important requirements of any existing AV method is a decision criterion, which aims to accept or reject a questioned authorship. A decision criterion can be expressed through a simple scalar threshold INLINEFORM4 or a more complex model INLINEFORM5 such as a hyperplane in a high-dimensional feature space. As a consequence of the above statements, the determination of INLINEFORM6 or INLINEFORM7 has to be performed solely on the basis of INLINEFORM8 ,",
            "The natural language template approach enables various options to formulate the WinoGrande commonsense reasoning task as a text-to-text problem with T5. Here we adopt a formulation similar to the MNLI template. Consider a concrete example: He never comes to my home, but I always go to his house because the _ is smaller. Option1: home; Option2: house In this case, the correct replacement for _ is Option1. We decompose the above problem into two source–target training examples, where _ is replaced with each option and annotated with the correct answer as the target token, as shown in Table TABREF2. In addition, we reformulate each example into a commonsense reasoning “template” with two statements: hypothesis (from _ to the end of the original problem statement) and premise (the remaining",
            "English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set. Detailed guidelines for both phases are available online at: https://multisimlex.com.",
            "Domain shift is a fundamental problem in machine learning, that has attracted a lot of attention in the natural language processing and vision communities BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . To understand and address this problem, generated by the lack of labeled data in a target domain, researchers have studied the behavior of machine learning methods in cross-domain settings BIBREF2 , BIBREF11 , BIBREF10 and came up with various domain adaptation techniques BIBREF12 , BIBREF5 , BIBREF6 , BIBREF9 . In cross-domain classification, a classifier is trained on data from a source domain and tested on data from a (different) target domain. The accuracy of machine learning methods is usually lower in the cross-domain"
        ]
    },
    {
        "title": "Interactive Machine Comprehension with Information Seeking Agents",
        "answer": "show, hide, ask, answer, stop, next, previous, go, back, forward, reveal, gather, issue, output, interact, exhaust, step, budget, answer, question, determine, observe, learn, IR, MRC, dataset, environment, behavior, study, solve, real-world, problems, search, interactive, information-seeking, modify, dataset, environment, behavior, study, solve, real-world, problems, search, interactive, information-seeking, modify, dataset, environment, behavior, study, solve, real-world, problems, search, interactive, information-seeking, modify, dataset, environment, behavior, study, solve, real-world, problems, search, interactive, information-seeking, modify, dataset, environment, behavior, study, solve, real-world, problems, search, interactive, information-seeking, modify, dataset, environment, behavior, study, solve, real-world, problems, search, interactive, information-seeking, modify, dataset, environment, behavior, study, solve, real-world, problems, search, interactive, information-seeking, modify, dataset, environment, behavior, study, solve, real-world, problems, search, interactive, information-seeking, modify, dataset, environment, behavior, study, solve, real-world,",
        "evidence": [
            "Model Setting",
            "requested by the user. As in BIBREF6 , our focus is on the problem of interpreting navigation directions. We assume that a robot can realize valid navigation plans according to the graph. We contribute a new end-to-end model for following directions in natural language under the behavioral navigation framework. Inspired by the information retrieval and question answering literature BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , we propose to leverage the behavioral graph as a knowledge base to facilitate the interpretation of navigation commands. More specifically, the proposed model takes as input user directions in text form, the behavioral graph of the environment encoded as INLINEFORM0 node; edge; node INLINEFORM1 triplets, and the initial location of the robot in the graph. The model then",
            "including non-embedding-based methods (e.g., pattern-based methods). We can use more than one entity expansion model to find related entities. For instance, general embedding models, such as those built on Wikipedia, might be a good choice in early iterations, whereas more domain-specific models trained on domain-specific corpora might be helpful in later iterations. LUWAK is flexible to change and use more than one Expansion API. This design encourages us to continuously refine the entity expansion module easily.",
            "models into an end-to-end cloud platform that provides both an easy-to-use annotation interface as well as an inference interface that allows users to upload documents and inspect model outputs.",
            "Do models generalize explicit supervision, or just memorize it? ::: Results",
            "special tokens (most pretrained Transformers models have a maximum sequence length they can handle, defined during their pretraining step). Tokenizers can be instantiated from existing configurations available through Transformers originating from the pretrained models or created more generally by the user from user-specifications. Model - All models follow the same hierarchy of abstraction: a base class implements the model's computation graph from encoding (projection on the embedding matrix) through the series of self-attention layers and up to the last layer hidden states. The base class is specific to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of",
            "than showing the entire paragraph $p$, we only show an agent the first sentence $s_1$ and withhold the rest. The agent must issue commands to reveal the hidden sentences progressively and thereby gather the information needed to answer question $q$. An agent decides when to stop interacting and output an answer, but the number of interaction steps is limited. Once an agent has exhausted its step budget, it is forced to answer the question.",
            "Experimental setup ::: Evaluation metrics ::: Information extraction-oriented metrics.",
            "(e.g., in an open domain QA dataset) to determine what text to observe next, with such behavior tantamount to learning to do IR. As mentioned, our idea for reformulating existing MRC datasets as partially observable and interactive environments is straightforward and general. Almost all MRC datasets can be used to study interactive, information-seeking behavior through similar modifications. We hypothesize that such behavior can, in turn, help in solving real-world MRC problems involving search.",
            "training phrase. The domain information is also selected with attention mechanism. DISPLAYFORM0   where INLINEFORM0 is attention distribution of auxiliary DC task, and INLINEFORM1 is the attentive information for DC task, which is fed into the final classifier to predict its domain INLINEFORM2 . Since INLINEFORM0 contains the domain information, we can use it to generate a more flexible query vector DISPLAYFORM0   where INLINEFORM0 is a shared learnable weight matrix and INLINEFORM1 is a task-specific bias vector. When we set INLINEFORM2 , the dynamic query is equivalent to the static one.",
            "These classes automatically instantiate the right configuration, tokenizer or model class instance from the name of the pretrained checkpoints.",
            "Analysis: Question Answering Models"
        ]
    },
    {
        "title": "Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models",
        "answer": "Forward Filtering Backwards Sampling algorithm (FFBS) and Multinomial-Dirichlet posterior.",
        "evidence": [
            "to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text.",
            "LSTM gating signals determine the flow of information. In other words, they indicate how LSTM reads the word sequences and how the information from different parts is captured and combined. LSTM gating signals are rarely analyzed, possibly due to their high dimensionality and complexity. In this work, we consider both the gating signals and their saliency, which is computed as the partial derivative of the score of the final decision with respect to each gating signal. Instead of considering individual dimensions of the gating signals, we aggregate them to consider their norm, both for the signal and for its saliency. Note that ESIM models have two LSTM layers, the first (input) LSTM performs the input encoding and the second (inference) LSTM generates the representation for inference. In",
            "the CNN and RCNN models, they perform slightly better than most of the SVM models but still, the content information is insufficient to achieve a good performance on the Uns posts. As to adding comment information to these models, since the commenters do not always hold the same stance as the author, simply adding comments and post contents together merely adds noise to the model. Among all UTCNN variations, we find that user information is most important, followed by topic and comment information. UTCNN without user information shows results similar to SVMs — it does well for Sup and Neu but detects no Uns. Its best f-scores on both Sup and Neu among all methods show that with enough training data, content-based models can perform well; at the same time, the lack of user information",
            "In this section, we describe the data augmentation methods we use to increase the amount of training data in order to make our NMT systems suffer less from the low-resourced situation in Japanese INLINEFORM0 Vietnamese translation. Although NMT systems can predict and generate the translation of unseen words on their vocabularies, but they only perform this well if the parallel corpus for training are sufficiently large. For many under-resourced languages, unfortunately, it hardly presents. In reality, although the monolingual data of Vietnamese and Japanese are immensely available due to the popularity of their speakers, the bilingual Japanese-Vietnamese corpora are very limited and often in low quality or in narrowly technical domains. Therefore, data augmentation methods to exploit",
            "The HMM training procedure is as follows: Initialization of HMM hidden states: (Discrete HMM) Random multinomial draw for each time step (i.i.d. across time steps). (Continuous HMM) K-means clusters fit on LSTM states, to speed up convergence relative to random initialization. At each iteration: Sample states using Forward Filtering Backwards Sampling algorithm (FFBS, BIBREF7 ). Sample transition parameters from a Multinomial-Dirichlet posterior. Let $n_{ij}$ be the number of transitions from state $i$ to state $j$ . Then the posterior distribution of the $i$ -th row of transition matrix $T$ (corresponding to transitions from state $i$ ) is: $T_i \\sim \\text{Mult}(n_{ij} | T_i) \\text{Dir}(T_i | \\alpha )$  where $\\alpha $ is the Dirichlet hyperparameter. (Continuous HMM) Sample multivariate",
            "it does use any commonsense knowledge. The incremental encoding scheme without MSA obtained the best grammar score and our full mode IE+MSA(GA) has the best logicality score. All the models have fairly good grammar scores (maximum is 2.0), while the logicality scores differ remarkably, much lower than the maximum score, indicating the challenges of this task. More specifically, incremental encoding is effective due to the facts: 1) IE is significantly better than Seq2Seq and HLSTM in grammar (Sign Test, 1.84 vs. $1.74/1.57$ , p-value= $0.046/0.037$ , respectively), and in logicality (1.10 vs. 0.70/0.84, p-value $<0.001/0.001$ ). 2) IE+MSA is significantly better than HLSTM+MSA in logicality (1.26 vs. 1.06, p-value= $0.014$ for GA; 1.24 vs. 1.02, p-value= $0.022$ for CA). This indicates",
            "to access to sentence-level information. The intuition behind the method is that each hidden vector INLINEFORM3 can capture useful information before and including the word INLINEFORM4 . Shortly after, lstm-minusconstituency use the same method on the task of constituency parsing, as the representation of a sentence span, extending the original uni-directional LSTM-Minus to the bi-directional case. More recently, inspired by the success of LSTM-Minus in both dependency and constituency parsing, lstm-minusdiscourse extend the technique to discourse parsing. They propose a two-stage model consisting of an intra-sentential parser and a multi-sentential parser, learning contextually informed representations of constituents with LSTM-Minus, at the sentence and document level, respectively.",
            "Three types of models were used in our system, a feed-forward neural network, an LSTM network and an SVM regressor. The neural nets were inspired by the work of Prayas BIBREF7 in the previous shared task. Different regression algorithms (e.g. AdaBoost, XGBoost) were also tried due to the success of SeerNet BIBREF8 , but our study was not able to reproduce their results for Spanish. For both the LSTM network and the feed-forward network, a parameter search was done for the number of layers, the number of nodes and dropout used. This was done for each subtask, i.e. different tasks can have a different number of layers. All models were implemented using Keras BIBREF9 . After the best parameter settings were found, the results of 10 system runs to produce our predictions were averaged (note",
            "Conclusion\nWe have shown that by exploiting negative examples explicitly, the syntactic abilities of LSTM-LMs greatly improve, demonstrating a new capacity of handling syntax robustly. Given a success of our approach using negative examples, and our final analysis for transferability, which indicates that the negative examples do not have to be complete, one interesting future direction is to extend our approach to automatically inducing negative examples themselves in some way, possibly with orthographic and/or distributional indicators or others.",
            "In this section we outline our pipeline for human communication comprehension: the Multi-attention Recurrent Network (MARN). MARN has two key components: Long-short Term Hybrid Memory and Multi-attention Block. Long-short Term Hybrid Memory (LSTHM) is an extension of the Long-short Term Memory (LSTM) by reformulating the memory component to carry hybrid information. LSTHM is intrinsically designed for multimodal setups and each modality is assigned a unique LSTHM. LSTHM has a hybrid memory that stores view-specific dynamics of its assigned modality and cross-view dynamics related to its assigned modality. The component that discovers cross-view dynamics across different modalities is called the Multi-attention Block (MAB). The MAB first uses information from hidden states of all LSTHMs at",
            "then be fine-tuned for the specific classification task. In sum, we attribute the success of MVCNN to: (i) designing variable-size convolution filters to extract variable-range features of sentences and (ii) exploring the combination of multiple public embedding versions to initialize words in sentences. We also employ two “tricks” to further enhance system performance: mutual learning and pretraining. In remaining parts, Section \"Related Work\" presents related work. Section \"Model Description\" gives details of our classification model. Section \"Model Enhancements\" introduces two tricks that enhance system performance: mutual-learning and pretraining. Section \"Experiments\" reports experimental results. Section \"Conclusion\" concludes this work.",
            "In an ASR system, a language model (LM) takes an important role by incorporating external knowledge into the system. Conventional ASR systems combine an LM with an acoustic model by FST giving a huge performance gain. This trend is also shown in general including hybrid ASR systems and neural network-based sequence-to-sequence ASR systems. The following experiments show a benefit of using a language model in decoding with the previous stage-2 transferred models. Although the performance gains in %CER are also generally observed over all target languages, the improvement in %WER was more distinctive. The results shown in the following Fig. FIGREF27 are in %WER. “whole” in each figure means we used all the available data for the target language as full set explained before.   We used a"
        ]
    },
    {
        "title": "BERT-Based Arabic Social Media Author Profiling",
        "answer": "1,100 users with gender tags, 298,929 tweets for dialect identification. ::: EXTENDED_Gender ::: gold labeled with 15 classes. ::: 162,829 tweets ::: 20,000 tweets for each class. ::: 18,929 tweets. ::: 550 female users ::: 550 male users. ::: BERT. ::: 298,929 tweets ::: 20,000 tweets ::: 18,929 tweets ::: 162,829 tweets ::: 1,100 users ::: 550 male users ::: 550 female users ::: 298,929 tweets ::: 20,000 tweets ::: 18,929 tweets ::: 162,829 tweets ::: 1,100 users ::: 550 male users ::: 550 female users ::: 298,929 tweets ::: 20,000 tweets ::: 18,929 tweets ::: 162,829 tweets ::: 1,100 users ::: 550 male users ::: 550 female users ::: 298,929 tweets ::: 20,000 tweets ::: 18,929 tweets ::: 162,829 tweets ::: 1,100 users ::: 550 male users ::: 550 female users ::: 298,929 tweets ::: 20,000 tweets ::: 18,929 tweets ::: 162,829 tweets ::: 1",
        "evidence": [
            "In-domain and out-of-domain data",
            "break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries, thesauri, WordNet) if required.  4. Annotators are kept anonymous, and are not able to communicate with each other during the annotation process. The selection criteria for the annotators required that all annotators must be native speakers of the target language. Preference to annotators with university education was given, but not required. Annotators were asked to complete a spreadsheet containing the translated pairs of words, as well as the part-of-speech, and a column to enter the score. The annotators did not have access to the original pairs in English. To ensure the quality of the collected ratings, we have employed an adjudication protocol similar to the one",
            "acknowledging the flaws in our data and drawing cautious and reasonable conclusions from them. In all cases, we should report the choices we have made when creating or re-using any dataset.",
            "Datasets Used for the RQE Study",
            "Implementation Details",
            "Data Construction",
            "Methodology ::: Data presentation",
            "To further improve the performance of our models, we introduce in-house labeled data that we use to fine-tune BERT. For the gender classification task, we manually label an in-house dataset of 1,100 users with gender tags, including 550 female users, 550 male users. We obtain 162,829 tweets by crawling the 1,100 users' timelines. We combine this new gender dataset with the gender TRAIN data (from shared task) to obtain an extended dataset, to which we refer as EXTENDED_Gender. For the dialect identification task, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task. In this way, we obtain 298,929 tweets (Sudan only has 18,929 tweets). We combine this new dialect data with the shared task dialect TRAIN data to",
            "Dictionary-based approaches",
            "Previous Work and Evaluation Data",
            "TutorialVQA Dataset ::: Data Collection",
            "Datasets"
        ]
    },
    {
        "title": "WiSeBE: Window-based Sentence Boundary Evaluation",
        "answer": "WiSeBE and standard metrics.",
        "evidence": [
            "Detection systems based on multi-reference (dis)agreement. We evaluate and compare the performance of different SBD systems over a set of Youtube transcripts using WiSeBE and standard metrics. This double evaluation gives an understanding of how WiSeBE is a more reliable metric for the SBD task.",
            "between the medical test questions and the open-domain questions, it also highlights the need for larger medical datasets to support deep learning approaches in dealing with the linguistic complexity of consumer health questions and the challenge of finding correct and complete answers. Another technique was used by ECNU-ICA team BIBREF34 based on learning question similarity via two long short-term memory (LSTM) networks applied to obtain the semantic representations of the questions. To construct a collection of similar question pairs, they searched community question answering sites such as Yahoo! and Answers.com. In contrast, the ECNU-ICA system achieved the best performance of 1.895 in the open-domain task but an average score of only 0.402 in the medical task. As the ECNU-ICA",
            "Results and Discussions",
            "from the epoch corresponding to the the best dev set AP is chosen. Several other optimizers—Adagrad BIBREF34 , Adadelta BIBREF35 , and Adam BIBREF36 —were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum.",
            "To boost performance, the SVM, LSTM, and feed-forward models were combined into an ensemble. For both the LSTM and feed-forward approach, three different models were trained. The first model was trained on the training data (regular), the second model was trained on both the training and translated training data (translated) and the third one was trained on both the training data and the semi-supervised data (silver). Due to the nature of the SVM algorithm, semi-supervised learning does not help, so only the regular and translated model were trained in this case. This results in 8 different models per subtask. Note that for the valence tasks no silver training data was obtained, meaning that for those tasks the semi-supervised models could not be used. Per task, the LSTM and feed-forward",
            "GLUE and SuperGLUE",
            "Cross-Lingual Evaluation ::: Models in Comparison",
            "Dictionary-based approaches",
            "1.17), the baseline 2 mean score is slightly higher than the baseline 1's: 2.86 (the stdev is 1.27), while SimplerVoice evaluation score is the highest one: 4.82 (the stdev is 0.35) which means the most effective approach to provide users with products' usage. Additionally, a paired-samples t-test was conducted to compare the MOS scores of users' responses among all products using baseline 1 and SimplerVoice system. There was a significant difference in the scores for baseline 1 (Mean = 2.57, Stdev = 1.17) and SimplerVoice (Mean = 4.82, Stdev = 0.35); t= -8.18224, p =1.19747e-07. These results show that there is a statistically significant difference in the MOS means between baseline 1 and SimplerVoice and that SimplerVoice performs more effectively than baseline 1 over different types of",
            "In general, distributed word representations are evaluated using a word similarity task. For instance, WordSim353 2002:PSC:503104.503110, MC BIBREF2 , RG BIBREF3 , and SCWS Huang:2012:IWR:2390524.2390645 have been used to evaluate word similarities in English. Moreover, baker-reichart-korhonen:2014:EMNLP2014 built a verb similarity dataset (VSD) based on WordSim353 because there was no dataset of verbs in the word-similarity task. Recently, SimVerb-3500 was introduced to evaluate human understanding of verb meaning Gerz:2016:EMNLP. It provides human ratings for the similarity of 3,500 verb pairs so that it enables robust evaluation of distributed representation for verbs. However, most of these datasets include English words only. There has been no Japanese dataset for the word-similarity",
            "SBD research has been focused on two different aspects; features and methods. Regarding the features, some work focused on acoustic elements like pauses duration, fundamental frequencies, energy, rate of speech, volume change and speaker turn BIBREF17 , BIBREF18 , BIBREF19 . The other kind of features used in SBD are textual or lexical features. They rely on the transcript content to extract features like bag-of-word, POS tags or word embeddings BIBREF20 , BIBREF18 , BIBREF21 , BIBREF22 , BIBREF15 , BIBREF6 , BIBREF23 . Mixture of acoustic and lexical features have also been explored BIBREF24 , BIBREF25 , BIBREF19 , BIBREF26 , which is advantageous when both audio signal and transcript are available. With respect to the methods used for SBD, they mostly rely on statistical/neural machine",
            "always perform better than the clustering method. We attribute it to that in SVD the most important singular values are taken into account. The corpus - SVD technique outperforms the word2vec algorithm for some corpora. When we do not take into account the 3-feats technique, the corpus-based SVD method yields the highest accuracies for the English Twitter dataset. We show that simple models can outperform more complex models, such as the concatenation of the three subapproaches or the word2vec algorithm. Another interesting finding is that for some cases the accuracy decreases when we utilise the polarity labels, as in the case for the English Twitter dataset. Since the TDK dictionary covers most of the domain-specific vocabulary used in the movie reviews, the dictionary method performs"
        ]
    },
    {
        "title": "Recognizing Musical Entities in User-generated Content",
        "answer": "LSTM, feed-forward, ensemble.",
        "evidence": [
            "Algorithms Used",
            "which takes into account a particular test set and tries to minimize the error rate for those particular test samples. Ifrim et al. BIBREF14 presented a transductive learning approach for text classification based on combining latent variable models for decomposing the topic-word space into topic-concept and concept-word spaces, and explicit knowledge models with named concepts for populating latent variables. Guo et al. BIBREF16 proposed a transductive subspace representation learning method to address domain adaptation for cross-lingual text classification. Zhuang et al. BIBREF3 presented a probabilistic model, by which both the shared and distinct concepts in different domains can be learned by the Expectation-Maximization process which optimizes the data likelihood. In BIBREF33 , an",
            "What is the Model Learning?",
            "Following the recent progress in deep learning, researchers and practitioners of machine learning are recognizing the importance of understanding and interpreting what goes on inside these black box models. Recurrent neural networks have recently revolutionized speech recognition and translation, and these powerful models could be very useful in other applications involving sequential data. However, adoption has been slow in applications such as health care, where practitioners are reluctant to let an opaque expert system make crucial decisions. If we can make the inner workings of RNNs more interpretable, more applications can benefit from their power. There are several aspects of what makes a model or algorithm understandable to humans. One aspect is model complexity or parsimony.",
            "a pretraining would cost approximately 100K USD. Contrary to this trend, the booming research in Machine Learning in general and Natural Language Processing in particular is arguably explained significantly by a strong focus on knowledge sharing and large-scale community efforts resulting in the development of standard libraries, an increased availability of published research code and strong incentives to share state-of-the-art pretrained models. The combination of these factors has lead researchers to reproduce previous results more easily, investigate current approaches and test hypotheses without having to redevelop them first, and focus their efforts on formulating and testing new hypotheses. To bring Transfer Learning methods and large-scale pretrained Transformers back into the",
            "At the time of writing, the authors have been mostly aware of FastBert, FARM, flair BIBREF22, BIBREF33, AllenNLP BIBREF21 and PyText but there are likely more interesting developments to be found, from research and internal projects to production packages.",
            "We will continue investigating other network architectures including transfer learning, as well as creation of a large collection of consumer health questions for training to improve the performance of DL models. Future work also includes exploring integration of a Question Focus Recognition module to enhance candidate question retrieval, and expanding our question-answer collection.",
            "Collectively, the success of large pretrained neural models, both encoder-only BERT-like architectures as well as encoder–decoder architectures like T5, raise interesting questions for the pursuit of commonsense reasoning. Researchers have discovered that previous models perform well on benchmark datasets because they pick up on incidental biases in the dataset that have nothing to do with the task; in contrast, the WinoGrande dataset has devoted considerable effort to reducing such biases, which may allow models to (inadvertently) “cheat” (for example, using simple statistical associations). While it is certainly true that datasets over-estimate the commonsense reasoning capabilities of modern models BIBREF1, there are alternative and complementary explanations as well: It has been a",
            "meaning that for those tasks the semi-supervised models could not be used. Per task, the LSTM and feed-forward model's predictions were averaged over 10 prediction runs. Subsequently, the predictions of all individual models were combined into an average. Finally, models were removed from the ensemble in a stepwise manner if the removal increased the average score. This was done based on their original scores, i.e. starting out by trying to remove the worst individual model and working our way up to the best model. We only consider it an increase in score if the difference is larger than 0.002 (i.e. the difference between 0.716 and 0.718). If at some point the score does not increase and we are therefore unable to remove a model, the process is stopped and our best ensemble of models has",
            "Experimenting with Transformers ::: Language understanding benchmarks",
            "Recently, the transformative potential of machine learning (ML) has propelled ML into the forefront of mainstream media. In Brazil, the use of such technique has been widely diffused gaining more space. Thus, it is used to search for patterns, regularities or even concepts expressed in data sets BIBREF0 , and can be applied as a form of aid in several areas of everyday life. Among the different definitions, ML can be seen as the ability to improve performance in accomplishing a task through the experience BIBREF1 . Thus, BIBREF2 presents this as a method of inferences of functions or hypotheses capable of solving a problem algorithmically from data representing instances of the problem. This is an important way to solve different types of problems that permeate computer science and other",
            "several machine learning algorithms to perform entity recognition combining task-specific and corpus-based features. We also show how to improve recognition results by jointly considering formal and user-generated content"
        ]
    },
    {
        "title": "Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER",
        "answer": "NER",
        "evidence": [
            "Discussion\nFor many of the languages examined, we were able to improve on BERT's zero-resource cross-lingual performance on the MLDoc classification and CoNLL NER tasks. Language-adversarial training was generally effective, though the size of the effect appears to depend on the task. We observed that adversarial training moves the embeddings of English text and their non-English translations closer together, which may explain why it improves cross-lingual performance. Future directions include adding the language-adversarial task during BERT pre-training on the multilingual Wikipedia corpus, which may further improve zero-resource performance, and finding better stopping criteria for zero-resource cross-lingual tasks besides using the English dev set.",
            "Text Classification",
            "Named Entity Recognition (NER), or alternatively Named Entity Recognition and Classification (NERC), is the task of detecting entities in an input text and to assign them to a specific class. It starts to be defined in the early '80, and over the years several approaches have been proposed BIBREF2 . Early systems were based on handcrafted rule-based algorithms, while recently several contributions by Machine Learning scientists have helped in integrating probabilistic models into NER systems. In particular, new developments in neural architectures have become an important resource for this task. Their main advantages are that they do not need language-specific knowledge resources BIBREF3 , and they are robust to the noisy and short nature of social media messages BIBREF4 . Indeed,",
            "Experiments on Additional Losses ::: Which additional loss works better?",
            "In this paper, we examine the use case of general adversarial networks (GANs) in the field of marketing. In particular, we analyze how GAN models can replicate text patterns from successful product listings on Airbnb, a peer-to-peer online market for short-term apartment rentals. To do so, we define the Diehl-Martinez-Kamalu (DMK) loss function as a new class of functions that forces the model's generated output to include a set of user-defined keywords. This allows the general adversarial network to recommend a way of rewording the phrasing of a listing description to increase the likelihood that it is booked. Although we tailor our analysis to Airbnb data, we believe this framework establishes a more general model for how generative algorithms can be used to produce text samples for the",
            "The performances of the NER experiments are reported separately for three different parts of the system proposed. Table 6 presents the comparison of the various methods while performing NER on the bot-generated corpora and the user-generated corpora. Results shown that, in the first case, in the training set the F1 score is always greater than 97%, with a maximum of 99.65%. With both test sets performances decrease, varying between 94-97%. In the case of UGC, comparing the F1 score we can observe how performances significantly decrease. It can be considered a natural consequence of the complex nature of the users' informal language in comparison to the structured message created by the bot. In Table 7, results of the schedule matching are reported. We can observe how the quality of the",
            "topic, and comment information. We attempt to learn user and topic representations which encode user interactions and topic influences to further enhance text classification, and we also incorporate comment information. We evaluate this model on a post stance classification task on forum-style social media platforms. The contributions of this paper are as follows: 1. We propose UTCNN, a neural network for text in modern social media channels as well as legacy social media, forums, and message boards — anywhere that reveals users, their tastes, as well as their replies to posts. 2. When classifying social media post stances, we leverage users, including authors and likers. User embeddings can be generated even for users who have never posted anything. 3. We incorporate a topic model to",
            "Bi-LSTM + CRF for Multimodal NER",
            "compete against. We here explore whether existing NLP systems can answer the questions \"What sentiment?\" and \"Towards whom?\" accurately for the dataset of political tweets provided by BIBREF2. In our analysis, we include NLP tools with publicly-available APIs, even if the tools were not specifically designed for short texts like tweets, and, in particular, political tweets. Our experiments reveal that the task of entity-level sentiment analysis is difficult for existing tools to answer accurately while the recognition of the entity, here, which politician, was easier.",
            "positive and negative non-ironies. Then we use the positive and negative non-ironies to train the sentiment classifier for non-irony.",
            "data $D$. The unlikelihood loss strengthens the signal to penalize undesirable words in a context by explicitly reducing the likelihood of negative tokens $x_i^*$. This is more direct learning signal than the binary classification loss.",
            "and nicho appears exclusively in nine sci.space training documents). On the contrary, the neural network model seems less affected by word counts regularities and systematically attributes the highest relevances to words semantically related to the considered target class. These results demonstrate that, subjectively, the neural network is better suited to identify relevant words in text documents than the BoW/SVM model."
        ]
    }
]