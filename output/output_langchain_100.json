[
    {
        "title": "Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations",
        "answer": "by incorporating the logic rules to train the aggregator, instead of depending solely on neural networks to learn from the data.  by aligning the second-order statistics of source and target distributions, without requiring any target labels.  by jointly optimizing the representation, the cross-domain transformation and the target label inference in an end-to-end fashion.  by minimizing the domain shift.  by incorporating class descriptions, class hierarchy, and the word-to-label paths in ConceptNet.  by combining with unlabeled data to yield new label oriented classifiers through posterior regularization.  by using a parallel neural network architecture.  by incorporating the logic rules to train the aggregator.  by jointly optimizing the representation, the cross-domain transformation and the target label inference in an end-to-end fashion.  by incorporating class descriptions, class hierarchy, and the word-to-label paths in ConceptNet.  by combining with unlabeled data to yield new label oriented classifiers through posterior regularization.  by using a parallel neural network architecture.  by incorporating the logic rules to train the aggregator.  by jointly optimizing the representation, the cross-domain transformation and the target label inference in an end-to-end fashion.  by incorporating class descriptions, class hierarchy, and the word-to-label paths in ConceptNet.  by combining with unlabeled",
        "evidence": [
            "ches of unlabeled data. Labeled data are input into the model to calculate the standard supervised loss for each mini-batch and the model weights are updated regularly. Meanwhile, each mini-batch of unlabeled data is selected randomly from the pool of all unlabeled data; the model computes the loss for CVT from the mini-batch of unlabeled data. This CVT loss is used to train auxiliary prediction modules, which see restricted views of the input, to match",
            " public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth. Besides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human experts.",
            " for single topic datasets, adding that latent topics still benefits performance: even when we are discussing the same topic, we use different arguments and supporting evidence. Lastly, we get 4.8% improvement when adding comment information and it achieves comparable performance to UTCNN without topic information, which shows that comments also benefit performance. For platforms where user IDs are pixelated or otherwise hidden, adding comments to a text model still improves performance. In its integration of user, content, and comment information, the full UTC",
            " better vector representation of words and thus will facilitate deep learning based automatic feature extraction.\n\n\n",
            " For example, the predicate $\\textit {head\\_honcho\\_N/N}$ was never seen in the training data, so it is replaced with $\\textit {unknown}$ ; however, it would be better to replace it with $\\textit {honcho\\_N/N}$ , which was seen in the training data. Finally, although using connected entities in Freebase as additional candidates during inference is helpful, it often over- or under-generates",
            " tab:thairesult,tab:engresult (row (g) vs row (h)). This experiment was conducted only on the UGWC dataset because no unlabeled data are available in the Orchid dataset, as mentioned in Section UID33 . The model improves the F1 score slightly, from 88.8% (row (g)) to 88.9% (row (h)) on the UGWC dataset. This result occurs because both the labeled and unlabeled data",
            " by significant margins. These results demonstrate the effectiveness of logic rules in assigning meaningful weights to the neighbors. Specifically, in order to generate representations for unseen entities, it is crucial to incorporate the logic rules to train the aggregator, instead of depending solely on neural networks to learn from the data. By combining the logic rules and neural networks, LAN takes a step further in outperforming all the other models. To find out whether the superiority of LAN to the baselines can generalize to other scoring functions,",
            " the source domain. In BIBREF7 , deep learning is employed to jointly optimize the representation, the cross-domain transformation and the target label inference in an end-to-end fashion. More recently, Sun et al. BIBREF8 proposed an unsupervised domain adaptation method that minimizes the domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Chang et al. BIBREF9 proposed a framework based on using a parallel",
            " $\\textsc {0shot-tc}$. JiangqngGuo enrich the embedding representations by incorporating class descriptions, class hierarchy, and the word-to-label paths in ConceptNet. DBLPMitchellSL18 assume that some natural language explanations about new labels are available. Then those explanations are parsed into formal constraints which are further combined with unlabeled data to yield new label oriented classifiers through posterior regularization. However, those explanatory statements about new labels are collected from crowd",
            " the model with different language tokens, as is seen in Table TABREF30 . Impressively, this even works when the source sequence is in the wrong script for the language, as is seen in the entry for Arabic.\n\n\nLanguage Embeddings\nBecause these language ID tokens are so useful, it would be good if they could be effectively estimated for unseen languages. ostling2017continuous found that the language vectors their models learned correlated well to genetic relationships, so it would be interesting",
            " pretrained entailment models work in this order for label-fully-unseen case: RTE $>$ FEVER $>$MNLI; on the contrary, if we fine-tune them on the label-partially-unseen case, the MNLI-based model performs best. This could be due to a possibility that, on one hand, the constructed situation entailment dataset is closer to the RTE dataset than to the MNLI dataset, so an RTE-",
            " Majority performs majority voting for classification tasks, and predicts the expected label for regression tasks. This baseline is useful as a lower bound of model performance. Human performance is calculated for CMU-MOSI dataset which offers per annotator results. This is the accuracy of human performance in a one-vs-rest classification/regression. Finally, MARN indicates our proposed model. Additionally, the modified baseline MARN (no MAB) removes the MAB and learns no dense cross-view dynamics"
        ]
    },
    {
        "title": "Artificial Error Generation with Machine Translation and Syntactic Patterns",
        "answer": "English, French, Spanish, Mandarin Chinese, Finnish, Portuguese, German, Italian, Russian, Hebrew, Croatian, Luxembourgish, Bavarian, Yiddish, Japanese, Italian, Russian, Hebrew, Croatian.",
        "evidence": [
            " eight different language families. This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili. We hope to further include additional languages and inspire other researchers to contribute to the effort over the lifetime of this project. The work on data collection can be divided into two crucial phases: 1) a translation phase where the extended English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages",
            " in an analogous way to how language ID is tagged in multilingual g2p.\n\n\n",
            " to control the topic influence in the documents. As one next step, we will compile additional and larger corpora to investigate the question whether the evaluation results of this paper hold more generally. Furthermore, we will address the important question how the results of AV methods can be interpreted in a more systematic manner, which will further influence the practicability of AV methods besides the proposed properties. This work was supported by the German Federal Ministry of Education and Research (BMBF) under the project \"DOR",
            " that questions are linguistically simple, elaborate and explicit. Workers were asked to formulate questions about details of such a situation, i.e. independent of a concrete narrative. This resulted in questions, the answer to which is not literally mentioned in the text. To cover a broad range of question types, we asked participants to write 3 temporal questions (asking about time points and event order), 3 content questions (asking about persons or details in the scenario) and 3 reasoning questions (asking how or why something",
            " resources are available. In both papers, NMT systems are trained with hyperparameters that are typical for high-resource settings, and the authors did not tune hyperparameters, or change network architectures, to optimize NMT for low-resource conditions.\n\n\nImproving Low-Resource Neural Machine Translation\nThe bulk of research on low-resource NMT has focused on exploiting monolingual data, or parallel data involving other language pairs. Methods to improve NMT with monolingual data range",
            "Acknowledgments\nWe would like to acknowledge the support of CHIST-ERA for funding this work through the Access Multilingual Information opinionS (AMIS), (France - Europe) project. We also like to acknowledge the support given by the Prof. Hanifa Boucheneb from VERIFORM Laboratory (École Polytechnique de Montréal).\n\n\n",
            " at least one noun is present in both segments, they remain independent. If there is no verb-nominal form, the segments remain independent.\n\n\nExperiments\nIn this first exploratory work, only documents in French were considered, but the system can be adapted to other languages. The evaluation is based on the correspondence of word pairs representing a border. In this way we compare the Annodis segmentation with the automatically produced segmentation. For each pair of reference segments, a $L",
            " terms is still low.\n\n\nFinal Remarks\nThis paper presents an analysis of the presence of gender bias in Portuguese word embeddings. Even though it is a work in progress, the proposal showed promising results in analyzing predicting models. A possible extension of the work involves deepening the analysis of the results obtained, seeking to achieve higher accuracy rates and fairer models to be used in machine learning techniques. Thus, these studies can involve tests with different methods of pre-processing the data to the use",
            " vectors their models learned correlated well to genetic relationships, so it would be interesting to see if the embeddings our source encoder learned for the language ID tokens showed anything similar. In a few cases they do (the languages closest to German in the vector space are Luxembourgish, Bavarian, and Yiddish, all close relatives). However, for the most part the structure of these vectors is not interpretable. Therefore, it would be difficult to estimate the embedding for an unseen language,",
            " similarity of 3,500 verb pairs so that it enables robust evaluation of distributed representation for verbs. However, most of these datasets include English words only. There has been no Japanese dataset for the word-similarity task. Apart from English, WordSim353 and SimLex-999 Hill:2015:CL have been translated and rescored in other languages: German, Italian and Russian Leviant:2015:arXiv. SimLex-999 has also been translated and rescored in Hebrew and Croatian",
            " complicates the reproduction of the experiments in other languages. Consequently, the development of multilingual systems using discursive analysis are yet to be developed. Diverse applications based on the latest technologies require at least one of the three steps mentioned above BIBREF1, BIBREF2, BIBREF3. In this context, the idea of exploring the architecture of a generic system that is able not only of segmenting a text correctly but also of adapting it to any language, was a great motivation",
            ", the representations of each concept are computed as averages over the last $H=4$ hidden layers in all experiments, as suggested by Wu:2019arxiv. Besides m-bert and xlm, covering multiple languages, we also analyze the performance of “language-specific” bert and xlm models for the languages where they are available: Finnish, Spanish, English, Mandarin Chinese, and French. The main goal of this comparison is to study the differences in performance"
        ]
    },
    {
        "title": "Generating Word and Document Embeddings for Sentiment Analysis",
        "answer": "Supervised scores are calculated by assigning +1 or -1 to the words based on their dictionary definitions. However, when the supervised score is utilised, words of opposite polarities get far away from each other as they are translated across coordinate regions. Positive words appear in quad 1, negative words in quad 3, and neutral words in quad 2. The supervised scores are then used to calculate the RP scores as relevance indicator.  The supervised scores are also used to delete a sequence of words from each document, re-classify the documents with “missing words”, and report the classification accuracy as a function of the number of deleted words.  The word-level relevances are computed on the original documents (with no words deleted).  The supervised scores are also used to train a word aligner based on IBM Model 2 and to set the fertilities of all source words to a fixed integer value.  The supervised scores are also used to train a separate fertility predictor.  The supervised scores are also used to calculate the RP scores as relevance indicator.  The supervised scores are also used to delete a sequence of words from each document, re-classify the documents with “missing words”, and report the classification accuracy as a function of the number of deleted words.",
        "evidence": [
            "arity scores of the reviews as explained in Section 3.5. As can be seen, at least one of our methods outperforms the baseline word2vec approach for all the Turkish and English corpora, and all categories. All of our approaches performed better when we used the supervised scores, which are extracted on a review basis, and concatenated them to word vectors. Mostly, the supervised 4-scores feature leads to the highest accuracies, since it employs the annotational information concerned with",
            "  where $\\sigma $ is the sigmoid function and $\\odot $ is the element-wise product. Note that all the vectors in the equations have the same dimension $n$ , called the cell size. The probability of the output word at the $t$ -th step, $y_t$ , is computed by  $$p(y_t|y_1,\\ldots ,y_{t-1},x) = {\\rm Softmax}(",
            " over 90k participants. The choice of supervised learning model is often guided by the task definition and the label types. For example, to identify stance towards rumors based on sequential annotations, an algorithm for learning from sequential BIBREF43 or time series data BIBREF44 could be used. The features (sometimes called variables or predictors) are used by the model to make the predictions. They may vary from content-based features such as single words, sequences of words, or information about their synt",
            "SLT dataset, from an overall F1 score of 64.5% (row (g)) to 65.3% (row (h)) and from a 2-class F1 score of 81.7% to 82.7%. Because both the labeled and unlabeled data were collected from TED talks, the number of vocabulary words grows substantially more than in the UGWC dataset because the talks cover various topics. In this dataset, average 1.225 new words found in each new",
            " query word overlap) to learn the relevance ranking, and query-independent features (e.g., term frequency) to learn the saliency ranking. Then, the two types of features are combined to train an overall ranking model. Note that the only supervision available is the reference summaries. Humans write summaries with the trade-off between relevance and saliency. Some salient content may not appear in reference summaries if it fails to respond to the query. Likewise, the content relevant to the query",
            " could be interested in how the respondent rates the experience with the program. Sentiment analysis is a common tool for such a task. It is based on dictionaries of words that are associated with positive or negative emotions. The sentiment of a document such as an open-ended question would then be based on relative word counts and the sentiment scores with which these words are associated. BIBREF15 find positive and negative keywords and count their frequency in Chinese newspaper articles that mention the United States. With this data",
            " obtained through latent semantic analysis (LSA) used to factorize the feature matrices, but two recently developed models have been introduced which rely on different logic: word2vec from Mikolov and the research group at Google, and the global vectors for word representations (GloVe) unsupervised learning algorithm from the Stanford Natural Language Processing group. We use GloVe introduced by BIBREF15 because while LSA tends to maximize the statistical information used, it does not perform well on",
            " sentimentally. In the corpus-based approach, we capture both of these characteristics and generate word embeddings specific to a domain. Firstly, we construct a matrix whose entries correspond to the number of cooccurrences of the row and column words in sliding windows. Diagonal entries are assigned the number of sliding windows that the corresponding row word appears in the whole corpus. We then normalise each row by dividing entries in the row by the maximum score in it. Secondly, we perform the principal",
            " the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity.  2. Each annotator must score the entire set of 1,888 pairs in the dataset. The pairs must not be shared between different annotators.  3. Annotators are able to break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries",
            ", the words of opposite polarities are found to be close to each other, since they have common words in their dictionary definitions. Only the information concerned with the dictionary definitions are used there, discarding the polarity scores. However, when we utilise the supervised score (+1 or -1), words of opposite polarities (e.g. “happy\" and “unhappy\") get far away from each other as they are translated across coordinate regions. Positive words now appear in quad",
            "RP scores as relevance indicator. More specifically, in accordance to the word-level relevances we delete a sequence of words from each document, re-classify the documents with “missing words”, and report the classification accuracy as a function of the number of deleted words. Hereby the word-level relevances are computed on the original documents (with no words deleted). For the deleting experiments, we consider only 20Newsgroups test documents that have a length greater or equal to",
            ", guided, and predicted. With constant, we set the fertilities of all source words to a fixed integer value INLINEFORM0 . With guided, we train a word aligner based on IBM Model 2 (we used fast_align in our experiments, BIBREF17 ) and, for each word in the vocabulary, we set the fertilities to the maximal observed value in the training data (or 1 if no alignment was observed). With the predicted strategy, we train a separate fertility predictor"
        ]
    },
    {
        "title": "Using General Adversarial Networks for Marketing: A Case Study of Airbnb",
        "answer": "000 Manhattan listings",
        "evidence": [
            "000 Manhattan listings that were posted on Airbnb during this defined time period. For each listing, we were given information of the amenities of the listing (number of bathrooms, number of bedrooms …), the listing’s zip code, the host’s description of the listing, the price of the listing, and the occupancy rate of the listing. Airbnb defines a home's occupancy rate, as the percentage of time that a listing is occupied over the time period that the listing is available. This gives",
            "ı Aydın is supported by TÜBİTAK BIDEB 2211E.\n\n\n",
            " the distribution over first words of questions posed by crowdworkers. We also observe low redundancy in the questions posed by crowdworkers over each policy, with each policy receiving ~49.94 unique questions despite crowdworkers independently posing questions. Questions are on average 8.4 words long. As declining to answer a question can be a legally sound response but is seldom practically useful, answers to questions where a minority of experts abstain to answer are filtered from the dataset. Privacy policies are ~3000 words long on average",
            " Table TABREF77 a) and articles + blog posts ( INLINEFORM2 0.09, Table TABREF77 b) in the logos dimension. If we break down the value with respect to the individual argument components, the agreement on claim and premise is substantial in the case of comments and forum posts (0.59 and 0.69, respectively). By contrast, these argument components were annotated only with a fair agreement in articles and blog posts (0.22 and 0.",
            " set of Amazon customer reviews, which in many ways parallels the tone and semantic structure of Airbnb listing descriptions. More generally, Bowman et al. demonstrate how the use of variational autoencoders presents a more accurate model for text generation, as compared to standard recurrent neural network language models [1]. For future work, we may also consider experimenting with different forms of word embeddings, such as improved word vectors (IWV) which were suggested by Rezaeinia et al [10].",
            "known peer-to-peer markets like Uber and Instacart use a centralized system that matches workers with assigned tasks via a series of complex algorithms [4]. Still, a number of other websites like Airbnb and eBay rely on sellers and buyers to organically find one-another in a decentralized fashion. In the case of these decentralized systems, sellers are asked to price and market their products in order to attract potential buyers. Without a large marketing team at their disposal, however, sellers most often rely on",
            "essions. In 10 sessions, single passenger was present (i.e., singletons), whereas the remaining 10 sessions include two passengers (i.e., dyads) interacting with the vehicle. The data is collected \"in the wild\" on the streets of Richmond, British Columbia, Canada. Each ride lasted about 1 hour or more. The vehicle is modified to hide the operator and the human acting as in-cabin agent from the passengers, using a variation of WoZ approach BIB",
            " GloVe vectors (840B token version, BIBREF4 ). The other parameters are initialized by randomly sampling from uniform distribution in [-0.1, 0.1]. The mini-batch size is set to 32. For each task, we take hyperparameters which achieve the best performance on the development set via a small grid search. We use ADAM optimizer BIBREF21 with the learning rate of INLINEFORM0 . The BiLSTM models have 200 dimensions in",
            " filings, we tried to normalize numbers that were written in a mixture of Arabic numerals and Chinese units (e.g., “UTF8gbsn亿”, the unit for $10^8$) and discarded partial results if simple rule-based rewrites were not successful. In the property lease agreements, the contract length, if not directly extracted by BERT, is computed from the extracted start and end dates. Note that these post processing steps were not applied",
            " by the UK Engineering and Physical Sciences Research Council (Grant number: EP/P011829/1).\n\n\n",
            ",193,514 word embeddings; training corpus: a Twitter corpus of 2B tweets with 27B tokens. (iv) SENNA. Size: 130,000 word embeddings; training corpus: Wikipedia. Note that we use their 50-dimensional embeddings. (v) Word2Vec. It has no 50-dimensional embeddings available online. We use released code to train skip-gram on English Gigaword Corpus BIBREF26 with setup: window",
            " among other possible labels. Instead we focus solely on API arguments for each type of conversation, meaning just the variables required to execute the transaction. For example, in dialogs about setting up UBER rides, we label the “to\" and “from\" locations along with the car type (UberX, XL, Pool, etc). For movie tickets, we label the movie name, theater, time, number of tickets, and sometimes screening type (e.g. 3D vs."
        ]
    },
    {
        "title": "Question Answering for Privacy Policies: Combining Computational and Legal Perspectives",
        "answer": "Yes",
        "evidence": [
            " baseline models as compared to the human upper bound and a random baseline. As can be seen, neural models have a clear advantage over the pure word overlap baseline, which performs worst, with an accuracy of INLINEFORM0 . The low accuracy is mostly due to the nature of correct answers in our data: Each correct answer has a low overlap with the text by design. Since the overlap model selects the answer with a high overlap to the text, it does not perform well. In particular, this also",
            " test set created by Krishnamurthy and Mitchell, which contains 220 queries generated in the same fashion as the training data from a separate section of ClueWeb. However, as they did not release a development set with their data, we used this set as a development set. For a final evaluation, we generated another, similar test set from a different held out section of ClueWeb, in the same fashion as done by Krishnamurthy and Mitchell. This final test set contains 307 queries",
            " depressive symptoms and no evidence of depression (both 15th percentile) suggesting fewer features are needed to reach optimal performance. In contrast, peak F1-score performances occurred at moderate percentiles for evidence of depression (30th percentile) and depressed mood (55th percentile) suggesting that more features are needed to reach optimal performance. However, one notable difference between these two classes is the dramatic F1-score improvements for depressed mood i.e., 20 point increase from the 1st percentile to the 20",
            " (score=2.0) from IE+MSA is much larger than that from IE (45.0%+5.0%/41.0%+4.0% vs. 36.0%+2.0% for GA/CA, respectively), and also remarkable larger than those from other baselines. Further, HLSTM equipped with MSA is better than those without MSA, indicating that commonsense knowledge is helpful. And the kappa measuring inter-",
            " previous shared task. Different regression algorithms (e.g. AdaBoost, XGBoost) were also tried due to the success of SeerNet BIBREF8 , but our study was not able to reproduce their results for Spanish. For both the LSTM network and the feed-forward network, a parameter search was done for the number of layers, the number of nodes and dropout used. This was done for each subtask, i.e. different tasks can have a",
            "arsest-level category. We also added entries for punctuations when the source lexicons did not contain any. We also performed experiments in which we retained the full original tags provided by the lexicons, with all morphological features included. On average, results were slightly better than those presented in the paper, although not statistically significantly. Moreover, the granularity of tag inventories in the lexicons is diverse, which makes it difficult to draw general conclusions about results based on full tags. This is",
            " of negative polarity. $N^{\\prime }_{t}$ and $N^{\\prime }$ denote similar values for the negative polarity corpus. We perform normalisation to prevent the imbalance problem and add a small number to both numerator and denominator for smoothing. As an alternative to multiplying with the supervised polarity scores, we also separately multiplied all the row scores with only +1 if the row word is a positive word, and with -1 if it is a negative",
            " of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as",
            " experiments, we first verify the efficiency of NCEL via theoretically comparing its time complexity with other collective alternatives. Afterwards, we train our neural model using collected Wikipedia hyperlinks instead of dataset-specific annotations, and perform evaluations on five public available benchmarks. The results show that NCEL consistently outperforms various baselines with a favorable generalization ability. Finally, we further present the performance on a challenging dataset WW BIBREF19 as well as qualitative results, investigating the effectiveness of each key module.\n\n\n",
            " knowledge of the products while baseline 2 might provide additional information by showing top images from search engines. With the baseline 2, we attempt to measure whether merely adding \"relevant\" or \"similar\" products' images would be sufficient to improve the end-users' ability to comprehend the product's intended use. Moreover, with SimplerVoice, we test if our system could provide users with the proper visual components to help them understand the products' usage based on the proposed techniques, and measure the usefulness of Sim",
            "-tuned BERT. Our full comparison including the BiDAF performance and evaluation on more metrics can be found in Appendix SECREF7 . Following previous RC research, we also include analysis on automatically-labeled question types in Appendix SECREF8 . As indicated by the results on METEOR and ROUGE-L (also indicated by a third metric, BLEU-1, as shown in Appendix SECREF7 ), both baselines perform worse on questions that require the understanding deep",
            " Table 6 presents the comparison of the various methods while performing NER on the bot-generated corpora and the user-generated corpora. Results shown that, in the first case, in the training set the F1 score is always greater than 97%, with a maximum of 99.65%. With both test sets performances decrease, varying between 94-97%. In the case of UGC, comparing the F1 score we can observe how performances significantly decrease. It can be considered a natural consequence of"
        ]
    },
    {
        "title": "Dynamic Compositional Neural Networks over Tree Structure",
        "answer": "food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering, food domain captioning, age, dialect, and gender, science question answering,",
        "evidence": [
            " and then asked to conjecture questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment to many users, we maintain a consistent set of video segments",
            " thank Dinesh Jayaraman, Yu-Chuan Su, Suyog Jain, and Chao-Yeh Chen for their assistance with experiments.\n\n\n",
            " and playful manner. Built by the authors on top of Transformers, Write with Transformer is an interactive interface that leverages the generative capabilities of pretrained architectures like GPT, GPT2 and XLNet to suggest text like an auto-completion plugin. Generating samples is also often used to qualitatively (and subjectively) evaluate the generation quality of language models BIBREF9. Given the impact of the decoding algorithm (top-K sampling, beam-search, etc.) on",
            " these tasks can be facilitated. Other machine translation approaches can also be tested, including the concatenation of this corpus with other multi-domain ones.\n\n\n",
            " are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for",
            "essions. In 10 sessions, single passenger was present (i.e., singletons), whereas the remaining 10 sessions include two passengers (i.e., dyads) interacting with the vehicle. The data is collected \"in the wild\" on the streets of Richmond, British Columbia, Canada. Each ride lasted about 1 hour or more. The vehicle is modified to hide the operator and the human acting as in-cabin agent from the passengers, using a variation of WoZ approach BIB",
            " limiting. We are actively exploring applying these techniques to science question answering BIBREF26 , for example, where existing KBs provide only partial coverage of the questions.\n\n\n",
            " Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia, Sudan, Tunisia, UAE, Yemen}. The gender task involves binary labels from the set {male, female}.\n\n\nExperiments\nAs explained earlier, the shared task is set up at the user level where the age, dialect, and gender of each user are the required predictions. In our experiments, we first model the task at the tweet level and then port these predictions at the user level. For our core modelling",
            " controls, the WinoGrande challenge still contains incidental biases that these more sophisticated models can exploit, or that we are genuinely making at least some progress in commonsense reasoning. The latter, in particular, challenges the notion that commonsense knowledge is (mostly) tacit. Perhaps it is the case that in a humongous corpus of natural language text, someone really has written about trying to stuff a tuba in a backpack?\n\n\nAcknowledgments\nThis research was supported in part by the Canada",
            " in NLP.\n\n\nAdaptation to food domain captioning\nThis experiment highlights a typical scenario in which domain adaptation is useful. Suppose that we have a large dataset of captioned images, which are taken from daily lives, but we would like to generate high quality captions for more specialized domain images such as minor sports and exotic food. However, captioned images for those domains are quite limited due to the annotation cost. We use domain adaptation methods to improve the captions of the target domain",
            " to create self-dialogs, this time as a written exercise. In this case, users are asked to pretend they have a personal assistant who can help them take care of various tasks in real time. They are told to imagine a scenario in which they are speaking to their assistant on the phone while the assistant accesses the services for one of the given tasks. They then write down the entire conversation. Figure FIGREF34 shows a sample set of instructions.\n\n\nThe Taskmaster Corpus :::",
            " . There are two clear differences between their work and ours. One is that trolling is concerned about not only abusive language but also a much larger range of language styles and addresses the intentions and interpretations of the commenters, which goes beyond the linguistic dimension. The other is that we are additionally interested in the reactions to trolling attempts, real or perceived, because we argued that this is a phenomenon that occurs in pairs through the interaction of at least two individuals, which is different from abusive language detection. Also, x"
        ]
    },
    {
        "title": "Automatic Discourse Segmentation: an evaluation in French",
        "answer": "F1 score.",
        "evidence": [
            "fold: to design a discursive segmenter using a minimum of resources and to establish an evaluation protocol to measure the performance of segmenters. The results show that we can build a simple version of the baseline, which employs only a list of markers and presents a very encouraging performance. Of course, the quality of the list is a preponderant factor for a correct segmentation. We have studied the impact of the marker which, even though it may seem fringe-worthy, contributes to improving the performance",
            " performance of each punctuation restoration model in a manner similar to sentence segmentation, the 2-class F1 score is calculated to measure model accuracy, as shown in f12class. The calculation of this metric is the same as that used in BIBREF35 . The metric considers only where the punctuation position is and ignores the type of restored punctuation. Therefore, this measure is similar to the metric sentence boundary F1, which only considers the position of the missing punctuation. INLINE",
            " to study the effectiveness of RQE for Medical Question Answering, by comparing the answers retrieved by the hybrid entailment-based approach, the IR method and the other QA systems participating to the medical task at TREC 2017 LiveQA challenge (LiveQA-Med).\n\n\nEvaluation Method\nWe developed an interface to perform the manual evaluation of the retrieved answers. Figure 5 presents the evaluation interface showing, for each test question, the top-10 answers of the evaluated",
            " the one using an embedded CNN on the Orchid and UGWC datasets.\n\n\nStatistical Tests for Thai sentence segmentation\nTo prove the significance of the model improvements, we compared the cross-validation results using paired t-tests to obtain the p-values, which are shown in Table TABREF57 for the Orchid dataset and Table TABREF58 for the UGWC dataset.\n\n\n",
            " The number of words per transcript oscilate between 271 and 1,602 with a total number of 8,080. We gave clear instructions to three evaluators ( INLINEFORM0 ) of how segmentation was needed to be perform, including the SU concept and how punctuation marks were going to be taken into account. Periods (.), question marks (?), exclamation marks (!) and semicolons (;) were considered SU delimiters (boundaries) while colons",
            " validated the alignment quality for 400 sentences randomly selected from the parsed corpus and assigned quality labels according Section SECREF9 . From all the evaluated sentences, 82.30% were correctly aligned, while 13.33% were partially aligned, and 4.35% presented no alignment. The small percentage of no alignment is probably due to the use of Hunalign tool with the provided EN/PT dictionary. Regarding the partial alignment, most of the problems are result of segmentation issues previous to the alignment, which",
            " du Vaucluse, and the right segment (SD), est un département du sud de la France.\n\n\nDescription of segmentation strategies ::: Segmentation with explicit use of a marker and POS labels\nThe Segmenter$_{\\mu +}$ system presents an improvement to the Segmenter$_{\\mu }$: inclusion of grammar categories with the TreeTagger tool. The advantage of this system is the detection of certain grammatical forms in order",
            " of using all the documents of the subcorpus “specialist” $E$, because the documents of the subcorpus of Annodis are not identical. Then we benchmarked the performance of the three systems automatically.\n\n\nExperiments ::: Results\nIn this section we will compare the results of the different segmentation systems through automatic evaluations. First of all, the human segmentation, from the subcorpus $D$ composed of common documents. The results",
            "Abstract\nSentence Boundary Detection (SBD) has been a major research topic since Automatic Speech Recognition transcripts have been used for further Natural Language Processing tasks like Part of Speech Tagging, Question Answering or Automatic Summarization. But what about evaluation? Do standard evaluation metrics like precision, recall, F-score or classification error; and more important, evaluating an automatic system against a unique reference is enough to conclude how well a SBD system is performing given the final application of the",
            " INLINEFORM1 Vietnamese as well: 0.29 BLEU points between (3) and (2) and 0.57 BLEU points between (4) and (3). This draws two conclusions: (i), despite using an unsupervised Vietnamese word segmentation which is fast, robust and does not require linguistic resources, our NMT systems performed better than those systems employing a complicate word segmentation method, (ii) BPE works significantly well for Japanese texts after we token",
            " did not limit the number of questions a worker can input to a corresponding segment and encouraged them to input a diverse set of questions which the span can answer. Along with the questions, the workers were also required to provide a justification as to why they made their questions. We manually checked this justification to filter out the questions with poor quality by removing those questions which were unrelated to the video. One initial challenge worth mentioning is that at first some workers input questions they had about the video and not questions which the",
            " automatic sentence segmentation was wrong. Given the “noise” in user-generated Web data (wrong or missing punctuation, casing, etc.), this was often the case. Annotators were also asked to rephrase (summarize) each annotated argument component into a simple statement when applicable, as shown in Figure FIGREF66 . This was used as a first sanity checking step, as each argument component is expected to be a coherent discourse unit. For example, if a"
        ]
    },
    {
        "title": "Multimodal Named Entity Recognition for Short Social Media Posts",
        "answer": "121,512",
        "evidence": [
            "15%), and test sets (15%). The captions data have average length of 30.7 characters (5.81 words) with vocabulary size 15,733, where 6,612 are considered unknown tokens from Stanford GloVE embeddings BIBREF22 . Named entities annotated in the SnapCaptions dataset include many of new and emerging entities, and they are found in various surface forms (various nicknames, typos, etc.) To the best of our knowledge, SnapCapt",
            " for Multimodal NER\nOur MNER model is built on a Bi-LSTM and CRF hybrid model. We use the following implementation for the entity Bi-LSTM.  it = (Wxiht-1 + Wcict-1) ct = (1-it) ct-1  + it tanh(Wxcxt + Whcht-1) ot = (Wxoxt + Whoht-1 + Wco",
            " classification systems based on each classifier's predictions for the 121,512 visual questions in the test dataset. We first show performance of the baseline and our two prediction systems using precision-recall curves. The goals are to achieve a high precision, to minimize wasting crowd effort when their efforts will be redundant, and a high recall, to avoid missing out on collecting the diversity of accepted answers from a crowd. We also report the average precision (AP), which indicates the area under a precision-recall",
            " do exist, they are limited in their size (e.g., 500 pairs in Turkish BIBREF26, 500 in Farsi BIBREF27, or 300 in Finnish BIBREF28) and coverage (e.g., all datasets which originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity dataset spanning 1,888",
            " or phrases according to the dictionary. Finally, we remove sentences which have more than two rare words (appearing less than three times) in order to constrain the size of vocabulary. Finally, we get 662,530 sentences after pre-processing. As neural networks are proved effective in irony detection, we decide to implement a neural classifier in order to classify the sentences into ironic and non-ironic sentences. However, the only high-quality irony dataset we can obtain is the dataset of",
            "REF16 . Although the formats of the datasets are almost the same, the model trained by the MS COCO dataset does not work well for the Flickr 30K dataset and vice versa. The word distributions of the captions are considerably different. If we ignore words with less than 30 counts, MS COCO has 3,655 words and Flicker30K has 2732 words; and only 1,486 words are shared. Also, the average lengths of captions are different. The average",
            " To this end, we chunk the novels into different sizes: 200-2000 words, at 200-word intervals, and evaluate our CNNs in the multi-class condition. Generalization-dataset experiments. To confirm that our models generalize, we pick the best models from the baseline-dataset experiments and evaluate on the novel-50 and IMDB62 datasets. For novel-50, the chunking size applied is 2000-word as per the baseline-dataset experiment",
            ": 246,122 word embeddings; training corpus: RCV1 corpus, one year of Reuters English newswire from August 1996 to August 1997. (ii) Huang. huang2012improving incorporated global context to deal with challenges raised by words with multiple meanings; size: 100,232 word embeddings; training corpus: April 2010 snapshot of Wikipedia. (iii) GloVe. Size: 1,193,514 word embeddings; training corpus: a Twitter corpus",
            " dataset is recorded across 5 sessions with 5 pairs of speakers. To ensure speaker independent learning, the dataset is split at the level of sessions: training is performed on 3 sessions (6 distinct speakers) while validation and testing are each performed on 1 session (2 distinct speakers).\n\n\nMultimodal Computational Descriptors\nAll the datasets consist of videos where only one speaker is in front of the camera. The descriptors we used for each of the modalities are as follows: Language All",
            " embed each token. For the neural models, the embeddings are used to initialize the token representations, and are refined during training. For the sliding similarity window approach, we set INLINEFORM0 . The vocabulary of the neural models was extracted from training and development data. For optimizing the bilinear model and the attentive reader, we used vanilla stochastic gradient descent with gradient clipping, if the norm of gradients exceeds 10. The size of the hidden layers was tuned to 64, with",
            "�New York Story” or “Thanksgiving Story”, which comprise snaps that are aggregated for various public events, venues, etc. All snaps were posted between year 2016 and 2017, and do not contain raw images or other associated information (only textual captions and obfuscated visual descriptor features extracted from the pre-trained InceptionNet are available). We split the dataset into train (70%), validation (15%), and test sets (15%). The captions data have average length",
            " Wall Street Journal (WSJ) dataset BIBREF25 , which contains 80 hours of clean read speech, and Librispeech BIBREF26 , which contains 1000 hours with separate train/dev/test splits for clean and noisy speech. Each dataset comes with official textual data to train language models, which contain 37 million tokens for WSJ, 800 million tokens for Librispeech. Our language models are trained separately for each dataset on the official text data only. These datasets were chosen"
        ]
    },
    {
        "title": "Neural Collective Entity Linking",
        "answer": "98.61% Accuracy.",
        "evidence": [
            " NCEL with the global models that report the performance on GERBIL. As shown in Table TABREF26 , NCEL achieves the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The baseline methods also achieve competitive results on some datasets but fail to adapt to the others. For example, AIDA and xLisa perform quite well on ACE2004 but poorly on other datasets, or WAT, PBoH, and WNED",
            " to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index from the comments and queried it for comments containing the word “troll” with an edit distance of 1",
            " time, we also design a combination of rewards for reinforcement learning and incorporate reinforcement learning with a pre-training process. Experimental results demonstrate that our model outperforms other generative models and our rewards are effective. Although our model design is effective, there are still many errors and we systematically analyze them. In the future, we are interested in exploring these directions and our work may extend to other kinds of ironies which are more difficult to model.\n\n\n",
            " improve discriminative signals of candidate entities by exploiting the rich structure underlying the correct entities. To alleviate the global computations, we propose to convolute on the subgraph of adjacent mentions. Thus, the overall coherence shall be achieved in a chain-like way via a sliding window over the document. To the best of our knowledge, this is the first effort to develop a unified model for neural collective entity linking. In experiments, we first verify the efficiency of NCEL via theoretically comparing its time",
            ", with 82.80% Accuracy on SNLI, 78.52% Accuracy on MultiNLI, and 83.62% Accuracy on Quora. Logistic Regression achieved the best Accuracy of 98.60% on Clinical-RQE. We also performed a 10-fold cross-validation on the full Clinical-QE data of 8,588 question pairs, which gave 98.61% Accuracy. In the second experiment, we used these datasets for training only and compared their",
            "\nAcknowledgments\nThis work was funded by the Amazon Academic Research Awards program.\n\n\n",
            " NCEL learns the mapping INLINEFORM2 through a neural network including three main modules: encoder, sub-graph convolution network (sub-GCN) and decoder. Next, we introduce them in turn. Encoder The function of this module is to integrate different features by a multi-layer perceptron (MLP): INLINEFORM0  where INLINEFORM0 is the hidden states of the current mention, INLINEFORM1 and INLINEFORM2 are trainable parameters",
            " 3-feats technique, we observed a great improvement, as can be expected. Success rates obtained for the movie corpus are much better than those for the Twitter dataset for most of our approaches, since tweets are, in general, much shorter and noisier. We also found out that, when choosing the p value as 0.05, our results are statistically significant compared to the baseline approach in Turkish BIBREF10. Some of our subapproaches also produce better success rates than those sentiment",
            " this task. More specifically, incremental encoding is effective due to the facts: 1) IE is significantly better than Seq2Seq and HLSTM in grammar (Sign Test, 1.84 vs. $1.74/1.57$ , p-value= $0.046/0.037$ , respectively), and in logicality (1.10 vs. 0.70/0.84, p-value $<0.001/0.001$",
            "tuning on an MNLI-based pretrained entailment model seems more powerful.\n\n\nExperiments ::: Label-fully-unseen evaluation\nRegarding this label-fully-unseen evaluation, apart from our entailment models and three unsupervised baselines “Majority”, “Word2Vec” and “ESA”, we also report the following baseline: Wikipedia-based: We train a binary classifier based on BERT on a dataset",
            " via: DISPLAYFORM0 \n\n\nEvaluation\nThe baseline and the models are evaluated on a corpus of 12 MOOC discussion forums. We train on 80% of the training data and report evaluation results on the held-out 20% of test data. We report INLINEFORM0 scores on the positive class (interventions), in line with prior work. We also argue that recall of the positive class is more important than precision, since it is costlier for instructors to miss intervening",
            " choose two typical NLP tasks involving six datasets. The qualitative and quantitative experiment results demonstrate the effectiveness of our models.\n\n\nAcknowledgments\nWe would like to thank the anonymous reviewers for their valuable comments and thank Kaiyu Qian, Jiachen Xu, Jifan Chen for useful discussions. This work was partially funded by National Natural Science Foundation of China (No. 61532011 and 61672162), Shanghai Municipal Science and Technology Commission (No. 16JC1420401).\n\n\n"
        ]
    },
    {
        "title": "Distant supervision for emotion detection using Facebook reactions",
        "answer": "public pages.",
        "evidence": [
            ". We collected Facebook posts and their corresponding reactions from public pages using the Facebook API, which we accessed via the Facebook-sdk python library. We chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but we did so mainly based on intuition (see Section SECREF4 ) and with an eye to the nature of the datasets available for evaluation (see Section SECREF5 ). The choice of which pages to select posts from is far from trivial, and we believe",
            " within the scope of a privacy policy (e.g., 'has Viber had any privacy breaches in the past?') and 16.9% of questions were identified as completely out-of-scope (e.g., `'will the app consume much space?'). In the questions identified as relevant, 32% were ill-formed questions that were phrased by the user in a manner considered vague or ambiguous. Of the questions that were both relevant as well as `well-formed',",
            "353. They used a convolutional neural network (CNN) model to classify a question into a restricted set of 10 question types and crawled \"relevant\" online web pages to find the answers. However, the results were lower than those achieved by the systems relying on finding similar answered questions. These results support the relevance of similar question matching for the end-to-end QA task as a new way of approaching QA instead of the classical QA approaches based on Question Analysis and Answer Retri",
            " as SQuAD BIBREF6 based on Wikipedia, TriviaQA BIBREF7 constructed from trivia questions with answer evidence from Wikipedia, or those from Hermann et al. based on CNN and Daily Mail articles BIBREF8 are factoid-based, meaning the answers typically involve a single entity. Differing from video transcripts, the structures of these data sources, namely paragraphs from Wikipedia and news sources, are typically straightforward since they are meant to be read. In contrast, video",
            "ability and superiority for different topics, languages, data distributions, and platforms. The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset",
            " to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index from the comments and queried it for comments containing the word “troll” with an edit distance of 1",
            " future, we will enhance our work by extracting facts from the suggested news articles. Results suggest that the news content cited in entity pages comes from the first paragraphs. However, challenging task such as the canonicalization and chronological ordering of facts, still remain.\n\n\n",
            "IBREF30 . Coordination was measured as a Gini index of Hashtags while centrality was measured by a count of followers of an account.\n\n\nFacebook\nSocial media has been used to estimate preferences as well. The advantage of social media compared to speeches or any other preference indicator is coverage. BIBREF31 use endorsement of official pages on Facebook to scale ideological positions of politicians from different levels of government and the public into a common space. Their method extends to other social media such as",
            " to myriads of social media posts or other articles across multiple platforms which often include both text and accompanying images. In addition, we proposed the modality attention module, a new neural mechanism which learns optimal integration of different modes of correlated information. In essence, the modality attention learns to attenuate irrelevant or uninformative modal information while amplifying the primary modality to extract better overall representations. We showed that the modality attention based model outperforms other state-of-the-art",
            " kumar2014accurately and guha2004propagation present a methodology to identify malicious individuals in a network based solely on the network's properties rather than on the textual content of comments. cambria2010not propose a method that involves NLP components, but fail to provide an evaluation of their system. There is extensive work on detecting offensive and abusive language in social media BIBREF2 and BIBREF3 . There are two clear differences between their work and ours. One is that",
            "-switching data for RC, we create artificial code-switching datasets by replacing some of the words in contexts or questions with their synonyms in another language. The synonyms are found by word-by-word translation with given dictionaries. We use the bilingual dictionaries collected and released in facebookresearch/MUSE GitHub repository. We substitute the words if and only if the words are in the bilingual dictionaries. Table TABREF14 shows that on all the code-switching datasets",
            " dataset was extended from that in BIBREF21 ; the data were collected from January 2017 to December 2017. The labeled dataset includes 48,374 passages. To support semi-supervised learning, the first 3 months of data (96,777 passages) are unlabeled. Because the data stem from social media, some text exists that cannot be considered as part of any sentence, such as product links, symbols unrelated to sentences, and space between sentences. These portions were not originally annotated as"
        ]
    },
    {
        "title": "MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge",
        "answer": "Amazon Mechanical Turk, CrowdFlower, and WebRTC.  Additionally, Yahoo!, Answers.com, and Quora.  Also, Google Cloud, and Crowdworkers.  Furthermore, ECNU-ICA, and ONR YIP N00014-12-1-0754.  Finally, Amazon Academic Research Awards program, and Office of Naval Research.  Additionally, National Science Foundation, and IIS-1065390.  Also, Dinesh Jayaraman, and Yu-Chuan Su.  Furthermore, Su Gao, and Su Gao.  Finally, ECNU-ICA system, and ECNU-ICA approach.  Additionally, Amazon Mechanical Turk platform, and Crowdworkers.  Also, Google Cloud, and TensiStrength.  Furthermore, Rosette Text Analytics, and Office of Naval Research.  Finally, National Science Foundation, and IIS-1065390.  Additionally, Dinesh Jayaraman, and Yu-Chuan Su.  Also, Su Gao, and Su Gao.  Furthermore, ECNU-ICA system, and ECNU-ICA approach.  Finally, Amazon Mechanical Turk platform, and Crowdworkers.  Additionally, Google Cloud, and TensiStrength.  Also, Rosette",
        "evidence": [
            ". Davidson et al. BIBREF9 used the Twitter API to accumulate 84.4 million tweets from 33,458 twitter users containing particular terms from a pre-defined lexicon of hate speech words and phrases, called Hatebased.org. To annotate collected tweets as Hate, Offensive, or Neither, they randomly sampled $25k$ tweets and asked users of CrowdFlower crowdsourcing platform to label them. In detail, the distribution of different classes in both datasets will be provided in Sub",
            "\nAcknowledgments\nThis work was funded by the Amazon Academic Research Awards program.\n\n\n",
            " to describe the entire system in detail, there are several platform features that help illustrate how the process works.  Modality: The agents playing the assistant type their input which is in turn played to the user via text-to-speech (TTS) while the crowdsourced workers playing the user speak aloud to the assistant using their laptop and microphone. We use WebRTC to establish the audio channel. This setup creates a digital assistant-like communication style.  Conversation and user quality control: Once",
            " pairs, they searched community question answering sites such as Yahoo! and Answers.com. In contrast, the ECNU-ICA system achieved the best performance of 1.895 in the open-domain task but an average score of only 0.402 in the medical task. As the ECNU-ICA approach also relied on a neural network for question matching, this result shows that training attention-based decoder-encoder networks on the Quora dataset generalized better to the medical domain than",
            "ments are made and the process is repeated, often with another researcher involved. The final annotations can be collected using a crowdsourcing platform, a smaller number of highly-trained annotators, or a group of experts. Which type of annotator to use should be informed by the complexity and specificity of the concept. For more complex concepts, highly-trained or expert annotators tend to produce more reliable results. However, complex concepts can sometimes be broken down into micro-tasks that can be performed independently",
            " access to a trusted third-party privacy assistant, to whom they can ask any privacy question about a given mobile application. We use the Amazon Mechanical Turk platform and recruit crowdworkers who have been conferred “master” status and are located within the United States of America. Turkers are asked to provide five questions per mobile application, and are paid $2 per assignment, taking ~eight minutes to complete the task.\n\n\nData Collection ::: Answer Selection\nTo identify legally sound answers, we",
            " crowd disagreement. Finally, we demonstrated how to employ the prediction system to accelerate the collection of diverse answers from a crowd by typically at least 20% over today's status quo of fixed redundancy allocation.\n\n\nAcknowledgments\nThe authors gratefully acknowledge funding from the Office of Naval Research (ONR YIP N00014-12-1-0754) and National Science Foundation (IIS-1065390). We thank Dinesh Jayaraman, Yu-Chuan Su, Su",
            " slight preprocessing. Essentially, we removed user names, time-stamps, URLs, multiple blanks as well as annotations that were not part of the original conversations from all chat lines. Moreover, we did not normalize words (for example, shorten words such as “nooooo” to “no”) as we believe that these represent important style markers. Furthermore, we did not remove newlines between the chat lines, as the positions of specific words might play an",
            ". In the results section we compare our model with related work.\n\n\nDataset\nWe tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show the applicability and superiority for different topics, languages, data distributions, and platforms.",
            "island states prioritizing climate change. Identifying such groups is often referred to as `dimension reduction' of data. Validation of unsupervised learning results is less straight-forward than with supervised learning. We use data external to our analysis to validate the findings BIBREF1 . A complementary approach to unsupervised and supervised learning is the use of crowdsourced human coders. BIBREF2 show that crowdsourcing text analysis is a way to achieve reliable and replicable results quickly and",
            " knowledge about the activity of planting a tree, which goes beyond what is mentioned in the text. Texts, questions, and answers were obtained through crowdsourcing. In order to ensure high quality, we manually validated and filtered the dataset. Due to our design of the data acquisition process, we ended up with a substantial subset of questions that require commonsense inference (27.4%).\n\n\nCorpus\nMachine comprehension datasets consist of three main components: texts, questions and answers. In this section,",
            " more than 30 percent points. Crowdworkers correctly identified 62% of the neutral, 85% of the positive, and 92% of the negative sentiments. Google Cloud correctly identified 88% of the neutral sentiments, but only 3% of the positive, and 19% of the negative sentiments. TensiStrength correctly identified 87.2% of the neutral sentiments, but 10.5% of the positive, and 8.1% of the negative sentiments. Rosette Text Analytics correctly identified 22.7%"
        ]
    },
    {
        "title": "Stochastic Answer Networks for SQuAD 2.0",
        "answer": "components: the span detector and unanswerable classifier. module for the span detector BIBREF1. Formally, at time step INLINEFORM0 in the range of INLINEFORM1, the state is defined by INLINEFORM2. The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM5, where INLINEFORM6. Here, INLINEFORM7 is computed from the previous state INLINEFORM8 and memory INLINEFORM9 : INLINEFORM10 and INLINEFORM11. a phrasal decoder. We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This  components: the span detector and unanswerable classifier. module for the span detector BIBREF1. Formally, at time step INLINEFORM0 in the range of INLINEFORM1, the state is defined by INLINEFORM2. The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM",
        "evidence": [
            " components: the span detector and unanswerable classifier.\n\n\nConclusion\nTo sum up, we proposed a simple yet efficient model based on SAN. It showed that the joint learning algorithm boosted the performance on SQuAD 2.0. We also would like to incorporate ELMo into our model in future.\n\n\nAcknowledgments\nWe thank Yichong Xu, Shuohang Wang and Sheng Zhang for valuable discussions and comments. We also thank Robin Jia for the help on S",
            "rained models or created more generally by the user from user-specifications. Model - All models follow the same hierarchy of abstraction: a base class implements the model's computation graph from encoding (projection on the embedding matrix) through the series of self-attention layers and up to the last layer hidden states. The base class is specific to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of the",
            "ality and network structure.\n\n\nIntroduction\nMany speech processing tasks – such as automatic speech recognition or spoken term detection – hinge on associating segments of speech signals with word labels. In most systems developed for such tasks, words are broken down into sub-word units such as phones, and models are built for the individual units. An alternative, which has been considered by some researchers, is to consider each entire word segment as a single unit, without assigning parts of it to sub-word units.",
            " module for the span detector BIBREF1 . Formally, at time step INLINEFORM0 in the range of INLINEFORM1 , the state is defined by INLINEFORM2 . The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM5 , where INLINEFORM6 . Here, INLINEFORM7 is computed from the previous state INLINEFORM8 and memory INLINEFORM9 : INLINEFORM10 and INLINEFORM11 .",
            " a phrasal decoder. We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This",
            "-view dynamics in the network architecture and continuously models both dynamics through time. In MARN, view-specific dynamics within each modality are modeled using a Long-short Term Hybrid Memory (LSTHM) assigned to that modality. The hybrid memory allows each modality's LSTHM to store important cross-view dynamics related to that modality. Cross-view dynamics are discovered at each recurrence time-step using a specific neural component called the Multi-attention Block (MAB",
            ". Unlike Baseline1 which avoids an explicit segmentation step, the Pointer-network can explicitly model which sentences are likely to be a boundary sentence. Moreover, the search space of the spans in the Pointer-network is $2n$ where $n$ is the number of sentences, because it selects only two boundary sentences. Note that the search space of Baseline1 is $n^2$. A much smaller search space might improve the accuracy by making the model consider fewer candidates.",
            " is that the model is confused by the large amount of questions that cannot be answered from the text directly, which might confound the computation of attention values. Also, the attentive reader was originally constructed for reconstructing literal text spans as answers. Our mode of answer collection, however, results in many correct answers that cannot be found verbatim in the text. This presents difficulties for the attention mechanism. The fact that an attention model outperforms a simple bilinear baseline only marginally shows that MCScript",
            " conditions with entailment/contradiction, we did not run all data size conditions given our limited computational resources. Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with the",
            " components can learn each other. The CNN architecture learns sentence features layer by layer, then those features are justified by all constituent words. During pretraining, all the model parameters, including mutichannel input, convolution parameters and fully connected layer, will be updated until they are mature to extract the sentence features. Subsequently, the same sets of parameters will be fine-tuned for supervised classification tasks. In sum, this pretraining is designed to produce good initial values for both model parameters and word",
            " component classification, but reveals a sub-task of finding the component boundaries. The best system achieved 0.32 on this measure. Vovk2013MT used this measure to annotate argument spans and his annotators achieved 0.36 boundary similarity score. Human annotators in BIBREF125 reached 0.53 boundary similarity score. The overall performance of the system is also affected by the accuracy of individual NPL tools used for extracting features. One particular problem is that the preprocessing models we rely",
            " we use a marked target copy, viewed as a fake source, which a generator encodes so as to fool a discriminator trained to distinguish a fake from a natural source. Our architecture contains two distinct encoders, one for the natural source and one for the pseudo-source. The latter acts as the generator ( INLINEFORM0 ) in the GAN framework, computing a representation of the pseudo-source that is then input to a discriminator ( INLINEFORM1 ), which has to"
        ]
    },
    {
        "title": "Winograd Schemas and Machine Translation",
        "answer": "English, French, Italian, Spanish, Hebrew, Arabic, Polish, Mandarin, Yue, Vietnamese.",
        "evidence": [
            " Romance) are all neighbors. In order to quantify exactly the effect of language affinity on the similarity scores, we run correlation analyses between these and language features. In particular, we extract feature vectors from URIEL BIBREF99, a massively multilingual typological database that collects and normalizes information compiled by grammarians and field linguists about the world's languages. In particular, we focus on information about geography (the areas where the language speakers are concentrated), family (the phylogenetic tree each language",
            " user study. Note that the 15 participated subjects had not used the 20 products before and were also not familiar with the packaged products including the chosen 20 products; hence, they were \"illiterate\" in terms of comprehending English and in terms of having used any of the products although they might be literate in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded",
            "iers, Spanians). NMT systems learn some syntactic disambiguation even with very little data, for example the translation of das and die as relative pronouns ('that', 'which', 'who'), while PBSMT produces less grammatical translation. On the flip side, the ultra low-resource NMT system ignores some unknown words in favour of a more-or-less fluent, but semantically inadequate translation: erobert ('conquered') is translated into doing, and",
            " changing landscape of Victorian scientific culture, allowing them to contrast Darwin's “foraging” in the scientific literature of his time to the ways in which that literature was itself produced. Finally, their methods provided a case study, and validation of technical approaches, for cognitive scientists who are interested in how people explore and exploit sources of knowledge. Questions about potential “dual use” may also arise. Returning to our introductory example, BIBREF0 started with a deceptively simple question",
            " defined will allow us to eliminate costly annotation efforts in the future, especially for the word-level slots and intent keywords. Once enough domain-specific multi-modal data is collected, our future work is to explore training end-to-end dialogue agents for our in-cabin use-cases. We are planning to exploit other modalities for improved understanding of the in-cabin dialogue as well.\n\n\n",
            " a very difficult challenge for the depth of understanding achieved by a machine translation program. The third person plural pronoun `they' has no gender in English and most other languages (unlike the third person singular pronouns `he', `she', and `it'). However Romance languages such as French, Italian, and Spanish, and Semitic languages such as Hebrew and Arabic distinguish between the masculine and the feminine third-person plural pronouns, at least in some grammatical cases. For instance in French, the masculine",
            " and playful manner. Built by the authors on top of Transformers, Write with Transformer is an interactive interface that leverages the generative capabilities of pretrained architectures like GPT, GPT2 and XLNet to suggest text like an auto-completion plugin. Generating samples is also often used to qualitatively (and subjectively) evaluate the generation quality of language models BIBREF9. Given the impact of the decoding algorithm (top-K sampling, beam-search, etc.) on",
            " to convey the nearest possible semantics given the above points (e.g., the English word homework is translated into the Polish MWE praca domowa). Satisfying the above rules when finding appropriate translations for each pair–while keeping to the spirit of the intended semantic relation in the English version–is not always straightforward. For instance, kinship terminology in Sinitic languages (Mandarin and Yue) uses different terms depending on whether the family member is older or younger, and whether the family member comes",
            " across nigeria between'). 3% of questions have an unresolvable coreference (e.g., `how do i get to Warsaw Missouri from here'). 4% of questions are vague, and a further 7% have unknown sources of error. 2% still contain false presuppositions (e.g., `what is the only fruit that does not have seeds?') and the remaining 42% do not have an answer within the document. This reinforces our belief that though they have been under",
            " a more exploratory nature. Here the purpose is to reveal patterns in the data that allow us to distinguish distinct groups whose differences are small, while variations across groups are large. In a set of text documents, we may be interested in the main topics about which respondents are talking. In social sciences, we may look for groups of nations within the international system that use a similar language or that describe similar issues, like small-island states prioritizing climate change. Identifying such groups is often referred",
            ", however imperfectly, by the presence of particular words semi-automatically extracted from a machine learning algorithm. Operationalizations are never perfect translations, and are often refined over the course of an investigation, but they are crucial. We begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results. The research process sounds more or less linear this way, but each of these phases overlaps, and",
            " and then asked to conjecture questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment to many users, we maintain a consistent set of video segments"
        ]
    },
    {
        "title": "How we do things with words: Analyzing text as social and cultural data",
        "answer": "subtleties of meaning and interpretation, external relevance, user interaction, off-topic comments, and others.",
        "evidence": [
            " analysis and close reading can complement each other. Computational approaches are valuable precisely because they help us identify patterns that would not otherwise be discernible. Yet these approaches are not a panacea. Examining thick social and cultural questions using computational text analysis carries significant challenges. For one, texts are culturally and socially situated. They reflect the ideas, values and beliefs of both their authors and their target audiences, and such subtleties of meaning and interpretation are difficult to incorporate in computational approaches. For another,",
            ", user interaction (asking questions, expressing agreement), off-topic comments, and others. Such characteristics are typical to on-line discussions in general, but they have not been examined with respect to argumentation or persuasion. Indeed, we observed that there are (1) documents that are completely unrelated and (2) documents that are related to the topic, but do not contain any argumentation. This issue has been identified among argumentation theorist; for example as external relevance by Paglieri.Cast",
            " several ways. In particular, they provide neither a unified API across models nor standardized ways to access the internals of the models. Targeting a more general machine-learning community, these Hubs lack the NLP-specific user-facing features provided by Transformers like tokenizers, dedicated processing scripts for common downstream tasks and sensible default hyper-parameters for high performance on a range of language understanding and generation tasks. The last direction is related to machine learning research frameworks that are specifically used to test,",
            " limiting. We are actively exploring applying these techniques to science question answering BIBREF26 , for example, where existing KBs provide only partial coverage of the questions.\n\n\n",
            " categorization task is much closer to the Wikipedia-based topic categorization task; emotion and situation categorizations, however, are relatively further. Our entailment models, pretrained on MNLI/FEVER/RTE respectively, perform more robust on the three $\\textsc {0shot-tc}$ aspects (except for the RTE on emotion). Recall that they are not trained on any text classification data, and never know the domain and the aspects in the test. This clearly shows the",
            " what features make it more or less ambiguous? In these cases, it is critical to communicate high-level patterns in terms that are recognizable. This contrasts with much of the work in computational text analysis, which tends to focus on automating tasks that humans perform inefficiently. These tasks range from core linguistically motivated tasks that constitute the backbone of natural language processing, such as part-of-speech tagging and parsing, to filtering spam and detecting sentiment. Many tasks are motivated by applications, for example to",
            " For example, we might focus on major historical moments (French Revolution, post-Milosevic Serbia) or critical epochs (Britain entering the Victorian era, the transition from Latin to proto-Romance). Or, we could look for records of how people conducted science, wrote and consumed literature, and worked out their philosophies. A growing number of researchers work with born-digital sources or data. Born-digital data, e.g., from social media, generally do not involve direct elicitation",
            " across nigeria between'). 3% of questions have an unresolvable coreference (e.g., `how do i get to Warsaw Missouri from here'). 4% of questions are vague, and a further 7% have unknown sources of error. 2% still contain false presuppositions (e.g., `what is the only fruit that does not have seeds?') and the remaining 42% do not have an answer within the document. This reinforces our belief that though they have been under",
            " the FBFans datset and related work on the CreateDebate dataset. We do not adopt oversampling on these models because the CreateDebate dataset is almost balanced. In previous work, integer linear programming (ILP) or linear-chain conditional random fields (CRFs) were proposed to integrate text features, author, ideology, and user-interaction constraints, where text features are unigram, bigram, and POS-dependencies; the author constraint tends to require that",
            " information from text documents. For example, researchers have analyzed speeches, legislative bills, religious texts, press communications, newspaper articles, stakeholder consultations, policy documents, and regulations. Such documents often contain many different dimensions or aspects of information and it is usually impossible to manually process them for systematic analysis. The analytical methods used to research the content of such documents are similar. We introduce prominent applications from the social sciences to provide an intuition about what can be done with such data.\n\n\nOpen-ended survey",
            "353. They used a convolutional neural network (CNN) model to classify a question into a restricted set of 10 question types and crawled \"relevant\" online web pages to find the answers. However, the results were lower than those achieved by the systems relying on finding similar answered questions. These results support the relevance of similar question matching for the end-to-end QA task as a new way of approaching QA instead of the classical QA approaches based on Question Analysis and Answer Retri",
            " translation process and the resolution of translation disagreements. More importantly, they also did not reannotate the translated pairs in the target languages. As we discussed in § SECREF6 and reiterate later in §SECREF5, semantic differences among languages can have a profound impact on the annotation scores; particulary, we show in §SECREF25 that these differences even roughly define language clusters based on language affinity. A core issue with the current datasets concerns a lack of one unified procedure that ensures the compar"
        ]
    },
    {
        "title": "Quasi-Recurrent Neural Networks",
        "answer": "max-pooling",
        "evidence": [
            " Conveniently, this is equivalent to stochastically setting a subset of the QRNN's INLINEFORM0 gate channels to 1, or applying dropout on INLINEFORM1 : DISPLAYFORM0   Thus the pooling function itself need not be modified at all. We note that when using an off-the-shelf dropout layer in this context, it is important to remove automatic rescaling functionality from the implementation if it is present. In many experiments, we also apply",
            "max$ function is calculated over a combined logits from both sides.  $$\\begin{split}\n&p(y)=\\frac{exp({e_y \\odot W_g[h_t^{dec};c_{wrd}]})+exp(s_c(y))}{Z}\\\\\n&s_c(y) = c_{wrd} \\odot {tanh(W_ch_t^{wrd})}\n",
            " transformer encoder are concatenated, and a matrix is produced. The convolutional operation is performed with a window of size (3, hidden size of BERT which is 768 in BERTbase model) and the maximum value is generated for each transformer encoder by applying max pooling on the convolution output. By concatenating these values, a vector is generated which is given as input to a fully connected network. By applying softmax on the input, the classification operation is performed",
            "\nEach layer of a quasi-recurrent neural network consists of two kinds of subcomponents, analogous to convolution and pooling layers in CNNs. The convolutional component, like convolutional layers in CNNs, allows fully parallel computation across both minibatches and spatial dimensions, in this case the sequence dimension. The pooling component, like pooling layers in CNNs, lacks trainable parameters and allows fully parallel computation across minibatch and feature dimensions. Given an input",
            ", if a sentence is highly related to the query, its pooling weight is large. On the other hand, if a sentence is salient in the document cluster, its embedding should be representative. As a result, the weighted-sum pooling generates the document representation which is automatically biased to embeddings of sentences match both documents and the query. AttSum simulates human attentive reading behavior, and the attention mechanism in it has actual meaning. The experiments to be presented in Section \"Query Re",
            " the well-known PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 . In regard to our experiments, however, we cannot use these corpora, due to the absence of relevant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our",
            " does not apply word-level pooling as an intermediate step and extracts only the relevant subspace of each word:  $$\\forall _i:~d_i = {\\textstyle \\sum _t} \\; R_{i,t} \\cdot x_{i,t}$$   (Eq. 13)  This last approach is particularly useful to address the problem of word homonymy, and will thus result in even finer semantic extraction from the document.",
            ". As a stabilizer value we use $\\epsilon = 0.01$ as introduced in BIBREF12 . After computation of the messages according to Equation 7 , the latter can be pooled onto the corresponding neuron by the formula $R_j = \\sum _k R_{j \\leftarrow k}$ . The relevance scores $R_j$ are then propagated through the max-pooling layer using the formula:  $$R_{j,t} = \\",
            " yu2014deep used one layer of convolution over phrases detected by a sliding window on a target sentence, then used max- or average-pooling to form a sentence representation. blunsom2014convolutional stacked multiple layers of one-dimensional convolution by dynamic k-max pooling to model sentences. We also adopt dynamic k-max pooling while our convolution layer has variable-size filters. kimEMNLP2014 also studied multichannel representation and variable-size",
            " an aggregator INLINEFORM0 essentially takes as input a collection of vectors INLINEFORM1 ( INLINEFORM2 ) and maps them to a single vector. With this observation, the following two types of functions seem to be natural choices for neighborhood aggregators, and have been adopted previously: Pooling Functions. A typical pooling function is mean-pooling, which is defined by INLINEFORM0 . Besides mean-pooling, other previously adopted choices include sum- and max-pool",
            " INLINEFORM7 belong to the same community, and 0 otherwise BIBREF34 . Since modularity optimization is an NP-complete problem, we utilize the greedy variant which is implemented through the igraph package. This approach has enjoyed widespread network science employment by applied mathematicians and physicists BIBREF35 , and has surfaced in political science studies of international trade BIBREF36 and international law BIBREF37 . As a direct measure of polarization, this approach has been used to measure roll call",
            ", and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks"
        ]
    },
    {
        "title": "Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction",
        "answer": "sentence",
        "evidence": [
            " classify each instance (snippet) as being subjective or objective. BIBREF26  IE Idiom enhanced sentiment classification. BIBREF27 . Each sentence contains at least one idiom. As shown in Table 3 , DC-TreeLSTM consistently outperforms RecNN, MV-RecNN, RNTN, and TreeLSTM by a large margin while achieving comparable results to the CNN and using much fewer parameters.(The number of parameters in our models is approximately 10K",
            " a head-tail position pair in $s_k$.\n\n\niMRC: Making MRC Interactive ::: Query Types\nGiven the question “When is the deadline of AAAI?”, as a human, one might try searching “AAAI” on a search engine, follow the link to the official AAAI website, then search for keywords “deadline” or “due date” on the website to jump to a specific paragraph. Humans",
            " In this context a sentence is defined (for English) by the Cambridge Dictionary as: “a group of words, usually containing a verb, that expresses a thought in the form of a statement, question, instruction, or exclamation and starts with a capital letter when written”. PMD carries certain complications, some given the ambiguity of punctuation marks within a sentence. A period can denote an acronym, an abbreviation, the end of the sentence or a combination of them as in the",
            " $ p(c(e)) &= \\sigma ( \\theta _c^T \\phi _e ) \\\\ p(r(e_1, e_2)) &= \\sigma (\n\\theta _r^T \\phi _{(e_1, e_2)} ) $  The probability of a predicate instance is the sigmoided inner product of the corresponding predicate and entity embeddings. Predicates with nearby embeddings will have",
            " the observed agreement between expert and lay annotators. Formally, for annotation task $t$ and instance $i$ :  $$\\text{Difficulty}_{ti} = \\frac{\\sum _{j=1}^n{f(\\text{label}_{ij}, y_i})}{n}$$   (Eq. 3)  where $f$ is a scoring function that measures the quality of the label from worker $j$ for sentence $i",
            " involves stance detection. In this case, the whole document (discussion post, article) is assumed to represent the writer's standpoint to the discussed topic. Since the topic is stated as a controversial question, the author is either for or against it. Somasundaran.Wiebe.2009 built a computational model for recognizing stances in dual-topic debates about named entities in the electronic products domain by combining preferences learned from the Web data and discourse markers from PDTB BIBREF48 . Hasan",
            ", domains? There are aspects of this task that make it both easier and more difficult than “traditional” IE. Even though they are expressed in natural language, business documents frequently take constrained forms, sometimes even “template-like” to a certain degree. As such, it may be easy to learn cue phrases and other fixed expressions that indicate the presence of some element (i.e., pattern matching). On the other hand, the structure and vocabulary of the texts may be very",
            " called ISOLATION, which performs saliency ranking and relevance ranking in isolation. Specifically, it directly uses the sum pooling over sentence embeddings to represent the document cluster. Therefore, the embedding similarity between a sentence and the document cluster could only measure the sentence saliency. To include the query information, we supplement the common hand-crafted feature TF-IDF cosine similarity to the query. This query-dependent feature, together with the embedding similarity, are used in sentence ranking",
            " in the figure). In this work, we use multichannel initialization for case (i) and random initialization for cases (ii) and (iii). Only fine-tuned multichannel representations (case (i)) are kept for subsequent supervised training. The rationale for this pretraining is similar to auto-encoder: for an object composed of smaller-granular elements, the representations of the whole object and its components can learn each other. The CNN architecture learns sentence features layer by layer",
            " or even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions. To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require background knowledge. We explicitly state the following items in the HIT instructions for question writing: No Yes-no questions should be asked. The question should have at",
            " controls, the WinoGrande challenge still contains incidental biases that these more sophisticated models can exploit, or that we are genuinely making at least some progress in commonsense reasoning. The latter, in particular, challenges the notion that commonsense knowledge is (mostly) tacit. Perhaps it is the case that in a humongous corpus of natural language text, someone really has written about trying to stuff a tuba in a backpack?\n\n\nAcknowledgments\nThis research was supported in part by the Canada",
            "(y_t=\\tilde{y}_t|y_{<t}, X),$$   (Eq. 28)   where $\\widetilde{x}_j^{(i)}$ means the reference word used for encoding at the $j$ -th position in sentence $i$ , and $\\tilde{y}_t$ represents the $j$ -th word in the reference ending. Such an approach does not mean that at each"
        ]
    },
    {
        "title": "TWEETQA: A Social Media Focused Question Answering Dataset",
        "answer": "By analyzing the predicate-argument structure of the tweets collected from news articles.",
        "evidence": [
            " In order to get enough data, we first extract the URLs of all section pages (e.g. World, Politics, Money, Tech) from the snapshot of each home page and then crawl all articles with tweets from these section pages. Note that another possible way to collect informative tweets is to download the tweets that are posted by the official Twitter accounts of news media. However, these tweets are often just the summaries of news articles, which are written in formal text. As our focus is to",
            " the radio schedule for recreating the musical context to analyze user-generated tweets, detecting when they are referring to a specific work or contributor recently played. We manage to associate to every track broadcasted a list of entities, thanks to the tweets automatically posted by the BBC Radio3 Music Bot, where it is described the track actually on air in the radio. In Table 3, examples of bot-generated tweets are shown. Afterwards, we detect the entities on the user-generated content by means of two",
            " and clustering Twitter users based on climate mentalities using network analysis BIBREF4 . Throughout, Twitter has been accepted as a powerful tool given the magnitude and reach of samples unattainable from standard surveys. However, the aforementioned studies are not scalable with regards to training data, do not use more recent sentiment analysis tools (such as neural nets), and do not consider unbiased comparisons pre- and post- various climate events (which would allow for a more concrete evaluation of shocks to climate change sentiment).",
            " 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}. For dialects, the data are labeled with 15 classes, from the set {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia",
            " is to identify trolling events, our datasets will be composed of suspected trolling attempts (i.e., comments that are suspected to be trolling attempts). In other words, some of these suspected trolling attempts will be real trolling attempts, and some of them won't. So, if a suspected trolling attempt is in fact not a trolling attempt, then its author will not be a troll. To cover both the troll and the recipients, we define a (suspected trolling attempt, responses) pair as the",
            ", @jdsutter, @justinkirkland and #cnnworldcup, #goldenglobes. In contrast, the SQuAD dataset rarely has usernames or hashtags that are used to signify events or refer to the authors. It is also worth noting that the data collected from social media can not only capture events and developments in real-time but also capture individual opinions and thus requires reasoning related to the authorship of the content as is illustrated in Table TAB",
            " criteria based on the work in Gender Studies and Critical Race Theory (CRT) that can annotate a corpus of more than $16k$ tweets as racism, sexism, or neither. To classify tweets, they used a logistic regression model with different sets of features, such as word and character $n$-grams up to 4, gender, length, and location. They found that their best model produces character $n$-gram as the most indicative features, and using location or",
            ". Annotators were asked to complete a spreadsheet containing the translated pairs of words, as well as the part-of-speech, and a column to enter the score. The annotators did not have access to the original pairs in English. To ensure the quality of the collected ratings, we have employed an adjudication protocol similar to the one proposed and validated by Pilehvar:2018emnlp. It consists of the following three rounds:  Round 1: All annotators are asked",
            "., the Gutenberg archive of British novels. This stage of the research also raises important questions about fairness. Are marginalized groups, for example, represented in the tweets we have collected? If not, what types of biases might result from analyses relying on those tweets? Local experts and “informants” can help navigate the data. They can help understand the role an archive plays in the time and place. They might tell us: Is this the central archive, or a peripheral one? What makes",
            " today - @IAmSteveHarvey\". This tweet is actually talking about an image attached to this tweet. Some other tweets with simple text structures may talk about an inserted link or even videos. To filter out these tweets that heavily rely on attached media to convey information, we utilize a state-of-the-art semantic role labeling model trained on CoNLL-2005 BIBREF15 to analyze the predicate-argument structure of the tweets collected from news articles and keep only the tweets with more",
            " hashtags, the performance is severely limited on the questions requiring such reasoning abilities. We believe that good segmentation, disambiguation and linking tools developed by the social media community for processing the userIDs and hashtags will significantly help these question types. Besides the easy questions requiring mainly paraphrasing skill, we also find that the questions requiring the understanding of authorship and oral/tweet English habits are not very difficult. We think this is due to the reason that, except for these tweet",
            " we use the Python library twitter_nlp, presented in BIBREF1 . In total, we define 26 features for describing each token: 1)POS tag; 2)Chunk tag; 3)Position of the token within the text, normalized between 0 and 1; 4)If the token starts with a capital letter; 5)If the token is a digit. Gazetteers: 6)Contributor first names; 7)Contributor last names; 8)Contributor types (\"s"
        ]
    },
    {
        "title": "Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation",
        "answer": "Affective Text dataset, Fairy Tales dataset, ISEAR dataset, Multi-SimLex datasets, ClueWeb, CNN/Daily Mail dataset. 8,162 stories. 100 samples from the test set. 220 queries, 307 queries. CreateDebate dataset.  Japanese word similarity dataset.  Social Media Processing (SMP) dataset.  SQuAD evaluations.  POS-Specific Subsets.  iFLYTEK Corporation dataset.  Krishnamurthy and Mitchell dataset.  National Natural Science Foundation of China dataset.  Shanghai Municipal Science and Technology Commission dataset.  English.  Fairy Tales dataset.  Japanese.  Chinese.  ClueWeb.  SMP dataset.  SQuAD.  CreateDebate.  Multi-SimLex.  Affective Text.  ISEAR.  Krishnamurthy and Mitchell.  iFLYTEK Corporation.  Social Media Processing.  Shanghai Municipal Science and Technology Commission.  National Natural Science Foundation of China.  English.  Japanese.  Chinese.  Fairy Tales.  ISEAR.  Affective Text.  Multi-SimLex.  ClueWeb.  SMP dataset.  SQuAD.  CreateDebate.  Krish",
        "evidence": [
            "FORM0  where INLINEFORM0 and INLINEFORM1 are the average precision and recall of the three class. We adopted the macro-average f-score as the evaluation metric for the overall performance because (1) the experimental dataset is severely imbalanced, which is common for contentious issues; and (2) for stance classification, content in minor-class posts is usually more important for further applications. For the CreateDebate dataset, accuracy was adopted as the evaluation metric to compare the results",
            " in future work.\n\n\nEmotion datasets\nThree datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them",
            "Abstract\nAn evaluation of distributed word representation is generally conducted using a word similarity task and/or a word analogy task. There are many datasets readily available for these tasks in English. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult. Therefore, as a first step toward evaluating distributed representations in Japanese, we constructed a Japanese word similarity dataset. To the best of our knowledge, our dataset is the first resource that can be used to evaluate distributed",
            " discussions and comments. We also thank Robin Jia for the help on SQuAD evaluations. \n\n\n",
            " datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size influences.",
            ", provide qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.\n\n\n",
            " choose two typical NLP tasks involving six datasets. The qualitative and quantitative experiment results demonstrate the effectiveness of our models.\n\n\nAcknowledgments\nWe would like to thank the anonymous reviewers for their valuable comments and thank Kaiyu Qian, Jiachen Xu, Jifan Chen for useful discussions. This work was partially funded by National Natural Science Foundation of China (No. 61532011 and 61672162), Shanghai Municipal Science and Technology Commission (No. 16JC1420401).\n\n\n",
            " evaluation data; this gap has now been filled with the introduction of the Multi-SimLex datasets. In all follow-up experiments, we always explicitly denote which post-processing configuration is used in evaluation.  POS-Specific Subsets. We present the results for subsets of word pairs grouped by POS class in Table TABREF46. Prior work based on English data showed that representations for nouns are typically of higher quality than those for the other POS classes BIBREF49, BIB",
            " test set created by Krishnamurthy and Mitchell, which contains 220 queries generated in the same fashion as the training data from a separate section of ClueWeb. However, as they did not release a development set with their data, we used this set as a development set. For a final evaluation, we generated another, similar test set from a different held out section of ClueWeb, in the same fashion as done by Krishnamurthy and Mitchell. This final test set contains 307 queries",
            " present the two tasks of the evaluation as well as the evaluation metrics. We then describe the released data for evaluation. Finally, we also show the evaluation results of the two tasks. As the evaluation data is provided by the iFLYTEK Corporation from their real online applications, we believe that the released data will further promote the research of human-computer dialogue and fill the blank of the data on the two tasks.\n\n\nAcknowledgements\nWe would like to thank the Social Media Processing (S",
            ", and the vocabulary size is set to 10,000 and the word vector dimension to 200. We applied 2-layer LSTM units with 512-dimension hidden states. These settings were applied to all the baselines. The parameters of the LSTMs (Eq. 14 and 15 ) are shared by the encoder and the decoder.\n\n\nAutomatic Evaluation\nWe conducted the automatic evaluation on the 8,162 stories (the entire test set). We generated endings from all the",
            "Experiments ::: Human Evaluation and Case Study\nWe also conduct human evaluation on the generated summaries. Similar to the previous work BIBREF3, BIBREF20, we randomly select 100 samples from the test set of CNN/Daily Mail dataset and ask 3 human testers to measure relevance and readability of each summary. Relevance is based on how much salient information does the summary contain, and readability is based on how fluent and grammatical the summary is. Given an article,"
        ]
    },
    {
        "title": "Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance",
        "answer": "TDC, SQuAD, NewsQA, TIMIT, Switchboard, Fisher, Wiktionary, SimLex, EDU, Annodis, Glozz, BIBREF10, BIBREF11, BIBREF12, TABREF10, FIGREF60.",
        "evidence": [
            ", where each corpus involves a specific challenge. The rest of this paper is structured as follows. Section SECREF2 discusses the related work that served as an inspiration for our analysis. Section SECREF3 comprises the proposed criteria and properties to characterize AV methods. Section SECREF4 describes the methodology, consisting of the used corpora, examined AV methods, selected performance measures and experiments. Finally, Section SECREF5 concludes the work and outlines future work.\n\n\nRelated Work\nOver the years, researchers",
            "Abstract\nIn Brazil, the governmental body responsible for overseeing and coordinating post-graduate programs, CAPES, keeps records of all theses and dissertations presented in the country. Information regarding such documents can be accessed online in the Theses and Dissertations Catalog (TDC), which contains abstracts in Portuguese and English, and additional metadata. Thus, this database can be a potential source of parallel corpora for the Portuguese and English languages. In this article, we present the development of a",
            " corpora (above 10 thousand sentences), causing large memory consumption. In these cases, the algorithm cuts the large corpus in smaller manageable chunks, which may affect dictionary building. The parallel abstracts were supplied to the aligner, which performed sentence segmentation followed by sentence alignment. A small modification in the sentence segmentation algorithm was performed to handle the fact that all words are in lowercase letters, which originally prevented segmentation. After sentence alignment, the following post-processing steps were performed: (i",
            " words); Reports from Institut Français de Relations Internationales (32 raports, 266 000 words). The corpora were noted using Glozz. Annodis aims at building an annotated corpus. The proposed annotations are on two levels of analysis, that is, two perspectives: Ascendant: part of EDU are used in the construction of more complex structures, through the relations of discourse; Descending: approaches the text in its entirety and relies on the various shallow indices to identify high",
            " with the current datasets concerns a lack of one unified procedure that ensures the comparability of resources in different languages. Further, concept pairs for different languages are sourced from different corpora (e.g., direct translation of the English data versus sampling from scratch in the target language). Moreover, the previous SimLex-based multilingual datasets inherit the main deficiencies of the English original version, such as the focus on nouns and highly frequent concepts. Finally, prior work mostly focused on languages that are widely spoken",
            " partially observable environments. Specifically, we \"occlude\" the majority of a document's text and add context-sensitive commands that reveal \"glimpses\" of the hidden text to a model. We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making. We believe that this setting can contribute in scaling models to web-level QA scenarios.\n\n",
            " BIBREF9. Surprisingly, as data is said to be “the new oil\", few data sets are available for ASR systems. The best known are corpora like TIMIT BIBREF10, Switchboard BIBREF11 or Fisher BIBREF12 which date back to the early 1990s. The scarceness of available corpora is justified by the fact that gathering and annotating audio data is costly both in terms of money and time. Telephone conversations and broadcast recordings have",
            " controversies, such as homeschooling, single-sex education, or mainstreaming. A novel aspect of the corpus is its coverage of different registers of user-generated Web content, such as comments to articles, discussion forum posts, blog posts, as well as professional newswire articles. Since the data come from a variety of sources and no assumptions about its actual content with respect to argumentation can be drawn, we conduct two extensive annotation studies. In the first study, we tackle the problem of relatively",
            "rapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10 . In addition to the raw IPA transcriptions extracted from Wiktionary, the corpus provides an automatically cleaned version of transcriptions. Cleaning is a necessary step because web-scraped data is often noisy and may be transcribed at an inconsistent level of detail. The data cleaning used",
            "This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. Dong Nguyen is supported with an Alan Turing Institute Fellowship (TU/A/000006). Maria Liakata is a Turing fellow at 40%. We would also like to thank the participants of the “Bridging disciplines in analysing text as social and cultural data” workshop held at the Turing Institute (2017) for insightful discussions. The workshop was funded by a Turing",
            " existing summaries as templates to assist with source article representations through an ingenious bi-directional selective layer. The enhanced article representations are expected to contribute towards better summarization eventually. We also developed the corresponding retrieval and re-ranking modules for obtaining quality templates. Extensive evaluations were conducted on a standard benchmark dataset and experimental results show that our model can quickly pick out high-quality templates from the training corpus, laying key foundation for effective article representations and summary generations. The results also show that our model outper",
            " ones, the final correlation scores seem substantially higher than the ones obtained by the single massively multilingual m-bert model. Finally, the results in Figure FIGREF60 again verify the usefulness of unsupervised post-processing also in cross-lingual settings. We observe improved performance with both m-bert and xlm-100 when mean centering (+mc) is applied, and further gains can be achieved by using abtt on the mean-centered vector spaces. A similar finding also holds"
        ]
    },
    {
        "title": "Knowledge Based Machine Reading Comprehension",
        "answer": "4.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2",
        "evidence": [
            "{\\prime } \\in \\mathcal {A}(q)} S(q,a^{\\prime })$$   (Eq. 16) \n\n\nThe Question Generation Model\nIn this section, we present the generation model which generates a question based on the semantics of a candidate answer. Afterward, we introduce how our paraphrasing model, which measures the semantic relevance between the generated question and the original question, is pretrained.\n\n\nHierarchical seq2",
            " appropriate question answering models can be developed for question answering on instructional videos.\n\n\n",
            " about what can be done with such data.\n\n\nOpen-ended survey questions\nOpen-ended questions are a rich source of information that should be leveraged to inform decision-making. We could be interested in several aspects of such a text document. One useful approach would be to find common, recurring topics across multiple respondents. This is an unsupervised learning task because we do not know what the topics are. Such models are known as topic models. They summarize multiple text documents into a number",
            "\nResearches on question classification, question taxonomies and QA system have been undertaken in recent years. There are two types of approaches for question classification according to Banerjee et al in BIBREF13 - by rules and by machine learning approach. Rule based approaches use some hard coded grammar rules to map the question to an appropriate answer type BIBREF14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in",
            "FORM6 .\n\n\nModelling Context in Forums\nContext has been used and modelled in various ways for different problems in discussion forums. In a work on a closely related problem of forum thread retrieval BIBREF2 models context using inter-post discourse e.g., Question-Answer. BIBREF3 models the structural dependencies and relationships between forum posts using a conditional random field in their problem to infer the reply structure. Unlike BIBREF2 , BIBREF3 can be used to",
            "-answer pairs. The annotation of this dataset is designed so that successfully answering the questions requires understanding and the knowledge involved in a document. We implement a framework consisting of both a question answering model and a question generation model, both of which take the knowledge extracted from the document as well as relevant facts from an external knowledge base such as Freebase/ProBase/Reverb/NELL. Results show that incorporating side information from external KB improves the accuracy of the baseline question answer system. We compare",
            ". When it comes to poetry generation using generative text models, Zhang and Lapata BIBREF9, Yi et al. BIBREF10 and Wang et al. BIBREF11 use language modeling to generate Chinese poems. However, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding. For the task of text generation, MaskGAN BIBREF12 uses a Reinforcement Learning signal from the discriminator,",
            ", at a game step $t$, the encoder reads observation string $o_t$ and question string $q$ to generate attention aggregated hidden representations $M_t$. Using $M_t$, the action generator outputs commands (defined in previous sections) to interact with iMRC. If the generated command is stop or the agent is forced to stop, the question answerer takes the current information at game step $t$ to generate head and tail pointers for answering the question;",
            "21 shows the distribution of question types in the dataset, which we identified using simple heuristics based on the first words of a question: Yes/no questions were identified as questions starting with an auxiliary or modal verb, all other question types were determined based on the question word. We found that 29% of all questions are yes/no questions. Questions about details of a situation (such as what/ which and who) form the second most frequent question category. Temporal questions (when and",
            " typically leads to disagreement for images showing many objects, and agreement otherwise. We employ a 2,492-dimensional feature vector to represent the question-based features. One feature is the number of words in the question. Intuitively, a longer question offers more information and we hypothesize additional information makes a question more precise. The remaining features come from two one-hot vectors describing each of the first two words in the question. Each one-hot vector is created using the learned vocabularies",
            " generation model. We can see that QGNet performs better than Seq2Seq in terms of BLEU score because many important words of low frequency from the input are replicated to the target sequence. However, the improvement is not significant for the QA task. We also incorporate each of the four KBs into QGNet, and observe slight improvements on NELL and Reverb. Despite the overall accuracy of QGNet being lower than PCNet and KVMemNet, combining",
            ", some of the question types that the RQE classifier learns will not be deemed important terms by traditional IR and the most relevant questions will not be ranked at the top of the list. Therefore, in our approach, when a question is submitted to the system, candidate questions are fetched using the IR models, then the RQE classifier is applied to filter out the non-entailed questions and re-rank the remaining candidates. Specifically, we denote INLINEFORM0 the list"
        ]
    },
    {
        "title": "Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health",
        "answer": "SNLI and multiNLI, Quora, Clinical-QE, and SemEval-cQA.",
        "evidence": [
            " reduced feature sets can produce comparable results to the much larger feature dataset.\n\n\nAcknowledgments\nResearch reported in this publication was supported by the National Library of Medicine of the [United States] National Institutes of Health under award numbers K99LM011393 and R00LM011393. This study was granted an exemption from review by the University of Utah Institutional Review Board (IRB 00076188). Note that in order to protect tweeter anonymity, we have not reproduced tweets verbat",
            "No common types).\n\n\nDatasets Used for the RQE Study\nWe evaluate the RQE methods (i.e. deep learning model and logistic regression classifier) using two datasets of sentence pairs (SNLI and multiNLI), and three datasets of question pairs (Quora, Clinical-QE, and SemEval-cQA). The Stanford Natural Language Inference corpus (SNLI) BIBREF13 contains 569,037 sentence pairs",
            " dataset is recorded across 5 sessions with 5 pairs of speakers. To ensure speaker independent learning, the dataset is split at the level of sessions: training is performed on 3 sessions (6 distinct speakers) while validation and testing are each performed on 1 session (2 distinct speakers).\n\n\nMultimodal Computational Descriptors\nAll the datasets consist of videos where only one speaker is in front of the camera. The descriptors we used for each of the modalities are as follows: Language All",
            " our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans. We conduct extensive experiments to study the generalization capabilities of the proposed model for following natural language instructions. We investigate both generalization to new instructions in known and in new environments. We conclude this paper by discussing the benefits of the proposed approach as well as opportunities for future research based on our findings.\n\n\nRelated work\n",
            " by the UK Engineering and Physical Sciences Research Council (Grant number: EP/P011829/1).\n\n\n",
            " dataset was extended from that in BIBREF21 ; the data were collected from January 2017 to December 2017. The labeled dataset includes 48,374 passages. To support semi-supervised learning, the first 3 months of data (96,777 passages) are unlabeled. Because the data stem from social media, some text exists that cannot be considered as part of any sentence, such as product links, symbols unrelated to sentences, and space between sentences. These portions were not originally annotated as",
            " the one using an embedded CNN on the Orchid and UGWC datasets.\n\n\nStatistical Tests for Thai sentence segmentation\nTo prove the significance of the model improvements, we compared the cross-validation results using paired t-tests to obtain the p-values, which are shown in Table TABREF57 for the Orchid dataset and Table TABREF58 for the UGWC dataset.\n\n\n",
            " , respectively. We found that model accuracy is highly sensitive to these parameters. Therefore, we use a grid search technique to find the best value for both parameters for the model. We apply two optimizers used in this work: Adagard BIBREF39 and Adam BIBREF40 , whose learning rates are set to 0.02 and 0.001 for the Thai and English datasets, respectively. To generalize the model, we also integrate L2 regularization with an alpha of 0.",
            ") . The third dataset (dataset-pn) is a subset of dataset-basic, containing 828 positive and 280 negative citations. Dataset-pn was used for the purposes of (1) evaluating binary classification (positive versus negative) performance using sent2vec; (2) Comparing the sentiment classification ability of PS-Embeddings with other embeddings.\n\n\nEvaluation Strategy\nOne-Vs-The-Rest strategy was adopted for the task of multi",
            " datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size influences.",
            " quantitatively evaluate the similarities between the embedding of the languages. The results can be found in the Appendix.\n\n\nWhat Does Zero-shot Transfer Model Learn? ::: Code-switching Dataset\nWe observe linguistic-agnostic representations in the last subsection. If tokens are represented in a language-agnostic way, the model may be able to handle code-switching data. Because there is no code-switching data for RC, we create artificial code-switching datasets",
            " with 0.49 average score using Wikipedia as the first answer source and Yahoo and Google searches as secondary answer sources. Each medical question was decomposed into several subquestions. To extract the answer from the selected text passage, a bi-directional attention model trained on the SQUAD dataset was used. Deep neural network models have been pushing the limits of performance achieved in QA related tasks using large training datasets. The results obtained by CMU-OAQA and PRNA showed that large open"
        ]
    },
    {
        "title": "A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media",
        "answer": "state-of-the-art NER models, end-to-end NER systems, conventional machine learning algorithms, supervised approaches, unary approaches, binary-intrinsic approaches, binary-extrinsic approaches, hybrid DNN/RNN-HMM systems, loopy belief propagation (LBP), ranking SVMs, graph pruning, PageRank/random walks.",
        "evidence": [
            "ic approach COAV, the remaining top performing methods belong to the binary-extrinsic category. This category of AV methods has also been superior in the PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 , where they outperformed binary-intrinsic and unary approaches three times in a row (2013–2015). The top performing approaches Caravel, COAV and NNCD deserve closer attention. All three are based on character-",
            " to the treatment group, but were not banned from Reddit. The goal is to estimate the counterfactual scenario: in this case, what would have happened had the site not taken action against these specific forums? An ideal control would make it possible to distinguish the effect of the treatment — closing the forums — from other idiosyncratic properties of texts that were treated. We also look for categories of documents that might not be useful. We might remove documents that are meta-discourse, like introductions and",
            " RNNLM using cold fusion approach during training to attain a better-trained model. In near future, we will incorporate all the above techniques to get comparable performance with the state-of-the-art hybrid DNN/RNN-HMM systems.\n\n\n",
            "), the H2020 project SUMMA (under grant agreement 688139), and a Google PhD Fellowship for the second author.\n\n\n",
            " there is a mismatch between the scores and systematized concept at this stage, the codebook may need to be adjusted, human coders retrained, more training data prepared, algorithms adjusted, or in some instances, even a new analytical method adopted. Other types of validation are also possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond",
            " close to the baseline level, with slight improvements in German. A first observation is that this method brings stupid BT models closer to conventional BT, at a greatly reduced computational cost. While French still remains 0.4 to 1.0 BLEU below very good backtranslation, both approaches are in the same ballpark for German - may be because BTs are better for the former system than for the latter. Finally note that the GAN architecture has two differences with basic copy-marked: (a)",
            " limiting. We are actively exploring applying these techniques to science question answering BIBREF26 , for example, where existing KBs provide only partial coverage of the questions.\n\n\n",
            " applied to other classification tasks, such as topic classification and concept mining. The experiments show that even unsupervised approaches, as in the corpus-based approach, can outperform supervised approaches in classification tasks. Combining some approaches, which can compensate for what others lack, can help us build better vectors. Our word vectors are created by conventional machine learning algorithms; however, they, as in the corpus-based model, produce state-of-the-art results. Although we preferred to use a",
            " second drawback of the global approach has been alleviated through approximate optimization techniques, such as PageRank/random walks BIBREF21 , graph pruning BIBREF22 , ranking SVMs BIBREF23 , or loopy belief propagation (LBP) BIBREF18 , BIBREF24 . However, these methods are not differentiable and thus difficult to be integrated into neural network models (the solution for the first limitation). To overcome the third issue of inadequate training data, BIB",
            " model has much steeper slope and converges to a much better minima in both the tasks. It can also be noticed that in the training procedure for the model which has both attention and conflict, the updates are much smoother.\n\n\nQualitative Comparison\nWe also show qualitative results where we can observe that our model with attention and conflict combined does better on cases where pairs are non-duplicate and has very small difference. We have observed that the conflict model is very sensitive to even minor",
            " discussions and comments. We also thank Robin Jia for the help on SQuAD evaluations. \n\n\n",
            " approaches outperform the state-of-the-art NER models (both with and without using additional visual contexts) on our new MNER dataset SnapCaptions, a large collection of informal and extremely short social media posts paired with unique images.\n\n\nRelated Work\nNeural models for NER have been recently proposed, producing state-of-the-art performance on standard NER tasks. For example, some of the end-to-end NER systems BIBREF4 ,"
        ]
    },
    {
        "title": "External Lexical Information for Multilingual Part-of-Speech Tagging",
        "answer": "English, Chinese Mandarin, Spanish, Welsh, Kiswahili, Polish, Russian, Finnish, Arabic, French, German, Italian, Japanese, Korean, Portuguese, Turkish. 11 languages. 11 target languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages. 11 languages",
        "evidence": [
            " eight different language families. This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili. We hope to further include additional languages and inspire other researchers to contribute to the effort over the lifetime of this project. The work on data collection can be divided into two crucial phases: 1) a translation phase where the extended English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages",
            " are absent just because they were filtered out before training); foreign words with separation into languages (fastText 2, web 2), words with special orthography or tokens in broken encoding (not presented here); names and surnames (RDT 8, fastText 3, web 3), including foreing names (fastText 9, web 6); toponyms (not presented here) and toponym descriptors (web 7); fairy tale characters (fastText 6); parts of speech and morphological forms (",
            ". We also observe lower absolute scores (and a larger number of OOVs) for languages with very rich and productive morphological systems such as the two Slavic languages (Polish and Russian) and Finnish. Since Polish and Russian are known to have large Wikipedias and Common Crawl data BIBREF33 (e.g., their Wikipedias are in the top 10 largest Wikipedias worldwide), the problem with coverage can be attributed exactly to the proliferation of morphological forms in",
            " defined will allow us to eliminate costly annotation efforts in the future, especially for the word-level slots and intent keywords. Once enough domain-specific multi-modal data is collected, our future work is to explore training end-to-end dialogue agents for our in-cabin use-cases. We are planning to exploit other modalities for improved understanding of the in-cabin dialogue as well.\n\n\n",
            " that questions are linguistically simple, elaborate and explicit. Workers were asked to formulate questions about details of such a situation, i.e. independent of a concrete narrative. This resulted in questions, the answer to which is not literally mentioned in the text. To cover a broad range of question types, we asked participants to write 3 temporal questions (asking about time points and event order), 3 content questions (asking about persons or details in the scenario) and 3 reasoning questions (asking how or why something",
            "-bert's subword vocabulary contains plenty of cmn-specific and yue-specific subwords which are exploited by the encoder when producing m-bert-based representations. Simultaneously, higher scores with m-bert (and xlm in Table TABREF46) are reported for resource-rich languages such as French, Spanish, and English, which are better represented in m-bert's training data. We also observe lower absolute scores (and a larger number of OOV",
            ", if someone is very angry about customer service, his or her complaint might be given priority over somewhat milder complaints.  BIBREF0 present four tasks in which systems have to automatically determine the intensity of emotions (EI) or the intensity of the sentiment (Valence) of tweets in the languages English, Arabic, and Spanish. The goal is to either predict a continuous regression (reg) value or to do ordinal classification (oc) based on a number of predefined categories.",
            "resource cross-lingual tasks besides using the English dev set.\n\n\nAcknowledgments\nWe would like to thank Jiateng Xie, Julian Salazar and Faisal Ladhak for the helpful comments and discussions.\n\n\n",
            " identify something other than the language, such as typological features about the language's phonemic inventory. This could enable better sharing of resources among languages. Such typological knowledge is readily available in databases like Phoible and WALS for a wide variety of languages. It would be interesting to explore if any of these features is a good predictor of a language's orthographic rules. It would also be interesting to apply the artificial token approach to other problems besides multilingual g2p. One closely related",
            ", however imperfectly, by the presence of particular words semi-automatically extracted from a machine learning algorithm. Operationalizations are never perfect translations, and are often refined over the course of an investigation, but they are crucial. We begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results. The research process sounds more or less linear this way, but each of these phases overlaps, and",
            " Unseen Language Dataset\nIt has been shown that extractive QA tasks like SQuAD may be tackled by some language independent strategies, for example, matching words in questions and context BIBREF20. Is zero-shot learning feasible because the model simply learns this kind of language independent strategies on one language and apply to the other? To verify whether multi-BERT largely counts on a language independent strategy, we test the model on the languages unseen during pre-training. To make",
            " the public” from $W_q$ and accordingly update the grammar. The controlled sampling ensures that each sampled question uses words from a single start-to-end path in $W_q$ . For example, we could sample a question what is Czech Republic 's language? by sampling words from the path (what, language, do, people 's, in, Czech, Republic, is speaking, ?) in Figure 1 . We repeat this sampling process to generate multiple potential paraphr"
        ]
    },
    {
        "title": "Disunited Nations? A Multiplex Network Approach to Detecting Preference Affinity Blocs using Texts and Votes",
        "answer": "SNLI and multiNLI, Quora, Clinical-QE, and SemEval-cQA.",
        "evidence": [
            "IBREF18 , researchers propose a system based on a densely connected LSTM network.\n\n\nOur Dataset\nIn this section, we describe how we build our dataset with tweets. First, we crawl over 2M tweets from twitter using GetOldTweets-python. We crawl English tweets from 04/09/2012 to /12/18/2018. We first remove all re-tweets and use langdetect to remove all non-English sentences. Then, we",
            " dataset is used for the training. Typically, the source dataset is bigger than that of the target, and this method sometimes works better than TgtOnly. Another method is All, in which the source and target datasets are combined and used for the training. Although this method uses all the data, the training criteria enforce the model to perform well on both of the domains, and therefore the performance on the target domain is not necessarily high.  An approach widely used in the neural network community is FineT",
            " in future work.\n\n\nEmotion datasets\nThree datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them",
            " are removed, and the data are partitioned into 10 parts containing equal numbers of sentences to support 10-fold cross-validation. Each training set is split into one part used for validation and the rest is used for model training. Subsequently, all the words in each dataset are concatenated and then separated into sequences with 200 words per instance. Each sequence always begins with the first word of a sentence. If a sequence ends with an unfinished sentence, the next sequence starts with that complete sentence",
            "No common types).\n\n\nDatasets Used for the RQE Study\nWe evaluate the RQE methods (i.e. deep learning model and logistic regression classifier) using two datasets of sentence pairs (SNLI and multiNLI), and three datasets of question pairs (Quora, Clinical-QE, and SemEval-cQA). The Stanford Natural Language Inference corpus (SNLI) BIBREF13 contains 569,037 sentence pairs",
            " datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size influences.",
            " use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions. We split the dataset into training (9,731 questions on 1,470 texts), development (1,411 questions on 219 texts), and test set (2,797 questions on 430 texts).",
            " troubled the researchers from the onset. Could the methodology be adopted to target the speech of groups like Black Lives Matter? Could it be adopted by repressive governments to minimize online dissent? While these concerns remained, they concluded that hypothetical dual use scenarios did not outweigh the tangible contribution this research could offer towards making the online environment more equal and just.\n\n\nData\nThe next step involves deciding on the data sources, collecting and compiling the dataset, and inspecting its metadata.\n\n\nData acquisition\nMany scholars in",
            " use their GPUs to perform the experiments mentioned in the paper. We also thank the anonymous reviewers for their careful reading of our paper and insightful comments.\n\n\n",
            "; in contrast, the WinoGrande dataset has devoted considerable effort to reducing such biases, which may allow models to (inadvertently) “cheat” (for example, using simple statistical associations). While it is certainly true that datasets over-estimate the commonsense reasoning capabilities of modern models BIBREF1, there are alternative and complementary explanations as well: It has been a fundamental assumption of the research community that commonsense reasoning is difficult because it comprises tacit rather than explicit knowledge",
            " can observe that INLINEFORM0 has a bigger similarity to S1 in 5 occasions compared to other two references, while INLINEFORM1 is more similar to S2 in 7 transcripts (underline). After computing the mean F1 scores over the transcripts, it can be concluded that in average S2 had a better performance segmenting the dataset compared to S1, obtaining a F1 score equal to 0.510. But... What about the complexity of the dataset? Regardless all references have been",
            " platform-based metadata BIBREF8, BIBREF9, BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be changing direction, with deep learning models being used for both feature extraction and the training of classifiers. These newer models are applying deep learning approaches such as Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs), etc.BIBREF6, BIBREF0 to enhance the"
        ]
    },
    {
        "title": "When to reply? Context Sensitive Models to Predict Instructor Interventions in MOOC Forums",
        "answer": "attention based models",
        "evidence": [
            "Abstract\nDue to time constraints, course instructors often need to selectively participate in student discussion threads, due to their limited bandwidth and lopsided student--instructor ratio on online forums. We propose the first deep learning models for this binary prediction problem. We propose novel attention based models to infer the amount of latent context necessary to predict instructor intervention. Such models also allow themselves to be tuned to instructor's preference to intervene early or late. Our three proposed attentive model variants to infer the latent context improve",
            " chunk before $x_i$, and its label. For example, in Fig. FIGREF8, $c_4=\\langle 3, 3, \\text{VP}\\rangle $ for $x_4=\\text{the}$. Chunks, $c$ are only used as conditioning context via $p(x_i \\mid x_{<i}, c_{\\leqslant i})$; they are not predicted. Because the $c$",
            " recall curve compared to the original date-adjusted model (1.156 vs. 0.959). Constructing affinity blocs based on texts and votes in tandem thus leads to a more substantively intuitive model, as well as increased predictive performance. Although this is a relatively large gain in predictive accuracy, these substantively small quantities confirm that the prediction of violent conflict onset remains an enduring challenge for scholars of IR. Nonetheless, the ability to exploit revealed preference information in speeches and votes in tandem appears to",
            "PA performs better than PPA and APA on low intervention courses (i.ratio INLINEFORM3 0.25) mainly because PPA and APA's performance drops steeply when i.ratio drops (see col.2 parenthesis and INLINEFORM4 of PPA and APA). While all the proposed models beat the baseline on every course except casebased-2. On medicalneuro-2 and compilers-4 which have the lowest i.ratio among",
            " represent context clues which are spanning in the story context. In addition, commonsense knowledge is applied through multi-source attention to facilitate story comprehension, and thus to help generate coherent and reasonable endings. Through building context clues and using implicit knowledge, the model is able to produce reasonable story endings. context clues implied in the post and make the inference based on it. Automatic and manual evaluation shows that our model can generate more reasonable story endings than state-of-the-art baselines.\n\n\nIntroduction",
            " matrix in order to model rich compositionality. TreeLSTM BIBREF9 : Recursive neural network with Long Short-Term Memory unit.\n\n\nDiscussion and Qualitative Analysis\nIn our models, the latent vector $\\mathbf {z}$ controls the process of predicting network's parameters and its dimensionality determines the number of model's parameters. Next, we will investigate how the controlling vector $\\mathbf {z}$ influences the performance of our models. Figure 4 shows the accur",
            " the thread structure and the sequence of posts explicitly improves performance. Instructors in different MOOCs from different subject areas intervene differently. For example, on a Science, Technology, Engineering and Mathematics (STEM) MOOC, instructors may often intervene early as possible to resolve misunderstanding of the subject material and prevent confusion. However, in a Humanities MOOC, instructors allow for the students to explore open-ended discussions and debate among themselves. Such instructors may prefer to intervene later in the discussion to encourage further",
            " annotations in specialized domains: Can we predict item difficulty? We define a training instance as difficult if a lay annotator or an automated model disagree on its labeling. We show that difficulty can be predicted, and that it is distinct from inter-annotator agreement. Further, such predictions can be used during training to improve information extraction models. Are there systematic differences between expert and lay annotations? We observe decidedly lower agreement between lay workers as compared to domain experts. Lay annotations have high precision but low recall with",
            " splash, mountain), and thus they often need additional contexts to correctly predict. In the training data, visual contexts make stronger indicators for these named entities (space needle, splash mountain), and the modality attention module successfully attends more to stronger signals. For text-only model (W+C), we observe that performance gains mostly come from the modality attention module better handling tokens unseen during training or unknown tokens from the pre-trained word embeddings matrix. For example, while WaRriO",
            " or link them to other users and posts for improved performance. The 17.5% improvement when adding user information suggests that user information is especially useful when the dataset is highly imbalanced. All models that consider user information predict the minority class successfully. UCTNN without topic information works well but achieves lower performance than the full UTCNN model. The 4.9% performance gain brought by LDA shows that although it is satisfactory for single topic datasets, adding that latent topics still benefits performance: even when",
            " use the post prior to the instructor's reply as a query over the contexts INLINEFORM1 to compute attention weights INLINEFORM2 , which are then used to compute the attended context representation INLINEFORM3 (recall again that the intervention text itself is not available for this purpose). This attention formulation makes an equivalence between the final INLINEFORM4 post and the prospective intervention, using Post INLINEFORM5 as the query for finding the appropriate context INLINEFORM6 , inclusive of itself",
            " representation that beyond the target sentence. The previously proposed contextual language models treat preceding sentences as a sequence of inputs, and they are suitable for document level context modeling. In dialog modeling, however, dialog interactions between speakers play an important role. Modeling utterances in a dialog as a sequence of inputs might not well capture the pauses, turn-taking, and grounding phenomena BIBREF12 in a dialog. In this work, we propose contextual RNN language models that specially track the interactions between speakers."
        ]
    },
    {
        "title": "A Hierarchical Model for Data-to-Text Generation",
        "answer": "1.0",
        "evidence": [
            " we can see that we obtain improvements on the F-measure over the baseline method in most experimental settings except BG with INLINEFORM0 . In a few more cases, the baseline model sometimes obtains higher accuracy scores for the reason described in UID38 . In our cross-lingual experiments shown in Table TABREF37 , we also note F-measure improvements over the baseline model with the exception of DA/SV when INLINEFORM0 . We observe that the improvements are on",
            "REF11 b). For example, our system accelerates the collection of 70% of the diversity by 21% over the Status Quo baseline. In addition, our system accelerates the collection of the 82% of diversity one would observe with VizWiz by 23% (i.e., average of 3.3 answers per visual question). In absolute terms, this means eliminating the collection of 92,180 answers with no loss to captured answer diversity. This translates to eliminating 19 40-hour work",
            " knowledge of the products while baseline 2 might provide additional information by showing top images from search engines. With the baseline 2, we attempt to measure whether merely adding \"relevant\" or \"similar\" products' images would be sufficient to improve the end-users' ability to comprehend the product's intended use. Moreover, with SimplerVoice, we test if our system could provide users with the proper visual components to help them understand the products' usage based on the proposed techniques, and measure the usefulness of Sim",
            " entirely fixed such that there is no further possibility to improve its performance from outside (without deviating from the definitions in the publication of the method), the method is considered to be non-optimizable. Non-optimizable AV methods are preferable in forensic settings as, here, the existence of a training/validation corpus is not always self-evident. Among the proposed AV approaches in the respective literature, we identified only a small fraction BIBREF21 , BIBREF2 , BIB",
            " A typical approach to this task is to use an IR method to find similar question candidates, then a more sophisticated method to select and re-rank the similar questions. We followed a similar approach for this evaluation by combining the LR method with the IR baseline provided in the context of SemEval-cQA. The hybrid method combines the score provided by the Logistic Regression model and the reciprocal rank from the IR baseline using a weight-based combination:  INLINEFORM0  The weight",
            " in the poorest performance on all datasets. Secondly, our LAN model outperforms all the other baselines significantly, especially on the Hit@k metrics. The improvement on the MR metric of LAN might not be considerable. This is due to the flaw of the MR metric since it is more sensitive to lower positions of the ranking, which is actually of less importance. The MRR metric is proposed for this reason, where we could observe consistent improvements brought by LAN. The effectiveness of LAN on link prediction valid",
            " of randomly selected out-of-domain natural sentences, with the same architecture and training parameters, running validation every 2000 updates with a patience of 10. Since BPE units are selected based only on the out-of-domain statistics, fine-tuning is performed on sentences that are slightly longer (ie. they contain more units) than for the initial training. This system defines an upper-bound of the translation performance and is denoted below as natural. Our baseline and topline results are",
            " in terms of explainability. Figure 3 furthermore suggests that LRP provides semantically more meaningful semantic extraction than the baseline methods. In the next section we will confirm these observations quantitatively.\n\n\nQuantitative Evaluation\nIn order to quantitatively validate the hypothesis that LRP is able to identify words that either support or inhibit a specific classifier decision, we conduct several word-deleting experiments on the CNN models using LRP scores as relevance indicator. More specifically, in accordance to the word-",
            "ability task, and find they considerably improve performance over a majority-class baseline. SVM: We define 3 sets of features to characterize each question. The first is a simple bag-of-words set of features over the question (SVM-BOW), the second is bag-of-words features of the question as well as length of the question in words (SVM-BOW + LEN), and lastly we extract bag-of-words features, length of the question",
            "-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance .\n\n\nBaseline Agent\nAs a baseline, we propose QA-",
            " the baselines which mitigate the KL collapse issue by carefully engineering the weight between the reconstruction loss and KL loss or choosing a different choice of prior, we provide a simple and elegant solution through holistic KL regularisation, which can effectively mitigate the KL collapse issue and achieve a better reconstruction error in both training and testing. Sentence reconstruction. Lastly, we show some sentence examples reconstructed by vMF-VAE (i.e., the best baseline) and our model in the inputless setting using sentences from",
            " on Clinical-QE gave the best performance with 73.18% Accuracy. The SNLI and multi-NLI models did not perform well when tested on medical RQE data. We performed additional evaluations using the RTE-1, RTE-2 and RTE-3 open-domain datasets provided by the PASCAL challenge and the results were similar. We have also tested the SemEval-cQA-2016 model and had a similar drop in performance on RQ"
        ]
    },
    {
        "title": "Fully Convolutional Speech Recognition",
        "answer": "WER. Our baseline system, trained on mel-filterbanks, and decoded with a n-gram language model has a INLINEFORM3 WER. Replacing the n-gram LM by a convolutional one reduces the WER to INLINEFORM2.  -art among end-to-end systems trained only on WSJ, and hence the most comparable to our system, uses lattice-free MMI on augmented data (with speed perturbation) and gets INLINEFORM2 WER. Our baseline system, trained on mel-filterbanks, and decoded with a n-gram language model has a INLINEFORM3 WER. Replacing the n-gram LM by a convolutional one reduces the WER to INLINEFORM2.  -art among end-to-end systems trained only on WSJ, and hence the most comparable to our system, uses lattice-free MMI on augmented data (with speed perturbation) and gets INLINEFORM2 WER. Our baseline system, trained on mel-filterbanks, and decoded with a n-gram language model has a INLINEFORM3 WER. Replacing the n-gram LM by a convolutional one reduces the WER to INLINEFORM2.  -art among end-to-end systems trained only on WSJ, and hence the most comparable to our",
        "evidence": [
            "-art among end-to-end systems trained only on WSJ, and hence the most comparable to our system, uses lattice-free MMI on augmented data (with speed perturbation) and gets INLINEFORM2 WER. Our baseline system, trained on mel-filterbanks, and decoded with a n-gram language model has a INLINEFORM3 WER. Replacing the n-gram LM by a convolutional one reduces the WER to INLINE",
            "-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement",
            " discussions and comments. We also thank Robin Jia for the help on SQuAD evaluations. \n\n\n",
            " `advocated', `they' would refer to the women and would be translated as `elles'. In the later versions of the thesis, published as (Winograd 1972), he changed `women' to `demonstrators'; this made the disambiguation clearer, but lost the point about translation.\n\n\nCurrent state of the art\nNo one familiar with the state of the art in machine translation technology or the state of the art of artificial intelligence generally will be surprised to learn",
            " GM, KRM, WS. Figure design: LA, GM, WS. Final drafting: all equally.\n\n\n",
            " Algorithms ::: Support Vector Machine (SVM)\nSVM gives an optimal hyper-plane and it maximizes the margin between classes. We use Radial Basis Function (RBF) kernel in our system to make decision boundary curve-shaped. For decision function shape, we use the original one-vs-one (ovo) decision function.\n\n\nImplementation of the System ::: Classification Algorithms ::: Naive Bayesian Classifier (NBC)\nNBC is",
            "\n\n\nIntroduction\nSupervised text classification has achieved great success in the past decades due to the availability of rich training data and deep learning techniques. However, zero-shot text classification ($\\textsc {0shot-tc}$) has attracted little attention despite its great potential in real world applications, e.g., the intent recognition of bank consumers. $\\textsc {0shot-tc}$ is challenging because we often have to deal with classes that are compound, ultra-fine-",
            "REF93, which roughly correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample.  3) Source: CARD-660 BIBREF78. 67 word pairs are taken from this dataset focused on rare word similarity, applying the same selection criteria a) to e) employed for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus BIBREF94. CARD-660",
            " is roughly 3x faster to train than (S7) using the same GPU resources, so (E1) is also 1.5x faster to train than a single model (S7).\n\n\n",
            " conclude how well a SBD system is performing given the final application of the transcript? In this paper we propose Window-based Sentence Boundary Evaluation (WiSeBE), a semi-supervised metric for evaluating Sentence Boundary Detection systems based on multi-reference (dis)agreement. We evaluate and compare the performance of different SBD systems over a set of Youtube transcripts using WiSeBE and standard metrics. This double evaluation gives an understanding of how WiSeBE is a more reliable",
            " hashtags, the performance is severely limited on the questions requiring such reasoning abilities. We believe that good segmentation, disambiguation and linking tools developed by the social media community for processing the userIDs and hashtags will significantly help these question types. Besides the easy questions requiring mainly paraphrasing skill, we also find that the questions requiring the understanding of authorship and oral/tweet English habits are not very difficult. We think this is due to the reason that, except for these tweet",
            " Parameters are optimized by SGD. For regularization, we apply dropout on word embeddings and outputs of every layer of LSTMs, with weight decay of 1.2e-6, and anneal the learning rate by 0.5 if the validation perplexity does not improve successively, checking every 5,000 mini-batches. Mini-batch size, dropout weight, and initial learning rate are tuned by perplexity on the dev set of Wikipedia dataset. The"
        ]
    },
    {
        "title": "Massively Multilingual Neural Grapheme-to-Phoneme Conversion",
        "answer": "novel-9, dataset-pn, dataset-basic, SNLI, multiNLI, Quora, Clinical-QE, SemEval-cQA, Affective Text dataset, Fairy Tales dataset, ISEAR dataset.",
        "evidence": [
            " was then added to our original training set. Again, the machine translation platform Apertium BIBREF5 was used for the translation of the datasets.\n\n\nAlgorithms Used\nThree types of models were used in our system, a feed-forward neural network, an LSTM network and an SVM regressor. The neural nets were inspired by the work of Prayas BIBREF7 in the previous shared task. Different regression algorithms (e.g. AdaBoost,",
            "Datasets\nThe statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors.",
            " thank Dinesh Jayaraman, Yu-Chuan Su, Suyog Jain, and Chao-Yeh Chen for their assistance with experiments.\n\n\n",
            ") . The third dataset (dataset-pn) is a subset of dataset-basic, containing 828 positive and 280 negative citations. Dataset-pn was used for the purposes of (1) evaluating binary classification (positive versus negative) performance using sent2vec; (2) Comparing the sentiment classification ability of PS-Embeddings with other embeddings.\n\n\nEvaluation Strategy\nOne-Vs-The-Rest strategy was adopted for the task of multi",
            "aset\nData Collection. To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. Paragraph",
            " troubled the researchers from the onset. Could the methodology be adopted to target the speech of groups like Black Lives Matter? Could it be adopted by repressive governments to minimize online dissent? While these concerns remained, they concluded that hypothetical dual use scenarios did not outweigh the tangible contribution this research could offer towards making the online environment more equal and just.\n\n\nData\nThe next step involves deciding on the data sources, collecting and compiling the dataset, and inspecting its metadata.\n\n\nData acquisition\nMany scholars in",
            " results were achieved using the largest feature set (01234). For mainstreaming and red-shirting, the best results were achieved using only the feature set 4 (embeddings). These two domains contain also fewer documents, compared to other domains (refer to Table TABREF71 ). We suspect that embeddings-based features convey important information when not enough in-domain data are available. This observation will become apparent in the next experiment. The cross-domain experiments yield rather poor results",
            " use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions. We split the dataset into training (9,731 questions on 1,470 texts), development (1,411 questions on 219 texts), and test set (2,797 questions on 430 texts).",
            "No common types).\n\n\nDatasets Used for the RQE Study\nWe evaluate the RQE methods (i.e. deep learning model and logistic regression classifier) using two datasets of sentence pairs (SNLI and multiNLI), and three datasets of question pairs (Quora, Clinical-QE, and SemEval-cQA). The Stanford Natural Language Inference corpus (SNLI) BIBREF13 contains 569,037 sentence pairs",
            " can observe that INLINEFORM0 has a bigger similarity to S1 in 5 occasions compared to other two references, while INLINEFORM1 is more similar to S2 in 7 transcripts (underline). After computing the mean F1 scores over the transcripts, it can be concluded that in average S2 had a better performance segmenting the dataset compared to S1, obtaining a F1 score equal to 0.510. But... What about the complexity of the dataset? Regardless all references have been",
            " In Figure 1, an overview of the system proposed is presented.\n\n\nDataset\nIn May 2018, we crawled Twitter using the Python library Tweepy, creating two datasets on which Contributor and Musical Work entities have been manually annotated, using IOB tags. The first set contains user-generated tweets related to the BBC Radio 3 channel. It represents the source of user-generated content on which we aim to predict the named entities. We create it filtering the messages containing hashtags",
            " in future work.\n\n\nEmotion datasets\nThree datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them"
        ]
    },
    {
        "title": "A Neural Approach to Irony Generation",
        "answer": "annotators, DualRL system and the current model.",
        "evidence": [
            " can achieve the second-best results in sentiment and content preservation. Nevertheless, DualRL system and ours get poor performances in irony accuracy. The reason may be that the other four baselines tend to generate common and even not fluent sentences which are irrelevant to the input sentences and are hard to be identified as ironies. So annotators usually mark these output sentences as non-ironic sentences, which causes these models to obtain better performances than DualRL and ours but much poorer results in sentiment and content preservation",
            " therefore unsuitable for analyzing argumentation. Three blog posts and two articles were found not to be argumentative (the authors took no stance to the discussed controversy) and one article was an interview, which the current model cannot capture (a dialogical argumentation model would be required). For each of the 340 documents, the gold standard annotations were obtained using the majority vote. If simple majority voting was not possible (different boundaries of the argument component together with a different component label), the gold standard was set",
            " and intra-coder reliability should be particularly high. Accuracy, as well as other measures such as precision, recall and F-score, are sometimes presented as a measure of validity, but if we do not have a genuinely objective determination of what something is supposed measure—as is often the case in text analysis—then accuracy is perhaps a better indication of reliability than of validity. In that case, validity needs to be assessed based on other techniques like those we discuss later in this section. It is",
            " by the European Court of Human Rights, and manually filtered the top SAGE words based on this definition. Not all identified words fitted the definition. The others included: the names of the subreddits themselves, names of related subreddits, community-specific jargon that was not directly related to hate speech, and terms such as IQ and welfare, which were frequently used in discourses of hate speech, but had significant other uses. The word lists provided the measurement instrument for their main result, which is that the use",
            " more than 30 percent points. Crowdworkers correctly identified 62% of the neutral, 85% of the positive, and 92% of the negative sentiments. Google Cloud correctly identified 88% of the neutral sentiments, but only 3% of the positive, and 19% of the negative sentiments. TensiStrength correctly identified 87.2% of the neutral sentiments, but 10.5% of the positive, and 8.1% of the negative sentiments. Rosette Text Analytics correctly identified 22.7%",
            " did not limit the number of questions a worker can input to a corresponding segment and encouraged them to input a diverse set of questions which the span can answer. Along with the questions, the workers were also required to provide a justification as to why they made their questions. We manually checked this justification to filter out the questions with poor quality by removing those questions which were unrelated to the video. One initial challenge worth mentioning is that at first some workers input questions they had about the video and not questions which the",
            " typical problems, such as implicitness or topic relevance. First, the authors often express their stance towards the topic implicitly, so it must be inferred by the reader. To do so, certain common-ground knowledge is required. However, such knowledge heavily depends on many aspects, such as the reader's familiarity with the topic or her cultural background, as well as the context of the source website or the discussion forum thread. This also applies for sarcasm and irony. Second, the decision whether a particular",
            "the-art performance, they still heavily relied on hand-crafted features. A few researches explored to directly measure similarity based on distributed representations. BIBREF5 trained a language model based on convolutional neural networks to project sentences onto distributed representations. BIBREF26 treated single document summarization as a sequence labeling task and modeled it by the recurrent neural networks. Others like BIBREF9 , BIBREF13 just used the sum of trained word embeddings to represent sentences or documents",
            " validated the alignment quality for 400 sentences randomly selected from the parsed corpus and assigned quality labels according Section SECREF9 . From all the evaluated sentences, 82.30% were correctly aligned, while 13.33% were partially aligned, and 4.35% presented no alignment. The small percentage of no alignment is probably due to the use of Hunalign tool with the provided EN/PT dictionary. Regarding the partial alignment, most of the problems are result of segmentation issues previous to the alignment, which",
            " semantic features to determine sentence offensiveness levels to identify cyberbullying, offensive or abusive comments on Youtube. On Youtube as well, dinakar2012common identified sensitive topics for cyberbullying. dadvar2014experts used expert systems to classify between bullying and no bullying in posts. van2015detection predict fine-grained categories for cyberbullying, distinguishing between insults and threats and identified user roles in the exchanges. Finally, hardaker2010trolling argues that trolling cannot be studied using",
            " of the following three rounds:  Round 1: All annotators are asked to follow the instructions outlined above, and to rate all 1,888 pairs with integer scores between 0 and 6.  Round 2: We compare the scores of all annotators and identify the pairs for each annotator that have shown the most disagreement. We ask the annotators to reconsider the assigned scores for those pairs only. The annotators may chose to either change or keep the scores. As in the case with Round 1",
            " only the post content to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns). Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant. Raw agreement between annotators is 0.91, indicating high agreement. Specifically, Cohen’s Kappa for Neu"
        ]
    },
    {
        "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer",
        "answer": "QANet BIBREF2.",
        "evidence": [
            "rained models or created more generally by the user from user-specifications. Model - All models follow the same hierarchy of abstraction: a base class implements the model's computation graph from encoding (projection on the embedding matrix) through the series of self-attention layers and up to the last layer hidden states. The base class is specific to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of the",
            " having a better architecture such as VGG-BLSTM helps in improving multilingual performance. Except Pashto, Georgian and Tokpisin, the multilingual VGG-BLSTM model gave 8.8 % absolute gain in average over monolingual model. In case of multilingual BLSTMP, except Pashto and Georgian an absolute gain of 5.0 % in average is observed over monolingual model. Even though the VGG-BLSTM gave improvements,",
            "Question Answering Model\nThe used architecture of the QA model is briefly summarized below. Here we choose QANet BIBREF2 as the base model due to the following reasons: 1) it achieves the second best performance on SQuAD, and 2) since there are completely no recurrent networks in QANet, its training speed is 5x faster than BiDAF BIBREF17 when reaching the same performance on SQuAD. The network architecture is illustrated in Figure FIG",
            "In this section, we describe our proposed method in two subsections. The first subsection specifies the model architecture and the details of each module. Our first and second contributions, which are local and distant representations, are mainly described in this subsection. Meanwhile, the second subsection expounds on how the model is trained with unlabeled data through the modified CVT, which is our third contribution.\n\n\nModel architecture\nIn this work, the model predicts the tags INLINEFORM0 for the tokens in a",
            " The softmax activation function is applied to the hidden layer to classify. 2. Insert nonlinear layers: Here, the first architecture is upgraded and an architecture with a more robust classifier is provided in which instead of using a fully connected network without hidden layer, a fully connected network with two hidden layers in size 768 is used. The first two layers use the Leaky Relu activation function with negative slope = 0.01, but the final layer, as the first architecture, uses softmax activation",
            " is described in BIBREF25. It uses the KALDI toolkit BIBREF26, following a standard Kaldi recipe. The acoustic model is based on a hybrid HMM-DNN architecture and trained on the data summarized in Table . Acoustic training data correspond to 100h of non-spontaneous speech type (mostly broadcast news) coming from both radio and TV shows. A 5-gram language model is trained from several French corpora (3,323M words in",
            "5]. The framework that GANs use for generating new data points employs an end-to-end neural network comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being directly exposed to the real samples. Instead, this model learns to reproduce the general patterns of the input via its interaction with the discriminator. The role of the discriminator, in turn, is to tell apart which data",
            "-view dynamics in the network architecture and continuously models both dynamics through time. In MARN, view-specific dynamics within each modality are modeled using a Long-short Term Hybrid Memory (LSTHM) assigned to that modality. The hybrid memory allows each modality's LSTHM to store important cross-view dynamics related to that modality. Cross-view dynamics are discovered at each recurrence time-step using a specific neural component called the Multi-attention Block (MAB",
            ") will be of limited use due to the paucity of relevant corpora. Yet, previous encoder-only architectures like RoBERTa that exploit a language modeling objective (that is, relying only on explicit textual knowledge) can clearly make headway in a commonsense reasoning task, and we can further improve upon these approaches with a sequence-to-sequence model. This leaves us with two possible explanations: despite careful controls, the WinoGrande challenge still contains incidental biases that these more",
            " components can learn each other. The CNN architecture learns sentence features layer by layer, then those features are justified by all constituent words. During pretraining, all the model parameters, including mutichannel input, convolution parameters and fully connected layer, will be updated until they are mature to extract the sentence features. Subsequently, the same sets of parameters will be fine-tuned for supervised classification tasks. In sum, this pretraining is designed to produce good initial values for both model parameters and word",
            " go to the end of the corridor and turn left, go straight to the end of the corridor and turn left again. After passing bookshelf on your left and table on your right, Enter the kitchen on your right.” For both routes, the proposed model was able to predict the correct sequence of navigation behaviors. This result suggests that the model is indeed using the input instructions and is not just approximating shortest paths in the behavioral graph. Other examples on the prediction of sub-obt",
            " decoders are utilized to decode ironic sentences and non-ironic sentences from latent representations respectively. In order to enforce a shared latent space, we share two layers on both the encoder side and the decoder side. Our model architecture is illustrated in Figure FIGREF13 . We denote irony encoder as INLINEFORM6 , irony decoder as INLINEFORM7 and non-irony encoder as INLINEFORM8 , non-irony decoder as INLINEFORM9 ."
        ]
    },
    {
        "title": "From Textual Information Sources to Linked Data in the Agatha Project",
        "answer": "overall accuracy after filtering and accuracy given that the segment is in the top 10 videos.",
        "evidence": [
            " distance distance to the query. Metrics. To evaluate our pipeline approach we use overall accuracy after filtering and accuracy given that the segment is in the top 10 videos. While the first metric is similar to SECREF17, the second can indicate if initially searching on the video space can be used to improve our selection:\n\n\nBaselines ::: Results\nTables TABREF20, TABREF21, TABREF22 show the results. First, the tables show that the two first",
            " of true state preferences and propensity to engage in armed conflict. However, these methods significantly improve our ability to identify meaningful patterns of preference similarity amongst the noise. Furthermore, the approach can assist any political study that seeks to measure position similarities from textual data and detect dense clusters of affinity or antagonism across multiple relational datasets. These might include social media actors who operate across multiple platforms, as well as contexts like legislatures where complex relations exist across votes, speeches, committee memberships, and others. Therefore, the",
            "\nAcknowledgments\nThis work was funded by the Amazon Academic Research Awards program.\n\n\n",
            "osing a large amount of human cost. Moreover, we believe that this design makes it easy to compare performances between interactive entity population pipelines and develop more sophisticated ones.\n\n\n",
            " an AV method can be rated as reliable. Moreover, we clarified a number of misunderstandings in previous research works and proposed three clear criteria that allow to classify the model category of an AV method, which in turn influences its design and the way how it should be evaluated. In regard to binary-extrinsic AV approaches, we explained which challenges exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV methods on three self-compiled corpora, where",
            ". Evaluation: We evaluate the methods on how well they predict the next observation in the validation set. For the HMM models, we do a forward pass on the validation set (no backward pass unlike the full FFBS), and compute the HMM state distribution vector $p_t$ for each time step $t$ . Then we compute the predictive likelihood for the next observation as follows: $ P(y_{t+1} | p_t) =\\sum _{x_",
            "0 absolute WER).\n\n\nConclusion\nWe introduced the first fully convolutional pipeline for speech recognition, that can directly process the raw waveform and shows state-of-the art performance on Wall Street Journal and on Librispeech among end-to-end systems. This first attempt at exploiting convolutional language models in speech recognition shows significant improvement over a 4-gram language model on both datasets. Replacing mel-filterbanks by a learnable front-end gives additional gains",
            " this task for the following reasons: Our collection of 47,457 question-answer pairs was collected from only 12 NIH institutes and is unlikely to contain more than 100 occurrences of the same focus-type pair. Each question was indexed with additional annotations for the question focus, its synonyms and the question type synonyms.\n\n\nEvaluating RQE for Medical Question Answering\nThe objective of this evaluation is to study the effectiveness of RQE for Medical Question Answering,",
            " this task. More specifically, incremental encoding is effective due to the facts: 1) IE is significantly better than Seq2Seq and HLSTM in grammar (Sign Test, 1.84 vs. $1.74/1.57$ , p-value= $0.046/0.037$ , respectively), and in logicality (1.10 vs. 0.70/0.84, p-value $<0.001/0.001$",
            " gates, we conducted an ablation study by discarding them respectively, and the results show that, while both of the gates are necessary, the template-to-article (T2A) gate tends to be more important than the article-to-template (A2T) gate. A human evaluation further validates the effectiveness of our model in generating informative, concise and readable summaries. 1.0 The contributions of this work include:\n\n\nThe Framework\nOur framework includes three key",
            " the highest one: 4.82 (the stdev is 0.35) which means the most effective approach to provide users with products' usage. Additionally, a paired-samples t-test was conducted to compare the MOS scores of users' responses among all products using baseline 1 and SimplerVoice system. There was a significant difference in the scores for baseline 1 (Mean = 2.57, Stdev = 1.17) and SimplerVoice (Mean = 4.",
            " experiments, we first verify the efficiency of NCEL via theoretically comparing its time complexity with other collective alternatives. Afterwards, we train our neural model using collected Wikipedia hyperlinks instead of dataset-specific annotations, and perform evaluations on five public available benchmarks. The results show that NCEL consistently outperforms various baselines with a favorable generalization ability. Finally, we further present the performance on a challenging dataset WW BIBREF19 as well as qualitative results, investigating the effectiveness of each key module.\n\n\n"
        ]
    },
    {
        "title": "Sparse and Constrained Attention for Neural Machine Translation",
        "answer": "cmn-spa, heb-est, rus-fin, cmn-heb, cmn-est, cmn-rus, cmn-pol, cmn-fin, cmn-spa, cmn-heb, cmn-est, cmn-rus, cmn-pol, cmn-fin, cmn-est, cmn-rus, cmn-pol, cmn-fin, cmn-spa, cmn-heb, cmn-est, cmn-rus, cmn-pol, cmn-fin, cmn-est, cmn-rus, cmn-pol, cmn-fin, cmn-spa, cmn-heb, cmn-est, cmn-rus, cmn-pol, cmn-fin, cmn-est, cmn-rus, cmn-pol, cmn-fin, cmn-spa, cmn-heb, cmn-est, cmn-rus, cmn-pol, cmn-fin, cmn-est, cmn-rus, cmn-pol, cmn-fin, cmn-spa, cmn-heb, cmn-est, cmn-rus, cmn-pol, cmn-fin, cmn-est, cmn-rus",
        "evidence": [
            " with vecmap for language pairs such as cmn-spa, heb-est, and rus-fin. However, typological differences (e.g., morphological richness) still play an important role as we observe very low scores when pairing cmn with morphologically rich languages such fin, est, pol, and rus. Similar to prior work of Vulic:2019we and doval2019onthe, given the fact that unsupervised vecmap is the most robust",
            " languages. A similar issue related to the mapping problem across languages arose in the Welsh concept pair yn llwye – yn gyfan gwbl, where Welsh speakers agreed that the two concepts are very similar. When asked, bilingual speakers considered the two Welsh concepts more similar than English equivalents purely – completely, potentially explaining why a higher average similarity score was reached in Welsh. The example of woman – wife can illustrate cultural differences or another translation-related issue where the word `wife' did not",
            " instructed to find direct or approximate translations for the 1,888 word pairs that satisfy the following rules. (1) All pairs in the translated set must be unique (i.e., no duplicate pairs); (2) Translating two words from the same English pair into the same word in the target language is not allowed (e.g., it is not allowed to translate car and automobile to the same Spanish word coche). (3) The translated pairs must preserve the semantic relations between the",
            " we propose contextual RNN language models that specially track the interactions between speakers. We expect such models to generate better representations of the dialog context. The remainder of the paper is organized as follows. In section 2, we introduce the background on contextual language modeling. In section 3, we describe the proposed dialog context language models. Section 4 discusses the evaluation procedures and results. Section 5 concludes the work.\n\n\nRNN Language Model\nA language model assigns a probability to a sequence of words $\\mathbf",
            "This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. Dong Nguyen is supported with an Alan Turing Institute Fellowship (TU/A/000006). Maria Liakata is a Turing fellow at 40%. We would also like to thank the participants of the “Bridging disciplines in analysing text as social and cultural data” workshop held at the Turing Institute (2017) for insightful discussions. The workshop was funded by a Turing",
            " can be a valuable language resource for MT and other NLP tasks. The development of parallel corpora from scientific texts has been researched by several authors, aiming at translation of biomedical articles BIBREF3 , BIBREF4 , or named entity recognition of biomedical concepts BIBREF5 . Regarding Portuguese/English and English/Spanish language pairs, the FAPESP corpus BIBREF6 , from the Brazilian magazine revista pesquisa FAPESP, contains more than 150,000",
            ", the 10K-20K segment of the bars represents the proportion of words in the dataset that occur in the list of most frequent words between the frequency rank of 10,000 and 20,000 in that language; likewise with other intervals. Frequency lists for the presented languages are derived from Wikipedia and Common Crawl corpora. While many concept pairs are direct or approximate translations of English pairs, we can see that the frequency distribution does vary across different languages, and is also related to inherent language properties",
            " which might not be available when it comes to low-resource languages. In this paper, we leverage pre-trained multilingual language representation, for example, BERT learned from multilingual un-annotated sentences (multi-BERT), in cross-lingual zero-shot RC. We fine-tune multi-BERT on the training set in source language, then test the model in target language, with a number of combinations of source-target language pair to explore the cross-ling",
            " similarity of 3,500 verb pairs so that it enables robust evaluation of distributed representation for verbs. However, most of these datasets include English words only. There has been no Japanese dataset for the word-similarity task. Apart from English, WordSim353 and SimLex-999 Hill:2015:CL have been translated and rescored in other languages: German, Italian and Russian Leviant:2015:arXiv. SimLex-999 has also been translated and rescored in Hebrew and Croatian",
            " then filtered to remove documents without both Portuguese and English abstracts, and additional metadata selected. After the initial filtering, the resulting documents were processed for language checking to make sure that there was no misplacing of English abstracts in the Portuguese field, or the other way around, removing the documents that presented such inconsistency. We also performed a case folding to lower case letters, since the TDC datasets present all fields with uppercase letters. In addition, we also removed newline/carriage",
            " manner to be lexically close to the text. In a second pilot, we investigated the option to take the questions collected for one text and show them as questions for another text of the same scenario. While this method resulted in a larger number of questions that required inference, we found the majority of questions to not make sense at all when paired with another text. Many questions were specific to a text (and not to a scenario), requiring details that could not be answered from other texts. Since the two",
            " , BIBREF11 rely on statistical information, such as sentence length ratio in two languages and align mode probability. However, these methods are designed for other bilingual language pairs that are written in different language characters (e.g. English-French, Chinese-Japanese). The ancient-modern Chinese has some characteristics that are quite different from other language pairs. For example, ancient and modern Chinese are both written in Chinese characters, but ancient Chinese is highly concise and its syntactical structure is different from"
        ]
    },
    {
        "title": "Dialog Context Language Modeling with Recurrent Neural Networks",
        "answer": "30 turns.",
        "evidence": [
            " conversation from becoming fully fledged, overly complex human discourse. This creates an idealized spoken environment, revealing how users would openly and candidly express themselves with an automated assistant that provided superior natural language understanding. Perhaps the most relevant work to consider here is the recently released MultiWOZ dataset BIBREF13, since it is similar in size, content and collection methodologies. MultiWOZ has roughly 10,000 dialogs which feature several domains and topics. The dialogs are annotated",
            " is no guarantee that swear words are used for insulting. This kind of error affects the precision and recall for the “troll” and “engage” classes. A solution to this problem may be the inclusion of longer parts of the conversation. It is typical in a troll-engaged comment scheme to observe longer than usual exchanges between two users, and the comments evolve in very agitated remarks. One may then use this information to disambiguate between the two classes.\n\n",
            " to handle specific scenarios such as uncooperative users and technical glitches. Uncooperative users typically involve those who either ignored agent input or who rushed through the conversation with short phrases. Technical issues involved dropped sessions (e.g. WebRTC connections failed) or cases in which the user could not hear the agent or vice-versa. In addition, weekly meetings were held with the agents to answer questions and gather feedback on their experiences. Agents typically work four hours per day with dialog types changing",
            "Abstract\nIn this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network (RNN) based contextual language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3% on perplexity. The",
            " of dialogue turns, we have a penalty rule that for a dialogue task, if the system cannot return the result (or accomplish the task) in 30 turns, the dialogue task is end by force. Meanwhile, if a system cannot accomplish a task in less than 30 dialogue turns, the number of dialogue turns is set to 30.\n\n\nEvaluation Data\nIn the evaluation, all the data for training, developing and test is provided by the iFLYTEK Corporation. For task 1",
            " language in the news context. The selected videos cover different topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 minutes. To encourage the diversity of content format we included newscasts, interviews, reports and round tables. During the transcription phase we opted for a manual transcription process because we observed that using transcripts from an ASR system will difficult in a large degree the manual segmentation process. The number of words per transcript oscilate between 271 and 1,",
            " therefore unsuitable for analyzing argumentation. Three blog posts and two articles were found not to be argumentative (the authors took no stance to the discussed controversy) and one article was an interview, which the current model cannot capture (a dialogical argumentation model would be required). For each of the 340 documents, the gold standard annotations were obtained using the majority vote. If simple majority voting was not possible (different boundaries of the argument component together with a different component label), the gold standard was set",
            " time period for the model we replicate spans 1965-2000, whereas our corpus of speeches covers 1970-2014. We limit our extension to the years of overlap, 1970-2000. Our inferential analysis first replicates their full conflict onset model BIBREF8 , which we successfully do. This same model is estimated over the constrained time range of our analysis to ensure that significance and direction of the coefficients on the covariates do not substantially differ. We find that the signs remain the same and the coefficients",
            "izier.2012 developed a Prolog engine using a lexicon of 1300 words and a set of 78 hand-crafted rules with the focus on a particular argument structure “reasons supporting conclusions” in French. Taking the dialogical perspective, Cabrio.Villata.2012 built upon an argumentation framework proposed by Dung.1995 which models arguments within a graph structure and provides a reasoning mechanism for resolving accepted arguments. For identifying support and attack, they relied on existing research on textual entail",
            " repair. The sample dialog in Figure FIGREF44 shows the result of a self-dialog exercise in which workers were told to write a conversation with various ticket availability issues that is ultimately unsuccessful. Two more benefits of the self-dialog approach are its efficiency and cost effectiveness. We were able to gather thousands of dialogs in just days without transcription or trained agents, and spent roughly six times less per dialog. Despite these advantages, the self-dialog written technique cannot recreate the disfluencies",
            ". Then, in the decoding step, they combine the word attention and section attention to obtain a context vector. This approach to capture discourse structure is however quite limited both in general and especially when you consider its application to extractive summarization. First, their hierarchical method has a large number of parameters and it is therefore slow to train and likely prone to overfitting. Secondly, it does not take the global context of the whole document into account, which may arguably be critical in extractive methods, when",
            " and presented a semi-supervised model for their classification. Their social act types were inspired by research in psychology and organizational behavior and were motivated by work in dialog understanding. They annotated a corpus in three languages using in-house annotators and achieved INLINEFORM0 in the range from 0.13 to 0.53. Georgila.et.al.2011 focused on cross-cultural aspects of persuasion or argumentation dialogs. They developed a novel annotation scheme stemming from different literature sources on"
        ]
    },
    {
        "title": "Shallow Syntax in Deep Water",
        "answer": "BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of the labels.  chunk boundary information.  chunk label embeddings.  BIOUL encoding of",
        "evidence": [
            " Syntactic Features\nOur second approach incorporates shallow syntactic information in downstream tasks via token-level chunk label embeddings. Task training (and test) data is automatically chunked, and chunk boundary information is passed into the task model via BIOUL encoding of the labels. We add randomly initialized chunk label embeddings to task-specific input encoders, which are then fine-tuned for task-specific objectives. This approach does not require a shallow syntactic encoder or chunk",
            " what features make it more or less ambiguous? In these cases, it is critical to communicate high-level patterns in terms that are recognizable. This contrasts with much of the work in computational text analysis, which tends to focus on automating tasks that humans perform inefficiently. These tasks range from core linguistically motivated tasks that constitute the backbone of natural language processing, such as part-of-speech tagging and parsing, to filtering spam and detecting sentiment. Many tasks are motivated by applications, for example to",
            " at tagging time. These lexical features can also be divided into local lexical features (for example the list of possible tags known to the external lexicon for the current word) and contextual lexical features (for example the list of possible tags known to the external lexicon for surrounding words). In particular, lexical contextual features provide a means to model the right context of the current word, made of words that have not yet been tagged by the system but for which the lexicon often provides a",
            " key attributes that make Taskmaster-1 both unique and effective for data-driven approaches to building dialog systems and for other research.  Spoken and written dialogs: While the spoken sources more closely reflect conversational language BIBREF20, written dialogs are significantly cheaper and easier to gather. This allows for a significant increase in the size of the corpus and in speaker diversity.  Goal-oriented dialogs: All dialogs are based on one of six tasks: ordering pizza, creating auto",
            " relies on a simple distance function, a fixed threshold INLINEFORM0 and predefined feature categories such as function words. Regarding the correctly recognized Y-cases, we noticed that conjunctive adverbs such as “hence”, “therefore” or “moreover” contributed mostly to AVeer's correct predictions. However, a more in-depth analysis is required in future work to figure out whether the decisions of the remaining methods are also primarily affected by these",
            " tale characters (fastText 6); parts of speech and morphological forms (cases and numbers of nouns and adjectives, tenses of verbs); capitalization (in fact, first positions in the sentences) and punctuation issues (e. g., non-breaking spaces); Wikipedia authors and words from Wikipedia discussion pages (fastText 5); other different semantic categories. We also made an attempt to describe obtained components automatically in terms of common contexts of common morphological and semantic tags using MyStem",
            " generated from scratch they tend to have a relatively worse performance on fluency and grammar. Furthermore, while abstractive summaries are typically less redundant, they may end up including misleading or even utterly false statements, because the methods to extract and aggregate information form the source document are still rather noisy. In this work, we focus on extracting informative sentences from a given document (without dealing with redundancy), especially when the document is relatively long (e.g., scientific articles). Most recent works on neural extractive",
            " better vector representation of words and thus will facilitate deep learning based automatic feature extraction.\n\n\n",
            " whose granularity can range from a simple morphosyntactic category, or part-of-speech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, etc.). The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov",
            " integrate the data already carefully sampled in prior work into a single, comprehensive, and representative dataset. This way, we can control for diversity, frequency, and other properties while avoiding to perform this time-consuming selection process from scratch. Note that, on the other hand, the word pairs chosen for English are scored from scratch as part of the entire Multi-SimLex annotation process, introduced later in §SECREF5. We now describe the external data sources for the final set of word pairs: ",
            " where the language speakers are concentrated), family (the phylogenetic tree each language belongs to), and typology (including syntax, phonological inventory, and phonology). Moreover, we consider typological representations of languages that are not manually crafted by experts, but rather learned from texts. BIBREF100 proposed to construct such representations by training language-identifying vectors end-to-end as part of neural machine translation models. The vector for similarity judgments and the vector of linguistic features for a given language",
            "-specific query vector of attention. The original tasks and the auxiliary task of domain classification (DC) are joint learned in our multi-task learning framework. The query vector INLINEFORM0 of DC task is static and needs be learned in training phrase. The domain information is also selected with attention mechanism. DISPLAYFORM0   where INLINEFORM0 is attention distribution of auxiliary DC task, and INLINEFORM1 is the attentive information for DC task, which is fed into the final class"
        ]
    },
    {
        "title": "TTTTTackling WinoGrande Schemas",
        "answer": "81% and 91% for datasets of Waseem and Davidson respectively.",
        "evidence": [
            " `advocated', `they' would refer to the women and would be translated as `elles'. In the later versions of the thesis, published as (Winograd 1972), he changed `women' to `demonstrators'; this made the disambiguation clearer, but lost the point about translation.\n\n\nCurrent state of the art\nNo one familiar with the state of the art in machine translation technology or the state of the art of artificial intelligence generally will be surprised to learn",
            " with a fully connected linear classifier (BERTbase) outperforms previous baselines yielding F1-score of 81% and 91% for datasets of Waseem and Davidson respectively. Inserting a CNN to pre-trained BERT model for fine-tuning on downstream task provides the best results as F1- score of 88% and 92% for datasets of Waseem and Davidson and it clearly exceeds the baselines. Intuitively, this makes sense that combining all pre-",
            " \\prod _{t=1}^{T_k}P(w^{\\mathbf {U}_{k}}_{t}|w^{\\mathbf {U}_{k}}_{< t}, \\mathbf {U}_{<k}) \\\\$$   (Eq. 6)  where $\\mathbf {U}_{<k}$ denotes all previous turns before $\\mathbf {U}_k$ , and $w^{\\math",
            " Development Program of China (2017YFB1002101), NSFC key project (U1736204, 61661146007), and THUNUS NExT Co-Lab. \n\n\n",
            " there is a mismatch between the scores and systematized concept at this stage, the codebook may need to be adjusted, human coders retrained, more training data prepared, algorithms adjusted, or in some instances, even a new analytical method adopted. Other types of validation are also possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond",
            " datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size influences.",
            " the state of the art might be achieved by integrating lexical information both from an external lexicon and from word vector representations into tagging models. In that regard, further work will be required to understand which class of models perform the best. An option would be to integrate feature-based models such as a CRF with an LSTM-based layer, following recent proposals such as the one proposed by BIBREF45 for named entity recognition.\n\n\n",
            " access to the gold-standards of SENTIPOLC'14, TASS'15 and SemEval 2015 & 2016, respectively. The authors also thank Elio Villaseñor for the helpful discussions in early stages of this research.\n\n\n",
            " or even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions. To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require background knowledge. We explicitly state the following items in the HIT instructions for question writing: No Yes-no questions should be asked. The question should have at",
            " code and strong incentives to share state-of-the-art pretrained models. The combination of these factors has lead researchers to reproduce previous results more easily, investigate current approaches and test hypotheses without having to redevelop them first, and focus their efforts on formulating and testing new hypotheses. To bring Transfer Learning methods and large-scale pretrained Transformers back into the realm of these best practices, the authors (and the community of contributors) have developed Transformers, a library for state-of-the art",
            " under these supervision types, finding no clear advantage from using expert annotators. This differs from our findings, perhaps owing to differences in design. The experts we used already hold advanced medical degrees, for instance, while those in prior work were medical students. Furthermore, the task considered here would appear to be of greater difficulty: even a system trained on $\\sim $ 5k instances performs reasonably, but far from perfect. By contrast, in some of the prior work where experts and crowd annotations were deemed equivalent",
            " a refinement gate $r_t$: where $\\sigma $ is the sigmoid activation function, $W_{s}^{r}$, $W_{a}^r$ and $b_r$ are learnable parameters. $r_t$ represents how much degree of the current attention should be updated. Small value of $r_{ti}$ indicates that the content of $i$-th position is not much relevant to current decoding state $s_t$,"
        ]
    },
    {
        "title": "A Question-Entailment Approach to Question Answering",
        "answer": "Deep learning model, logistic regression classifier, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs), Recurrent neural network (RNN), Support Vector Machine, and Universal Sentence Encoder (USE).",
        "evidence": [
            "No common types).\n\n\nDatasets Used for the RQE Study\nWe evaluate the RQE methods (i.e. deep learning model and logistic regression classifier) using two datasets of sentence pairs (SNLI and multiNLI), and three datasets of question pairs (Quora, Clinical-QE, and SemEval-cQA). The Stanford Natural Language Inference corpus (SNLI) BIBREF13 contains 569,037 sentence pairs",
            " platform-based metadata BIBREF8, BIBREF9, BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be changing direction, with deep learning models being used for both feature extraction and the training of classifiers. These newer models are applying deep learning approaches such as Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs), etc.BIBREF6, BIBREF0 to enhance the",
            "?\n\n\nAcknowledgments\nThis research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada. We would like to thank Google Colab for providing support in terms of computational resources.\n\n\n",
            " RNNLM using cold fusion approach during training to attain a better-trained model. In near future, we will incorporate all the above techniques to get comparable performance with the state-of-the-art hybrid DNN/RNN-HMM systems.\n\n\n",
            " of QA-DQN. We adopt the Rainbow algorithm proposed by BIBREF23, which integrates several extensions to the original Deep Q-Learning algorithm BIBREF24. Rainbox exhibits state-of-the-art performance on several RL benchmark tasks (e.g., Atari games). During game playing, we use a mini-batch of size 10 and push all transitions (observation string, question string, generated command, reward) into a replay buffer of size 500,",
            "VR model: Pearson's correlation coefficients range from 0.550 to 0.622. The regression correlations are the lowest. The RNN model realizes the strongest performance among the stand-alone (non-ensemble) models, outperforming variants that exploit CNN and USE representations. Combining the RNN and USE further improves results. We hypothesize that this is due to complementary sentence information encoded in universal representations. For all models, correlations for Intervention and Outcomes are higher than for Population, which",
            " but also which emoticons are used in a similar context. We take advantage of distant supervision by using Facebook reactions as proxies for emotion labels, which to the best of our knowledge hasn't been done yet, and we train a set of Support Vector Machine models for emotion recognition. Our models, differently from existing ones, exploit information which is acquired entirely automatically, and achieve competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. For explanatory purposes",
            " data, the LR+IR method outperformed the best system in all measures, with 80.57% Accuracy and 77.47% MAP (official system ranking measure in SemEval-cQA). On the cQA-2017 test data, the LR+IR method obtained 44.66% MAP and outperformed the cQA-2017 best system in Accuracy with 67.27%.\n\n\nDiscussion of RQE Results\nWhen trained and tested on the same corpus, the",
            "max classifier. The sentence embedding model sums the Recurrent neural network (RNN) embeddings of its words. The word embeddings are first initialized with pretrained GloVe vectors. This adaptation provided the best performance in previous experiments with RQE data. GloVe is an unsupervised learning algorithm to generate vector representations for words BIBREF42 . Training is performed on aggregated word co-occurrence statistics from a large corpus, and the resulting representations show",
            " the features in the text.\n\n\nIntroduction\nFollowing the recent progress in deep learning, researchers and practitioners of machine learning are recognizing the importance of understanding and interpreting what goes on inside these black box models. Recurrent neural networks have recently revolutionized speech recognition and translation, and these powerful models could be very useful in other applications involving sequential data. However, adoption has been slow in applications such as health care, where practitioners are reluctant to let an opaque expert system make crucial decisions. If we can make",
            ", some of the question types that the RQE classifier learns will not be deemed important terms by traditional IR and the most relevant questions will not be ranked at the top of the list. Therefore, in our approach, when a question is submitted to the system, candidate questions are fetched using the IR models, then the RQE classifier is applied to filter out the non-entailed questions and re-rank the remaining candidates. Specifically, we denote INLINEFORM0 the list",
            " BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11. The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance. Nonetheless, RC on languages other than English has"
        ]
    },
    {
        "title": "Argumentation Mining in User-Generated Web Discourse",
        "answer": "lexical and semantic challenges of domain-specific data that often include advanced argumentation and complex contextual information.  the higher sparseness of relevant information sources, and the more pronounced lack of similarities between users' profiles.  the lexical and semantic challenges of domain-specific data that often include advanced argumentation and complex contextual information.  the higher sparseness of relevant information sources, and the more pronounced lack of similarities between users' profiles.  the lexical and semantic challenges of domain-specific data that often include advanced argumentation and complex contextual information.  the higher sparseness of relevant information sources, and the more pronounced lack of similarities between users' profiles.  the lexical and semantic challenges of domain-specific data that often include advanced argumentation and complex contextual information.  the higher sparseness of relevant information sources, and the more pronounced lack of similarities between users' profiles.  the lexical and semantic challenges of domain-specific data that often include advanced argumentation and complex contextual information.  the higher sparseness of relevant information sources, and the more pronounced lack of similarities between users' profiles.  the lexical and semantic challenges of domain-specific data that often include advanced argumentation and complex contextual information.  the higher sparseness of relevant information sources, and the more pronounced lack of",
        "evidence": [
            " vs. private school domain, meaning that the more complicated the text in terms of readability is, the lower agreement was reached. We observed no significant correlation in sentence coverage and average sentence length measures. We cannot draw any general conclusion from these results, but we can state that some registers and topics, given their properties, are more challenging to annotate than others. Another qualitative analysis of disagreements between annotators was performed by constructing a probabilistic confusion matrix BIBREF77 on the token level.",
            " these tasks can be facilitated. Other machine translation approaches can also be tested, including the concatenation of this corpus with other multi-domain ones.\n\n\n",
            " the availability of rich data on users' locations, profiles and search history, personalization has become the leading trend in large-scale information retrieval. However, efficiency through personalization is not yet the most suitable model when tackling domain-specific searches. This is due to several factors, such as the lexical and semantic challenges of domain-specific data that often include advanced argumentation and complex contextual information, the higher sparseness of relevant information sources, and the more pronounced lack of similarities between users'",
            " into multiple AV problems is especially important in such scenarios, where the presence of the true author of INLINEFORM4 in the candidate set cannot be guaranteed. In the past two decades, researchers from different fields including linguistics, psychology, computer science and mathematics proposed numerous techniques and concepts that aim to solve the AV task. Probably due to the interdisciplinary nature of this research field, AV approaches were becoming more and more diverse, as can be seen in the respective literature. In 2013, for example,",
            " did not limit the number of questions a worker can input to a corresponding segment and encouraged them to input a diverse set of questions which the span can answer. Along with the questions, the workers were also required to provide a justification as to why they made their questions. We manually checked this justification to filter out the questions with poor quality by removing those questions which were unrelated to the video. One initial challenge worth mentioning is that at first some workers input questions they had about the video and not questions which the",
            " On the other hand, there still exists some problems to be solved. We observe that translating proper nouns and personal pronouns (such as names, place names and ancient-specific appellations) is very difficult for all of these models. For instance, the ancient Chinese appellation `Zhen' should be translated into `Wo' in modern Chinese. Unfortunately, we calculate the accurate rate of some special words (such as `Zhen',`Chen' and `Gua'), and find that",
            " encoder trained on out-of-domain data, and a task-specific encoder. Experiments on a complex biomedical information extraction task using expert and lay annotators show that: (i) simply excluding from the training data instances predicted to be difficult yields a small boost in performance; (ii) using difficulty scores to weight instances during training provides further, consistent gains; (iii) assigning instances predicted to be difficult to domain experts is an effective strategy for task routing. Our experiments confirm the expectation",
            "?\n\n\nAcknowledgments\nThis research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada. We would like to thank Google Colab for providing support in terms of computational resources.\n\n\n",
            " natively support any form of recurrent dropout. That is, running an LSTM that uses a state-of-the-art regularization scheme at cuDNN-like speeds would likely require an entirely custom kernel.\n\n\nCharacter-level Neural Machine Translation\nWe evaluate the sequence-to-sequence QRNN architecture described in SECREF5 on a challenging neural machine translation task, IWSLT German–English spoken-domain translation, applying fully character-level segmentation. This",
            " theory based on empirical research has been developed, thus feasibility of adapting argumentation models to the Web discourse represents an open issue. These challenges can be formulated into the following research questions: In this article, we push the boundaries of the argumentation mining field by focusing on several novel aspects. We tackle the above-mentioned research questions as well as the previously discussed challenges and issues. First, we target user-generated Web discourse from several domains across various registers, to examine how argumentation is communicated in different",
            " all possible queries and use an entity's (or entity pair's) membership in each set as a feature in our predicate models. There are two problems with this approach: (1) the set of all possible queries is intractably large, so we need a mechanism similar to a semantic parser's lexicon to select a small set of queries for each word; and (2) executing hundreds or thousands of queries at runtime for each predicate and entity is not computationally tractable. To solve these",
            " is the fact that two different class domains are involved. On the one hand, there is the class domain of authors, where the task is to distinguish INLINEFORM7 and INLINEFORM8 . On the other hand, there is the elevated or lifted domain of verification problem classes, which are Y and N. The training phase of binary-intrinsic approaches is used for learning to distinguish these two classes, and the verification task can be understood as putting the verification problem as a whole into"
        ]
    },
    {
        "title": "Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model",
        "answer": "linguistic-agnostic representations.",
        "evidence": [
            " quantitatively evaluate the similarities between the embedding of the languages. The results can be found in the Appendix.\n\n\nWhat Does Zero-shot Transfer Model Learn? ::: Code-switching Dataset\nWe observe linguistic-agnostic representations in the last subsection. If tokens are represented in a language-agnostic way, the model may be able to handle code-switching data. Because there is no code-switching data for RC, we create artificial code-switching datasets",
            " plateau epochs, then the learning rate is reduced. Training stops when reducing the learning rate no longer improves dev set AP. Then, the model from the epoch corresponding to the the best dev set AP is chosen. Several other optimizers—Adagrad BIBREF34 , Adadelta BIBREF35 , and Adam BIBREF36 —were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum. \n\n\nSi",
            " a function returning the number of $x$. In practice, for each mini-batch for $L_{lm}$, we calculate $L_{add}$ for the same set of sentences and add these two to obtain a total loss for updating parameters. As we mentioned in Section intro, this loss does not exploit negative examples explicitly; essentially a model is only informed of a key position (target word) that determines the grammaticality. This is rather an indirect learning signal, and we expect",
            " MAB) removes the MAB and learns no dense cross-view dynamics code $z$ . This model can be seen as three disjoint LSTMs and is used to investigate the importance of modeling temporal cross-view dynamics. The next modified baseline MARN (no $\\mathcal {A}$ ) removes the $\\mathcal {A}$ deep network and sets all $K$ attention coefficients $a^k_t = 1$ ( $h^k_t =",
            "}$ datasets of open aspects. This label-fully-unseen setup is more like the dataless learning principle BIBREF0, in which no task-specific annotated data is provided for training a model (since usually this kind of model fails to generalize in other domains and other tasks), therefore, we are encouraged to learn models with open-data or test-agnostic data. In this way, the learned models behave more like humans.\n\n\nAn entailment model for",
            " merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation on all the hidden states, allowing a better regularisation of the model learning process. Empirical results show that our model can effectively mitigate the latent variable collapse issue while giving a better predictive performance than the baselines.\n\n\nAcknowledgment\nThis work is supported by the award made by the UK Engineering and Physical Sciences Research Council (Grant number: EP/",
            " simply assign Option1 as the answer. The table also reports “zero-shot” performance, i.e., performing inference on the development set without any model fine tuning. Condition #2 represents our submission to the official leaderboard, which achieves 0.7673 AUC on the held-out test set. From these results, we see that the logit trick clearly improves performance, which is consistent with the observations of Nogueira et al. BIBREF7. In fact",
            " we define here as Definition-Restrictive: Definition-Restrictive ($\\textsc {0shot-tc}$). Given labeled instances belonging to a set of seen classes $S$, $\\textsc {0shot-tc}$ aims at learning a classifier $f(\\cdot ): X \\rightarrow Y$, where $Y=S\\cup U$; $U$ is a set of unseen classes and belongs to the same aspect as $S$. In this work",
            " rate INLINEFORM0 , embedding dimension INLINEFORM1 , and margin INLINEFORM2 for all datasets. We randomly sample 64 neighbors for each entity. Zero padding is used when the number of neighbors is less than 64. L2-regularization is applied on the parameters of LAN. The regularization rate is INLINEFORM3 . We search the best hyper-parameters of all models according to the performance on validation set. In detail, we search learning rate INLINEFORM0",
            " reduces NLL from 96 to 79 and PPL from 98 to 43 in PTB, respectively. In the inputless setting, our performance gain is even higher, i.e., NLL reduced from 117 to 85 and PPL from 262 to 54. A similar pattern can be observed for the E2E dataset. These observations suggest that our approach can learn a better generative model for data. Loss analysis. To conduct a more thorough evaluation, we further investigate model behaviours in terms of both",
            " . Due to the newness of the subject matter, the definition of `zero-resource' varies somewhat from author to author. For the experiments that follow, `zero-resource' means that, during model training, we do not use labels from non-English data, nor do we use human or machine-generated parallel text. Only labeled English text and unlabeled non-English text are used during training, and hyperparameters are selected using English evaluation sets. Our contributions are the following",
            " property lease agreements, we show results on all documents (left) and only over those with unseen templates (right). Examining these results, we see that although there is some degradation in effectiveness between all documents and only unseen templates, it appears that BERT is able to generalize to previously-unseen expressions of the content elements. Specifically, it is not the case that the model is simply memorizing fixed patterns or key phrases—otherwise, we could just craft a bunch of regular expression patterns"
        ]
    },
    {
        "title": "Automated News Suggestions for Populating Wikipedia Entity Pages",
        "answer": "salience feature, authority and novelty features, relative entity frequency from the salience feature group.  authority and novelty features account to a further improvement in terms of precision, by adding roughly a 7%-10% increase.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the salience feature group.  authority and novelty features.  relative entity frequency from the sal",
        "evidence": [
            "elines. Relative entity frequency from the salience feature, models the entity salience as an exponentially decaying function based on the positional index of the paragraph where the entity appears. The performance of INLINEFORM0 with relative entity frequency from the salience feature group is close to that of all the features combined. The authority and novelty features account to a further improvement in terms of precision, by adding roughly a 7%-10% increase. However, if both feature groups are considered separately, they significantly outperform",
            " state A says “nuclear weapons are bad,\" and state B says “atom bombs are terrible,\" the only feature in common is the term “are,\" which leads to near-orthogonality in their BOW vectors and low similarity scores. If a third state C says “atom bombs are good,\" then B and C would exhibit the highest cosine similarity of the three, despite having the opposite expressed policy positions. Word embeddings and term-document matrices are located",
            ", where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity relatedness) that are unavailable to local model BIBREF15 . For example (Figure FIGREF1 ), an EL model seemly cannot find sufficient disambiguation clues for the mention England from its surrounding words, unless it utilizes the coherence information of consistent topic “cricket\" among adjacent mentions England, Hussain, and Essex. Although the",
            " Entities with very high authority have many news items associated with them and only the most relevant of these should be included in Wikipedia whereas for entities of lower authority the threshold for inclusion of a news article will be lower. Third, a good recommendation should be able to identify novel news by minimizing redundancy coming from multiple news articles. Finally, addition of facts is facilitated if the recommendations are fine-grained, i.e., recommendations are made on the section level rather than the page level (placement).",
            " We propose to examine the saliency of attention. Specifically, given a premise-hypothesis pair and the model's decision $y$ , we consider the similarity between a pair of premise and hypothesis hidden states $e_{ij}$ as a variable. The score of the decision $S(y)$ is thus a function of $e_{ij}$ for all $i$ and $j$ . The saliency of $e_{ij}$ is then defined to be $",
            " text, thus introducing a “global view.” The GR features, on the other hand, are more restricted to a “local view” on entities between consecutive sentences. While a deeper empirical investigation is needed, one can intuitively imagine that identifying authorship by focusing on the local transitions between grammatical relations (as in GR) is more difficult than observing how the entire text is organized (as in RST).\n\n\nConclusion\nWe have conducted an in-depth investigation of",
            " by its neighboring concepts (and corresponding relations) in the knowledge base. Note that entities in ConceptNet are common words (such as tree, leaf, animal), we thus use word vectors to represent h/r/t directly, instead of using geometric embedding methods (e.g., TransE) to learn entity and relation embeddings. In this way, there is no need to bridge the representation gap between geometric embeddings and text-contextual embeddings (i.e",
            "the-art performance, they still heavily relied on hand-crafted features. A few researches explored to directly measure similarity based on distributed representations. BIBREF5 trained a language model based on convolutional neural networks to project sentences onto distributed representations. BIBREF26 treated single document summarization as a sequence labeling task and modeled it by the recurrent neural networks. Others like BIBREF9 , BIBREF13 just used the sum of trained word embeddings to represent sentences or documents",
            " the comparative relevance of the news article to the different entities occurring in it. As an example, let us consider the meeting of the Sudanese bishop Elias Taban with Hillary Clinton. Both entities are salient for the meeting. However, in Taban's Wikipedia page, this meeting is discussed prominently with a corresponding news reference, whereas in Hillary Clinton's Wikipedia page it is not reported at all. We believe this is not just an omission in Clinton's page but mirrors the fact that for the lesser known Tab",
            " results for the article–entity (AEP) placement task. We only report the evaluation metrics for the `relevant' news-entity pairs. A detailed explanation on why we focus on the `relevant' pairs is provided in Section SECREF16 . Baselines. We consider the following baselines for this task. B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 . B2. The second baseline assigns the value relevant to",
            ". Then we apply a machine learning tool to reveal the significance of each feature. We propose features based on the observation that answer agreement often arises when 1) a lay person's attention can be easily concentrated to a single, undisputed region in an image and 2) a lay person would find the requested task easy to address. We employ five image-based features coming from the salient object subitizing BIBREF22 (SOS) method, which produces five probabilities that indicate whether an image contains",
            " entities makes it harder a) for the encoder to encode semantics of the objects contained in the table and b) for the attention mechanism to extract salient entities/records. Second, the comparison between scenario Hierarchical-kv and Hierarchical-k shows that omitting entirely the influence of the record values in the attention mechanism is more effective: this last variant performs slightly better in all metrics excepted CS-R%, reinforcing our intuition that focusing on the structure modeling is an important part"
        ]
    },
    {
        "title": "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
        "answer": "English.",
        "evidence": [
            " policies included in the corpus are in English, and were collected before April 1, 2018, predating many companies' GDPR-focused BIBREF41 updates. We leave it to future studies BIBREF42 to look at the impact of the GDPR (e.g., to what extent GDPR requirements contribute to making it possible to provide users with more informative answers, and to what extent their disclosures continue to omit issues that matter to users).\n\n\nData Collection ::: Crowdsourced",
            " time, or expertise required. The application of natural language processing (NLP) and machine learning (ML) can make feasible the speedy analysis of qualitative data on a large scale. We start with a brief description of the main approaches to NLP and how they can be augmented by human coding. We then move on to the issue of working with multiple languages and different document formats. We then provide an overview of the recent application of these techniques in a United Nations Development Program (UNDP) study.",
            " can be divided into two parts: High resource results, computed with wFSTs trained on a combination of Wiktionary pronunciation data and g2p rules extracted from Wikipedia IPA Help pages. They report high resource results for 85 languages. Adapted results, where they apply various mapping strategies in order to adapt high resource models to other languages. The final adapted results they reported include most of the 85 languages with high resource results, as well as the various languages they were able to adapt them for,",
            "English spoken-domain translation, applying fully character-level segmentation. This dataset consists of 209,772 sentence pairs of parallel training data from transcribed TED and TEDx presentations, with a mean sentence length of 103 characters for German and 93 for English. We remove training sentences with more than 300 characters in English or German, and use a unified vocabulary of 187 Unicode code points. Our best performance on a development set (TED.tst2013) was achieved using a four-layer encoder–",
            " in an analogous way to how language ID is tagged in multilingual g2p.\n\n\n",
            "fer, Damyana Gateva, Leonie Harter, Sarah Mameche, Stefan GrÃ¼newald and Tatiana Anikina for help with the annotations. This research was funded by the German Research Foundation (DFG) as part of SFB 1102 `Information Density and Linguistic Encoding' and EXC 284 `Multimodal Computing and Interaction'.\n\n\n",
            "SV when INLINEFORM0 . We observe that the improvements are on average stronger when INLINEFORM1 . This suggests that our model performs well with very little data due to its flexibility to generate any tag set, including those not observed in the training data. The strongest improvements are observed for FI/HU. This is likely because the number of unique tags is the highest in this language pair and our method scales well with the number of tags due to its ability to make use of correlations between",
            "resource cross-lingual tasks besides using the English dev set.\n\n\nAcknowledgments\nWe would like to thank Jiateng Xie, Julian Salazar and Faisal Ladhak for the helpful comments and discussions.\n\n\n",
            " Romance) are all neighbors. In order to quantify exactly the effect of language affinity on the similarity scores, we run correlation analyses between these and language features. In particular, we extract feature vectors from URIEL BIBREF99, a massively multilingual typological database that collects and normalizes information compiled by grammarians and field linguists about the world's languages. In particular, we focus on information about geography (the areas where the language speakers are concentrated), family (the phylogenetic tree each language",
            " 1,888 pairs in this work versus 500 pairs in the work of Camacho:2017semeval. As a result, each of our cross-lingual datasets contains a substantially larger number of concept pairs, as shown in Table TABREF30. The cross-lingual Multi-Simlex datasets are constructed automatically, leveraging word pair translations and annotations collected in all 12 languages. This yields a total of 66 cross-lingual datasets, one for each possible combination of languages. Table T",
            "This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. Dong Nguyen is supported with an Alan Turing Institute Fellowship (TU/A/000006). Maria Liakata is a Turing fellow at 40%. We would also like to thank the participants of the “Bridging disciplines in analysing text as social and cultural data” workshop held at the Turing Institute (2017) for insightful discussions. The workshop was funded by a Turing",
            " and conclude.\n\n\nData\nFor the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100"
        ]
    },
    {
        "title": "Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation",
        "answer": "Vietnamese part of the Japanese-Vietnamese parallel corpus.",
        "evidence": [
            "-Source, instead of using a subsampled monolingual corpus, we use the Vietnamese part of the Japanese-Vietnamese parallel corpus in order to learn the multilingual information in the same domain. Our datasets are listed in Table TABREF14 .\n\n\nPreprocessing\nAfter using KyTea to tokenize the Japanese texts, we learn and apply Sennrich's BPE from the tokenized texts. For Vietnamese texts, first we use Moses scripts to normalize the texts from",
            ". In the results section we compare our model with related work.\n\n\nDataset\nWe tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show the applicability and superiority for different topics, languages, data distributions, and platforms.",
            " is added to the new training set. If INLINEFORM2 or INLINEFORM3 , it is added to the auxiliary set, which serves as existing neighbors for unseen entities in INLINEFORM4 . Finally, for a triplet INLINEFORM0 in the original validation set, if INLINEFORM1 or INLINEFORM2 , it is removed from the validation set. The statistics for the resulting INLINEFORM0 datasets using Subject and Object strategies are in Table TABREF34 .\n\n",
            "aset\nData Collection. To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. Paragraph",
            "Abstract\nNeural machine translation (NMT) systems have recently obtained state-of-the art in many machine translation systems between popular language pairs because of the availability of data. For low-resourced language pairs, there are few researches in this field due to the lack of bilingual data. In this paper, we attempt to build the first NMT systems for a low-resourced language pairs:Japanese-Vietnamese. We have also shown significant improvements when combining advanced methods to reduce",
            ") . The third dataset (dataset-pn) is a subset of dataset-basic, containing 828 positive and 280 negative citations. Dataset-pn was used for the purposes of (1) evaluating binary classification (positive versus negative) performance using sent2vec; (2) Comparing the sentiment classification ability of PS-Embeddings with other embeddings.\n\n\nEvaluation Strategy\nOne-Vs-The-Rest strategy was adopted for the task of multi",
            "Although”). The last contribution is an adaptation of CVT, which allows the model to utilize unlabeled data to produce effective local and distant representations. The experiment was conducted on two Thai datasets, Orchid and UGWC, and one English punctuation restoration dataset, IWSLT. English punctuation restoration is similar to our Thai sentence segmentation. On the Thai sentence segmentation task, our model achieves F1 scores of 92.5% and 88.9% on the",
            " base/anchor language to create the dataset. In simple words, if one translates to two languages $L_1$ and $L_2$ starting from the same set of pairs in English, it is higly likely that $L_1$ and $L_2$ will diverge from English in different ways. Therefore, the similarity between $L_1$-eng and $L_2$-eng is expected to be higher than between $L_1$-$",
            "ity Dataset (RW) Luong-etal:conll13:morpho. We extracted Japanese word pairs from the Evaluation Dataset of Japanese Lexical Simplification kodaira. It targeted content words (nouns, verbs, adjectives, adverbs). It included 10 contexts about target words annotated with their lexical substitutions and rankings. Figure FIGREF1 shows an example of the dataset. A word in square brackets in the text is represented as a",
            " , respectively. We found that model accuracy is highly sensitive to these parameters. Therefore, we use a grid search technique to find the best value for both parameters for the model. We apply two optimizers used in this work: Adagard BIBREF39 and Adam BIBREF40 , whose learning rates are set to 0.02 and 0.001 for the Thai and English datasets, respectively. To generalize the model, we also integrate L2 regularization with an alpha of 0.",
            " across five different datasets. Besides, further analysis of the impacts of main modules as well as qualitative results demonstrates its effectiveness. In the future, we will extend our method into cross-lingual settings to help link entities in low-resourced languages by exploiting rich knowledge from high-resourced languages, and deal with NIL entities to facilitate specific applications.\n\n\nAcknowledgments\nThe work is supported by National Key Research and Development Program of China (2017YFB1002101), NSFC key",
            "QuAD BIBREF2. The Chinese dataset is DRCD BIBREF14, a Chinese RC dataset with 30,000+ examples in the training set and 10,000+ examples in the development set. The Korean dataset is KorQuAD BIBREF15, a Korean RC dataset with 60,000+ examples in the training set and 10,000+ examples in the development set, created in exactly the same procedure as SQuAD. We always use the development sets of S"
        ]
    },
    {
        "title": "Ancient-Modern Chinese Translation with a Large Training Dataset",
        "answer": "ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.",
        "evidence": [
            "aset\nData Collection. To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. Paragraph",
            " with the current datasets concerns a lack of one unified procedure that ensures the comparability of resources in different languages. Further, concept pairs for different languages are sourced from different corpora (e.g., direct translation of the English data versus sampling from scratch in the target language). Moreover, the previous SimLex-based multilingual datasets inherit the main deficiencies of the English original version, such as the focus on nouns and highly frequent concepts. Finally, prior work mostly focused on languages that are widely spoken",
            " Development Program of China (2017YFB1002101), NSFC key project (U1736204, 61661146007), and THUNUS NExT Co-Lab. \n\n\n",
            "ability and superiority for different topics, languages, data distributions, and platforms. The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset",
            " we intend to measure. Unfortunately, these steps tend to be underreported, but documenting the pre-processing choices made is essential and is analogous to recording the decisions taken during the production of a scholarly edition or protocols in biomedical research. Data may also vary enormously in quality, depending on how it has been generated. Many historians, for example, work with text produced from an analogue original using Optical Character Recognition (OCR). Often, there will be limited information available regarding the accuracy of the OCR",
            " Twitter tweets. We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13 . The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\"). If a tweet is annotated evidence of depression, then",
            "This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. Dong Nguyen is supported with an Alan Turing Institute Fellowship (TU/A/000006). Maria Liakata is a Turing fellow at 40%. We would also like to thank the participants of the “Bridging disciplines in analysing text as social and cultural data” workshop held at the Turing Institute (2017) for insightful discussions. The workshop was funded by a Turing",
            "QuAD BIBREF2. The Chinese dataset is DRCD BIBREF14, a Chinese RC dataset with 30,000+ examples in the training set and 10,000+ examples in the development set. The Korean dataset is KorQuAD BIBREF15, a Korean RC dataset with 60,000+ examples in the training set and 10,000+ examples in the development set, created in exactly the same procedure as SQuAD. We always use the development sets of S",
            " base/anchor language to create the dataset. In simple words, if one translates to two languages $L_1$ and $L_2$ starting from the same set of pairs in English, it is higly likely that $L_1$ and $L_2$ will diverge from English in different ways. Therefore, the similarity between $L_1$-eng and $L_2$-eng is expected to be higher than between $L_1$-$",
            "Abstract\nWith social media becoming increasingly pop-ular on which lots of news and real-time eventsare reported, developing automated questionanswering systems is critical to the effective-ness of many applications that rely on real-time knowledge. While previous datasets haveconcentrated on question answering (QA) forformal text like news and Wikipedia, wepresent the first large-scale dataset for QA oversocial media data. To ensure that the tweetswe collected are useful, we only gather",
            " 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}. For dialects, the data are labeled with 15 classes, from the set {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia",
            " datasets differ with respect to text type (Wikipedia texts, examination texts, etc.), mode of answer selection (span-based, multiple choice, etc.) and test systems regarding different aspects of language understanding, but they do not explicitly address commonsense knowledge. Two notable exceptions are the NewsQA and TriviaQA datasets. NewsQA BIBREF20 is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection"
        ]
    },
    {
        "title": "UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish",
        "answer": "abtt.  uncovec.  L2 regularization.  dropout.  self-attention mechanism.  bidirectional LSTMs.  semi-supervised machine learning methods.  semi-supervised approach.  semi-supervised learning.  semi-supervised machine learning.  semi-supervised.  semi-supervised learning methods.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-supervised.  semi-sup",
        "evidence": [
            ", assigning tweets to an emotion if an annotated emotion-word occurred in the tweet. Our semi-supervised approach is quite straightforward: first a model is trained on the training set and then this model is used to predict the labels of the silver data. This silver data is then simply added to our training set, after which the model is retrained. However, an extra step is applied to ensure that the silver data is of reasonable quality. Instead of training a single model initially, ten different",
            "focused summarization, i.e., to measure the saliency of a sentence and its relevance to a user's query. After a long period of research, learning-based models like Logistic Regression BIBREF1 etc. have become growingly popular in this area. However, most current supervised summarization systems often perform the two tasks in isolation. Usually, they design query-dependent features (e.g., query word overlap) to learn the relevance ranking, and query-independent features",
            "he, given the fact that unsupervised vecmap is the most robust unsupervised CLWE method at present BIBREF158, our results again question the usefulness of fully unsupervised approaches for a large number of languages, and call for further developments in the area of unsupervised and weakly supervised cross-lingual representation learning. The scores of m-bert and xlm-100 lead to similar conclusions as in the monolingual settings. Reasonable correlation scores are achieved",
            "Abstract\nWe propose a simple domain adaptation method for neural networks in a supervised setting. Supervised domain adaptation is a way of improving the generalization performance on the target domain by using the source domain dataset, assuming that both of the datasets are labeled. Recently, recurrent neural networks have been shown to be successful on a variety of NLP tasks such as caption generation; however, the existing domain adaptation techniques are limited to (1) tune the model parameters by the target dataset after the training by the source",
            " of a supervised learning result is relatively straightforward as it requires comparing prediction results with actual outcomes. Furthermore, there is no need to label all text documents (or interview data from each respondent) prior to analyzing them. Rather, a sufficiently large set of documents can be labelled to train an algorithm and then used to classify the remaining documents. In unsupervised learning, the outcome variable is unknown. The exercise is, therefore, of a more exploratory nature. Here the purpose is to reveal patterns in the",
            "REF4 to the use of bias removal techniques BIBREF5 in fact. Approaches linked to the data pre-processing step usually consist of methods based on improving the quality of the dataset after which the usual classification tools can be used to train a classifier. So, it starts from a baseline already stipulated by the execution of itself. On the other side of the spectrum, there are Unsupervised and semi-supervised learning techniques, that are attractive because they do not imply the cost",
            " the perturbation is completely unsupervised (i.e., self-contained) and does not inject any external (semantic similarity oriented) knowledge into the vector space. Note that different perturbations can also be stacked: e.g., we can apply uncovec and then use abtt on top the output uncovec vectors. When using uncovec and abtt we always length-normalize and mean-center the data first (i.e., we apply the",
            " dataset was extended from that in BIBREF21 ; the data were collected from January 2017 to December 2017. The labeled dataset includes 48,374 passages. To support semi-supervised learning, the first 3 months of data (96,777 passages) are unlabeled. Because the data stem from social media, some text exists that cannot be considered as part of any sentence, such as product links, symbols unrelated to sentences, and space between sentences. These portions were not originally annotated as",
            " public into a common space. Their method extends to other social media such as Twitter where endorsements and likes could be leveraged.\n\n\nWeibo, RenRen, and Chinese microblogs\nThe most prominent example of supervised classification with social media data involves the first large scale study of censorship in China. BIBREF32 automatically downloaded Chinese blogposts as they appeared online. Later they returned to the same posts and checked whether or not they had been censored. Furthermore, they analyzed the content of the blog",
            " model, we also integrate L2 regularization with an alpha of 0.01 to the loss function for model updating. Moreover, dropout is applied to the local representation vectors, recurrent representation vectors, between all bidirectional LSTMs and enclosed by the self-attention mechanism in the high-level module. During training, both the supervised and semi-supervised models are trained until the validation metrics stop improving; the metrics are (1) sentence boundary F1 score and (2)",
            " semi-supervised machine learning methods on the gold data Toulmin dataset introduced in section SECREF51 . An argument consists of different components (such as premises, backing, etc.) which are implicitly linked to the claim. In principle one document can contain multiple independent arguments. However, only 4% of the documents in our dataset contain arguments for both sides of the issue. Thus we simplify the task and assume there is only one argument per document. Given the low inter-annotator agreement on the",
            " we can train competitive NMT systems without relying on auxiliary resources. This has practical relevance for languages where large amounts of monolingual data, or multilingual data involving related languages, are not available. Even though we focused on only using parallel data, our results are also relevant for work on using auxiliary data to improve low-resource MT. Supervised systems serve as an important baseline to judge the effectiveness of semisupervised or unsupervised approaches, and the quality of supervised systems trained on little"
        ]
    },
    {
        "title": "Conflict as an Inverse of Attention in Sequence Relationship",
        "answer": "unseen languages. 10-fold cross-validation.  classification of reviews.  conflict onset network.  classification of visual questions.  classification of arguments.  classification of documents.  classification of AV methods.  classification of dialog interactions.  classification of user intents.  classification of user behaviors.  classification of user preferences.  classification of user feedback.  classification of user satisfaction.  classification of user experience.  classification of user engagement.  classification of user retention.  classification of user churn.  classification of user loyalty.  classification of user trust.  classification of user satisfaction.  classification of user experience.  classification of user engagement.  classification of user retention.  classification of user churn.  classification of user loyalty.  classification of user trust.  classification of user satisfaction.  classification of user experience.  classification of user engagement.  classification of user retention.  classification of user churn.  classification of user loyalty.  classification of user trust.  classification of user satisfaction.  classification of user experience.  classification of user engagement.  classification of user retention.  classification of user churn.  classification of user loyalty.  classification of user trust.  classification of user satisfaction.  classification of user experience.  classification of user engagement",
        "evidence": [
            " where two sequences have a contradicting relationship based on the task it performs. To alleviate this problem and further improve the performance, we propose a conflict mechanism that tries to capture how two sequences repel each other. This acts like the inverse of attention and, empirically, we show that how conflict and attention together can improve the performance. Future research work should be based on alternative design of conflict mechanism using other difference operators other than element wise difference which we use.\n\n\n",
            " to handle specific scenarios such as uncooperative users and technical glitches. Uncooperative users typically involve those who either ignored agent input or who rushed through the conversation with short phrases. Technical issues involved dropped sessions (e.g. WebRTC connections failed) or cases in which the user could not hear the agent or vice-versa. In addition, weekly meetings were held with the agents to answer questions and gather feedback on their experiences. Agents typically work four hours per day with dialog types changing",
            " We performed 10-fold cross-validation for both of the datasets. We used the approximate randomisation technique to test whether our results are statistically significant. Here, we tried to predict the labels of reviews and assess the performance. We obtained varying accuracies as shown in Table TABREF17. “3 feats\" features are those hand-crafted features we extracted, which are the minimum, mean, and maximum polarity scores of the reviews as explained in Section 3.5. As can",
            " later they had to define two new argumentation schemes that they discovered in PDTB. Similarly, Song.et.al.2014 admitted that the schemes are ambiguous and hard to directly apply for annotation, therefore they modified the schemes and created new ones that matched the data. Although Macagno.Konstantinidou.2012 show several examples of two argumentation schemes applied to few selected arguments in classroom experiments, empirical evidence presented by Anthony.Kim.2014 reveals many practical and theoretical difficulties of",
            " we test the model on the languages unseen during pre-training. To make sure the languages have never been seen before, we artificially make unseen languages by permuting the whole vocabulary of existing languages. That is, all the words in the sentences of a specific language are replaced by other words in the same language to form the sentences in the created unseen language. It is assumed that if multi-BERT used to find answers by language independent strategy, then multi-BERT should also do well on unseen",
            " the supporting evidence they identify is not identical i.e full overlap. They disagree on the remaining 26%. Sources of apparent disagreement correspond to situations when different experts: have differing interpretations of question intent (11%) (for example, when a user asks 'who can contact me through the app', the questions admits multiple interpretations, including seeking information about the features of the app, asking about first party collection/use of data or asking about third party collection/use of data), identify different sources of evidence for",
            " that all three methods perform very similar to each other, whereas big discrepancies between GLAD and COAV can be observed regarding c@1. When we consider the current and maximum achievable results (depicted by the circles and triangles, respectively) it becomes apparent that GLAD's model behaves stable, while the one of COAV becomes increasingly vulnerable the more the documents are shortened. When looking at the ROC curve of Caravel, it can be clearly seen that the actual and maximum achievable results are",
            "-of-fit (GOF) of exponential random graph models, it is common to simulate network statistics which were not specified in the model to see how well the fitted model can simulate statistics on the outcome network. In our case, the outcome network is the conflict onset network, and we perform 50 simulations over each of the 30 time steps for 1,500 total simulations for each of the four models. The in-sample areas under the ROC and PR curves for the three models of primary concern",
            " of up to one “careless/spam\" answer per visual question. The outcome of our labeling scheme is that a disagreement label is agnostic to the specific cause of disagreement and rather represents the many causes (described above). For our first system, we use domain knowledge to guide the learning process. We compile a set of features that we hypothesize inform whether a crowd will arrive at an undisputed, single answer. Then we apply a machine learning tool to reveal the significance of each feature",
            "29 , BIBREF37 , BIBREF32 , BIBREF1 , BIBREF2 ). These methods use external documents during a potentially existing training phase and – more importantly – during testing. In these approaches, the decision between INLINEFORM0 and INLINEFORM1 is put into the focus, where the external documents aim to construct the counter class INLINEFORM2 . Based on the above observations, we conclude that the key requirement for judging the model category of an AV method depends",
            " is sometimes experienced between interlocutors in the two-person approach. In addition, all the self-dialogs follow a reasonable path even when the user is constructing conversations that include understanding errors or other types of dialog glitches such as when a particular choice is not available. As it turns out, crowdsourced workers are quite effective at recreating various types of interactions, both error-free and those containing various forms of linguistic repair. The sample dialog in Figure FIGREF44 shows the result of a",
            " it is evident that the results obtained on the test set are not always in line with those achieved on the development set. Especially on the anger subtask for both EI-Reg and EI-Oc, the scores are considerably lower on the test set in comparison with the results on the development set. Therefore, a small error analysis was performed on the instances where our final model made the largest errors.\n\n\nError Analysis\nDue to some large differences between our results on the dev and test"
        ]
    },
    {
        "title": "A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification",
        "answer": "Zhang et al and Md. Aminul Islam et al in BIBREF13 BIBREF14 BIBREF15.  BIBREF16 BIBREF17 BIBREF18 BIBREF19 BIBREF20 BIBREF21 BIBREF22 BIBREF23 BIBREF24 BIBREF25 BIBREF26 BIBREF27 BIBREF28 BIBREF29 BIBREF30 BIBREF31 BIBREF32 BIBREF33 BIBREF34 BIBREF35 BIBREF36 BIBREF37 BIBREF38 BIBREF39 BIBREF40 BIBREF41 BIBREF42 BIBREF43 BIBREF44 BIBREF45 BIBREF46 BIBREF47 BIBREF48 BIBREF49 BIBREF50 BIBREF51 BIBREF52 BIBREF53 BIBREF54 BIBREF55 BIBREF56 BIBREF57 BIBREF58 BIBREF59 BIBREF60 BIBREF61 BIBREF62 BIBREF63 BIBREF64 BIBREF65 BIBREF66 BIBREF67 BIBREF68 BIBREF69 BIBREF70 BIBREF71 BIBREF72 B",
        "evidence": [
            " approaches perform similarly and reach state-of-the-art results. Yet better performances are obtained with our feature-based models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, feature-based approaches enriched",
            " IR-based method and the hybrid QA method (IR+RQE) according to the same reference answers used in LiveQA-Med. The answers were anonymized (the method names were blinded) and presented to 3 assessors: a medical doctor (Assessor A), a medical librarian (B) and a researcher in medical informatics (C). None of the assessors participated in the development of the QA methods. Assessors B and C evaluated 1,000 answers",
            "14 compiled an overview of important algorithmic building blocks for AV where, among other things, they also formulated three AV problems as decision problems. In 2009, Stamatatos BIBREF15 coined the phrases profile- and instance-based approaches that initially were used in the field of AA, but later found their way also into AV. In 2013 and 2014, Stamatatos et al. BIBREF11 , BIBREF16 introduced the terms intrinsic- and extrinsic models that aim to",
            " respectively), HLSTM+MSA and HLSTM (1.06/1.02 vs. 0.84, p-value $<0.001/0.001$ for GA/CA, respectively), and HLSTM+MSA and HLSTM+Copy (1.06/1.02 vs. 0.90, p-value= $0.044/0.048$ , respectively) all approve this claim. In addition, similar results between",
            " available datasets to verify the linking performance as well as generalization ability. We also conduct an extensive analysis of time complexity, the impact of key modules, and qualitative results, which demonstrate the effectiveness and efficiency of our proposed method.\n\n\nIntroduction\n This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ Entity linking (EL), mapping entity mentions in texts to a given knowledge base",
            "\" ML interface called themis-ml. In this interface, the main idea is to pick up a data set from a modified dataset. Themis-ml implements two methods for training fairness-aware models. The tool relies on two methods to make agnostic model type predictions: Reject Option Classification and Discrimination-Aware Ensemble Classification, these procedures being used to post-process predictions in a way that reduces potentially discriminatory predictions. According to the authors, it is possible to perceive the potential use",
            " RNNLM using cold fusion approach during training to attain a better-trained model. In near future, we will incorporate all the above techniques to get comparable performance with the state-of-the-art hybrid DNN/RNN-HMM systems.\n\n\n",
            ", BIBREF104, BIBREF157. Recent empirical studies BIBREF158, BIBREF135, BIBREF159 have compared a variety of unsupervised and weakly supervised mapping-based CLWE methods, and vecmap emerged as the most robust and very competitive choice. Therefore, we focus on 1) its fully unsupervised variant (unsuper) in our comparisons. For several language pairs, we also report scores with two other vecmap model variants: 2)",
            "LEU points compared to not using them.\n\n\nRelated Works\nJapanese-Vietnamese MT is firstly mentioned in 2005 BIBREF18 . The authors focused on the difference from embedding structures between Japanese and Vietnamese, and then proposed rules for MT system and experiment on very small dataset (714 Japanese embedding sentences). This approach is suitable for small system applied in a specific domain or language, but it is not easily extendable to other domains or languages due to the expensiveness of",
            " applicability of the Caravel approach remains highly questionable, as it requires a corpus where the information regarding Y/N-distribution is known beforehand in order to set the threshold. In regard to the two examined unary AV approaches MOCC and OCCAV, we observed that these perform poorly on all three corpora in comparison to the binary-intrinsic and binary-extrinsic methods. Most likely, this is caused by the wrong threshold setting, as both tend to",
            "itatively evaluate and compare the ML models in combination with a relevance decomposition or explanation technique, we apply the evaluation method described in section \"Measuring Model Explanatory Power through Extrinsic Validation\" . That is, we compute the accuracy of an external classifier (here KNN) on the classification of document summary vectors (obtained with the ML model's predicted class). For these experiments we remove test documents which are empty or contain only one word after preprocessing (this amounts to remove",
            "\nResearches on question classification, question taxonomies and QA system have been undertaken in recent years. There are two types of approaches for question classification according to Banerjee et al in BIBREF13 - by rules and by machine learning approach. Rule based approaches use some hard coded grammar rules to map the question to an appropriate answer type BIBREF14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in"
        ]
    },
    {
        "title": "Extractive Summarization of Long Documents by Combining Global and Local Context",
        "answer": "global context: the whole document; local context: words with multiple meanings.",
        "evidence": [
            " correction at ( INLINEFORM0 ). From these tables, we can see that on both datasets the performance significantly improves when local topic information (i.e. local context) is added. And the improvement is even greater when we only consider long documents. Rather surprisingly, this is not the case for the global context. Adding a representation of the whole document (i.e. global context) never significantly improves performance. In essence, it seems that all the benefits of our model come exclusively from modeling",
            " in an analogous way to how language ID is tagged in multilingual g2p.\n\n\n",
            " related to this work, and to the three anonymous reviewers of this draft for their constructive feedback. Finally, the authors would like to thank all crowdworkers who consented to participate in this study.\n\n\n",
            ": 246,122 word embeddings; training corpus: RCV1 corpus, one year of Reuters English newswire from August 1996 to August 1997. (ii) Huang. huang2012improving incorporated global context to deal with challenges raised by words with multiple meanings; size: 100,232 word embeddings; training corpus: April 2010 snapshot of Wikipedia. (iii) GloVe. Size: 1,193,514 word embeddings; training corpus: a Twitter corpus",
            " (HLSTM): The story context is represented by a hierarchical LSTM: a word-level LSTM for each sentence and a sentence-level LSTM connecting the four sentences BIBREF29 . A hierarchical attention mechanism is applied, which attends to the states of the two LSTMs respectively. HLSTM+Copy: The copy mechanism BIBREF32 is applied to hierarchical states to copy the words in the story context for generation. HLSTM+Graph Attention(",
            " that questions are linguistically simple, elaborate and explicit. Workers were asked to formulate questions about details of such a situation, i.e. independent of a concrete narrative. This resulted in questions, the answer to which is not literally mentioned in the text. To cover a broad range of question types, we asked participants to write 3 temporal questions (asking about time points and event order), 3 content questions (asking about persons or details in the scenario) and 3 reasoning questions (asking how or why something",
            " largest weight to some of everything to be just right (everything $\\rightarrow $ everything, perfect $\\rightarrow $ right). The example illustrates how the connection between context clues are built through incremental encoding and use of commonsense knowledge. The chain of context clues, such as ${be\\_cooking}\\rightarrow {want\\_everything\\_be\\_right}\\rightarrow {perfect\\_everything}\\rightarrow {lay\\_down}\\rightarrow {get\\_back}$",
            " boolean value indicating presence or absence is associated to each emotion. For a whole sentence, a global score per emotion can be obtained by summing the vectors for all content words of that sentence included in the lexicon, and used as feature. As additional feature, we also included Word Embeddings, namely distributed representations of words in a vector space, which have been exceptionally successful in boosting performance in a plethora of NLP tasks. We use three different embeddings: Google embeddings: pre",
            " the sentences are read for one entity: we collect transitions for all the non-empty roles (e.g., INLINEFORM9 , but not INLINEFORM10 ). For the RST discourse features, we process non-empty discourse relations also through either local or global reading. In the local reading, we read all the discourse relations in a sentence (a row) then move on to the next sentence. In the global reading, we read in discourse relations for one entity at a time.",
            "IBREF4 has experimented with global and local attention based on the how many hidden states the attention function takes into account. Their experiments have revolved around three attention functions - dot, concat and general. Their findings include that dot product works best for global attention. Our work also belongs to the global attention family as we consider all the hidden states of the sequence. Attention has been widely used in pair-classification problems like natural language inference. Wang et al. BIBREF5 introduced BIM",
            " should be small. Moreover, the whole distribution of the difference over all input positions should have a flat shape. Similar to the definition of local variance loss, the global variance loss is formulated as: where $g_i$ represents the difference between the accumulated attention weight and maximum attention weight at $i$-th position.\n\n\nProposed model ::: Model Training\nThe model is firstly pre-trained to minimize the maximum-likelihood loss, which is widely used in sequence generation tasks",
            " key modules in NCEL and give qualitative results for comprehensive analysis.\n\n\nBaselines and Datasets\nWe compare NCEL with the following state-of-the-art EL methods including three local models and three types of global models: Local models: He BIBREF29 and Chisholm BIBREF6 beat many global models by using auto-encoders and web links, respectively, and NTEE BIBREF16 achieves the best performance based on joint embeddings"
        ]
    },
    {
        "title": "Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets",
        "answer": "F-score, classification error, precision, recall, macro-averaged F1, MAP, Accuracy, MOS score. 0shot-tc, Definition-Restrictive, Definition-Wild, Label-partially-unseen. INLINEFORM0, INLINEFORM2. LR+IR method. SemEval-cQA. cQA-2017. IMDB62. SVM2 and SVM2-PV. macro-averaged F1. 11 classes. token level. entity-value pairs. IE system. gold descriptions. RotoWire train dataset. BIBREF10. BIBREF8. BIBREF23. BIBREF24. MMR. 0shot-tc. Definition-Restrictive. Definition-Wild. Label-partially-unseen. INLINEFORM0. INLINEFORM2. LR+IR method. SemEval-cQA. cQA-2017. IMDB62. SVM2 and SVM2-PV. macro-averaged F1. 11 classes. token level. entity-value pairs. IE system. gold descriptions. RotoWire train dataset. BIBREF10. BIBREF8. BIBREF23. BIBREF24. MMR. 0shot-tc. Definition-R",
        "evidence": [
            " setup ::: Evaluation metrics ::: Information extraction-oriented metrics.\nThese metrics estimate the ability of our model to integrate elements from the table in its descriptions. Particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ. To do so, we follow the protocol presented in BIBREF10. First, we apply an information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset. Entity-value pairs",
            " discussions and comments. We also thank Robin Jia for the help on SQuAD evaluations. \n\n\n",
            " there is a mismatch between the scores and systematized concept at this stage, the codebook may need to be adjusted, human coders retrained, more training data prepared, algorithms adjusted, or in some instances, even a new analytical method adopted. Other types of validation are also possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond",
            " The INLINEFORM0 scores might seem very low at the first glance. One obvious reason is the actual performance of the system, which gives a plenty of room for improvement in the future. But the main cause of low INLINEFORM2 numbers is the evaluation measure — using 11 classes on the token level is very strict, as it penalizes a mismatch in argument component boundaries the same way as a wrongly predicted argument component type. Therefore we also report two another evaluation metrics that help to put our results",
            " data, the LR+IR method outperformed the best system in all measures, with 80.57% Accuracy and 77.47% MAP (official system ranking measure in SemEval-cQA). On the cQA-2017 test data, the LR+IR method obtained 44.66% MAP and outperformed the cQA-2017 best system in Accuracy with 67.27%.\n\n\nDiscussion of RQE Results\nWhen trained and tested on the same corpus, the",
            "�I feel so depressed!”, “What a bad day!”, etc. There may be infinite number of responses for an open domain messages. Hence, it is difficult to construct a gold standard (usually a reference set) to evaluate a response which is generated by an open domain chit-chat system. For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc.,",
            " size applied is 2000-word as per the baseline-dataset experiment results, and for IMDB62, texts are not chunked (i.e., we feed the models with the original reviews directly). For model comparison, we also run the SVMs (i.e., SVM2 and SVM2-PV) used in the baseline-dataset experiment. All the experiments conducted here are multi-class classification with macro-averaged F1 evaluation. Model configurations",
            ", provide qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.\n\n\n",
            "Abstract\nSentence Boundary Detection (SBD) has been a major research topic since Automatic Speech Recognition transcripts have been used for further Natural Language Processing tasks like Part of Speech Tagging, Question Answering or Automatic Summarization. But what about evaluation? Do standard evaluation metrics like precision, recall, F-score or classification error; and more important, evaluating an automatic system against a unique reference is enough to conclude how well a SBD system is performing given the final application of the",
            " the six basic emotions of Ekman's standard model BIBREF8 . Valence was to be determined as positive or negative. Classification of emotion and valence were treated as separate tasks. Emotion labels were not considered as mututally exclusive, and each emotion was assigned a score from 0 to 100. Training/developing data amounted to 250 annotated headlines (Affective development), while systems were evaluated on another 1000 (Affective test). Evaluation was done using two different methods:",
            " products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\" (MOS) BIBREF23 , BIBREF24 to measure the effectiveness: how similar a response were comparing to the correct product's usage. The MOS score range is 1-5 (1-Bad, 2",
            " ::: Evaluation.\nOur standardized evaluations correspond to the Definition-Restrictive and Definition-Wild. i) Label-partially-unseen evaluation. This corresponds to the commonly studied $\\textsc {0shot-tc}$ defined in Definition-Restrictive: for the set of labels of a specific aspect, given training data for a part of labels, predicting in the full label set. This is the most basic setup in $\\textsc {0shot-tc}$. It"
        ]
    },
    {
        "title": "Creative GANs for generating poems, lyrics, and metaphors",
        "answer": "novel-9, dataset-pn, dataset-basic, SNLI, multiNLI, Quora, Clinical-QE, SemEval-cQA, Affective Text dataset, Fairy Tales dataset, ISEAR dataset, BBC Radio 3 dataset, Multi-SimLex dataset.",
        "evidence": [
            " was then added to our original training set. Again, the machine translation platform Apertium BIBREF5 was used for the translation of the datasets.\n\n\nAlgorithms Used\nThree types of models were used in our system, a feed-forward neural network, an LSTM network and an SVM regressor. The neural nets were inspired by the work of Prayas BIBREF7 in the previous shared task. Different regression algorithms (e.g. AdaBoost,",
            "Datasets\nThe statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors.",
            " dataset is used for the training. Typically, the source dataset is bigger than that of the target, and this method sometimes works better than TgtOnly. Another method is All, in which the source and target datasets are combined and used for the training. Although this method uses all the data, the training criteria enforce the model to perform well on both of the domains, and therefore the performance on the target domain is not necessarily high.  An approach widely used in the neural network community is FineT",
            ") . The third dataset (dataset-pn) is a subset of dataset-basic, containing 828 positive and 280 negative citations. Dataset-pn was used for the purposes of (1) evaluating binary classification (positive versus negative) performance using sent2vec; (2) Comparing the sentiment classification ability of PS-Embeddings with other embeddings.\n\n\nEvaluation Strategy\nOne-Vs-The-Rest strategy was adopted for the task of multi",
            " troubled the researchers from the onset. Could the methodology be adopted to target the speech of groups like Black Lives Matter? Could it be adopted by repressive governments to minimize online dissent? While these concerns remained, they concluded that hypothetical dual use scenarios did not outweigh the tangible contribution this research could offer towards making the online environment more equal and just.\n\n\nData\nThe next step involves deciding on the data sources, collecting and compiling the dataset, and inspecting its metadata.\n\n\nData acquisition\nMany scholars in",
            " is added to the new training set. If INLINEFORM2 or INLINEFORM3 , it is added to the auxiliary set, which serves as existing neighbors for unseen entities in INLINEFORM4 . Finally, for a triplet INLINEFORM0 in the original validation set, if INLINEFORM1 or INLINEFORM2 , it is removed from the validation set. The statistics for the resulting INLINEFORM0 datasets using Subject and Object strategies are in Table TABREF34 .\n\n",
            "No common types).\n\n\nDatasets Used for the RQE Study\nWe evaluate the RQE methods (i.e. deep learning model and logistic regression classifier) using two datasets of sentence pairs (SNLI and multiNLI), and three datasets of question pairs (Quora, Clinical-QE, and SemEval-cQA). The Stanford Natural Language Inference corpus (SNLI) BIBREF13 contains 569,037 sentence pairs",
            " 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}. For dialects, the data are labeled with 15 classes, from the set {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia",
            " in future work.\n\n\nEmotion datasets\nThree datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them",
            " In Figure 1, an overview of the system proposed is presented.\n\n\nDataset\nIn May 2018, we crawled Twitter using the Python library Tweepy, creating two datasets on which Contributor and Musical Work entities have been manually annotated, using IOB tags. The first set contains user-generated tweets related to the BBC Radio 3 channel. It represents the source of user-generated content on which we aim to predict the named entities. We create it filtering the messages containing hashtags",
            " knowledge about the activity of planting a tree, which goes beyond what is mentioned in the text. Texts, questions, and answers were obtained through crowdsourcing. In order to ensure high quality, we manually validated and filtered the dataset. Due to our design of the data acquisition process, we ended up with a substantial subset of questions that require commonsense inference (27.4%).\n\n\nCorpus\nMachine comprehension datasets consist of three main components: texts, questions and answers. In this section,",
            " integrate the data already carefully sampled in prior work into a single, comprehensive, and representative dataset. This way, we can control for diversity, frequency, and other properties while avoiding to perform this time-consuming selection process from scratch. Note that, on the other hand, the word pairs chosen for English are scored from scratch as part of the entire Multi-SimLex annotation process, introduced later in §SECREF5. We now describe the external data sources for the final set of word pairs: "
        ]
    },
    {
        "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text",
        "answer": "S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2",
        "evidence": [
            " years, (ii) the entity class specific performance, and (iii) the impact on entity profile expansion by suggesting missing sections to entities based on the pre-computed templates. Baselines. To the best of our knowledge, we are not aware of any comparable approach for this task. Therefore, the baselines we consider are the following: S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2 ",
            ":: Tweet-Level Models ::: Baseline GRU.\nOur baseline is a GRU network for each of the three tasks. We use the same network architecture across the 3 tasks. For each network, the network contains a layer unidirectional GRU, with 500 units and an output linear layer. The network is trained end-to-end. Our input embedding layer is initialized with a standard normal distribution, with $\\mu =0$, and $\\sigma =1$, i",
            " of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as",
            "kit.\n\n\nBaseline Decoder\nOur baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations:\n\n\nDecoder Speed Improvements\nThis section describes a number of speedups that can be made to a CPU-based attentional sequence-to-sequence beam decoder. Crucially, none of these speedups affect the actual mathematical computation of the decoder, so they can be applied to any network architecture with a guarantee that they will not affect the",
            " top images result from Google and Bing. We also provided the product title along with the images. SimplerVoice: We shown the generated key messages (Tab. TABREF19 ), and visual description including 2 components: photorealistics images and pictographs (Fig. FIGREF20 ) from SimplerVoice system. Intuitively, baseline 1 shows how much information a user would receive from the products' packages without prior knowledge of the products while baseline 2 might provide additional information by showing top images",
            " of randomly selected out-of-domain natural sentences, with the same architecture and training parameters, running validation every 2000 updates with a patience of 10. Since BPE units are selected based only on the out-of-domain statistics, fine-tuning is performed on sentences that are slightly longer (ie. they contain more units) than for the initial training. This system defines an upper-bound of the translation performance and is denoted below as natural. Our baseline and topline results are",
            " a baseline model. A pictorial representation of this ontology is given in Figure FIGREF16 Considering the criminal law domain case study, we made a few changes to the original SEM ontology. The entities of the model are: Actor: person involved with event Place: location of the event Time: time of the event Object: that actor act upon Organization: organization involved with event Currency: money involved with event The proposed ontology was designed in such a manner that it can incorporate information extracted from multiple",
            " out of the Internet Archive—we should be open about the limitations of our analyses, acknowledging the flaws in our data and drawing cautious and reasonable conclusions from them. In all cases, we should report the choices we have made when creating or re-using any dataset.\n\n\nCompiling data\nAfter identifying the data source(s), the next step is compiling the data. This step is fundamental: if the sources cannot support a convincing result, no result will be convincing. In many cases, this",
            "Labeled Question Types\nBesides the analysis on different reasoning types, we also look into the performance over questions with different first tokens in the development set, which provide us an automatic categorization of questions. According to the results in Table TABREF46 , the three neural baselines all perform the best on “Who” and “Where” questions, to which the answers are often named entities. Since the tweet contexts are short, there are only a small number of named entities to",
            " a higher KL loss in the inputless setting than the standard setting, as $\\mathbf {z}$ is required to encode more information about the input data for reconstruction. In terms of overall performance, our model outperforms all the baselines in both datasets (i.e., PTB and E2E). For instance, when comparing with the strongest baseline vMF-VAE in the standard setting, our model reduces NLL from 96 to 79 and PPL from 98 to 43 in",
            ". Unlike Baseline1 which avoids an explicit segmentation step, the Pointer-network can explicitly model which sentences are likely to be a boundary sentence. Moreover, the search space of the spans in the Pointer-network is $2n$ where $n$ is the number of sentences, because it selects only two boundary sentences. Note that the search space of Baseline1 is $n^2$. A much smaller search space might improve the accuracy by making the model consider fewer candidates.",
            " classification systems based on each classifier's predictions for the 121,512 visual questions in the test dataset. We first show performance of the baseline and our two prediction systems using precision-recall curves. The goals are to achieve a high precision, to minimize wasting crowd effort when their efforts will be redundant, and a high recall, to avoid missing out on collecting the diversity of accepted answers from a crowd. We also report the average precision (AP), which indicates the area under a precision-recall"
        ]
    },
    {
        "title": "Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity",
        "answer": "manually.",
        "evidence": [
            " presented in Table 6 . Rows 1 and 2 repeat the performance of the models trained on difficult subset and random subset with expert annotations only respectively. The third row is the model trained by combining difficult and random subsets with expert annotations. There are around 250 abstracts in the overlap of these two sets, so there are total 1.75k abstracts used for training the D+R model. Rows 4 to 6 are the models trained on all 5k abstracts with mixed annotations, where",
            " SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC). These captions are collected exclusively from snaps submitted to public and crowd-sourced stories (aka Snapchat Live Stories or Our Stories). Examples of such public crowd-sourced stories are “New York Story” or “Thanksgiving Story”,",
            "_i,s_j)$ is the Spearman's correlation between annotators $i$ and $j$'s scores ($s_i$,$s_j$) for all pairs in the dataset, and $N$ is the number of annotators. APIAA has been used widely as the standard measure for inter-annotator agreement, including in the original SimLex paper BIBREF14. It simply averages the pairwise Spearman's correlation between all annotators",
            "fer, Damyana Gateva, Leonie Harter, Sarah Mameche, Stefan GrÃ¼newald and Tatiana Anikina for help with the annotations. This research was funded by the German Research Foundation (DFG) as part of SFB 1102 `Information Density and Linguistic Encoding' and EXC 284 `Multimodal Computing and Interaction'.\n\n\n",
            ", and are able to use external sources (e.g. dictionaries, thesauri, WordNet) if required.  4. Annotators are kept anonymous, and are not able to communicate with each other during the annotation process. The selection criteria for the annotators required that all annotators must be native speakers of the target language. Preference to annotators with university education was given, but not required. Annotators were asked to complete a spreadsheet containing the translated pairs of",
            "-answer pairs. The annotation of this dataset is designed so that successfully answering the questions requires understanding and the knowledge involved in a document. We implement a framework consisting of both a question answering model and a question generation model, both of which take the knowledge extracted from the document as well as relevant facts from an external knowledge base such as Freebase/ProBase/Reverb/NELL. Results show that incorporating side information from external KB improves the accuracy of the baseline question answer system. We compare",
            " collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants: While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance”",
            " in future work.\n\n\nEmotion datasets\nThree datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them",
            " INLINEFORM0 was 0.70. The research has also been active on non-English datasets. Goudas.et.al.2014 focused on user-generated Greek texts. They selected 204 documents and manually annotated sentences that contained an argument (760 out of 16,000). They distinguished claims and premises, but the claims were always implicit. However, the annotation agreement was not reported, neither was the number of annotators or the guidelines. A study on annotation of arguments was conducted",
            " In Figure 1, an overview of the system proposed is presented.\n\n\nDataset\nIn May 2018, we crawled Twitter using the Python library Tweepy, creating two datasets on which Contributor and Musical Work entities have been manually annotated, using IOB tags. The first set contains user-generated tweets related to the BBC Radio 3 channel. It represents the source of user-generated content on which we aim to predict the named entities. We create it filtering the messages containing hashtags",
            " documents were pre-segmented using the Stanford Core NLP sentence splitter BIBREF69 embedded in the DKPro Core framework BIBREF70 . Annotators were asked to stick to the sentence level by default and label entire pre-segmented sentences. They should switch to annotations on the token level only if (a) a particular sentence contained more than one argument component, or (b) if the automatic sentence segmentation was wrong. Given the “noise”",
            " citations? The proposed method constructed sentence vectors (sent2vec) by averaging the word embeddings, which were learned from Anthology Collections (ACL-Embeddings). I also investigated polarity-specific word embeddings (PS-Embeddings) for classifying positive and negative citations. The sentence vectors formed a feature space, to which the examined citation sentence was mapped to. Those features were input into classifiers (support vector machines) for supervised classification. Using 10-cross"
        ]
    },
    {
        "title": "Multi-attention Recurrent Network for Human Communication Comprehension",
        "answer": "Long-short Term Hybrid Memory is formulated in Algorithm 1. Given a set of $M$ modalities in the domain of the data, subsequently $M$ LSTHMs are built in the MARN pipeline. For each modality $m \\in M$, the input to the $m$ th LSTHM is of the form $\\mathbf {X}^m=\\lbrace",
        "evidence": [
            " for that modality. This allows the memory to function in a hybrid manner. The Long-short Term Hybrid Memory is formulated in Algorithm 1. Given a set of $M$ modalities in the domain of the data, subsequently $M$ LSTHMs are built in the MARN pipeline. For each modality $m \\in M$ , the input to the $m$ th LSTHM is of the form $\\mathbf {X}^m=\\lbrace",
            ".g., convolutionally), but some aspects require long-distance context and must be computed recurrently. Many existing neural network architectures either fail to take advantage of the contextual information or fail to take advantage of the parallelism. QRNNs exploit both parallelism and context, exhibiting advantages from both convolutional and recurrent neural networks. QRNNs have better predictive accuracy than LSTM-based models of equal hidden size, even though they use fewer parameters and run substantially faster. Our experiments",
            "seq LSTM for intent keywords & slots extraction) + Level-2 (Separate-0: Seq2one LSTM for utterance-level intent detection) Hierarchical & Separate-1: Level-1 (Seq2seq Bi-LSTM for intent keywords & slots extraction) + Level-2 (Separate-1: Seq2one Bi-LSTM for utterance-level intent detection) Hierarchical & Separate-2",
            " RNNLM using cold fusion approach during training to attain a better-trained model. In near future, we will incorporate all the above techniques to get comparable performance with the state-of-the-art hybrid DNN/RNN-HMM systems.\n\n\n",
            " and also discuss the limitation. The latter part of this paper is a detailed analysis of the trained models and introduced losses. Our second question is about the true limitation of LSTM-LMs: are there still any syntactic constructions that the models cannot handle robustly even with our direct learning signals? This question can be seen as a fine-grained one raised by BIBREF5 with a stronger tool and improved evaluation metric. Among tested constructions, we find that syntactic agreement",
            " the LSTM and feed-forward model's predictions were averaged over 10 prediction runs. Subsequently, the predictions of all individual models were combined into an average. Finally, models were removed from the ensemble in a stepwise manner if the removal increased the average score. This was done based on their original scores, i.e. starting out by trying to remove the worst individual model and working our way up to the best model. We only consider it an increase in score if the difference is larger",
            ", whereas the inference LSTM primarily focuses on “nap”, which is the key word suggesting a Contradiction. Note that ESIM uses attention between the input and inference LSTM layers to align/contrast the sentences, hence it makes sense that the inference LSTM is more focused on the critical differences between the sentences. This is also observed for the Neutral example as well. It is worth noting that, while revealing similar general trends, the backward LSTM",
            "IBREF27 , BIBREF28 . In an LSTM RNN, at each time frame both the hidden state INLINEFORM0 and an associated “cell memory\" vector INLINEFORM1 , are updated and passed on to the next time frame. In other words, each forward edge in Figure FIGREF1 can be viewed as carrying both the cell memory and hidden state vectors. The updates are modulated by the values of several gating vectors, which control the degree to which",
            " data, which suggests that the joint hybrid is more helpful for smaller data sets. The joint hybrid is an order of magnitude slower than the sequential hybrid, as the SGD-based HMM is slower to train than the FFBS-based HMM. We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures",
            "r \\in \\mathbb {R}^{2d \\times d}$ and $\\mathbf {b}_r \\in \\mathbb {R}^{d}$ are learnt parameters. The low-level encoder aims at encoding a collection of records belonging to the same entity while the high-level encoder encodes the whole set of entities. Both the low-level and high-level encoders consider their input elements as unordered. We use the Transformer",
            "eman is particularly interesting. Indeed, these two models share the same intuition to flatten the data-structure. The only difference stands on the encoder mechanism: bi-LSTM vs. Transformer, for Wiseman and Flat respectively. Results shows that our Flat scenario obtains a significant higher BLEU score (16.7 vs. 14.5) and generates fluent descriptions with accurate mentions (RG-P%) that are also included in the gold descriptions (CS-R%).",
            "-LSTM) are also quasi-recurrent. However, some QRNN models (including those with attention or skip-connections) are not “strongly typed”. In particular, a T-RNN differs from a QRNN as described in this paper with filter size 1 and f-pooling only in the absence of an activation function on INLINEFORM0 . Similarly, T-GRUs and T-LSTMs differ from QRNNs with filter size"
        ]
    },
    {
        "title": "Construction of a Japanese Word Similarity Dataset",
        "answer": "social media.",
        "evidence": [
            " out of the Internet Archive—we should be open about the limitations of our analyses, acknowledging the flaws in our data and drawing cautious and reasonable conclusions from them. In all cases, we should report the choices we have made when creating or re-using any dataset.\n\n\nCompiling data\nAfter identifying the data source(s), the next step is compiling the data. This step is fundamental: if the sources cannot support a convincing result, no result will be convincing. In many cases, this",
            " by the UK Engineering and Physical Sciences Research Council (Grant number: EP/P011829/1).\n\n\n",
            " answers. A) TREC 2017 LiveQA-Med Participants' Results:  B) Our IR-based QA System: C) Our IR+RQE QA System:\n\n\n",
            " dataset was extended from that in BIBREF21 ; the data were collected from January 2017 to December 2017. The labeled dataset includes 48,374 passages. To support semi-supervised learning, the first 3 months of data (96,777 passages) are unlabeled. Because the data stem from social media, some text exists that cannot be considered as part of any sentence, such as product links, symbols unrelated to sentences, and space between sentences. These portions were not originally annotated as",
            " also like to thank LISP - Laboratory of Informatics, Systems and Parallelism.\n\n\n",
            " future, we will enhance our work by extracting facts from the suggested news articles. Results suggest that the news content cited in entity pages comes from the first paragraphs. However, challenging task such as the canonicalization and chronological ordering of facts, still remain.\n\n\n",
            " a collection. It creates a set of probability distributions over the vocabulary of the collection, which, when combined together in different proportions, best match the content of the collection. We can sort the words in each of these distributions in descending order by probability, take some arbitrary number of most-probable words, and get a sense of what (if anything) the topic is “about”. Each of the text segments also has its own distribution over the topics, and we can sort these segments",
            "points needs to be taken into account in systems' design. Characteristics that are not considered as relevant in a given task can be encapsulated in data nonetheless, and lead to bias performance. Being aware of the demographic skews our data set might contain is a first step to track the life cycle of a training data set and a necessary step to control the tools we develop.\n\n\n",
            "aset\nData Collection. To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. Paragraph",
            "\nData Collection ::: Answer Selection\nTo identify legally sound answers, we recruit seven experts with legal training to construct answers to Turker questions. Experts identify relevant evidence within the privacy policy, as well as provide meta-annotation on the question's relevance, subjectivity, OPP-115 category BIBREF49, and how likely any privacy policy is to contain the answer to the question asked.\n\n\nData Collection ::: Analysis\nTable.TABREF17 presents aggregate statistics of the",
            " or even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions. To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require background knowledge. We explicitly state the following items in the HIT instructions for question writing: No Yes-no questions should be asked. The question should have at",
            "? using the lexical and phrasal rules from the PPDB. Once $G_{\\mathrm {syn}}^{\\prime }$ is generated, we sample paraphrases of the input question $q$ . These paraphrases are further filtered with a classifier to improve the precision of the generated paraphrases. We train the L-PCFG $G_{\\mathrm {syn}}$ on the Paralex corpus BIBREF9 . Paralex"
        ]
    },
    {
        "title": "Assessing the Applicability of Authorship Verification Methods",
        "answer": "a self-compiled corpus is a dataset created by the researcher themselves.",
        "evidence": [
            " from future requests.\n\n\nThe Taskmaster Corpus ::: Self-dialogs (one-person written dataset)\nWhile the two-person approach to data collection creates a realistic scenario for robust, spoken dialog data collection, this technique is time consuming, complex and expensive, requiring considerable technical implementation as well as administrative procedures to train and manage agents and crowdsourced workers. In order to extend the Taskmaster dataset at minimal cost, we use an alternative self-dialog approach in which crowdsourced workers",
            " for anonymising the source corpus – error statistics from a private corpus can be aggregated and applied to a different target text, obscuring sensitive information in the original examination scripts. However, the task of creating incorrect data is somewhat more difficult than might initially appear – naive methods for error generation can create data that does not resemble natural errors, thereby making downstream systems learn misleading or uninformative patterns. Previous work on artificial error generation (AEG) has focused on specific error types, such as prepos",
            " Corpus containing 2,500M and 800M tokens, respectively, and has two models named BERTbase and BERTlarge. BERTbase contains an encoder with 12 layers (transformer blocks), 12 self-attention heads, and 110 million parameters whereas BERTlarge has 24 layers, 16 attention heads, and 340 million parameters. Extracted embeddings from BERTbase have 768 hidden dimensions BIBREF11. As the BERT model is pre-trained on general corpora",
            " automatically produced segmentation. For each pair of reference segments, a $L_r$ list of word pairs is provided: the last word of the first segment and the first word of the second. For example, considering the reference text wik1_01_02-04-2006.seg, from Annodis corpus: [Le Ban Amendment]$_1$ [Après avoir adopté la Convention,]_2 [un certain nombre de PED et",
            " applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus was to focus on a different aspect of the methods applicability. Our findings regarding the examined approaches can be summarized as follows: Despite of the good performance of the five AV methods GenIM, ImpGI, Unmasking, Caravel and SPATIUM, none of them can be truly considered as reliable and therefore applicable in real forensic cases. The reason for this is not only the non-",
            ".\n\n\nCorpus and Annotation\nReddit is popular website that allows registered users (without identity verification) to participate in fora grouped by topic or interest. Participation consists of posting stories that can be seen by other users, voting stories and comments, and comments in the story's comment section, in the form of a forum. The forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories",
            " Statistical and Neural Machine Translation systems (e.g. SMT and NMT). Acquiring a high-quality parallel corpus that is large enough to train MT systems, specially NMT ones, is not a trivial task, since it usually demands human curating and correct alignment. In light of that, the automated creation of parallel corpora from freely available resources is extremely important in Natural Language Processing (NLP), enabling the development of accurate MT solutions. Many parallel corpora are already available, some",
            " sentimentally. In the corpus-based approach, we capture both of these characteristics and generate word embeddings specific to a domain. Firstly, we construct a matrix whose entries correspond to the number of cooccurrences of the row and column words in sliding windows. Diagonal entries are assigned the number of sliding windows that the corresponding row word appears in the whole corpus. We then normalise each row by dividing entries in the row by the maximum score in it. Secondly, we perform the principal",
            "versarial techniques induce features that are language-independent.\n\n\nSelf-training Techniques\nSelf-training, where an initial model is used to generate labels on an unlabeled corpus for the purpose of domain or cross-lingual adaptation, was studied in the context of text classification BIBREF11 and parsing BIBREF13 , BIBREF14 . A similar idea based on expectation-maximization, where the unobserved label is treated as a latent variable, has also been applied",
            "rapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10 . In addition to the raw IPA transcriptions extracted from Wiktionary, the corpus provides an automatically cleaned version of transcriptions. Cleaning is a necessary step because web-scraped data is often noisy and may be transcribed at an inconsistent level of detail. The data cleaning used",
            "] purpleOur approach integrates recent findings on neural extractive summarization in a parameter lean and modular architecture.[3] purpleWe evaluate our model and compare with previous works in both extractive and abstractive summarization on two large scientific paper datasets, which contain documents that are much longer than in previously used corpora.[4] purpleOur model not only achieves state-of-the-art on these two datasets, but in an additional experiment, in which we consider documents with increasing length, it becomes",
            "vel, it can be clearly seen that the actual and maximum achievable results are very close to each other. This is not surprising, due to the fact that Caravel's threshold always lies at the median point of the ROC curve, provided that the given corpus is balanced. While inspecting the 250 characters long documents in more detail, we identified that they share similar vocabularies consisting of chat abbreviations such as “lol” (laughing out loud) or “k�"
        ]
    },
    {
        "title": "Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach",
        "answer": "BIBREF20, BIBREF23, BIBREF6, TABREF55, TABREF56, EP/REF7, natural, MARN, MAB.",
        "evidence": [
            ":: Tweet-Level Models ::: Baseline GRU.\nOur baseline is a GRU network for each of the three tasks. We use the same network architecture across the 3 tasks. For each network, the network contains a layer unidirectional GRU, with 500 units and an output linear layer. The network is trained end-to-end. Our input embedding layer is initialized with a standard normal distribution, with $\\mu =0$, and $\\sigma =1$, i",
            "\nAcknowledgments\nThis work was funded by the Amazon Academic Research Awards program.\n\n\n",
            " on Clinical-QE gave the best performance with 73.18% Accuracy. The SNLI and multi-NLI models did not perform well when tested on medical RQE data. We performed additional evaluations using the RTE-1, RTE-2 and RTE-3 open-domain datasets provided by the PASCAL challenge and the results were similar. We have also tested the SemEval-cQA-2016 model and had a similar drop in performance on RQ",
            " close to the baseline level, with slight improvements in German. A first observation is that this method brings stupid BT models closer to conventional BT, at a greatly reduced computational cost. While French still remains 0.4 to 1.0 BLEU below very good backtranslation, both approaches are in the same ballpark for German - may be because BTs are better for the former system than for the latter. Finally note that the GAN architecture has two differences with basic copy-marked: (a)",
            " natural language instructions into a navigation plan against alternative deep-learning models: [align=left,leftmargin=0em,labelsep=0.4em,font=] The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6",
            " average score for each aspect. As the results show in Table TABREF55 , our model not only performs much better than the baselines, it also shows quite comparable performance with the reference summaries. In Table TABREF56 we present two real examples, which show the templates found by our model are indeed related to the source articles, and with their aid, our model succeeds to keep the main content of the source articles for summarization while discarding unrelated words like `US' and `",
            " specific phrase in Portuguese (um estado), the baseline model predicted {POS: Det, Gender: Masc, Number: Sing} INLINEFORM0 , {POS: Noun, Gender: Fem (X), Number: Sing} INLINEFORM1 , whereas our model was able to get the gender correct because of the transition factors in our model.\n\n\nWhat is the Model Learning?\nOne of the major advantages of our model is the ability to interpret what the model has learned by",
            " merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation on all the hidden states, allowing a better regularisation of the model learning process. Empirical results show that our model can effectively mitigate the latent variable collapse issue while giving a better predictive performance than the baselines.\n\n\nAcknowledgment\nThis work is supported by the award made by the UK Engineering and Physical Sciences Research Council (Grant number: EP/",
            " classification systems based on each classifier's predictions for the 121,512 visual questions in the test dataset. We first show performance of the baseline and our two prediction systems using precision-recall curves. The goals are to achieve a high precision, to minimize wasting crowd effort when their efforts will be redundant, and a high recall, to avoid missing out on collecting the diversity of accepted answers from a crowd. We also report the average precision (AP), which indicates the area under a precision-recall",
            "REF7 ), both baselines perform worse on questions that require the understanding deep semantics and userID&hashtags. The former kind of questions also appear in other benchmarks and is known to be challenging for many current models. The second kind of questions is tweet-specific and is related to specific properties of social media data. Since both models are designed for formal-text passages and there is no special treatment for understanding user IDs and hashtags, the performance is severely limited on the questions requiring such reasoning abilities",
            " of randomly selected out-of-domain natural sentences, with the same architecture and training parameters, running validation every 2000 updates with a patience of 10. Since BPE units are selected based only on the out-of-domain statistics, fine-tuning is performed on sentences that are slightly longer (ie. they contain more units) than for the initial training. This system defines an upper-bound of the translation performance and is denoted below as natural. Our baseline and topline results are",
            " of the coefficients $a_t$ are crucial is an important research question. From the results tables, we notice that the MARN (no $\\mathcal {A}$ ) baseline severely under-performs compared to MARN. This supports the importance of the attentions in the MAB. Without these attentions, MARN is not able to accurately model the cross-view dynamics. RQ3: In our experiments the MARN with only one attention (like conventional attention models)"
        ]
    },
    {
        "title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU",
        "answer": "fast baseline decoder.",
        "evidence": [
            " outputs achieve 36.2 BLEU and are 99.9%+ identical. The largest improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount. Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder. Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target",
            "\n\n\nBaseline Agent\nAs a baseline, we propose QA-DQN, an agent that adopts components from QANet BIBREF18 and adds an extra command generation module inspired by LSTM-DQN BIBREF19. As illustrated in Figure FIGREF6, the agent consists of three components: an encoder, an action generator, and a question answerer. More precisely, at a game step $t$, the encoder reads observation string $",
            " neural networks and continuous sentence features, which outperforms traditional methods on the DailyMail dataset. In particular, they develop a general encoder-decoder architecture, where a CNN is used as sentence encoder, a uni-directional LSTM as document encoder, with another uni-directional LSTM as decoder. To decrease the number of parameters while maintaining the accuracy, summarunner present SummaRuNNer, a simple RNN-based sequence classifier",
            " $\\textbf {c}_{\\textbf {l},j}^{(i)}$ is the context vector which is an attentive read of the preceding sentence $X_{i-1}$ , conditioned on $\\textbf {h}^{(i)}_{j-1}$ . We will describe the context vector in the next section. During the decoding process, the decoder obtains a context vector from the last sentence $X_{K}$ in the context to utilize",
            " We use the BIO encoding, so each token belongs to one of the following 11 classes: O (not a part of any argument component), Backing-B, Backing-I, Claim-B, Claim-I, Premise-B, Premise-I, Rebuttal-B, Rebuttal-I, Refutation-B, Refutation-I. This is the minimal encoding that is able to distinguish two adjacent argument components of the same type. In our",
            "REF28 proposed a delayed copy mechanism where their decoder also acts in two steps: 1) using a classical LSTM decoder to generate delexicalized text and 2) using a pointer network BIBREF30 to replace placeholders by records from the input data. Closer to our work, very recent work BIBREF11, BIBREF19, BIBREF13 have proposed to take into account the data structure. More particularly, Puduppully et al. B",
            " used the dot products of these encoder hidden states with the decoder's last layer's un-gated hidden states, applying a INLINEFORM1 along the encoder timesteps, to weight the encoder states into an attentional sum INLINEFORM2 for each decoder timestep. This context, and the decoder state, are then fed into a linear layer followed by the output gate: DISPLAYFORM0   where INLINEFORM0 is the last layer.",
            "\n\n\nNMT setups and performance \nOur baseline NMT system implements the attentional encoder-decoder approach BIBREF6 , BIBREF7 as implemented in Nematus BIBREF8 on 4 million out-of-domain parallel sentences. For French we use samples from News-Commentary-11 and Wikipedia from WMT 2014 shared translation task, as well as the Multi-UN BIBREF9 and EU-Bookshop BIBREF10 corpora. For German",
            "GA/ASIC-based decoders require specialized hardware deployment and logistical constraints such as batch processing. Efficient CPU decoders can also be used for on-device mobile translation. We focus on single-threaded decoding and single-sentence processing, since multiple threads can be used to reduce latency but not total throughput. We approach this problem from two angles: In Section \"Decoder Speed Improvements\" , we describe a number of techniques for improving the speed of the decoder, and",
            " is 128.\n\n\nExperimental Setup ::: Baselines\nWe compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base: A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue BIBREF0; VAE-CNN: A variational autoencoder model with a LSTM enc",
            " using the baseline system trained on mel-filterbanks. The decoder parameters are tuned using the grid search described in Section SECREF3 , a beam size is fixed to 2500 and a beam score to 30. Correlation between perplexity and WER Figure FIGREF18 shows the correlation between perplexity and WER as the training progresses. As perplexity decreases, the WER on both dev-clean and dev-other also decreases following the same trend. It illustrates that perplexity on the linguistic",
            " vectors their models learned correlated well to genetic relationships, so it would be interesting to see if the embeddings our source encoder learned for the language ID tokens showed anything similar. In a few cases they do (the languages closest to German in the vector space are Luxembourgish, Bavarian, and Yiddish, all close relatives). However, for the most part the structure of these vectors is not interpretable. Therefore, it would be difficult to estimate the embedding for an unseen language,"
        ]
    },
    {
        "title": "A Stable Variational Autoencoder for Text Modelling",
        "answer": "precision and recall (denoted respectively CS-P% and CS-R%) of unique relations $r$ extracted from $\\hat{y}_{1:T}$ that also appear in $s$. $\\bullet $ Content Selection (CS) measures how well the generated document matches the gold document in terms of mentioned records. We measure the precision and recall (denoted respectively CS-P% and CS-R%) of unique relations $r$ extracted from $\\hat{y}_{1:T}$ that also appear in $s$. $\\bullet $ Content Selection (CS) measures how well the generated document matches the gold document in terms of mentioned records. We measure the precision and recall (denoted respectively CS-P% and CS-R%) of unique relations $r$ extracted from $\\hat{y}_{1:T}$ that also appear in $s$. $\\bullet $ Content Selection (CS) measures how well the generated document matches the gold document in terms of mentioned records. We measure the precision and recall (denoted respectively CS-P% and CS-R%) of unique relations $r$ extracted from $\\hat{y}_{1:T}$ that also appear in $s$. $\\bullet $ Content Selection (CS) measures how well the generated document matches the gold document in terms of mentioned records. We measure the",
        "evidence": [
            " equivalent (one of them is incomplete) and INLINEFORM4 are totally inconsistent. The answers collected at this step are also used to measure the human performance. We have 59 individual workers participated in this process.\n\n\nTask and Evaluation\nAs described in the question-answer writing process, the answers in our dataset are different from those in some existing extractive datasets. Thus we consider the task of answer generation for TweetQA and we use several standard metrics for natural language generation to evaluate QA",
            ", since the introduction of GANs, research groups have studied how variational autoencoders can be incorporated into these models, to increase the clarity and accuracy of the models’ outputs. Wang et al. in particular demonstrate how using a variational autoencoder in the generator model can increase the accuracy of generated text samples [11]. Most promisingly, the data used by the group was a large data set of Amazon customer reviews, which in many ways parallels the tone and semantic",
            " based on their importance, and form the summary as an aggregate of these sentences. Usually, summaries generated in this way have a better performance on fluency and grammar, but they may contain much redundancy and lack in coherence across sentences. In contrast, abstractive methods attempt to mimic what humans do by first extracting content from the source document and then produce new sentences that aggregate and organize the extracted information. Since the sentences are generated from scratch they tend to have a relatively worse performance on fluency and",
            ". When it comes to poetry generation using generative text models, Zhang and Lapata BIBREF9, Yi et al. BIBREF10 and Wang et al. BIBREF11 use language modeling to generate Chinese poems. However, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding. For the task of text generation, MaskGAN BIBREF12 uses a Reinforcement Learning signal from the discriminator,",
            " we intend to measure. Unfortunately, these steps tend to be underreported, but documenting the pre-processing choices made is essential and is analogous to recording the decisions taken during the production of a scholarly edition or protocols in biomedical research. Data may also vary enormously in quality, depending on how it has been generated. Many historians, for example, work with text produced from an analogue original using Optical Character Recognition (OCR). Often, there will be limited information available regarding the accuracy of the OCR",
            " text, thus introducing a “global view.” The GR features, on the other hand, are more restricted to a “local view” on entities between consecutive sentences. While a deeper empirical investigation is needed, one can intuitively imagine that identifying authorship by focusing on the local transitions between grammatical relations (as in GR) is more difficult than observing how the entire text is organized (as in RST).\n\n\nConclusion\nWe have conducted an in-depth investigation of",
            " a collection. It creates a set of probability distributions over the vocabulary of the collection, which, when combined together in different proportions, best match the content of the collection. We can sort the words in each of these distributions in descending order by probability, take some arbitrary number of most-probable words, and get a sense of what (if anything) the topic is “about”. Each of the text segments also has its own distribution over the topics, and we can sort these segments",
            " train the model and assume that they are the gold standard, we give both articles and reference summaries to the annotator to score the generated summaries. In other words, we compare the generated summaries against the reference ones and the original article to obtain the (relative) scores in Table 3. Each perspective is assessed with a score from 1 (worst) to 5 (best). The result in Table TABREF21 demonstrate that our model performs better under both criteria w.r.t.",
            " likely, this is caused by the wrong threshold setting, as both tend to generate more N-predictions. From the remaining approaches, GLAD and COAV seem to be a good choice for realistic scenarios. However, the former has been shown to be more robust in regard to varying text lengths given a fixed model, while the latter requires a retraining of the model (note that both performed almost equal in terms of AUC). Our hypothesis, which we leave open for future work, is that",
            " generate text containing factual (i.e., correct) records. We measure the precision and absolute number (denoted respectively RG-P% and RG-#) of unique relations $r$ extracted from $\\hat{y}_{1:T}$ that also appear in $s$. $\\bullet $ Content Selection (CS) measures how well the generated document matches the gold document in terms of mentioned records. We measure the precision and recall (denoted respectively CS-P% and CS",
            " are able to significantly improve error detection performance.\n\n\nConclusion\nThis paper investigated two AEG methods, in order to create additional training data for error detection. First, we explored a method using textual patterns learned from an annotated corpus, which are used for inserting errors into correct input text. In addition, we proposed formulating error generation as an MT framework, learning to translate from grammatically correct to incorrect sentences. The addition of artificial data to the training process was evaluated on three error detection annotations",
            " We defined two metrics - grammar and logicality for manual evaluation. Score 0/1/2 is applied to each metric during annotation. Whether an ending is natural and fluent. Score 2 is for endings without any grammar errors, 1 for endings with a few errors but still understandable and 0 for endings with severe errors and incomprehensible. Whether an ending is reasonable and coherent with the story context in logic. Score 2 is for reasonable endings that are coherent in logic, 1 for relevant endings but with some discrepancy between"
        ]
    },
    {
        "title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances",
        "answer": "layer/architecture.",
        "evidence": [
            " to 1.\n\n\nResults\nWe would like to investigate effectiveness the proposed joint model. To do so, the same shared layer/architecture is employed in the following variants of the proposed model: The results in terms of EM and F1 is summarized in Table TABREF20 . We observe that Joint SAN outperforms the SAN baseline with a large margin, e.g., 67.89 vs 69.27 (+1.38) and 70.68 vs 72.20 (+1",
            "rained models or created more generally by the user from user-specifications. Model - All models follow the same hierarchy of abstraction: a base class implements the model's computation graph from encoding (projection on the embedding matrix) through the series of self-attention layers and up to the last layer hidden states. The base class is specific to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of the",
            " models are known as topic models. They summarize multiple text documents into a number of common, semantic topics. BIBREF8 use a structural topic model (STM) that allows for the evaluation of the effects of structural covariates on topical structure, with the aim of analyzing several survey experiments and open-ended questions in the American National Election Study.\n\n\nReligious statements\n BIBREF9 analyze Islamic fatwas to determine whether Jihadist clerics write about different topics to those by non-J",
            " model is learned by the shared layer. To learn a better shareable sentence representation, we propose a new information-sharing scheme for multi-task learning in this paper. In our proposed scheme, the representation of every sentence is fully shared among all different tasks. To extract the task-specific feature, we utilize the attention mechanism and introduce a task-dependent query vector to select the task-specific information from the shared sentence representation. The query vector of each task can be regarded as learnable parameters (",
            ". This stark difference between the votes- and speeches-cluster coefficients provides further indication of underlying heterogeneity in the network graphs. Both indicate that membership in affinity communities is associated with a decrease in the likelihood of conflict onset but appear to capture different manifestations of latent preferences. The multiplex model displays coefficients closer to Models 1 and 2. The multiplex bloc indicates that membership in affinity communities as located across vote and speech graphs is associated with a decrease in the likelihood that a given pair of states will engage in",
            " are split into train, validation and test sets that include no identical speakers between sets, i.e. all the speakers in the test set are different from the train and validation sets. All models are re-trained on the same train/validation/test splits. To train the MARN for different tasks, the final outputs $h_T$ and neural cross-view dynamics code $z_T$ are the inputs to another deep neural network that performs classification (categorical cross-",
            "mode representation of the cross-layer communities for a given year where an edge between INLINEFORM0 and INLINEFORM1 exists if the extraction procedure identifies INLINEFORM2 and INLINEFORM3 as belonging to the same community. If the extraction procedure fails to locate multilayer communities, we can conclude that no information is lost by using the more common aggregation procedure of considering the union of edges in the two layers INLINEFORM4 and consider an edge to exist between INLINEFORM5 and",
            "VR model: Pearson's correlation coefficients range from 0.550 to 0.622. The regression correlations are the lowest. The RNN model realizes the strongest performance among the stand-alone (non-ensemble) models, outperforming variants that exploit CNN and USE representations. Combining the RNN and USE further improves results. We hypothesize that this is due to complementary sentence information encoded in universal representations. For all models, correlations for Intervention and Outcomes are higher than for Population, which",
            "eling multimodal human communication has been studied previously. Past approaches can be categorized as follows: Non-temporal Models: Studies have focused on simplifying the temporal aspect of cross-view dynamics BIBREF5 , BIBREF6 , BIBREF7 in order to model co-occurrences of information across the modalities. In these models, each modality is summarized in a representation by collapsing the time dimension, such as averaging the modality information through time BIBREF8",
            " sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11) . The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like.\n\n\nTask 1: Quora Dupl",
            " are changed to $\\Vert \\theta _g \\Vert ^2 + \\Vert \\theta _s \\Vert ^2$ and $\\Vert \\theta _g \\Vert ^2 + \\Vert \\theta _t \\Vert ^2$ , respectively. Since the parameters $\\theta _g$ are shared, we cannot optimize the problems separately. This change of the objective function can be understood as adding additional regularization terms $\n2(\\Vert \\theta _g \\",
            "-words model and default parameters as per the Scikit-learn implementation BIBREF13 on different combinations of pages. Based on results of the attempted combinations as well as on the distribution of emotions in the development dataset (Figure FIGREF9 ), we selected a best model (B-M), namely the combined set of Time, The Guardian and Disney, which yields the highest results on development data. Time and The Guardian perform well on most emotions but Disney helps to boost the performance for the Joy class"
        ]
    },
    {
        "title": "Is there Gender bias and stereotype in Portuguese Word Embeddings?",
        "answer": "word2vec, AWEs, CBOW, HLBL, Brown clusters, C&W embeddings, LSA, LDA, PS-Embeddings, sent2vec, Anthology Collections (ACL-Embeddings)  and CLWEs.  and  turian2010word, luo2014pre, Ponti:2019emnlp.  and  BIBREF2, BIBREF3, BIBREF6, BIBREF12, BIBREF21, BIBREF28.  and  BIBREF146.  and  BIBREF20.  and  TABREF20.  and  BIBREF2.  and  BIBREF3.  and  BIBREF6.  and  BIBREF12.  and  BIBREF21.  and  BIBREF28.  and  BIBREF146.  and  TABREF20.  and  BIBREF20.  and  BIBREF2.  and  BIBREF3.  and  BIBREF6.  and  BIBREF12.  and  BIBREF21.  and  BIBREF28.  and  BIBREF",
        "evidence": [
            " terms is still low.\n\n\nFinal Remarks\nThis paper presents an analysis of the presence of gender bias in Portuguese word embeddings. Even though it is a work in progress, the proposal showed promising results in analyzing predicting models. A possible extension of the work involves deepening the analysis of the results obtained, seeking to achieve higher accuracy rates and fairer models to be used in machine learning techniques. Thus, these studies can involve tests with different methods of pre-processing the data to the use",
            "W that has begun to be explored is the use of acoustic word embeddings (AWEs), or vector representations of spoken word segments. AWEs are representations that can be learned from data, ideally such that the embeddings of two segments corresponding to the same word are close, while embeddings of segments corresponding to different words are far apart. Once word segments are represented via fixed-dimensional embeddings, computing distances is as simple as measuring a cosine or Euclidean",
            ", is a word-embedding based convolutional neural network (CNN) model similar to the one proposed in BIBREF28 for sentence classification, which itself is a slight variant of the model introduced in BIBREF6 for semantic role labeling. This architecture is depicted in Figure 1 (left) and is composed of several layers. As previously described, in a first step we map each word in the document to its word2vec vector. Denoting by $D$ the word embedding",
            ", we experimented with a wide range of linguistically motivated features and found that (1) the largest feature set (including n-grams, structural features, syntactic features, topic distribution, sentiment distribution, semantic features, coreference feaures, discourse features, and features based on word embeddings) performs best in both in-domain and all-data cross validation, while (2) features based only on word embeddings yield best results in cross-domain evaluation. Since there",
            " many studies have been employed on generating word vectors including both semantic and sentimental components, generating and analysing the effects of different types of embeddings on different tasks is an emerging field for Turkish. Latent Dirichlet allocation (LDA) is used in BIBREF2 to extract mixture of latent topics. However, it focusses on finding the latent topics of a document, not the word meanings themselves. In BIBREF3, LSA is utilised to generate word vectors,",
            " citations? The proposed method constructed sentence vectors (sent2vec) by averaging the word embeddings, which were learned from Anthology Collections (ACL-Embeddings). I also investigated polarity-specific word embeddings (PS-Embeddings) for classifying positive and negative citations. The sentence vectors formed a feature space, to which the examined citation sentence was mapped to. Those features were input into classifiers (support vector machines) for supervised classification. Using 10-cross",
            " also inspired by studies that compared the performance of different word embedding versions or investigated the combination of them. For example, turian2010word compared Brown clusters, C&W embeddings and HLBL embeddings in NER and chunking tasks. They found that Brown clusters and word embeddings both can improve the accuracy of supervised NLP systems; and demonstrated empirically that combining different word representations is beneficial. luo2014pre adapted CBOW BIBREF12 to train word embed",
            " first letters) or visual contexts (concert scenes, associated contexts of which often include named entities in the training dataset). Sensitivity to Word Embeddings Vocabulary Size: In order to isolate the effectiveness of the modality attention module on textual models in handling missing tokens, we report the performance with varying word embeddings vocabulary sizes in Table TABREF20 . By increasing the number of missing tokens artificially by randomly removing words from the word embeddings matrix (original vocab size: 400K",
            "_FORM3) to find out how similar two word vectors (rows) are. Thirdly, all the values in the matrix are subtracted from 1 to create a dissimilarity matrix. Then, we feed the matrix as input into the fuzzy c-means clustering algorithm. We chose the number of clusters as 200, as it is considered a standard for word embeddings in the literature. After clustering, the dimension i for a corresponding word indicates the degree to which this word",
            " approaches are not only intuitively desirable, but also increase classification accuracy in machine learning tasks. Vector space approaches involve the embedding of document features from one dimension per word space to a continuous, lower dimensional vector space. Each document feature is represented as a real-valued vector and it has been shown that these representations retain desirable syntactical qualities, such as context and structure of speech. Traditionally, dimensionality reduction could be obtained through latent semantic analysis (LSA) used to factorize the feature",
            " translated from English to the target language via a shared cross-lingual word vector space. To this end, following Ponti:2019emnlp we rely on cross-lingual word embeddings (CLWEs) BIBREF146 available online, which are based on Wiki ft vectors. Following that, a constraint refinement step is applied in the target language which aims to eliminate the noise inserted during the translation process. This is done by training a relation classification tool: it is trained again",
            " words and all components. The other approaches use human markup. Such measures need additional data, and it is difficult to study them algebraically. Also, unlike topic modeling, word embeddings are not probabilistic: both positive and negative values of coordinates should be considered. Let all word vectors be normalized and $W$ be the word matrix. Inspired by BIBREF21 , where vector space models are used for evaluating topic coherence, we suggest to estimate the interpretability of $k$"
        ]
    },
    {
        "title": "An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models",
        "answer": "BERT, LSTM, CRF, MEMM, FastText, CNN, LSTM-based models, bi-LSTM-based models, LSTM gating signals, attention, LSTM language models, LSTM-based models, LSTM, CRF, MEMM, FastText, CNN, BERT, LSTM, CRF, MEMM, FastText, CNN, LSTM-based models, bi-LSTM-based models, LSTM gating signals, attention, LSTM language models, LSTM-based models, LSTM, CRF, MEMM, FastText, CNN, BERT, LSTM, CRF, MEMM, FastText, CNN, LSTM-based models, bi-LSTM-based models, LSTM gating signals, attention, LSTM language models, LSTM-based models, LSTM, CRF, MEMM, FastText, CNN, BERT, LSTM, CRF, MEMM, FastText, CNN, LSTM-based models, bi-LSTM-based models, LSTM gating signals, attention, LSTM language models, LSTM-based models, LSTM, CRF, MEMM, FastText, CNN, BERT, LSTM, CRF, MEMM, FastText, CNN, LSTM-based models, bi-LSTM-based models, LSTM gating signals, attention, LSTM language models, LSTM-based models, LSTM, CRF",
        "evidence": [
            " models (MEMMs, CRFs) and that of word vector representations to bi-LSTM-based models as reported by BIBREF20 . As mentioned above, work on the contribution of word vector representations to feature-based approaches has been carried out by BIBREF18 . However, the exploitation of existing morphosyntactic or morphological lexicons in neural models is a less studied question. Improvements over the state of the art might be achieved by integrating lexical information both from",
            "Abstract\nDeep learning models have achieved remarkable success in natural language inference (NLI) tasks. While these models are widely explored, they are hard to interpret and it is often unclear how and why they actually work. In this paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for NLI. In particular, we propose to interpret the intermediate layers of NLI models by visualizing the saliency of attention and LSTM gating signals",
            " several ways. In particular, they provide neither a unified API across models nor standardized ways to access the internals of the models. Targeting a more general machine-learning community, these Hubs lack the NLP-specific user-facing features provided by Transformers like tokenizers, dedicated processing scripts for common downstream tasks and sensible default hyper-parameters for high performance on a range of language understanding and generation tasks. The last direction is related to machine learning research frameworks that are specifically used to test,",
            " we test the model on the languages unseen during pre-training. To make sure the languages have never been seen before, we artificially make unseen languages by permuting the whole vocabulary of existing languages. That is, all the words in the sentences of a specific language are replaced by other words in the same language to form the sentences in the created unseen language. It is assumed that if multi-BERT used to find answers by language independent strategy, then multi-BERT should also do well on unseen",
            " multiple short sentences, like the phrase “so young” that forms an independent sentence in the example), but also from tweet-specific expressions (such as inferring that it is “Jay Sean” feeling sad about Paul's death because he posted the tweet). Furthermore, we show the distinctive nature of TweetQA by comparing the collected data with traditional QA datasets collected primarily from formal domains. In particular, we demonstrate empirically that three strong neural models which achieve good performance on formal",
            "Abstract\nIn this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network (RNN) based contextual language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3% on perplexity. The",
            " embed each token. For the neural models, the embeddings are used to initialize the token representations, and are refined during training. For the sliding similarity window approach, we set INLINEFORM0 . The vocabulary of the neural models was extracted from training and development data. For optimizing the bilinear model and the attentive reader, we used vanilla stochastic gradient descent with gradient clipping, if the norm of gradients exceeds 10. The size of the hidden layers was tuned to 64, with",
            " resources are available. In both papers, NMT systems are trained with hyperparameters that are typical for high-resource settings, and the authors did not tune hyperparameters, or change network architectures, to optimize NMT for low-resource conditions.\n\n\nImproving Low-Resource Neural Machine Translation\nThe bulk of research on low-resource NMT has focused on exploiting monolingual data, or parallel data involving other language pairs. Methods to improve NMT with monolingual data range",
            " Multi-SimLex initiative and corresponding datasets are also aligned with the recent efforts on procuring multilingual benchmarks that can help advance computational modeling of natural language understanding across different languages. For instance, pretrained multilingual language models such as multilingual bert BIBREF29 or xlm BIBREF30 are typically probed on XNLI test data BIBREF84 for cross-lingual natural language inference. XNLI was created by translating examples from the English MultiNLI dataset",
            ". They do not model the internal structure of the LSTM, but instead approximate the entire architecture as a black box. BIBREF4 showed that in LSTM language models, around 10% of the memory state dimensions can be interpreted with the naked eye by color-coding the text data with the state values; some of them track quotes, brackets and other clearly identifiable aspects of the text. Building on these results, we take a somewhat more systematic approach to looking for interpretable hidden",
            " As one of the first attempts in neural network models, Djuric et al. BIBREF16 proposed a two-step method including a continuous bag of words model to extract paragraph2vec embeddings and a binary classifier trained along with the embeddings to distinguish between hate speech and clean content. Badjatiya et al. BIBREF0 investigated three deep learning architectures, FastText, CNN, and LSTM, in which they initialized the word embeddings with",
            "ricket\" among adjacent mentions England, Hussain, and Essex. Although the global model has achieved significant improvements, its limitation is threefold: To mitigate the first limitation, recent EL studies introduce neural network (NN) models due to its amazing feature abstraction and generalization ability. In such models, words/entities are represented by low dimensional vectors in a continuous space, and features for mention as well as candidate entities are automatically learned from data BIBREF4 . However, existing NN-based"
        ]
    },
    {
        "title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset",
        "answer": "English, Arabic, Chinese, French, German, Spanish.",
        "evidence": [
            " . We put the main focus on identifying argument components in the discourse. To comply with the machine learning terminology, in this section we will use the term domain as an equivalent to a topic (remember that our dataset includes six different topics; see section SECREF38 ). We evaluate three different scenarios. First, we report ten-fold cross validation over a random ordering of the entire data set. Second, we deal with in-domain ten-fold cross validation for each of the six domains. Third,",
            " project was to gather insights in cross-cultural aspects of emotional reactions. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of seven major emotions (joy, fear, anger, sadness, disgust, shame and guilt). In each case, the questions covered the way they had appraised a given situation and how they reacted. The final dataset contains reports by approximately 3000 respondents from all over the world, for a total of 7665 sentences labelled with",
            " 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}. For dialects, the data are labeled with 15 classes, from the set {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia",
            " obtained by CMU-OAQA and PRNA showed that large open-domain datasets were beneficial for the medical domain. However, the best system (CMU-OAQA) relying on the same training data obtained a score of 1.139 on the LiveQA open-domain task. While this gap in performance can be explained in part by the discrepancies between the medical test questions and the open-domain questions, it also highlights the need for larger medical datasets to support deep learning approaches",
            " a different (source) domain. Domain adaptation techniques can be roughly divided into graph-based methods BIBREF1 , BIBREF29 , BIBREF9 , BIBREF30 , probabilistic models BIBREF3 , BIBREF4 , knowledge-based models BIBREF14 , BIBREF31 , BIBREF11 and joint optimization frameworks BIBREF12 . The transfer learning methods from the literature show promising results in a variety of real-world applications, such",
            " All models used in the experiments are trained with batch size 20, using adam with learning rate INLINEFORM0 and the early stop strategy. The dimension of the hidden state is set to 96 for all layers, and the number of self-attention heads is set to 2. The setup is slightly different but better than the setting suggested by the original QAnet.\n\n\nResults\nFirst, we highlight the domain mismatch phenomenon in our experiments shown in Table TABREF9 . Row (a",
            " dialogs are based on one of six tasks: ordering pizza, creating auto repair appointments, setting up rides for hire, ordering movie tickets, ordering coffee drinks and making restaurant reservations.  Two collection methods: The two-person dialogs and self-dialogs each have pros and cons, revealing interesting contrasts.  Multiple turns: The average number of utterances per dialog is about 23 which ensures context-rich language behaviors.  API-based annotation: The dataset uses a simple annotation schema providing sufficient",
            " presented in Table 6 . Rows 1 and 2 repeat the performance of the models trained on difficult subset and random subset with expert annotations only respectively. The third row is the model trained by combining difficult and random subsets with expert annotations. There are around 250 abstracts in the overlap of these two sets, so there are total 1.75k abstracts used for training the D+R model. Rows 4 to 6 are the models trained on all 5k abstracts with mixed annotations, where",
            " have two SQA models (QANets), one trained from target domain data (ASR hypotheses) and another trained from source domain data (reference transcriptions). Because the two domains share common information, some layers in these two models can be tied in order to model the shared features. Hence, we can choose whether each layer in the QA model should be shared. Tying the weights between the source layer and the target layer in order to learn a symmetric mapping is to project both source",
            " are of interest to us in segmenting the videos. Each segment can be seen as a subtask within a larger video dictating an example. We thus chose these videos because of the amount of procedural information stored in each video for which the user may ask. Though there is only one domain, each video corresponds to a different overall goal.\n\n\nTutorialVQA Dataset ::: Data Collection\nWe downloaded 76 videos from a tutorial website about an image editing program . Each video is",
            " supervised domain adaptation of neural networks. On captioning datasets, we have shown that the method outperforms other standard adaptation methods applicable to neural networks. The proposed method only decomposes the output word parameters, where other parameters, such as word embedding, are completely shared across the domains. Augmentation of parameters in the other part of the network would be an interesting direction of future work.\n\n\n",
            " and are fixed during training. Character embeddings are initialized by 200-dimension random vectors. A convolutional layer with 96 kernels of size 5 is used to aggregate the sequence of characters. We use a max pooling layer on the character dimension, then a multi-layer perceptron (MLP) of size 96 is used to aggregate the concatenation of word- and character-level representations. A highway network BIBREF21 is used on top of this MLP. The resulting"
        ]
    },
    {
        "title": "Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge",
        "answer": "Freebase",
        "evidence": [
            " knowledge base is one kind of repository typically used to store answers to questions or solutions to problems enabling rapid search, retrieval, and reuse, either by an ontology or directly by those requesting support. For a more detailed description of ontologies and knowledge bases, see for instance BIBREF17. For designing the ontology adequate for our goals, we referred to the Simple Event Model (SEM) BIBREF18 as a baseline model. A pictorial representation of this ontology is given in",
            " use their GPUs to perform the experiments mentioned in the paper. We also thank the anonymous reviewers for their careful reading of our paper and insightful comments.\n\n\n",
            " can be answered (e.g. \"What is my diagnosis for such symptoms\") and build classifiers to detect such questions and warn the users about the dangers of looking for their answers online. More generally, medical QA systems should follow some strict guidelines regarding the goal and background knowledge and resources of each system in order to protect the consumers from misleading or harmful information. Such guidelines could be based (i) on the source of the information such as health and medical information websites sponsored by the U.",
            " use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions. We split the dataset into training (9,731 questions on 1,470 texts), development (1,411 questions on 219 texts), and test set (2,797 questions on 430 texts).",
            " second drawback of the global approach has been alleviated through approximate optimization techniques, such as PageRank/random walks BIBREF21 , graph pruning BIBREF22 , ranking SVMs BIBREF23 , or loopy belief propagation (LBP) BIBREF18 , BIBREF24 . However, these methods are not differentiable and thus difficult to be integrated into neural network models (the solution for the first limitation). To overcome the third issue of inadequate training data, BIB",
            "Abstract\nTraditional semantic parsers map language onto compositional, executable queries in a fixed schema. This mapping allows them to effectively leverage the information contained in large, formal knowledge bases (KBs, e.g., Freebase) to answer questions, but it is also fundamentally limiting---these semantic parsers can only assign meaning to language that falls within the KB's manually-produced schema. Recently proposed methods for open vocabulary semantic parsing overcome this limitation by learning execution models for arbitrary language, essentially using a",
            "ments are made and the process is repeated, often with another researcher involved. The final annotations can be collected using a crowdsourcing platform, a smaller number of highly-trained annotators, or a group of experts. Which type of annotator to use should be informed by the complexity and specificity of the concept. For more complex concepts, highly-trained or expert annotators tend to produce more reliable results. However, complex concepts can sometimes be broken down into micro-tasks that can be performed independently",
            " as SQuAD BIBREF6 based on Wikipedia, TriviaQA BIBREF7 constructed from trivia questions with answer evidence from Wikipedia, or those from Hermann et al. based on CNN and Daily Mail articles BIBREF8 are factoid-based, meaning the answers typically involve a single entity. Differing from video transcripts, the structures of these data sources, namely paragraphs from Wikipedia and news sources, are typically straightforward since they are meant to be read. In contrast, video",
            " the supporting evidence they identify is not identical i.e full overlap. They disagree on the remaining 26%. Sources of apparent disagreement correspond to situations when different experts: have differing interpretations of question intent (11%) (for example, when a user asks 'who can contact me through the app', the questions admits multiple interpretations, including seeking information about the features of the app, asking about first party collection/use of data or asking about third party collection/use of data), identify different sources of evidence for",
            " document collection because it uses the pattern-based approach. After a user inputs initial seed entities, SPIED generates regular expression patterns to find entity candidates from a given document collection. This approach incurs a huge computational cost for calculating the scores of every regular expression pattern and every entity candidate in each iteration. Furthermore, SPIED adopts a bootstrapping approach, which does not involve user feedback for each iteration. This approach can easily result in semantic drift. Interactive Knowledge Extraction BIBREF9 (",
            " question answering model and question generation model. We further incorporate four open KBs as external knowledge into both question answering and generative approaches, and demonstrate that incorporating additional evidence from open KBs improves total accuracy. We conduct extensive model analysis and error analysis to show the advantages and limitations of our approaches. \n\n\n",
            " For example, the predicate $\\textit {head\\_honcho\\_N/N}$ was never seen in the training data, so it is replaced with $\\textit {unknown}$ ; however, it would be better to replace it with $\\textit {honcho\\_N/N}$ , which was seen in the training data. Finally, although using connected entities in Freebase as additional candidates during inference is helpful, it often over- or under-generates"
        ]
    },
    {
        "title": "SimplerVoice: A Key Message&Visual Description Generator System for Illiteracy",
        "answer": "SimplerVoice",
        "evidence": [
            ". When it comes to poetry generation using generative text models, Zhang and Lapata BIBREF9, Yi et al. BIBREF10 and Wang et al. BIBREF11 use language modeling to generate Chinese poems. However, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding. For the task of text generation, MaskGAN BIBREF12 uses a Reinforcement Learning signal from the discriminator,",
            " where INLINEFORM0 denote an assignment to the subset of variables adjacent to factor INLINEFORM1 , and INLINEFORM2 is the assignment for variable INLINEFORM3 . Message updates are performed asynchronously in our model. Our message passing schedule was similar to that of foward-backward: the forward pass sends all messages from the first time step in the direction of the last. Messages to/from pairwise factors are included in this forward pass. The backward pass sends messages in",
            " numbers or dates, while keeping abbreviations and compound words. We do not apply any further preprocessing, as for instance stop-word removal or stemming, except for the SVM classifier where we additionally perform lowercasing, as this is a common setup for bag-of-words models. We truncate the resulting sequence of tokens to a chosen fixed length of 400 in order to simplify neural network training (in practice our CNN can process any arbitrary sized document). Lastly, we build the neural",
            "Abstract\nWe introduce SimplerVoice: a key message and visual description generator system to help low-literate adults navigate the information-dense world with confidence, on their own. SimplerVoice can automatically generate sensible sentences describing an unknown object, extract semantic meanings of the object usage in the form of a query string, then, represent the string as multiple types of visual guidance (pictures, pictographs, etc.). We demonstrate SimplerVoice system in a case study of generating grocery products'",
            ", because our models have access to a large KB, the formal component of the model can always give a score to any entity pair in the KB. This allows our model to considerably improve question answering performance on rare entities. It would be computationally intractable to consider all Freebase entities as answers to queries, and so we use a simple candidate entity generation technique to consider only a small set of likely entities for a given query. We first find all entities in the query, and consider as candidates",
            " the source is minimal, so we only perform ablation on the target. For the source, we use a 3-layer 512-dim bidi GRU in all models (S1)-(S6). Model (S1) and (S2) are one and two layer baselines. Model (S4), which uses 7 intermediate FC layers, has similar decoding cost to (S2) while doubling the improvement over (S1) to 1.2 BLEU. We",
            "5]. The framework that GANs use for generating new data points employs an end-to-end neural network comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being directly exposed to the real samples. Instead, this model learns to reproduce the general patterns of the input via its interaction with the discriminator. The role of the discriminator, in turn, is to tell apart which data",
            " user instructions to a high-level navigation plan. Our model utilized an attention mechanism to merge relevant information from the navigation instructions with a behavioral graph of the environment. The model then used a decoder to predict a sequence of navigation behaviors that matched the input commands. As part of this effort, we contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments. Our model achieved the best performance in this dataset in comparison to a two-step baseline approach for interpreting",
            " and global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 . Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph",
            " models find consistent patterns. When authors repeatedly use a particular theme or discourse, that repetition creates a consistent pattern. But other factors can also create similar patterns, which look as good to the algorithm. We might notice a topic that has highest probability on French stopwords, indicating that we need to do a better job of filtering by language. We might notice a topic of word fragments, such as “ing”, “tion”, “inter”, indicating that we are",
            " requiring any additional annotated data.  \n\n\nError Generation Methods\nWe investigate two alternative methods for AEG. The models receive grammatically correct text as input and modify certain tokens to produce incorrect sequences. The alternative versions of each sentence are aligned using Levenshtein distance, allowing us to identify specific words that need to be marked as errors. While these alignments are not always perfect, we found them to be sufficient for practical purposes, since alternative alignments of similar sentences often result in",
            " The approach to generate the subject (S) is similar to the verb (V). SimplerVoice also uses heuristics, and n-grams model to find the suitable actor S. In regard to heuristics method, we apply a rule-based method via the object's title, and object's category to generate S since there are objects only used by a specific group of S. For an example, if the object's title contains the word \"woman, women\", the S will"
        ]
    },
    {
        "title": "A Parallel Corpus of Theses and Dissertations Abstracts",
        "answer": "RNN-based NMT model, Transformer based NMT model, SMT.  NMT, SMT, Moses, OpenNMT, GT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based NMT model, Transformer based NMT model, SMT.  RNN-based",
        "evidence": [
            " and reasonable NMT systems that can be reproducible in order to serve as the baseline for further researches in the direction. Furthermore, we conduct experiments using some advanced methods to improve the quality of the systems. An important criteria for those methods is that they must be scalable and language-independent as much as possible. The criteria ensures the basic principle of NMT as well as the reproducibility of the systems. On the other hand, the methods are chosen in the direction that they would help alleviate",
            "NN language modeling framework.\n\n\nNMT with Source Syntax\nAmong the first proposals for using source syntax in NMT was that of luong2015multi, who introduced a multi-task system in which the source data was parsed and translated using a shared encoder and two decoders. More radical changes to the standard NMT paradigm have also been proposed. eriguchi2016tree introduced tree-to-sequence NMT; this model took parse trees as input using a tree-",
            " the preceding speedups are presented in Table 1 , measured on WMT English-French NewsTest2014. The NMT architecture evaluated here uses 3-layer 512-dimensional bidirectional GRU for the source, and a 1-layer 1024-dimensional attentional GRU for the target. Each sentence is decoded independently with a beam of 6. Since these speedups are all mathematical identities excluding quantization noise, all outputs achieve 36.2 BLEU and are 99.9%+",
            "FST models here are purely monolingual – they do not use data adaptation because there is sufficient training data for each of them. Full results are presented in Table TABREF26 . We omit models trained on the Adapted languages because they were not trained on high resource languages with unique writing systems, such as Georgian and Greek, and consequently performed very poorly on them. In contrast to the larger-scale Adapted results, in the High Resource experiments none of the sequence-to-sequence approaches",
            " complete in that they encompass the beginning and end of a task. In total, we identified 408 segments from the 76 videos. Second we asked AMT workers to provide question annotations for the videos. Our AMT experiment consisted of two parts. In the first part, we presented the workers with the video content of a segment. For each segment, we asked workers to generate questions that can be answered by the presented segment. We did not limit the number of questions a worker can input to a corresponding segment",
            " improve the performance of the basic RNN-based NMT model. However, we find that word2vec and reversal tricks seem no obvious improvement when trained the RNN-based NMT and Transformer models on augmented parallel corpus. For SMT, it performs better than NMT models when they were trained on the unaugmented dataset. Nevertheless, when trained on the augmented dataset, both the RNN-based NMT model and Transformer based NMT model outperform the SMT",
            " used to selectively improve parts of the model. Round trip training is another important source of inspiration, as it can be viewed as a way to use BT to perform semi-unsupervised BIBREF32 or unsupervised BIBREF33 training of NMT. The most convincing attempt to date along these lines has been proposed by BIBREF26 , who propose to use GANs to mitigate the difference between artificial and natural data.\n\n\nConclusion \nIn this paper, we have",
            " as two recently proposed coverage models: We also experimented combining the strategies above with the sparsemax transformation. As evaluation metrics, we report tokenized BLEU, METEOR ( BIBREF22 , as well as two new metrics that we describe next to account for over and under-translation.\n\n\nConclusions\nWe proposed a new approach to address the coverage problem in NMT, by replacing the softmax attentional transformation by sparse and constrained alternatives: sparsemax, constrained softmax,",
            "-max layer as well as reducing a substantial number of parameters to be learnt, but also has the ability to generate unseen words. In those languages, a word can be a compound word or comprised by sub-components, each has its own raw meaning or contains morphological information. Segmenting words into sub-components allows NMT to learn to translate them with considerably fewer data. For example, it is definitely less chance to see this popular German word “Wohnungsre",
            " in fact NMT model tend to produce more fluent results, specially regarding verbal regency. Human translation: this paper presents a study of efficiency and power management in a packaging industry and plastic films. OpenNMT: this work presents a study of efficiency and electricity management in a packaging industry and plastic films. Moses: in this work presents a study of efficiency and power management in a packaging industry and plastic films. GT: this paper presents a study of the efficiency and management of electric power in a packaging",
            " domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014.\n\n\nNMT setups and performance \nOur baseline NMT system implements",
            " follow up of previous discussions, we analyze the effect of BT on the internals of the network. Arguably, using a copy of the target sentence instead of a natural source should not be helpful for the encoder, but is it also the case with a strong BT? What are the effects on the attention model?\n\n\nParameter freezing protocol\nTo investigate these questions, we run the same fine-tuning using the copy-marked, backtrans-nmt and backtrans-n"
        ]
    },
    {
        "title": "Data Innovation for International Development: An overview of natural language processing for qualitative data analysis",
        "answer": "NLP, machine learning, lexical information, word vector representations, tagging models, CRF, LSTM-based layer, q-grams, stopwords, stemming, negations, part of speech, affective lexicons.",
        "evidence": [
            " time, or expertise required. The application of natural language processing (NLP) and machine learning (ML) can make feasible the speedy analysis of qualitative data on a large scale. We start with a brief description of the main approaches to NLP and how they can be augmented by human coding. We then move on to the issue of working with multiple languages and different document formats. We then provide an overview of the recent application of these techniques in a United Nations Development Program (UNDP) study.",
            " the state of the art might be achieved by integrating lexical information both from an external lexicon and from word vector representations into tagging models. In that regard, further work will be required to understand which class of models perform the best. An option would be to integrate feature-based models such as a CRF with an LSTM-based layer, following recent proposals such as the one proposed by BIBREF45 for named entity recognition.\n\n\n",
            " sentence. The rationale of q-grams is also to tackle misspelled sentences from the approximate pattern matching perspective BIBREF12 .\n\n\nLanguage Dependent Features\nThe following features are language dependent because they use specific information from the language concerned. Usually, the use of stopwords, stemming and negations are traditionally used in Sentiment Analysis. The users of this approach could add other features such as part of speech, affective lexicons, etc. to improve the performance BIBREF13",
            " language model that consider information at inter-document level. They claimed that by utilizing the structural information from a tree-structured document set, language modeling performance was largely improved.\n\n\nMethods\nThe previously proposed contextual language models focus on applying context by encoding preceding text, without considering interactions in dialogs. These models may not be well suited for dialog language modeling, as they are not designed to capture dialog interactions, such as clarifications and confirmations. By making special design in learning dialog interactions,",
            " what features make it more or less ambiguous? In these cases, it is critical to communicate high-level patterns in terms that are recognizable. This contrasts with much of the work in computational text analysis, which tends to focus on automating tasks that humans perform inefficiently. These tasks range from core linguistically motivated tasks that constitute the backbone of natural language processing, such as part-of-speech tagging and parsing, to filtering spam and detecting sentiment. Many tasks are motivated by applications, for example to",
            " methods; Some are the complicate ones which require linguistic resources or human-crafted rules. Thus, they are not language-independent and expensive to obtain for low-resourced languages. Byte-Pair Encoding, otherwise, is a simple but robust technique to do subword segmentation. Since it is an unsupervised and fast technique, it has great effects when applied to build NMT systems for morphologically rich languages. BPE is originally proposed in BIBREF9 as a data compression",
            "\n\n\nExperiments\nTo show the challenge of TweetQA for existing approaches, we consider four representative methods as baselines. For data processing, we first remove the URLs in the tweets and then tokenize the QA pairs and tweets using NLTK. This process is consistent for all baselines.\n\n\nQuery Matching Baseline\nWe first consider a simple query matching baseline similar to the IR baseline in Kocisk2017TheNR. But instead of only considering several genres of spans",
            " yet informative text, and visual description for illiterate people. This includes supporting adult basic education (ABE), and the English as a second language acquisition (SLA) training by performing natural language processing, and information retrieval techniques, such as: automatically generating sensible texts, word-sense-disambiguation and image-sense-disambiguation mechanism, and retrieving the optimal visual components. We propose the overall framework, and demonstrate the system in a case study of grocery shopping where Simpler",
            "ritz, and Manuela Veloso for help with the various languages and discussion of gendered pronouns.\n\n\nReferences\nE. Davis, “Qualitative Spatial Reasoning in Interpreting Text and Narrative” Spatial Cognition and Computation, 13:4, 2013, 264-294. H. Levesque, E. Davis, and L. Morgenstern, “The Winograd Schema Challenge,” KR 2012. L. Morg",
            ". In the broader natural language processing (NLP) literature, there has been a surge of research devoted to the development of distributional representations of speech which retain syntactical language qualities in ways that the bag-of-words (BOW) approach typically used in political text analysis research is not equipped to retain. The hypothesis claims that words that occur in similar contexts tend to have similar meanings BIBREF12 . When operationalized, the unique intuition is that similar words and phrases, such as",
            " can involve tests with different methods of pre-processing the data to the use of different models, as well as other factors that may influence the results generated. This deepening is necessary since the model's accuracy is not high. To conclude, we believe that the presence of gender bias and stereotypes in the Portuguese language is found in different spheres of language, and it is important to study ways of mitigating different types of discrimination. As such, it can be easily applied to analyze racists bias into the language, such",
            " or less linear this way, but each of these phases overlaps, and in some instances turns back upon itself. The analysis phase, for example, often feeds back into the original research questions, which may continue to evolve for much of the project. At each stage, our discussion is critically informed by insights from the humanities and social sciences, fields that have focused on, and worked to tackle, the challenges of textual analysis—albeit at smaller scales—since their inception. In describing our experiences with computational"
        ]
    },
    {
        "title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages",
        "answer": "crowdworkers, Google Cloud, TensiStrength, Rosette Text Analytics, and CARD-660.",
        "evidence": [
            " and negative labels, TASS uses the accuracy and SENTIPOLC uses a custom metric (see BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 ).\n\n\nExperimental Results\nWe tested our framework on two kinds of datasets. On one hand, we compare our performance on three languages having well known sentiment analysis contests; here, we compare our work against competitors of those challenges. On the other hand, we selected five languages without popular opinion mining",
            "vec technique has the potential to capture sentiment information in the citations, but hand-crafted features have better performance. \n\n\n",
            " policy positions. Word embeddings and term-document matrices are located for each year in the corpus, 1970-2014, and state dyad RWMD distances are calculated, converted to similarity scores, and stored in an INLINEFORM1 matrix INLINEFORM2 for each year. For texts to be considered as a useful complement to roll call data, we should see differences in the positions expressed in speeches versus votes. This would indicate that the two sources reveal different preference information and that using one",
            "In this paper we benchmark MARN's understanding of human communication on three tasks: 1) multimodal sentiment analysis, 2) multimodal speaker traits recognition and 3) multimodal emotion recognition. We perform experimentations on six publicly available datasets and compare the performance of MARN with the performance of state-of-the-art approaches on the same datasets. To ensure generalization of the model, all the datasets are split into train, validation and test sets that include no identical speakers between",
            "Abstract\nSentiments of words differ from one corpus to another. Inducing general sentiment lexicons for languages and using them cannot, in general, produce meaningful results for different domains. In this paper, we combine contextual and supervised information with the general semantic representations of words occurring in the dictionary. Contexts of words help us capture the domain-specific information and supervised scores of words are indicative of the polarities of those words. When we combine supervised features of words with the features extracted from their dictionary definitions",
            " classification whose main contributions are in two aspects. On one hand, our approach can serve as a baseline to compare other classification systems. It considers techniques for text representation such as spelling features, emoticons, word-based n-grams, character-based q-grams and language dependent features. On the other hand, our approach is a framework for practitioners or researchers looking for a bootstrapping sentiment classifier method in order to build more elaborated systems. Besides the text-transformations, the",
            " in California see a 0.06 point decrease in overall sentiment post-event, but a 0.09 point increase in within-cohort sentiment. Methodologically, we assert that overall averages are not robust results to use in sentiment analyses. We now comment on the two events yielding similar results between overall and within-cohort comparisons. Most tweets regarding the Bomb Cyclone have negative sentiment, though sentiment increases by 0.02 and 0.04 points post-event for overall and within-",
            "Abstract\nIn this paper, we introduce UniSent a universal sentiment lexica for 1000 languages created using an English sentiment lexicon and a massively parallel corpus in the Bible domain. To the best of our knowledge, UniSent is the largest sentiment resource to date in terms of number of covered languages, including many low resource languages. To create UniSent, we propose Adapted Sentiment Pivot, a novel method that combines annotation projection, vocabulary expansion, and unsupervised domain adaptation. We evaluate the",
            "Abstract\nWe report results of a comparison of the accuracy of crowdworkers and seven NaturalLanguage Processing (NLP) toolkits in solving two important NLP tasks, named-entity recognition (NER) and entity-level sentiment(ELS) analysis. We here focus on a challenging dataset, 1,000 political tweets that were collected during the U.S. presidential primary election in February 2016. Each tweet refers to at least one of four presidential candidates,i.e., four named entities.",
            " more than 30 percent points. Crowdworkers correctly identified 62% of the neutral, 85% of the positive, and 92% of the negative sentiments. Google Cloud correctly identified 88% of the neutral sentiments, but only 3% of the positive, and 19% of the negative sentiments. TensiStrength correctly identified 87.2% of the neutral sentiments, but 10.5% of the positive, and 8.1% of the negative sentiments. Rosette Text Analytics correctly identified 22.7%",
            "REF93, which roughly correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample.  3) Source: CARD-660 BIBREF78. 67 word pairs are taken from this dataset focused on rare word similarity, applying the same selection criteria a) to e) employed for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus BIBREF94. CARD-660",
            " and clustering Twitter users based on climate mentalities using network analysis BIBREF4 . Throughout, Twitter has been accepted as a powerful tool given the magnitude and reach of samples unattainable from standard surveys. However, the aforementioned studies are not scalable with regards to training data, do not use more recent sentiment analysis tools (such as neural nets), and do not consider unbiased comparisons pre- and post- various climate events (which would allow for a more concrete evaluation of shocks to climate change sentiment)."
        ]
    },
    {
        "title": "TutorialVQA: Question Answering Dataset for Tutorial Videos",
        "answer": "WordNet11 BIBREF29 and FB15K BIBREF11.",
        "evidence": [
            " new entities unseen during training. For the task of triplet classification, we directly use the datasets released by BIBREF9 ijcai2017-250 which are based on WordNet11 BIBREF29 . Since they do not conduct experiments on the link prediction task, we construct the required datasets based on FB15K BIBREF11 following a similar protocol used in BIBREF9 ijcai2017-250 as follows. Sampling unseen entities. Firstly, we randomly sample INLINE",
            " by the UK Engineering and Physical Sciences Research Council (Grant number: EP/P011829/1).\n\n\n",
            " structures. Its main characteristics are: Two annotations: Rhetorical relations and multilevel structures. Documents (687 000 words) taken from four sources: the Est Républicain newspaper (39 articles, 10 000 words); Wikipedia (30 articles + 30 summaries, 242 000 words); Proceedings of the conference Traitement Automatique des Langues Naturelles (TALN) 2008 (25 articles, 169 000 words); Reports from Institut Français de Relations Internationales (32 rap",
            " two typical KG completion tasks.\n\n\nAcknowledgements\nWe thank the three anonymous authors for their constructive comments. This work is supported by the National Natural Science Foundation of China (61472453, U1401256, U1501252, U1611264, U1711261, U1711262).\n\n\n",
            " text as an answer, given a question and video clip, our task aims to identify a span of a video segment as an answer which contains instructional details with various granularities. This work focuses on screencast tutorial videos pertaining to an image editing program. We introduce a dataset, TutorialVQA, consisting of about 6,000manually collected triples of (video, question, answer span). We also provide experimental results with several baselines algorithms using the video transcripts. The results indicate that",
            " the source is minimal, so we only perform ablation on the target. For the source, we use a 3-layer 512-dim bidi GRU in all models (S1)-(S6). Model (S1) and (S2) are one and two layer baselines. Model (S4), which uses 7 intermediate FC layers, has similar decoding cost to (S2) while doubling the improvement over (S1) to 1.2 BLEU. We",
            "W}_{\\textbf {t}}\\textbf {t}_i),$$   (Eq. 23)   where $(h_i, r_i, t_i) = R_i \\in \\mathbf {G}(x)$ is the $i$ -th triple in the graph. We use word vectors to represent concepts, i.e. $\\textbf {h}_i = \\mathbf {e}(h",
            " and labor-intensive. Each presents challenges. And especially when working across disciplines, the research often involves a fair amount of discussion—even negotiation—about what means of operationalization and approaches to analysis are appropriate and feasible. And yet, with a bit of perseverance and mutual understanding, conceptually sound and meaningful work results so that we can truly make use of the exciting opportunities rich textual data offers.\n\n\nAcknowledgements\nThis work was supported by The Alan Turing Institute under the EPSRC grant EP",
            " as SQuAD BIBREF6 based on Wikipedia, TriviaQA BIBREF7 constructed from trivia questions with answer evidence from Wikipedia, or those from Hermann et al. based on CNN and Daily Mail articles BIBREF8 are factoid-based, meaning the answers typically involve a single entity. Differing from video transcripts, the structures of these data sources, namely paragraphs from Wikipedia and news sources, are typically straightforward since they are meant to be read. In contrast, video",
            " hand, if the quality of the Back Translation system is not adequate, using the synthesis data might bring adverse effects to our NMT. In this paper, we subsample an amount of Vietnamese monolingual data so that we can create a synthetic corpus having the same size with the Japanese-Vietnamese parallel corpus. In the end, the data we have is double in size compared to the original one.\n\n\nMix-Source Approach\nAnother data augmentation method considered useful in this low-",
            "-style knowledge facilitates the interpretability of the decision making process. Triplets organize the document together with KBs as a graph, where a well-designed model such as PCNet, which we will describe in a later section, expressly reveal rationales for their predictions. Third, representing the documents as knowledge allows for ease of accessing and leveraging the knowledge from external/background knowledge because the knowledge representation of a document is easily consistent with both manually curated and automatically extracted KBs. In this paper, we",
            " time for all models except the baseline. We input the triplets from the graph to our proposed model in alphabetical order, and consider a modification where the triplets that surround the start location of the robot are provided first in the input graph sequence. We hypothesized that such rearrangement would help identify the starting location (node) of the robot in the graph. In turn, this could facilitate the prediction of correct output sequences. In the remaining of the paper, we refer to models that were provided"
        ]
    },
    {
        "title": "Transductive Learning with String Kernels for Cross-Domain Text Classification",
        "answer": "AdaBoost, MLP, SVM, K-NN, LSTM, feed-forward neural network.  Hidden Markov, bigram, trigram, Transformers, Irony Detection, pre-training process with reinforcement learning.  Corpus-based approach, unsupervised approaches.  MLP, SVM, K-NN, LSTM, feed-forward neural network.  Hidden Markov, bigram, trigram, Transformers, Irony Detection, pre-training process with reinforcement learning.  Corpus-based approach, unsupervised approaches.  MLP, SVM, K-NN, LSTM, feed-forward neural network.  Hidden Markov, bigram, trigram, Transformers, Irony Detection, pre-training process with reinforcement learning.  Corpus-based approach, unsupervised approaches.  MLP, SVM, K-NN, LSTM, feed-forward neural network.  Hidden Markov, bigram, trigram, Transformers, Irony Detection, pre-training process with reinforcement learning.  Corpus-based approach, unsupervised approaches.  MLP, SVM, K-NN, LSTM, feed-forward neural network.  Hidden Markov, bigram, trigram, Transformers, Irony Detection, pre-training process with reinforcement learning.  Corpus-based approach, unsupervised approaches.  MLP, SVM",
        "evidence": [
            " was then added to our original training set. Again, the machine translation platform Apertium BIBREF5 was used for the translation of the datasets.\n\n\nAlgorithms Used\nThree types of models were used in our system, a feed-forward neural network, an LSTM network and an SVM regressor. The neural nets were inspired by the work of Prayas BIBREF7 in the previous shared task. Different regression algorithms (e.g. AdaBoost,",
            " features such as single words, sequences of words, or information about their syntactic structure, to meta-information such as user or network information. Deciding on the features requires experimentation and expert insight and is often called feature engineering. For insight-driven analysis, we are often interested in why a prediction has been made and features that can be interpreted by humans may be preferred. Recent neural network approaches often use simple features as input (such as word embeddings or character sequences), which requires less feature",
            " whose granularity can range from a simple morphosyntactic category, or part-of-speech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, etc.). The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov",
            " this as a method of inferences of functions or hypotheses capable of solving a problem algorithmically from data representing instances of the problem. This is an important way to solve different types of problems that permeate computer science and other areas. One of the main uses of ML is in text processing, where the analysis of the content the entry point for various learning algorithms. However, the use of this content can represent the insertion of different types of bias in training and may vary with the context worked. This work",
            " and uses back-propagation for updating parameter weights using loss function calculated on training set into classification. K-NN has shown the weakest performance overall, as this algorithm has a bad reputation of not working well in high dimensional data BIBREF28. MLP and SVM have shown similar performance. MLP takes advantage of multiple hidden layers in order to take non-linearly separable samples in a linearly separable condition. SVM accomplishes this same feat by taking the samples to",
            ", based on the Transformers library, are available online. Using in production To facilitate the transition from research to production, all the models in the library are compatible with TorchScript, an intermediate representation of a PyTorch model that can then be run either in Python in a more efficient way, or in a high-performance environment such as C++. Fine-tuned models can thus be exported to production-friendly environment. Optimizing large machine learning models for production is an ongoing effort in the community",
            " better vector representation of words and thus will facilitate deep learning based automatic feature extraction.\n\n\n",
            " performance of classifiers to a large extent. Meanwhile, such models are usually sensitive to parameters and difficult to train. In contrast, we combine a pre-training process with reinforcement learning to build up a stable language model and design special rewards for our task. Irony Detection: With the development of social media, irony detection becomes a more important task. Methods for irony detection can be mainly divided into two categories: methods based on feature engineering and methods based on neural networks. As for methods based on feature",
            " time, or expertise required. The application of natural language processing (NLP) and machine learning (ML) can make feasible the speedy analysis of qualitative data on a large scale. We start with a brief description of the main approaches to NLP and how they can be augmented by human coding. We then move on to the issue of working with multiple languages and different document formats. We then provide an overview of the recent application of these techniques in a United Nations Development Program (UNDP) study.",
            " hidden dimensions, dropout rate, etc.). This configuration object can be saved and loaded for reproducibility or simply modified for architecture search. The configuration defines the architecture of the model but also architecture optimizations like the heads to prune. Configurations are agnostic to the deep learning framework used. Tokenizers - A Tokenizer class (inheriting from a base class `PreTrainedTokenizer`) is available for each model. This class stores the vocabulary token-to-index map for",
            "\nResearches on question classification, question taxonomies and QA system have been undertaken in recent years. There are two types of approaches for question classification according to Banerjee et al in BIBREF13 - by rules and by machine learning approach. Rule based approaches use some hard coded grammar rules to map the question to an appropriate answer type BIBREF14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in",
            " applied to other classification tasks, such as topic classification and concept mining. The experiments show that even unsupervised approaches, as in the corpus-based approach, can outperform supervised approaches in classification tasks. Combining some approaches, which can compensate for what others lack, can help us build better vectors. Our word vectors are created by conventional machine learning algorithms; however, they, as in the corpus-based model, produce state-of-the-art results. Although we preferred to use a"
        ]
    },
    {
        "title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches",
        "answer": "SNLI and multiNLI, Quora, Clinical-QE, and SemEval-cQA.",
        "evidence": [
            "IBREF18 , researchers propose a system based on a densely connected LSTM network.\n\n\nOur Dataset\nIn this section, we describe how we build our dataset with tweets. First, we crawl over 2M tweets from twitter using GetOldTweets-python. We crawl English tweets from 04/09/2012 to /12/18/2018. We first remove all re-tweets and use langdetect to remove all non-English sentences. Then, we",
            " dataset is used for the training. Typically, the source dataset is bigger than that of the target, and this method sometimes works better than TgtOnly. Another method is All, in which the source and target datasets are combined and used for the training. Although this method uses all the data, the training criteria enforce the model to perform well on both of the domains, and therefore the performance on the target domain is not necessarily high.  An approach widely used in the neural network community is FineT",
            " in future work.\n\n\nEmotion datasets\nThree datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them",
            " are removed, and the data are partitioned into 10 parts containing equal numbers of sentences to support 10-fold cross-validation. Each training set is split into one part used for validation and the rest is used for model training. Subsequently, all the words in each dataset are concatenated and then separated into sequences with 200 words per instance. Each sequence always begins with the first word of a sentence. If a sequence ends with an unfinished sentence, the next sequence starts with that complete sentence",
            "No common types).\n\n\nDatasets Used for the RQE Study\nWe evaluate the RQE methods (i.e. deep learning model and logistic regression classifier) using two datasets of sentence pairs (SNLI and multiNLI), and three datasets of question pairs (Quora, Clinical-QE, and SemEval-cQA). The Stanford Natural Language Inference corpus (SNLI) BIBREF13 contains 569,037 sentence pairs",
            " datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size influences.",
            " use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions. We split the dataset into training (9,731 questions on 1,470 texts), development (1,411 questions on 219 texts), and test set (2,797 questions on 430 texts).",
            " troubled the researchers from the onset. Could the methodology be adopted to target the speech of groups like Black Lives Matter? Could it be adopted by repressive governments to minimize online dissent? While these concerns remained, they concluded that hypothetical dual use scenarios did not outweigh the tangible contribution this research could offer towards making the online environment more equal and just.\n\n\nData\nThe next step involves deciding on the data sources, collecting and compiling the dataset, and inspecting its metadata.\n\n\nData acquisition\nMany scholars in",
            " use their GPUs to perform the experiments mentioned in the paper. We also thank the anonymous reviewers for their careful reading of our paper and insightful comments.\n\n\n",
            "; in contrast, the WinoGrande dataset has devoted considerable effort to reducing such biases, which may allow models to (inadvertently) “cheat” (for example, using simple statistical associations). While it is certainly true that datasets over-estimate the commonsense reasoning capabilities of modern models BIBREF1, there are alternative and complementary explanations as well: It has been a fundamental assumption of the research community that commonsense reasoning is difficult because it comprises tacit rather than explicit knowledge",
            " can observe that INLINEFORM0 has a bigger similarity to S1 in 5 occasions compared to other two references, while INLINEFORM1 is more similar to S2 in 7 transcripts (underline). After computing the mean F1 scores over the transcripts, it can be concluded that in average S2 had a better performance segmenting the dataset compared to S1, obtaining a F1 score equal to 0.510. But... What about the complexity of the dataset? Regardless all references have been",
            " platform-based metadata BIBREF8, BIBREF9, BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be changing direction, with deep learning models being used for both feature extraction and the training of classifiers. These newer models are applying deep learning approaches such as Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs), etc.BIBREF6, BIBREF0 to enhance the"
        ]
    },
    {
        "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
        "answer": "22",
        "evidence": [
            " the convolutional network model called Inception (GoogLeNet) BIBREF23 , BIBREF24 trained on the ImageNet dataset BIBREF25 to classify multiple objects in the scene. Our implementation of the Inception model has deep 22 layers, training of which is made possible via “network in network\" principles and several dimension reduction techniques to improve computing resource utilization. The final layer representation encodes discriminative information describing what objects are shown in an image, which provide",
            " same hyper-parameters. For the encoder module, both the low-level and high-level encoders use a two-layers multi-head self-attention with two heads. To fit with the small number of record keys in our dataset (39), their embedding size is fixed to 20. The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300. We use dropout at rate 0.5.",
            " are smaller (400 vs. 650) the total model sizes are comparable (40M for ours vs. 39M for theirs). Nonetheless, we will see in the first experiment that our carefully tuned three-layer model achieves much higher syntactic performance than their model (Section exp), being a stronger baseline to our extensions, which we introduce next.\n\n\nLearning with Negative Examples\nmethod Now we describe four additional losses for exploiting negative examples. The first two are existing ones, proposed for a similar purpose",
            "rained models or created more generally by the user from user-specifications. Model - All models follow the same hierarchy of abstraction: a base class implements the model's computation graph from encoding (projection on the embedding matrix) through the series of self-attention layers and up to the last layer hidden states. The base class is specific to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of the",
            " the source is minimal, so we only perform ablation on the target. For the source, we use a 3-layer 512-dim bidi GRU in all models (S1)-(S6). Model (S1) and (S2) are one and two layer baselines. Model (S4), which uses 7 intermediate FC layers, has similar decoding cost to (S2) while doubling the improvement over (S1) to 1.2 BLEU. We",
            " is of reasonable quality. Instead of training a single model initially, ten different models were trained which predict the labels of the silver instances. If the highest and lowest prediction do not differ more than a certain threshold the silver instance is maintained, otherwise it is discarded. This results in two parameters that could be optimized: the threshold and the number of silver instances that would be added. This method can be applied to both the LSTM and feed-forward networks that were used. An overview of the characteristics",
            " conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script",
            " parameter count that is quadratic in the number of layers. When applying this technique to the QRNN, we include connections between the input embeddings and every QRNN layer and between every pair of QRNN layers. This is equivalent to concatenating each QRNN layer's input to its output along the channel dimension before feeding the state into the next layer. The output of the last layer alone is then used as the overall encoding result. Encoder–Decoder Models To demonstrate the gener",
            " and some hidden layers. Input layer receives the signal, the output layer gives a decision or prediction about the input and the computation of the MLP is conducted in the hidden layers. In our system, we use 100 layers. For weight optimization, we use Limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS) optimization algorithm.\n\n\nImplementation of the System ::: Classification Algorithms ::: Support Vector Machine (SVM)\nSVM",
            " and a dense layer with sigmoid activation. We apply an Adam optimizer on the binary crossentropy loss. Implementing this simple, one-layer LSTM allows us to surpass the other traditional machine learning classification methods. Note the 13-point spread between validation and test accuracies achieved. Ideally, the training, validation, and test datasets have the same underlying distribution of tweet sentiments; the assumption made with our labeling technique is that the influential accounts chosen are representative of all Twitter accounts.",
            " layer, where the input’s text is converted to a GloVe vector. These GloVe vectors are learned via a global word-word co-occurrence matrix using our corpus of Airbnb listing descriptions [8]. At each time step, the GloVe vectors are then fed into an LSTM layer. For each layer, the model forward propagates the output of the LSTM layer to the next time-step’s LSTM layer via a rectified",
            "LSTM-CRF, as shown in Fig. . The model is divided into three modules. The first, low-level module, consists of two separate structures: local and distant structures. The second, high-level module, contains a sequence of stacked bidirectional LSTM and self-attention layers. The final module, the prediction module, is responsible for predicting the tags INLINEFORM0 . Each module is described more completely in the next three subsections. A sequence of"
        ]
    },
    {
        "title": "\"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach",
        "answer": "LRP decomposition method.",
        "evidence": [
            " (without requiring the true labels). Since worse results are obtained when using the SA scores or the uniform or TFIDF weighting, this indicates that the explanations produced by LRP are semantically more meaningful than the latter. Finally, we confirm quantitatively the observations made before, namely that (1) the LRP decomposition method provides better explanations than SA and that (2) the CNN model outperforms the BoW/SVM classifier in terms of explanatory power.\n\n\nExperimental",
            " query, people will naturally pay more attention to the query relevant sentences and summarize the main ideas from them. Similar to human attentive reading behavior, AttSum, the system to be illustrated in this section, ranks the sentences with its focus on the query. The overall framework is shown in Fig. 1 . From the bottom to up, AttSum is composed of three major layers. The rest of this section describes the details of the three layers.\n\n\nCNN Layer\nConvolutional Neural Networks (",
            ".g., convolutionally), but some aspects require long-distance context and must be computed recurrently. Many existing neural network architectures either fail to take advantage of the contextual information or fail to take advantage of the parallelism. QRNNs exploit both parallelism and context, exhibiting advantages from both convolutional and recurrent neural networks. QRNNs have better predictive accuracy than LSTM-based models of equal hidden size, even though they use fewer parameters and run substantially faster. Our experiments",
            " our experiments the MARN with only one attention (like conventional attention models) under-performs compared to the models with multiple attentions. One could argue that the models with more attentions have more parameters, and as a result their better performance may not be due to better modeling of cross-view dynamics, but rather due to more parameters. However we performed extensive grid search on the number of parameters in MARN with one attention. Increasing the number of parameters further (by increasing dense layers, L",
            " close to the baseline level, with slight improvements in German. A first observation is that this method brings stupid BT models closer to conventional BT, at a greatly reduced computational cost. While French still remains 0.4 to 1.0 BLEU below very good backtranslation, both approaches are in the same ballpark for German - may be because BTs are better for the former system than for the latter. Finally note that the GAN architecture has two differences with basic copy-marked: (a)",
            " use their GPUs to perform the experiments mentioned in the paper. We also thank the anonymous reviewers for their careful reading of our paper and insightful comments.\n\n\n",
            "Comparison of CNN and n-gram models for local representation\nJacovi A. et al. BIBREF13 proposed that a CNN can be used as an n-gram detector to capture local text features. Therefore, we also performed an experiment to compare a CNN and n-gram embedded as local structures. The results in Table TABREF56 show that the model using the embedded n-gram yields greater improvement than the one using an embedded CNN on the Orchid and UGWC datasets",
            " much fewer parameters.(The number of parameters in our models is approximately 10K while in CNN the number of parameters is about 400K). Compared with RecNN, DC-RecNN performs better, indicating the effectiveness of the dynamic compositional function. Additionally, both DC-RecNN and DC-TreeLSTM achieve substantial improvement on IE dataset, which covers the richness of compositionality (idiomaticity). We attribute the success on IE to its power in modeling more complicated compositionality.\n\n\n",
            " that model size alone explains the performance gain. However, when coupled with the observations of Nogueira et al. BIBREF7, T5's “generative capability”, i.e., its ability to generate fluent text, honed through pretraining, seems to play an important role. The fact that the choice of target tokens affects prediction accuracy is consistent with this observation. How and why is the subject of ongoing work.\n\n\nImplications\nCollectively, the success",
            " in the figure). In this work, we use multichannel initialization for case (i) and random initialization for cases (ii) and (iii). Only fine-tuned multichannel representations (case (i)) are kept for subsequent supervised training. The rationale for this pretraining is similar to auto-encoder: for an object composed of smaller-granular elements, the representations of the whole object and its components can learn each other. The CNN architecture learns sentence features layer by layer",
            "ated difficult subset (D+Other). In sum, the results clearly indicate that mixing expert and crowd annotations leads to better models than using solely crowd data, and better than using expert data alone. More importantly, there is greater gain in performance when instances are routed according to difficulty, as compared to randomly selecting the data for expert annotators. These findings align with our motivating hypothesis that annotation quality for difficult instances is important for final model performance. They also indicate that mixing annotations from expert and crowd could be",
            " reluctant to let an opaque expert system make crucial decisions. If we can make the inner workings of RNNs more interpretable, more applications can benefit from their power. There are several aspects of what makes a model or algorithm understandable to humans. One aspect is model complexity or parsimony. Another aspect is the ability to trace back from a prediction or model component to particularly influential features in the data BIBREF0 BIBREF1 . This could be useful for understanding mistakes made by neural networks,"
        ]
    },
    {
        "title": "Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks",
        "answer": "CCG supertagging, part-of-speech tagging, named entity recognition, base-phrase chunking, grammar error detection, semantic tagging, preposition supersense identification, and event factuality detection.",
        "evidence": [
            " and then asked to conjecture questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment to many users, we maintain a consistent set of video segments",
            " thank Dinesh Jayaraman, Yu-Chuan Su, Suyog Jain, and Chao-Yeh Chen for their assistance with experiments.\n\n\n",
            " these tasks can be facilitated. Other machine translation approaches can also be tested, including the concatenation of this corpus with other multi-domain ones.\n\n\n",
            " are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for",
            "bing Tasks\nThe ten different probing tasks we used include CCG supertagging BIBREF26, part-of-speech tagging from PTB BIBREF13 and EWT (Universal Depedencies BIBREF27), named entity recognition BIBREF20, base-phrase chunking BIBREF12, grammar error detection BIBREF28, semantic tagging BIBREF29, preposition supersense identification BIBREF30, and event factuality detection BIBREF31",
            " schedules used for transfer learning and transformers models with customizable options including warmup schedules which are relevant when training with Adam.\n\n\nExperimenting with Transformers\nIn this section, we present some of the major tools and examples provided in the library to experiment on a range of downstream Natural Language Understanding and Natural Language Generation tasks.\n\n\nExperimenting with Transformers ::: Language understanding benchmarks\nThe language models provided in Transformers are pretrained with a general purpose training objective, usually a variant of language modeling",
            "servable, and thus an agent is required to explore and memorize the environment via interaction; the action space in our setting (especially for the Ctrl+F command as defined in later section) is arguably larger than the tree sampling action space in DocQN. Closely related to iMRC is work by BIBREF15, in which the authors introduce a collection of synthetic tasks to train and test information-seeking capabilities in neural models. We extend that work by developing a realistic and challenging",
            " controls, the WinoGrande challenge still contains incidental biases that these more sophisticated models can exploit, or that we are genuinely making at least some progress in commonsense reasoning. The latter, in particular, challenges the notion that commonsense knowledge is (mostly) tacit. Perhaps it is the case that in a humongous corpus of natural language text, someone really has written about trying to stuff a tuba in a backpack?\n\n\nAcknowledgments\nThis research was supported in part by the Canada",
            ", we focus on three experiments, which are based on the corpora introduced in Section SECREF21 : The Effect of Stylistic Variation Across Large Time Spans The Effect of Topical Influence The Effect of Limited Text Length In the following each experiment is described in detail. In this experiment, we seek to answer the question if the writing style of an author INLINEFORM0 can be recognized, given a large time span between two documents of INLINEFORM1 . The motivation behind this experiment",
            " set of collected candidates. For approximately 11.5% of the questions, at least one answer was replaced. 135 questions (approx. 1%) were excluded from the dataset because no appropriate correct or incorrect answer could be found.\n\n\nData Statistics\nFor all experiments, we admitted only experienced MTurk workers who are based in the US. One HIT consisted of writing one text for the text collection, formulating 15 questions for the question collection, or finding 15 pairs of answers for the answer",
            " Assistants (FA8750-15-2-0277). The US Government is authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright notation. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the NSF, DARPA, or the US Government. The authors would like to extend their gratitude to Elias Wright, Gian Mascioli, Kiara Pillay",
            " the effect appears to depend on the task. We observed that adversarial training moves the embeddings of English text and their non-English translations closer together, which may explain why it improves cross-lingual performance. Future directions include adding the language-adversarial task during BERT pre-training on the multilingual Wikipedia corpus, which may further improve zero-resource performance, and finding better stopping criteria for zero-resource cross-lingual tasks besides using the English dev set.\n\n\n"
        ]
    },
    {
        "title": "Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation",
        "answer": "4.4x",
        "evidence": [
            " baseline models as compared to the human upper bound and a random baseline. As can be seen, neural models have a clear advantage over the pure word overlap baseline, which performs worst, with an accuracy of INLINEFORM0 . The low accuracy is mostly due to the nature of correct answers in our data: Each correct answer has a low overlap with the text by design. Since the overlap model selects the answer with a high overlap to the text, it does not perform well. In particular, this also",
            " (score=2.0) from IE+MSA is much larger than that from IE (45.0%+5.0%/41.0%+4.0% vs. 36.0%+2.0% for GA/CA, respectively), and also remarkable larger than those from other baselines. Further, HLSTM equipped with MSA is better than those without MSA, indicating that commonsense knowledge is helpful. And the kappa measuring inter-",
            " by significant margins. These results demonstrate the effectiveness of logic rules in assigning meaningful weights to the neighbors. Specifically, in order to generate representations for unseen entities, it is crucial to incorporate the logic rules to train the aggregator, instead of depending solely on neural networks to learn from the data. By combining the logic rules and neural networks, LAN takes a step further in outperforming all the other models. To find out whether the superiority of LAN to the baselines can generalize to other scoring functions,",
            " merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation on all the hidden states, allowing a better regularisation of the model learning process. Empirical results show that our model can effectively mitigate the latent variable collapse issue while giving a better predictive performance than the baselines.\n\n\nAcknowledgment\nThis work is supported by the award made by the UK Engineering and Physical Sciences Research Council (Grant number: EP/",
            " NCEL with the global models that report the performance on GERBIL. As shown in Table TABREF26 , NCEL achieves the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The baseline methods also achieve competitive results on some datasets but fail to adapt to the others. For example, AIDA and xLisa perform quite well on ACE2004 but poorly on other datasets, or WAT, PBoH, and WNED",
            " the LSTM and feed-forward model's predictions were averaged over 10 prediction runs. Subsequently, the predictions of all individual models were combined into an average. Finally, models were removed from the ensemble in a stepwise manner if the removal increased the average score. This was done based on their original scores, i.e. starting out by trying to remove the worst individual model and working our way up to the best model. We only consider it an increase in score if the difference is larger",
            " This results in an effective and more light-weight model. Experimental evaluation on the RotoWire benchmark shows that our model outperforms competitive baselines in terms of BLEU score and is generally better on qualitative metrics. This way of representing structured databases may lead to automatic inference and enrichment, e.g., by comparing entities. This direction could be driven by very recent operation-guided networks BIBREF39, BIBREF40. In addition, we note that our approach can still lead to",
            " that our results are robust), we consider the out-of-sample predictive accuracy for the models in which the multiplex clusters yielded a statistically significant relationship with conflict onset. This was the case for 7 out of the 11 thresholds tested. The sums of the areas under the precision recall curve for these models are plotted in Figure FIGREF41 . These box plots make clear that our multiplex models do indeed display increased out-of-sample predictive capability relative to the baseline model across various thresholds. The above",
            "WC, respectively, and it outperforms all the baseline models. The second experiment was executed on the IWSLT dataset BIBREF22 and involves an English-language punctuation restoration task. This experiment demonstrates that our model is generalizable to different languages. Our model, which does not require pretrained word vectors, improved the overall F1 score by 0.9% compared to the baselines, including a model that uses pretrained word vectors. There are five sections in the remainder of",
            " outputs achieve 36.2 BLEU and are 99.9%+ identical. The largest improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount. Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder. Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target",
            " in the loss function (Equation. DISPLAY_FORM12) is set to 0.3 and 0.1, respectively.\n\n\nExperiments ::: Automatic Evaluation Result\nAs shown in Table TABREF13 (the performance of other models is collected from their papers), our model exceeds the PGN baseline by 3.85, 2.1 and 3.37 in terms of R-1, R-2 and R-L respectively and receives over 3.23 point boost on",
            "PA performs better than PPA and APA on low intervention courses (i.ratio INLINEFORM3 0.25) mainly because PPA and APA's performance drops steeply when i.ratio drops (see col.2 parenthesis and INLINEFORM4 of PPA and APA). While all the proposed models beat the baseline on every course except casebased-2. On medicalneuro-2 and compilers-4 which have the lowest i.ratio among"
        ]
    },
    {
        "title": "Sentiment Analysis of Citations Using Word2vec",
        "answer": "Accuracy, precision, recall, F-score, intra-coder reliability, macro-averaged F1 evaluation, Hit@k metrics, MR metric, MRR metric.",
        "evidence": [
            " setup ::: Evaluation metrics ::: Information extraction-oriented metrics.\nThese metrics estimate the ability of our model to integrate elements from the table in its descriptions. Particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ. To do so, we follow the protocol presented in BIBREF10. First, we apply an information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset. Entity-value pairs",
            " and intra-coder reliability should be particularly high. Accuracy, as well as other measures such as precision, recall and F-score, are sometimes presented as a measure of validity, but if we do not have a genuinely objective determination of what something is supposed measure—as is often the case in text analysis—then accuracy is perhaps a better indication of reliability than of validity. In that case, validity needs to be assessed based on other techniques like those we discuss later in this section. It is",
            "points needs to be taken into account in systems' design. Characteristics that are not considered as relevant in a given task can be encapsulated in data nonetheless, and lead to bias performance. Being aware of the demographic skews our data set might contain is a first step to track the life cycle of a training data set and a necessary step to control the tools we develop.\n\n\n",
            " we can train competitive NMT systems without relying on auxiliary resources. This has practical relevance for languages where large amounts of monolingual data, or multilingual data involving related languages, are not available. Even though we focused on only using parallel data, our results are also relevant for work on using auxiliary data to improve low-resource MT. Supervised systems serve as an important baseline to judge the effectiveness of semisupervised or unsupervised approaches, and the quality of supervised systems trained on little",
            " we observe? These questions are also influenced by the availability and accessibility of data sources. For example, the choice to work with data from a particular social media platform may be partly determined by the fact that it is freely available, and this will in turn shape the kinds of questions that can be asked. A key output of this phase are the concepts to measure, for example: influence; copying and reproduction; the creation of patterns of language use; hate speech. Computational analysis of text motivated by these",
            " insightful comments. This work was supported by the European Research Council (ERC StG DeepSPIN 758969) and by the Fundação para a Ciência e Tecnologia through contracts UID/EEA/50008/2013, PTDC/EEI-SII/7092/2014 (LearnBig), and CMUPERI/TIC/0046/2014 (GoLocal).\n\n\nProof of Proposition \nWe provide here a detailed proof",
            " size applied is 2000-word as per the baseline-dataset experiment results, and for IMDB62, texts are not chunked (i.e., we feed the models with the original reviews directly). For model comparison, we also run the SVMs (i.e., SVM2 and SVM2-PV) used in the baseline-dataset experiment. All the experiments conducted here are multi-class classification with macro-averaged F1 evaluation. Model configurations",
            "\nMethodology ::: Supervised Contextual 4-scores\nOur last component is a simple metric that uses four supervised scores for each word in the corpus. We extract these scores as follows. For a target word in the corpus, we scan through all of its contexts. In addition to the target word's polarity score (the self score), out of all the polarity scores of words occurring in the same contexts as the target word, minimum, maximum, and average scores are taken into",
            "�I feel so depressed!”, “What a bad day!”, etc. There may be infinite number of responses for an open domain messages. Hence, it is difficult to construct a gold standard (usually a reference set) to evaluate a response which is generated by an open domain chit-chat system. For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc.,",
            " rigorous criteria for performance assessment. As reported in the main paper, clusters based on partitions of our speech graph and our speech-vote multiplex graph are both statistically significant predictors of conflict onset. Here, we assess the goodness-of-fit of our models, the out-of-sample predictive accuracy of the models, and robustness checks of our results at different tie similarity thresholds. To assess the in-sample goodness-of-fit (GOF) of exponential random graph models, it",
            ", provide qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.\n\n\n",
            " in the poorest performance on all datasets. Secondly, our LAN model outperforms all the other baselines significantly, especially on the Hit@k metrics. The improvement on the MR metric of LAN might not be considerable. This is due to the flaw of the MR metric since it is more sensitive to lower positions of the ranking, which is actually of less importance. The MRR metric is proposed for this reason, where we could observe consistent improvements brought by LAN. The effectiveness of LAN on link prediction valid"
        ]
    },
    {
        "title": "Neural Factor Graph Models for Cross-lingual Morphological Tagging",
        "answer": "English, French, Spanish, Chinese Mandarin, Welsh, Kiswahili, Hebrew, Croatian, German, Italian, Portuguese, Dutch, Russian, Japanese, Arabic, Hindi, Greek, Turkish, Vietnamese, Swahili, Polish, Swedish, Danish, Norwegian, Finnish, Romanian, Bulgarian, Ukrainian, Estonian, Latvian, Lithuanian, Hungarian, Czech, Slovak, Icelandic, Albanian, Azerbaijani, Belarusian, Bosnian, Georgian, Kazakh, Kyrgyz, Macedonian, Mongolian, Pashto, Serbian, Tajik, Uzbek, Amharic, Azerbaijani, Belarusian, Bosnian, Bulgarian, Croatian, Czech, Estonian, Finnish, Georgian, German, Greek, Hebrew, Hungarian, Icelandic, Italian, Japanese, Kazakh, Korean, Kurdish, Kyrgyz, Latvian, Lithuanian, Macedonian, Mongolian, Pashto, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Swahili, Swedish, Tajik, Turkish, Ukrainian, Uzbek, Vietnamese, Welsh, Yiddish, and many others. 11 target languages. 100 languages. 30 languages. 10 languages. 8 language families.",
        "evidence": [
            " eight different language families. This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili. We hope to further include additional languages and inspire other researchers to contribute to the effort over the lifetime of this project. The work on data collection can be divided into two crucial phases: 1) a translation phase where the extended English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages",
            "iers, Spanians). NMT systems learn some syntactic disambiguation even with very little data, for example the translation of das and die as relative pronouns ('that', 'which', 'who'), while PBSMT produces less grammatical translation. On the flip side, the ultra low-resource NMT system ignores some unknown words in favour of a more-or-less fluent, but semantically inadequate translation: erobert ('conquered') is translated into doing, and",
            " at least one noun is present in both segments, they remain independent. If there is no verb-nominal form, the segments remain independent.\n\n\nExperiments\nIn this first exploratory work, only documents in French were considered, but the system can be adapted to other languages. The evaluation is based on the correspondence of word pairs representing a border. In this way we compare the Annodis segmentation with the automatically produced segmentation. For each pair of reference segments, a $L",
            " that questions are linguistically simple, elaborate and explicit. Workers were asked to formulate questions about details of such a situation, i.e. independent of a concrete narrative. This resulted in questions, the answer to which is not literally mentioned in the text. To cover a broad range of question types, we asked participants to write 3 temporal questions (asking about time points and event order), 3 content questions (asking about persons or details in the scenario) and 3 reasoning questions (asking how or why something",
            "ERT) for adaptation using the pretraining objectives. Some models fine-tuned on downstream tasks such as SQuAD1.1 are also available. Overall, more than 30 pretrained weights are provided through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .\n\n\nRelated work\nThe design of Transformers was inspired by earlier libraries",
            " defined will allow us to eliminate costly annotation efforts in the future, especially for the word-level slots and intent keywords. Once enough domain-specific multi-modal data is collected, our future work is to explore training end-to-end dialogue agents for our in-cabin use-cases. We are planning to exploit other modalities for improved understanding of the in-cabin dialogue as well.\n\n\n",
            " SimLex-999 has also been translated and rescored in Hebrew and Croatian Mrksic:2017:TACL. SimLex-999 explicitly targets at similarity rather than relatedness and includes adjective, noun and verb pairs. However, this dataset contains only frequent words. In addition, the distributed representation of words is generally learned using only word-level information. Consequently, the distributed representation for low-frequency words and unknown words cannot be learned well with conventional models. However, low-frequency words",
            "-bert's subword vocabulary contains plenty of cmn-specific and yue-specific subwords which are exploited by the encoder when producing m-bert-based representations. Simultaneously, higher scores with m-bert (and xlm in Table TABREF46) are reported for resource-rich languages such as French, Spanish, and English, which are better represented in m-bert's training data. We also observe lower absolute scores (and a larger number of OOV",
            " identify something other than the language, such as typological features about the language's phonemic inventory. This could enable better sharing of resources among languages. Such typological knowledge is readily available in databases like Phoible and WALS for a wide variety of languages. It would be interesting to explore if any of these features is a good predictor of a language's orthographic rules. It would also be interesting to apply the artificial token approach to other problems besides multilingual g2p. One closely related",
            " complicates the reproduction of the experiments in other languages. Consequently, the development of multilingual systems using discursive analysis are yet to be developed. Diverse applications based on the latest technologies require at least one of the three steps mentioned above BIBREF1, BIBREF2, BIBREF3. In this context, the idea of exploring the architecture of a generic system that is able not only of segmenting a text correctly but also of adapting it to any language, was a great motivation",
            ", however imperfectly, by the presence of particular words semi-automatically extracted from a machine learning algorithm. Operationalizations are never perfect translations, and are often refined over the course of an investigation, but they are crucial. We begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results. The research process sounds more or less linear this way, but each of these phases overlaps, and",
            " we test the model on the languages unseen during pre-training. To make sure the languages have never been seen before, we artificially make unseen languages by permuting the whole vocabulary of existing languages. That is, all the words in the sentences of a specific language are replaced by other words in the same language to form the sentences in the created unseen language. It is assumed that if multi-BERT used to find answers by language independent strategy, then multi-BERT should also do well on unseen"
        ]
    },
    {
        "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
        "answer": "BERT, DistilBERT, RoBERTa, ALBERT, XLNet, Longformer, FlanT5, T5, BigBird, and others.",
        "evidence": [
            " form of training-computation-cost-sharing so that low-resource users can reuse pretrained models without having to train them from scratch. These models are accessed through a simple and unified API that follows a classic NLP pipeline: setting up configuration, processing data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to ensure they match the original author",
            " 16. We used the large version of the universal sentence encoder with a transformer BIBREF23 . We did not update the pretrained sentence encoder parameters during training. All hyperparamaters for all models (including hidden layers, hidden sizes, and dropout) were tuned using Vizier BIBREF24 via 10-fold cross validation on the training set maximizing for F1. As a baseline, we also trained a linear Support-Vector Regression BIBREF25 model on $n",
            " better vector representation of words and thus will facilitate deep learning based automatic feature extraction.\n\n\n",
            " subject of ongoing work.\n\n\nImplications\nCollectively, the success of large pretrained neural models, both encoder-only BERT-like architectures as well as encoder–decoder architectures like T5, raise interesting questions for the pursuit of commonsense reasoning. Researchers have discovered that previous models perform well on benchmark datasets because they pick up on incidental biases in the dataset that have nothing to do with the task; in contrast, the WinoGrande dataset has devoted considerable effort to",
            " workings of each individual architecture. Additional wrapper classes are built on top of the base class, adding a specific head on top of the base model hidden states. Examples of these heads are language modeling or sequence classification heads. These classes follow similar naming pattern: XXXForSequenceClassification or XXXForMaskedLM where XXX is the name of the model and can be used for adaptation (fine-tuning) or pre-training. All models are available both in PyTorch and TensorFlow",
            "izers implement additional useful features for the users, by offering values to be used with a model; these range from token type indices in the case of sequence classification to maximum length sequence truncating taking into account the added model-specific special tokens (most pretrained Transformers models have a maximum sequence length they can handle, defined during their pretraining step). Tokenizers can be instantiated from existing configurations available through Transformers originating from the pretrained models or created more generally by the user from user-specifications.",
            "Abstract\nRecent advances in modern Natural Language Processing (NLP) research have been dominated by the combination of Transfer Learning methods with large-scale language models, in particular based on the Transformer architecture. With them came a paradigm shift in NLP with the starting point for training a model on a downstream task moving from a blank specific model to a general-purpose pretrained architecture. Still, creating these general-purpose models remains an expensive and time-consuming process restricting the use of these methods to a",
            ", Google, and Yandex with the introduction of neural machine translation BIBREF3 . They provide services that are free of charge that can be easily integrated into standard programming languages like Python and R. Open source neural machine translation systems are also being made available BIBREF4 . In a recent application, BIBREF5 estimate the policy preferences of Swiss legislators using debates in the federal parliament. With speeches delivered in multiple languages, the authors first translate from German, French, and Italian into English",
            "ERT) for adaptation using the pretraining objectives. Some models fine-tuned on downstream tasks such as SQuAD1.1 are also available. Overall, more than 30 pretrained weights are provided through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .\n\n\nRelated work\nThe design of Transformers was inspired by earlier libraries",
            " describe a number of techniques for improving the speed of the decoder, and obtain a 4.4x speedup over a highly efficient baseline. These speedups do not affect decoding results, so they can be applied universally. In Section \"Model Improvements\" , we describe a simple but powerful network architecture which uses a single RNN (GRU/LSTM) layer at the bottom with a large number of fully-connected (FC) layers on top, and obtains improvements similar to a",
            "ches of unlabeled data. Labeled data are input into the model to calculate the standard supervised loss for each mini-batch and the model weights are updated regularly. Meanwhile, each mini-batch of unlabeled data is selected randomly from the pool of all unlabeled data; the model computes the loss for CVT from the mini-batch of unlabeled data. This CVT loss is used to train auxiliary prediction modules, which see restricted views of the input, to match",
            " a low entry barrier, the number of user-facing abstractions to learn was strongly limited and reduced to just three standard classes: configuration, models and tokenizers, which all can be initialized in a simple and unified way by using a common `from_pretrained()` instantiation method.\n\n\nIntroduction ::: Interpretability and diversity\nThere is a growing field of study, sometimes referred as BERTology from BERT BIBREF13, concerned with investigating the inner working of large-"
        ]
    },
    {
        "title": "Multichannel Variable-Size Convolution for Sentence Classification",
        "answer": "MVCNN extends the network in BIBREF5 by hierarchical convolution architecture and further exploration of multichannel and variable-size feature detectors.  Its convolution operation is not only more flexible to extract features of variable-range phrases, but also able to model dependency among all dimensions of representations.  MVCNN has rich feature maps as input and as output of each convolution layer.  Its convolution operation is not only more flexible to extract features of variable-range phrases, but also able to model dependency among all dimensions of representations.  MVCNN has rich feature maps as input and as output of each convolution layer.  Its convolution operation is not only more flexible to extract features of variable-range phrases, but also able to model dependency among all dimensions of representations.  MVCNN has rich feature maps as input and as output of each convolution layer.  Its convolution operation is not only more flexible to extract features of variable-range phrases, but also able to model dependency among all dimensions of representations.  MVCNN has rich feature maps as input and as output of each convolution layer.  Its convolution operation is not only more flexible to extract features of variable-range phrases, but also able to model dependency among all dimensions of representations.  MVCNN has rich feature maps as input and as output of each convolution layer",
        "evidence": [
            "size filters in a convolution layer. Compared to BIBREF4 , MVCNN has rich feature maps as input and as output of each convolution layer. Its convolution operation is not only more flexible to extract features of variable-range phrases, but also able to model dependency among all dimensions of representations. MVCNN extends the network in BIBREF5 by hierarchical convolution architecture and further exploration of multichannel and variable-size feature detectors.\n\n\nModel Enhancements\nThis",
            " the comparative relevance of the news article to the different entities occurring in it. As an example, let us consider the meeting of the Sudanese bishop Elias Taban with Hillary Clinton. Both entities are salient for the meeting. However, in Taban's Wikipedia page, this meeting is discussed prominently with a corresponding news reference, whereas in Hillary Clinton's Wikipedia page it is not reported at all. We believe this is not just an omission in Clinton's page but mirrors the fact that for the lesser known Tab",
            " comment information. UTCNN without user information shows results similar to SVMs — it does well for Sup and Neu but detects no Uns. Its best f-scores on both Sup and Neu among all methods show that with enough training data, content-based models can perform well; at the same time, the lack of user information results in too few clues for minor-class posts to either predict their stance directly or link them to other users and posts for improved performance. The 17.",
            " generative performance as indicated by the high reconstruction loss. The KL loss for both VAE-CNN and vMF-VAE are nonzero, where the former mitigates the KL collapse issue with a KL loss clipping strategy and the latter by replacing the standard normal distribution for the prior with the vMF distribution (i.e., with the vMF distribution, the KL loss only depends on a fixed concentration parameter, and hence results in a constant KL loss). Although both VAE-CNN",
            "353. They used a convolutional neural network (CNN) model to classify a question into a restricted set of 10 question types and crawled \"relevant\" online web pages to find the answers. However, the results were lower than those achieved by the systems relying on finding similar answered questions. These results support the relevance of similar question matching for the end-to-end QA task as a new way of approaching QA instead of the classical QA approaches based on Question Analysis and Answer Retri",
            "urrent Network (MARN). Our approach is designed to model both view-specific dynamics as well as cross-view dynamics continuously through time. View-specific dynamics are modeled using a Long-short Term Hybrid Memory (LSTHM) for each modality. Various cross-view dynamics are identified at each time-step using the Multi-attention Block (MAB) which outputs a multimodal neural code for the hybrid memory of LSTHM. MARN achieves state-of-the",
            "icated by blue coloring), while on the other hand the CNN2 heatmap is consistently more sparse and concentrated on semantically meaningful words. This sparsity property can be attributed to the max-pooling non-linearity which for each feature map in the neural network selects the first most relevant feature that occurs in the document. As can be seen, it significantly simplifies the interpretability of the results by a human. Another disadvantage of the SVM model is that it relies entirely on local and global",
            ") dropout averaging at test time of 1000 different masks, making it computationally more expensive to run. When training on the PTB dataset with an NVIDIA K40 GPU, we found that the QRNN is substantially faster than a standard LSTM, even when comparing against the optimized cuDNN LSTM. In Figure FIGREF15 we provide a breakdown of the time taken for Chainer's default LSTM, the cuDNN LSTM, and QRNN to perform a",
            " much fewer parameters.(The number of parameters in our models is approximately 10K while in CNN the number of parameters is about 400K). Compared with RecNN, DC-RecNN performs better, indicating the effectiveness of the dynamic compositional function. Additionally, both DC-RecNN and DC-TreeLSTM achieve substantial improvement on IE dataset, which covers the richness of compositionality (idiomaticity). We attribute the success on IE to its power in modeling more complicated compositionality.\n\n\n",
            "e., $\n{\\bf {v}} (s) &=\\text{CNN}(s) \\\\\n{\\bf {v}} (q) &=\\text{CNN}(q)\n$  A basic CNN contains a convolution operation on the top of word embeddings, which is followed by a pooling operation. Let ${\\bf {v}}(w_i) \\in \\mathbb {R}^k$ refer to the $k$",
            " datasets differ with respect to text type (Wikipedia texts, examination texts, etc.), mode of answer selection (span-based, multiple choice, etc.) and test systems regarding different aspects of language understanding, but they do not explicitly address commonsense knowledge. Two notable exceptions are the NewsQA and TriviaQA datasets. NewsQA BIBREF20 is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection",
            " numbers or dates, while keeping abbreviations and compound words. We do not apply any further preprocessing, as for instance stop-word removal or stemming, except for the SVM classifier where we additionally perform lowercasing, as this is a common setup for bag-of-words models. We truncate the resulting sequence of tokens to a chosen fixed length of 400 in order to simplify neural network training (in practice our CNN can process any arbitrary sized document). Lastly, we build the neural"
        ]
    },
    {
        "title": "A Lightweight Front-end Tool for Interactive Entity Population",
        "answer": "JavaScript",
        "evidence": [
            " can be found. As can be seen in the first two rows, multiple types of questions can be answered within the same video (but different segments). The last two rows display questions which belong to the same segment but correspond to different properties of the same entity, 'crop tool'. Here we observe different types of questions, such as \"why\", \"how\", \"what\", and \"where\", and can see why the answers may involve multiple steps. Some questions that the worked paraphrased were in",
            " paper has presented LUWAK, a lightweight front-end tool for interactive entity population. LUWAK provides a set of basic functions such as entity expansion and user feedback assignment. We have implemented LUWAK in pure JavaScript with LocalStorage to make it an installation-free tool. We believe that LUWAK plays an important role in delivering the values of existing entity expansion techniques to potential users including nontechnical people without supposing a large amount of human cost. Moreover, we believe that this design",
            ", based on the Transformers library, are available online. Using in production To facilitate the transition from research to production, all the models in the library are compatible with TorchScript, an intermediate representation of a PyTorch model that can then be run either in Python in a more efficient way, or in a high-performance environment such as C++. Fine-tuned models can thus be exported to production-friendly environment. Optimizing large machine learning models for production is an ongoing effort in the community",
            " in an analogous way to how language ID is tagged in multilingual g2p.\n\n\n",
            ", a tool for argumentation diagramming which supports both convergent and linked arguments, missing premises (enthymemes), and refutations. They also released the AracuariaDB corpus which has later been used for experiments in the argumentation mining field. However, the creation of the dataset in terms of annotation guidelines and reliability is not reported – these limitations as well as its rather small size have been identified BIBREF10 . Biran.Rambow.2011 identified justifications for",
            " potential of qualitative data to inform programming and gain a better understanding of values, behaviors and attitudes of people and communities affected by their efforts. Some organizations mainly rely on interview or focus group data, some also consider policy documents and reports, and others have started tapping into social media data. Regardless of where the data comes from, analyzing it in a systematic way to inform quick decision-making poses challenges, in terms of high costs, time, or expertise required. The application of natural language processing (NLP",
            " numbers or dates, while keeping abbreviations and compound words. We do not apply any further preprocessing, as for instance stop-word removal or stemming, except for the SVM classifier where we additionally perform lowercasing, as this is a common setup for bag-of-words models. We truncate the resulting sequence of tokens to a chosen fixed length of 400 in order to simplify neural network training (in practice our CNN can process any arbitrary sized document). Lastly, we build the neural",
            " and TensiStrength BIBREF13 are rule-based tools, relying on various dictionaries of emoticons, slangs, idioms, and ironic phrases, and set of rules that can detect the sentiment of a sentence overall or a targeted sentiment. Given a list of keywords, TensiStrength (similar to SentiStrength) reports the sentiment towards selected entities in a sentence, based on five levels of relaxation and five levels of stress. Among commercial NLP toolkits (e.g",
            " MCScript forms the basis of a shared task organized at SemEval 2018. The dataset is available at http://www.sfb1102.uni-saarland.de/?page_id=2582.\n\n\nAcknowledgements\nWe thank the reviewers for their helpful comments. We also thank Florian Pusse for the help with the MTurk experiments and our student assistants Christine SchÃ¤fer, Damyana Gateva, Leonie Harter, Sarah M",
            "\" ML interface called themis-ml. In this interface, the main idea is to pick up a data set from a modified dataset. Themis-ml implements two methods for training fairness-aware models. The tool relies on two methods to make agnostic model type predictions: Reject Option Classification and Discrimination-Aware Ensemble Classification, these procedures being used to post-process predictions in a way that reduces potentially discriminatory predictions. According to the authors, it is possible to perceive the potential use",
            ", an original entity, which was used to find the entity, and the score, which denotes the confidence score. A user can directly edit the table by adding, renaming, and deleting entities. Moreover, the entity inactivation function allows a user to manually inactivate entities, so that entity expansion algorithms do not use the inactivated entities. The table implements a page switching function, a search function, and a sorting function to ensure visibility even when there is a large number of entities in the table",
            " and INLINEFORM1 generated by the encoder layer are combined using an attention mechanism. We use one-way attention because the graph contains information about the whole environment, while the instruction has (potentially incomplete) local information about the route of interest. The use of attention provides our model with a two-step strategy to interpret commands. This resembles the way people find paths on a map: first, relevant parts on the map are selected according to their affinity to each of the words in the input"
        ]
    },
    {
        "title": "Modeling Trolling in Social Media Conversations",
        "answer": "7665",
        "evidence": [
            " conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script",
            " To this end, we chunk the novels into different sizes: 200-2000 words, at 200-word intervals, and evaluate our CNNs in the multi-class condition. Generalization-dataset experiments. To confirm that our models generalize, we pick the best models from the baseline-dataset experiments and evaluate on the novel-50 and IMDB62 datasets. For novel-50, the chunking size applied is 2000-word as per the baseline-dataset experiment",
            " datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size influences.",
            " BIBREF10 and commonsense inference BIBREF11. While this approach has shown impressive improvements on benchmarks and evaluation metrics, the exponential increase in the size of the pretraining datasets as well as the model sizes BIBREF5, BIBREF12 has made it both difficult and costly for researchers and practitioners with limited computational resources to benefit from these models. For instance, RoBERTa BIBREF5 was trained on 160 GB of text using 1024 32GB V100. On Amazon-",
            " from all over the world, for a total of 7665 sentences labelled with an emotion, making this the largest dataset out of the three we use.\n\n\nOverview of datasets and emotions\nWe summarise datasets and emotion distribution from two viewpoints. First, because there are different sets of emotions labels in the datasets and Facebook data, we need to provide a mapping and derive a subset of emotions that we are going to use for the experiments. This is shown in Table TABREF8 , where in",
            " dataset is recorded across 5 sessions with 5 pairs of speakers. To ensure speaker independent learning, the dataset is split at the level of sessions: training is performed on 3 sessions (6 distinct speakers) while validation and testing are each performed on 1 session (2 distinct speakers).\n\n\nMultimodal Computational Descriptors\nAll the datasets consist of videos where only one speaker is in front of the camera. The descriptors we used for each of the modalities are as follows: Language All",
            " consider each result is affected by at least three factors: (1) training corpus, (2) data size, (3) whether the source corpus is translated into the target language. To study the effect of data-size, we conducted an extra experiment where we down-sampled the size of English data to be the same as Chinese corpus, and used the down-sampled corpus to train. Then We carried out one-way ANOVA test and found out the significance of the three factors",
            "English spoken-domain translation, applying fully character-level segmentation. This dataset consists of 209,772 sentence pairs of parallel training data from transcribed TED and TEDx presentations, with a mean sentence length of 103 characters for German and 93 for English. We remove training sentences with more than 300 characters in English or German, and use a unified vocabulary of 187 Unicode code points. Our best performance on a development set (TED.tst2013) was achieved using a four-layer encoder–",
            "?\n\n\nAcknowledgments\nThis research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada. We would like to thank Google Colab for providing support in terms of computational resources.\n\n\n",
            "No common types).\n\n\nDatasets Used for the RQE Study\nWe evaluate the RQE methods (i.e. deep learning model and logistic regression classifier) using two datasets of sentence pairs (SNLI and multiNLI), and three datasets of question pairs (Quora, Clinical-QE, and SemEval-cQA). The Stanford Natural Language Inference corpus (SNLI) BIBREF13 contains 569,037 sentence pairs",
            " as the size of a dataset's vocabulary size (depending on different query type settings, we mask out words in the vocabulary that are not query candidates). The overall Q-value is simply the sum of the two components:\n\n\nBaseline Agent ::: Model Structure ::: Question Answerer\nFollowing BIBREF18, we append two extra stacks of aggregation transformer blocks on top of the encoder to compute head and tail positions: Here, $M_{head}$ and $M_{",
            " keys, and the average number of records (resp. entities) in a single data-structure is 628 (resp. 28). Entities are of two types, either team or player, and player descriptions depend on their involvement in the game. We followed the data partitions introduced with the dataset and used a train/validation/test sets of respectively $3,398$/727/728 (data-structure, description) pairs.\n\n\nExperimental setup ::: Evaluation metrics"
        ]
    },
    {
        "title": "Rotations and Interpretability of Word Embeddings: the Case of the Russian Language",
        "answer": "0.5",
        "evidence": [
            " singular vectors in SVD of $M_2$7 .\n\n\nInterpretability measures\nOne of traditional measures of interpretability in topic modeling looks as follows BIBREF29 , BIBREF19 . For each component, $n$ most probable words are selected. Then for each pair of selected words some co-occurrence measure such as PMI is calculated. These values are averaged over all pairs of selected words and all components. The other approaches use human markup. Such measures need",
            " use their GPUs to perform the experiments mentioned in the paper. We also thank the anonymous reviewers for their careful reading of our paper and insightful comments.\n\n\n",
            " paper presents a study of the efficiency and management of electric power in a packaging and plastic film industry. Human translation: this fact corroborates the difficulty in modeling human behavior. OpenNMT: this fact corroborates the difficulty in modeling human behavior. Moses: this fact corroborated the difficulty in model the human behavior. GT: this fact corroborates the difficulty in modeling human behavior.\n\n\nSentence alignment quality\nWe manually validated the alignment quality for 400 sentences randomly selected from the parsed corpus and assigned",
            "” and “not sure if trolling or not”. While the presence of a question like these seems to give us a hint of the responder's interpretation, we cannot be sure of his interpretation without also considering the context. One way to improve interpretation is to exploit the response strategy, but the response strategy in our model is predicted independently of interpretation. So one solution could be similar to the one proposed above for the disclosure task problem: jointly learning classifiers that predict both variables simultaneously.",
            " annotators must explicitly write down the (inferred) stance of the author. By definition, the Toulmin's model is intended to model single argument, with the claim in its center. However, we observed in our data, that some authors elaborate on both sides of the controversy equally and put forward an argument for each side (by argument here we mean the claim and its premises, backings, etc.). Therefore we allow multiple arguments to be annotated in one document. At the same time",
            " purposes, since it enables us to estimate the number of commonsense inference questions. For questions that did not make sense at all given a text, unfitting was assigned. If a question made sense for a text, but it was impossible to find an answer, the label unknown was used. In a second step, we told participants to formulate a plausible correct and a plausible incorrect answer candidate to answerable questions (text-based or script-based). To level out the effort between answerable and non",
            ", although presenting similar classification performance may largely differ in terms of explainability.  The work is organized as follows. In section \"Representing Words and Documents\" we describe the related work for explaining classifier decisions with respect to input space variables. In section \"Predicting Category with a Convolutional Neural Network\" we introduce our neural network ML model for document classification, as well as the LRP decomposition procedure associated to its predictions. We describe how LRP relevance scores can be used to",
            "Q^T X Q = \\operatorname{tr}Q^{-1} X Q = \\operatorname{tr}X$ . It turns out that in average the interpretability is constant under any orthogonal transformation. But it is possible to make the first components more interpretable due to the other components. For example, $\n(Q^T W^T W W^T W Q)_{1, 1} = \\left(q^T",
            " for prediction, but harder to interpret. The value we place on interpretability can therefore influence the approach we choose. There is an increasing interest in developing interpretable or transparent models in the NLP and machine learning communities.\n\n\nAnnotation\nMany studies involve human coders. Sometimes the goal is to fully code the data, but in a computational analysis we often use the labels (or annotations) to train machine learning models to automatically recognize them, and to identify language patterns that are associated with these",
            " to control the topic influence in the documents. As one next step, we will compile additional and larger corpora to investigate the question whether the evaluation results of this paper hold more generally. Furthermore, we will address the important question how the results of AV methods can be interpreted in a more systematic manner, which will further influence the practicability of AV methods besides the proposed properties. This work was supported by the German Federal Ministry of Education and Research (BMBF) under the project \"DOR",
            " there is a final multi-layer perceptron(MLP) followed with a sigmoid activation function indicating the confidence score for selecting each sentence: INLINEFORM0 \n\n\nExperiments\nTo validate our method, we set up experiments on the two scientific paper datasets (arXiv and PubMed). With ROUGE and METEOR scores as automatic evaluation metrics, we compare with previous works, both abstractive and extractive.\n\n\nTraining\nThe weighted negative log-likelihood",
            ". Annotators were asked to complete a spreadsheet containing the translated pairs of words, as well as the part-of-speech, and a column to enter the score. The annotators did not have access to the original pairs in English. To ensure the quality of the collected ratings, we have employed an adjudication protocol similar to the one proposed and validated by Pilehvar:2018emnlp. It consists of the following three rounds:  Round 1: All annotators are asked"
        ]
    },
    {
        "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study",
        "answer": "repetition, dropped words, mistranslations, biases from data collection, rules of annotation, hallucinations, erroneous facts, contradictory descriptions.  Also, the paper mentions the difficulty in modeling human behavior, and the challenges faced in computational analyses of textual data.  Additionally, the paper notes the importance of constraining the model on the data structure to prevent inaccurate or contradictory descriptions.  Furthermore, the paper mentions the need to fine-tune the embeddings and use early stop.  Finally, the paper notes the high complexity and expensive costs of collective methods compared to local methods.  Also, the paper mentions the difficulty in discriminating against implicit abuses, and the importance of examining more samples.  Additionally, the paper notes the need to identify the research question and interpret the results.  Finally, the paper mentions the importance of measuring social or cultural concepts using computational methods.  Also, the paper notes the challenges faced in measuring hate speech, and the importance of identifying false presuppositions.  Furthermore, the paper mentions the need to resolve unresolvable coreference, and the importance of handling vague questions.  Finally, the paper notes the importance of addressing unknown sources of error.  Also, the paper mentions the need to prevent inaccurate or contradictory descriptions, and the importance of constraining",
        "evidence": [
            " paper presents a study of the efficiency and management of electric power in a packaging and plastic film industry. Human translation: this fact corroborates the difficulty in modeling human behavior. OpenNMT: this fact corroborates the difficulty in modeling human behavior. Moses: this fact corroborated the difficulty in model the human behavior. GT: this fact corroborates the difficulty in modeling human behavior.\n\n\nSentence alignment quality\nWe manually validated the alignment quality for 400 sentences randomly selected from the parsed corpus and assigned",
            " access to the gold-standards of SENTIPOLC'14, TASS'15 and SemEval 2015 & 2016, respectively. The authors also thank Elio Villaseñor for the helpful discussions in early stages of this research.\n\n\n",
            " of the following three rounds:  Round 1: All annotators are asked to follow the instructions outlined above, and to rate all 1,888 pairs with integer scores between 0 and 6.  Round 2: We compare the scores of all annotators and identify the pairs for each annotator that have shown the most disagreement. We ask the annotators to reconsider the assigned scores for those pairs only. The annotators may chose to either change or keep the scores. As in the case with Round 1",
            " the parts of the reference that are correct and red highlights the corresponding problematic parts of translations, including repetitions, dropped words or mistranslations.\n\n\n",
            " the model successfully concentrates on the salient information, however, it also suffers from serious repetition problem. Further optimized by the variance loss, our model can avoid repetition and generate summary with salient information. Besides, our generated summary contains fewer trivial facts compared to the PGN+Coverage model.\n\n\n",
            ",348 QA pairs were extracted from this resource. Figure FIGREF55 presents examples of QA pairs generated from A.D.A.M encyclopedia. MedlinePlus Drugs: We extracted free text from 1,316 articles about Drugs and generated 12,889 QA pairs. MedlinePlus Herbs and Supplements: We extracted free text from 99 articles and generated 792 QA pairs. The final collection contains 47,457 annotated question-answer pairs about Diseases, Drugs and other named entities",
            " noticed that even for a human it is difficult to discriminate against this kind of implicit abuses. By examining more samples and with respect to recently studies BIBREF2, BIBREF24, BIBREF19, it is clear that many errors are due to biases from data collection BIBREF19 and rules of annotation BIBREF24 and not the classifier itself. Since Waseem et al.BIBREF5 created a small ad-hoc set of keywords and Davidson et al",
            " sample of the many opportunities and challenges faced in computational analyses of textual data. New possibilities and frustrating obstacles emerge at every stage of research, from identification of the research question to interpretation of the results. In this article, we take the reader through a typical research process that involves measuring social or cultural concepts using computational methods, discussing both the opportunities and complications that often arise. In the Reddit case, for example, hate speech is measured, however imperfectly, by the presence of particular words semi-autom",
            " across nigeria between'). 3% of questions have an unresolvable coreference (e.g., `how do i get to Warsaw Missouri from here'). 4% of questions are vague, and a further 7% have unknown sources of error. 2% still contain false presuppositions (e.g., `what is the only fruit that does not have seeds?') and the remaining 42% do not have an answer within the document. This reinforces our belief that though they have been under",
            " comments. This paper is based on results obtained from projects commissioned by the New Energy and Industrial Technology Development Organization (NEDO).\n\n\n",
            "REF40. In addition, we note that our approach can still lead to erroneous facts or even hallucinations. An interesting perspective might be to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions.\n\n\nAcknowledgements\nWe would like to thank the H2020 project AI4EU (825619) which partially supports Laure Soulier and Patrick Gallinari.\n\n\n",
            " use early stop and fine tune the embeddings. With a batch size of 16, nearly 3 epochs cost less than 15 minutes on the server with 20 core CPU and the GeForce GTX 1080Ti GPU with 12Gb memory. We use standard Precision, Recall and F1 at mention level (Micro) and at the document level (Macro) as measurements. Complexity Analysis Compared with local methods, the main disadvantage of collective methods is high complexity and expensive costs. Suppose there are INLINEFORM"
        ]
    },
    {
        "title": "Attention Optimization for Abstractive Document Summarization",
        "answer": "precision, recall, F-score, classification error.",
        "evidence": [
            ", provide qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.\n\n\n",
            "�I feel so depressed!”, “What a bad day!”, etc. There may be infinite number of responses for an open domain messages. Hence, it is difficult to construct a gold standard (usually a reference set) to evaluate a response which is generated by an open domain chit-chat system. For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc.,",
            " we are 3.5 times better. In Table TABREF58 we show the overall scores for the evaluation metrics for B1 and INLINEFORM4 . Finally, for B2 we achieve much poorer performance, with average scores of P=0.21, R=0.20 and F1=0.21. Robustness. In Table TABREF58 , we show the overall performance for the years between 2009 and 2013. An interesting observation we make is that we have a",
            "Abstract\nSentence Boundary Detection (SBD) has been a major research topic since Automatic Speech Recognition transcripts have been used for further Natural Language Processing tasks like Part of Speech Tagging, Question Answering or Automatic Summarization. But what about evaluation? Do standard evaluation metrics like precision, recall, F-score or classification error; and more important, evaluating an automatic system against a unique reference is enough to conclude how well a SBD system is performing given the final application of the",
            " the gold descriptions of the RotoWire train dataset. Entity-value pairs are extracted from the descriptions. For example, in the sentence Isaiah Thomas led the team in scoring, totaling 23 points [...]., an IE tool will extract the pair (Isaiah Thomas, 23, PTS). Second, we compute three metrics on the extracted information: $\\bullet $ Relation Generation (RG) estimates how well the system is able to generate text containing factual (i.e., correct) records. We measure",
            " discussions and comments. We also thank Robin Jia for the help on SQuAD evaluations. \n\n\n",
            " but also which emoticons are used in a similar context. We take advantage of distant supervision by using Facebook reactions as proxies for emotion labels, which to the best of our knowledge hasn't been done yet, and we train a set of Support Vector Machine models for emotion recognition. Our models, differently from existing ones, exploit information which is acquired entirely automatically, and achieve competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. For explanatory purposes",
            " section describes our evaluation of the proposed approach for interpreting navigation commands in natural language. We provide both quantitative and qualitative results.\n\n\nEvaluation Metrics\nWhile computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-",
            " approach to be very strict because for those boundaries where no agreement between references existed, the system was going to be partially wrong even the fact that it has correctly predicted the boundary. Their second approach tried to moderate the number of unjust penalizations. For this case, a classification was considered incorrect only if it didn't match either of the two references. These two examples exemplify the real need and some straightforward solutions for multi-reference evaluation metrics. However, we think that it is possible to consider in a",
            " public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth. Besides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human experts.",
            " \\cdot 10^{-4}$, and save model checkpoints every 5000 steps. It takes 130k steps to converge for the XL data size (see below). At inference time, we use greedy decoding and select for evaluation the model checkpoint that achieves the highest score on the development set. We did not experiment with T5-11B due to limited computational resources.\n\n\nResults\nExperimental results on the WinoGrande development set are reported in Table TABREF7 for different training",
            ". For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency:"
        ]
    },
    {
        "title": "Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling",
        "answer": "seq2seq, HLSTM, HLSTM+Copy, HLSTM+Copy+MSA, HLSTM+Copy+MSA+IE, HLSTM+Copy+MSA+IE+GA, HLSTM+Copy+MSA+IE+CA, HLSTM+Copy+MSA+IE+GA+CA, HLSTM+Copy+MSA+IE+GA+CA+IE, HLSTM+Copy+MSA+IE+GA+CA+IE+GA, HLSTM+Copy+MSA+IE+GA+CA+IE+GA+CA, HLSTM+Copy+MSA+IE+GA+CA+IE+GA+CA+IE, HLSTM+Copy+MSA+IE+GA+CA+IE+GA+CA+IE+GA, HLSTM+Copy+MSA+IE+GA+CA+IE+GA+CA+IE+GA+CA, HLSTM+Copy+MSA+IE+GA+CA+IE+GA+CA+IE+GA+CA+IE, HLSTM+Copy+MSA+IE+GA+CA+IE+GA+CA+IE+GA+CA+IE+GA, HLSTM+Copy",
        "evidence": [
            " with sequence-to-sequence (seq2seq) framework. There are three main ways to build up models. The first is to learn a latent style-independent content representation and generate sentences with the content representation and another style BIBREF6 , BIBREF7 . The second is to directly transfer sentences from one style to another under the control of classifiers and reinforcement learning BIBREF8 . The third is to remove style attribute words from the input sentence and combine the remaining content with new",
            " a phrasal decoder. We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This",
            " workings of each individual architecture. Additional wrapper classes are built on top of the base class, adding a specific head on top of the base model hidden states. Examples of these heads are language modeling or sequence classification heads. These classes follow similar naming pattern: XXXForSequenceClassification or XXXForMaskedLM where XXX is the name of the model and can be used for adaptation (fine-tuning) or pre-training. All models are available both in PyTorch and TensorFlow",
            " the multilingual model is trained on a much smaller set of languages (17 versus 100), and further improvements can be achieved by training a dedicated bilingual model. Finally, leveraging bilingual parallel data seems to offer additional slight gains, but a tiny difference between xlm-2 and xlm-2++ also suggests that this rich bilingual information is not used in the optimal way within the xlm architecture for semantic similarity. In summary, these results indicate that, in order to improve performance in cross",
            "2seq baseline. In addition, the lexicalized multi-source systems yields slightly higher BLEU scores than the unlexicalized multi-source systems; this is surprising because the lexicalized systems have significantly longer sequences than the unlexicalized ones. Finally, it is interesting to compare the seq2seq and parse2seq baselines. Parse2seq outperforms seq2seq by only a small amount compared to multi-source; thus, while adding syntax to NMT",
            ") will be of limited use due to the paucity of relevant corpora. Yet, previous encoder-only architectures like RoBERTa that exploit a language modeling objective (that is, relying only on explicit textual knowledge) can clearly make headway in a commonsense reasoning task, and we can further improve upon these approaches with a sequence-to-sequence model. This leaves us with two possible explanations: despite careful controls, the WinoGrande challenge still contains incidental biases that these more",
            "2Seq, HLSTM, and HLSTM+Copy) have predicted improper entities (cake), generated repetitive contents (her family), or copied wrong words (eat). The models equipped with incremental encoding or knowledge through MSA(GA/CA) perform better in this example. The ending by IE+MSA is more coherent in logic, and fluent in grammar. We can see that there may exist multiple reasonable endings for the same story context. In order to verify the ability of our",
            " models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given model to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog",
            " (a) for sample network architecture of the seq2one Bi-LSTM network. Note that in the Bi-LSTM implementation for seq2one learning (i.e., when not returning sequences), the outputs of backward/reverse LSTM is actually ordered in reverse time steps ( INLINEFORM0 ... INLINEFORM1 ). Thus, as illustrated in Fig. FIGREF24 (a), we actually concatenate the hidden state outputs of forward LSTM at last",
            " related to intent/slots). Therefore, the proposed approach would result in reducing the sequence length nearly by half at the input layer of level-2 for utterance-level recognition. For hierarchical & separate models, we experimented with 4 variations based on which separate model used at the second level of the hierarchy, and these are summarized as follows: Hierarchical & Separate-0: Level-1 (Seq2seq LSTM for intent keywords & slots extraction) + Level-2",
            " for training the neural architecture, one pre-trained with 2B of tweets, publicy downloadable, one trained with a corpora of 300K tweets collected during the 2014-2017 BBC Proms Festivals and disjoint from the data used in our experiments. The first model considered for this task has been the John Platt's sequential minimal optimization algorithm for training a support vector classifier BIBREF21 , implemented in WEKA BIBREF22 . Indeed, in BIBREF23 results",
            " the overall encoding result. Encoder–Decoder Models To demonstrate the generality of QRNNs, we extend the model architecture to sequence-to-sequence tasks, such as machine translation, by using a QRNN as encoder and a modified QRNN, enhanced with attention, as decoder. The motivation for modifying the decoder is that simply feeding the last encoder hidden state (the output of the encoder's pooling layer) into the decoder's recurrent pooling layer,"
        ]
    },
    {
        "title": "Leveraging Discourse Information Effectively for Authorship Attribution",
        "answer": "RoBERTa.",
        "evidence": [
            " `advocated', `they' would refer to the women and would be translated as `elles'. In the later versions of the thesis, published as (Winograd 1972), he changed `women' to `demonstrators'; this made the disambiguation clearer, but lost the point about translation.\n\n\nCurrent state of the art\nNo one familiar with the state of the art in machine translation technology or the state of the art of artificial intelligence generally will be surprised to learn",
            " with a fully connected linear classifier (BERTbase) outperforms previous baselines yielding F1-score of 81% and 91% for datasets of Waseem and Davidson respectively. Inserting a CNN to pre-trained BERT model for fine-tuning on downstream task provides the best results as F1- score of 88% and 92% for datasets of Waseem and Davidson and it clearly exceeds the baselines. Intuitively, this makes sense that combining all pre-",
            " comments. This paper is based on results obtained from projects commissioned by the New Energy and Industrial Technology Development Organization (NEDO).\n\n\n",
            ". All text was lowercased. In a post-processing step, it was ensured that each emoji is tokenized as a single token.\n\n\nWord Embeddings\nTo be able to train word embeddings, Spanish tweets were scraped between November 8, 2017 and January 12, 2018. We chose to create our own embeddings instead of using pre-trained embeddings, because this way the embeddings would resemble the provided data set: both are based on Twitter",
            " it with two baselines: the CRF-based model with n-gram embedding, which is currently the state-of-the-art for Thai sentence segmentation, and the Bi-LSTM-CRF model, which is currently the deep learning state-of-the-art approach for sequence tagging.\n\n\nEnglish punctuation restoration\nMost languages use a symbol that functions as a sentence boundary; however, a few do not use sentence markers including Thai, Lao and",
            " code and strong incentives to share state-of-the-art pretrained models. The combination of these factors has lead researchers to reproduce previous results more easily, investigate current approaches and test hypotheses without having to redevelop them first, and focus their efforts on formulating and testing new hypotheses. To bring Transfer Learning methods and large-scale pretrained Transformers back into the realm of these best practices, the authors (and the community of contributors) have developed Transformers, a library for state-of-the art",
            " module to retain the attention on the salient parts while weakening the attention on the irrelevant parts of input. As illustrated in Figure FIGREF5, the attention weight distribution $a_t$ at timestep $t$ (the first red histogram) is fed through the ARU module. In the ARU module, current decoding state $s_t$ and attention distribution $a_t$ are combined to calculate a refinement gate $r_t$: where $\\sigma $ is",
            " their gratitude to Elias Wright, Gian Mascioli, Kiara Pillay, Harrison Kay, Eliel Talo, Alexander Fagella and N. Cameron Russell for providing their valuable expertise and insight to this effort. The authors are also grateful to Eduard Hovy, Lorrie Cranor, Florian Schaub, Joel Reidenberg, Aditya Potukuchi and Igor Shalyminov for helpful discussions related to this work, and to the three anonymous reviewers of this draft for",
            "2 . Early systems were based on handcrafted rule-based algorithms, while recently several contributions by Machine Learning scientists have helped in integrating probabilistic models into NER systems. In particular, new developments in neural architectures have become an important resource for this task. Their main advantages are that they do not need language-specific knowledge resources BIBREF3 , and they are robust to the noisy and short nature of social media messages BIBREF4 . Indeed, according to a performance analysis of several Named",
            " conditions with entailment/contradiction, we did not run all data size conditions given our limited computational resources. Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with the",
            ".tst2013) was achieved using a four-layer encoder–decoder QRNN with 320 units per layer, no dropout or INLINEFORM0 regularization, and gradient rescaling to a maximum magnitude of 5. Inputs were supplied to the encoder reversed, while the encoder convolutions were not masked. The first encoder layer used convolutional filter width INLINEFORM1 , while the other encoder layers used INLINEFORM2 . Optimization was performed for",
            " close to the baseline level, with slight improvements in German. A first observation is that this method brings stupid BT models closer to conventional BT, at a greatly reduced computational cost. While French still remains 0.4 to 1.0 BLEU below very good backtranslation, both approaches are in the same ballpark for German - may be because BTs are better for the former system than for the latter. Finally note that the GAN architecture has two differences with basic copy-marked: (a)"
        ]
    },
    {
        "title": "Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents",
        "answer": "F1-score.",
        "evidence": [
            " setup ::: Evaluation metrics ::: Information extraction-oriented metrics.\nThese metrics estimate the ability of our model to integrate elements from the table in its descriptions. Particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ. To do so, we follow the protocol presented in BIBREF10. First, we apply an information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset. Entity-value pairs",
            " the applicability of the methods in real forensic settings. Then, we proposed several properties that enable a better characterization and by this a better comparison between AV methods. Among others, we explained that the performance measure AUC is meaningless in regard to unary or specific non-optimizable AV methods, which involve a fixed decision criterion (for example, NNCD). Additionally, we mentioned that determinism must be fulfilled such that an AV method can be rated as reliable. Moreover, we clarified a number",
            " purposes, since it enables us to estimate the number of commonsense inference questions. For questions that did not make sense at all given a text, unfitting was assigned. If a question made sense for a text, but it was impossible to find an answer, the label unknown was used. In a second step, we told participants to formulate a plausible correct and a plausible incorrect answer candidate to answerable questions (text-based or script-based). To level out the effort between answerable and non",
            " - UID21 illustrates the results on each challenge, all competitors are ordered in score's descending order (higher is better). The achieved performance of our approach is marked with a horizontal line on each figure. Figure UID22 briefly describes each challenge and summarizes our performance on each contest; also, we added three standard measures to simplify the insight's creation of the reader. The winner method in SENTIPOLC'14 (Italian) is reported in BIBREF22 . This method uses three groups of",
            " performances by applying the following three-tiered feature elimination approach: Reduction We reduced the dataset encoded for each class by eliminating features that occur less than twice in the full dataset. Selection We iteratively applied Chi-Square feature selection on the reduced dataset, selecting the top percentile of highest ranked features in increments of 5 percent to train and test the support vector model using a linear kernel and 5-fold, stratified cross-validation. Rank We cumulatively plotted the average F1-score performances",
            "064 speech utterances produced by 1,268 speakers for a total of 70 hours of speech. Training data by show, medium and speech type is summarized in Table and evaluation data in Table . Evaluation data has a higher variety of shows with both prepared (P) and spontaneous (S) speech type (accented speech from African radio broadcast is also included in the evaluation set).\n\n\nMethodology ::: Methodology for descriptive analysis of gender representation in training data\nWe first describe the gender",
            "the-art performance, they still heavily relied on hand-crafted features. A few researches explored to directly measure similarity based on distributed representations. BIBREF5 trained a language model based on convolutional neural networks to project sentences onto distributed representations. BIBREF26 treated single document summarization as a sequence labeling task and modeled it by the recurrent neural networks. Others like BIBREF9 , BIBREF13 just used the sum of trained word embeddings to represent sentences or documents",
            ", provide qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.\n\n\n",
            " ideal points of -0.797 and -0.739, respectively. Therefore, both sources of data appear to provide useful signals of different aspects of underlying state preferences. Before proceeding to the description of our word embeddings approach, it is useful to first explore the corpus through commonly used measures of disagreement, namely Wordscore and Euclidean distance. Further, the network polarization measure known as modularity is also used to explore the levels of polarization exhibited in roll call data versus speeches. First",
            " products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\" (MOS) BIBREF23 , BIBREF24 to measure the effectiveness: how similar a response were comparing to the correct product's usage. The MOS score range is 1-5 (1-Bad, 2",
            " datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size influences.",
            " spam and detecting sentiment. Many tasks are motivated by applications, for example to automatically block online trolls. Success, then, is often measured by performance, and communicating why a certain prediction was made—for example, why a document was labeled as positive sentiment, or why a word was classified as a noun—is less important than the accuracy of the prediction itself. The approaches we use and what we mean by `success' are thus guided by our research questions. Domain experts and fellow researchers can provide feedback"
        ]
    },
    {
        "title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled Data",
        "answer": "Hurricane Katrina, Odisha Cyclone, New Orleans Cyclone, Bomb Cyclone, Blizzard.  # 5 natural disasters were examined.  Hurricane Katrina, Odisha Cyclone, New Orleans Cyclone, Bomb Cyclone, Blizzard.  # 5 natural disasters were examined.  Hurricane Katrina, Odisha Cyclone, New Orleans Cyclone, Bomb Cyclone, Blizzard.  # 5 natural disasters were examined.  Hurricane Katrina, Odisha Cyclone, New Orleans Cyclone, Bomb Cyclone, Blizzard.  # 5 natural disasters were examined.  Hurricane Katrina, Odisha Cyclone, New Orleans Cyclone, Bomb Cyclone, Blizzard.  # 5 natural disasters were examined.  Hurricane Katrina, Odisha Cyclone, New Orleans Cyclone, Bomb Cyclone, Blizzard.  # 5 natural disasters were examined.  Hurricane Katrina, Odisha Cyclone, New Orleans Cyclone, Bomb Cyclone, Blizzard.  # 5 natural disasters were examined.  Hurricane Katrina, Odisha Cyclone, New Orleans Cyclone, Bomb Cyclone, Blizzard.  # 5 natural disasters were examined.  Hurricane Katrina, Odisha Cyclone, New Orleans Cyclone, Bomb Cyclone, Blizzard.  # 5 natural",
        "evidence": [
            "�negative\" sample (labeled as -1 in the data). All data were downloaded from Twitter in two separate batches using the “twint\" scraping tool BIBREF5 to sample historical tweets for several different search terms; queries always included either “climate change\" or “global warming\", and further included disaster-specific search terms (e.g., “bomb cyclone,\" “blizzard,\" “snowstorm,\" etc.). We refer to the first data",
            " Initiative\nIn 2015, the United Nations Development Programme (UNDP) Regional Hub for Europe and CIS launched a Fragments of Impact Initiative (FoI) that helped to collect qualitative (micro-narratives) and quantitative data from multiple countries. Within a six-month period, around 10,000 interviews were conducted in multiple languages. These covered the perception of the local population in countries including Tajikistan, Yemen, Serbia, Kyrgyzstan and Moldova on peace and reconciliation,",
            "\nAcknowledgments\nThis work was funded by the Amazon Academic Research Awards program.\n\n\n",
            " we selected 96 comments/forum posts, 8 blog posts, and 8 articles for this phase. A detailed inter-annotator agreement study on documents from this final phase will be reported in section UID75 . The annotations were very time-consuming. In total, each annotator spent 35 hours by annotating in the course of five weeks. Discussions and consolidation of the gold data took another 6 hours. Comments and forum posts required on average of 4 minutes per document to annotate, while blog posts",
            " project was to gather insights in cross-cultural aspects of emotional reactions. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of seven major emotions (joy, fear, anger, sadness, disgust, shame and guilt). In each case, the questions covered the way they had appraised a given situation and how they reacted. The final dataset contains reports by approximately 3000 respondents from all over the world, for a total of 7665 sentences labelled with",
            " Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when facts",
            " across nigeria between'). 3% of questions have an unresolvable coreference (e.g., `how do i get to Warsaw Missouri from here'). 4% of questions are vague, and a further 7% have unknown sources of error. 2% still contain false presuppositions (e.g., `what is the only fruit that does not have seeds?') and the remaining 42% do not have an answer within the document. This reinforces our belief that though they have been under",
            " We performed 10-fold cross-validation for both of the datasets. We used the approximate randomisation technique to test whether our results are statistically significant. Here, we tried to predict the labels of reviews and assess the performance. We obtained varying accuracies as shown in Table TABREF17. “3 feats\" features are those hand-crafted features we extracted, which are the minimum, mean, and maximum polarity scores of the reviews as explained in Section 3.5. As can",
            " of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as",
            "\" fashion. As climate change becomes more extreme, it remains to be seen what degree of predictive power exists in our current model regarding climate change sentiments with regards to natural disasters.\n\n\n",
            " recent example of an event that made it easier or harder to support how your family lives.” While the analysis and visualization of quantitative data was not problematic, systematic analysis and visualization of qualitative data, collected in a format of micro-narratives, would have been impossible. To find a way to deal with the expensive body of micro-narrative data, UNDP engaged a group of students from the School of Public Policy, University College London, under the supervision of Prof Slava M",
            "wings did not have an `Accidents' section before this year's disaster, which was the first in the history of the airline. Even if sections are missing for certain entities, similar sections usually occur in other entities of the same class (e.g. other airlines had disasters and therefore their pages have an accidents section). We exploit such homogeneity of section structure and construct templates that we use to expand entity profiles. The learning objective for INLINEFORM0 takes into account the following properties: Section"
        ]
    },
    {
        "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding",
        "answer": "two knowledge graph completion tasks",
        "evidence": [
            " completion tasks, we experimentally validate LAN's superiority in terms of the desired properties.\n\n\nIntroduction\nKnowledge graphs (KGs) such as Freebase BIBREF0 , DBpedia BIBREF1 , and YAGO BIBREF2 play a critical role in various NLP tasks, including question answering BIBREF3 , information retrieval BIBREF4 , and personalized recommendation BIBREF5 . A typical KG consists of numerous facts about a predefined set of entities",
            " these tasks can be facilitated. Other machine translation approaches can also be tested, including the concatenation of this corpus with other multi-domain ones.\n\n\n",
            " and then asked to conjecture questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment to many users, we maintain a consistent set of video segments",
            "2Seq, HLSTM, and HLSTM+Copy) have predicted improper entities (cake), generated repetitive contents (her family), or copied wrong words (eat). The models equipped with incremental encoding or knowledge through MSA(GA/CA) perform better in this example. The ending by IE+MSA is more coherent in logic, and fluent in grammar. We can see that there may exist multiple reasonable endings for the same story context. In order to verify the ability of our",
            " This is analogous to games with larger maps in the RL literature, where the environment is partially observable. A better exploration (in our case, jumping) strategy may help QA-DQN to master such harder games.\n\n\nExperimental Results ::: Generalizing to Test Set\nTo study QA-DQN's ability to generalize, we select the best performing agent in each experimental setting on the validation set and report their performance on the test set. The agent's test performance",
            " work can be performed to systematize large quantities of text. Additionally, we can learn about the sentiment that specific groups express towards specific topics. Networks can be uncovered, the spread of information or ideas can be traced, and influential actors identified. We can classify documents based on human coding of a subset of documents and establish which topics predict/correlate with predefined outcomes such as successful employment or completion of a program. While the application used here to illustrate the discussion focuses on texts in the form",
            " and playful manner. Built by the authors on top of Transformers, Write with Transformer is an interactive interface that leverages the generative capabilities of pretrained architectures like GPT, GPT2 and XLNet to suggest text like an auto-completion plugin. Generating samples is also often used to qualitatively (and subjectively) evaluate the generation quality of language models BIBREF9. Given the impact of the decoding algorithm (top-K sampling, beam-search, etc.) on",
            ", they do not attempt to learn execution models for those predicates, nor can they answer questions using those predicates. Yao and Van Durme yao-2014-info-extraction-freebase-qa and Dong et al. dong-2015-freebase-qa-mccnn proposed question answering models that use similar features to those used in this work. However, they did not produce semantic parses of language, instead using methods that are non-compositional and do",
            " of which pages to select posts from is far from trivial, and we believe this is actually an interesting aspect of our approach, as by using different Facebook pages one can intrinsically tackle the domain-adaptation problem (See Section SECREF6 for further discussion on this). The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking",
            " as entities during the matching, and when the string similarity thresholds increase, accepting only candidates with an higher degree of similarity. The behaviour of the Recall score is inverted. Finally, we test the impact of using the schedule matching together with a biLSTM-CRF network. In this experiment, we consider the network trained using all the features proposed, and the embeddings not pre-trained. Table 8 reports the results obtained. We can observe how generally the system benefits from the use of",
            " neighbors. However, their neighborhood aggregators neglect the unordered and unequal natures of an entity's neighbors. To this end, we summarize the desired properties that may lead to effective neighborhood aggregators. We also introduce a novel aggregator, namely, Logic Attention Network (LAN), which addresses the properties by aggregating neighbors with both rules- and network-based attention weights. By comparing with conventional aggregators on two knowledge graph completion tasks, we experimentally validate LAN's superiority in terms of the desired",
            "% of instances have an answer that is exactly the same as the corresponding subject/object in the extracted triplets. To do knowledge-based MRC, We build a new dataset consisting of 40,047 examples for the knowledge based MRC task. The annotation of this dataset is designed so that successfully answering the questions requires understanding and the knowledge involved in a document. Each instance is composed of a question, a set of triplets derived from a document, and the answer. We implement a framework consisting"
        ]
    },
    {
        "title": "The First Evaluation of Chinese Human-Computer Dialogue Technology",
        "answer": "Accuracy, F1-score, c@1, AUC, AUC@1, EER, MAP, MRR, grammar, logicality, task completion ratio, user satisfaction degree, response fluency.",
        "evidence": [
            " setup ::: Evaluation metrics ::: Information extraction-oriented metrics.\nThese metrics estimate the ability of our model to integrate elements from the table in its descriptions. Particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ. To do so, we follow the protocol presented in BIBREF10. First, we apply an information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset. Entity-value pairs",
            " our extensive literature research, numerous measures (e. g., Accuracy, F INLINEFORM0 , c@1, AUC, AUC@1, INLINEFORM1 or EER) have been used so far to assess the performance of AV methods. In regard to our experiments, we decided to use c@1 and AUC for several reasons. First, Accuracy, F INLINEFORM2 and INLINEFORM3 are not applicable in cases where AV methods leave verification problems unanswered,",
            ", provide qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.\n\n\n",
            "�I feel so depressed!”, “What a bad day!”, etc. There may be infinite number of responses for an open domain messages. Hence, it is difficult to construct a gold standard (usually a reference set) to evaluate a response which is generated by an open domain chit-chat system. For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc.,",
            "Abstract\nSentence Boundary Detection (SBD) has been a major research topic since Automatic Speech Recognition transcripts have been used for further Natural Language Processing tasks like Part of Speech Tagging, Question Answering or Automatic Summarization. But what about evaluation? Do standard evaluation metrics like precision, recall, F-score or classification error; and more important, evaluating an automatic system against a unique reference is enough to conclude how well a SBD system is performing given the final application of the",
            " to it, we extract three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores, from each review. These polarity scores of words are computed as in (DISPLAY_FORM4). For example, if a review consists of five words, it would have five polarity scores and we utilise only three of these sentiment scores as mentioned. Lastly, we concatenate these three scores to the averaged word vector per review. That is, each review is",
            " public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth. Besides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human experts.",
            ". For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency:",
            " a higher score than the best results achieved in the medical challenge at LiveQA'17. Evaluating the RQE system alone is not relevant, as applying RQE on the full collection for each user question is not feasible for a real-time system because of the extended execution time.\n\n\nEvaluation of the top ten answers\nIn this evaluation, we used Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) which are commonly used in QA",
            " unlabeled mini-batches INLINEFORM0 used for training between supervised mini-batches. Third, rather than using the same dropout rate for the local representation vectors, a new dropout rate is assigned. The details of hyperparameters such as the hidden size of each layer and dropout rate are given in Section SECREF7 .\n\n\nEvaluation\nDuring the evaluation, each task is assessed using different metrics based on previous works. For Thai sentence segmentation, three",
            " reference endings.\n\n\nManual Evaluation\nManual evaluations are indispensable to evaluate the coherence and logic of generated endings. For manual evaluation, we randomly sampled 200 stories from the test set and obtained 1,600 endings from the eight models. Then, we resorted to Amazon Mechanical Turk (MTurk) for annotation. Each ending will be scored by three annotators and majority voting is used to select the final label. We defined two metrics - grammar and logicality for manual evaluation. Score 0",
            "itatively evaluate and compare the ML models in combination with a relevance decomposition or explanation technique, we apply the evaluation method described in section \"Measuring Model Explanatory Power through Extrinsic Validation\" . That is, we compute the accuracy of an external classifier (here KNN) on the classification of document summary vectors (obtained with the ML model's predicted class). For these experiments we remove test documents which are empty or contain only one word after preprocessing (this amounts to remove"
        ]
    },
    {
        "title": "BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization",
        "answer": "manually.",
        "evidence": [
            " data. However, it is unrealistic to create all the templates manually since this work requires considerable domain knowledge and is also labor-intensive. Fortunately, the summaries of some specific training articles can provide similar guidance to the summarization as hard templates. Accordingly, these summaries are referred to as soft templates, or templates for simplicity, in this paper. Despite their potential in relieving the verbosity and insufficiency problems of natural language data, templates have not been exploited to full advantage. For example",
            " features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some",
            " role of the discriminator, in turn, is to tell apart which data points are ‘real’ and which have been created by the generator. On each run through the model, the generator then adapts its constructed output so as to more effectively ‘trick’ the discriminator into not being able to distinguish the real from the generated data. The end-to-end nature of the model then forces both the generator and discriminator to learn in parallel [7]. While G",
            " new entities unseen during training. For the task of triplet classification, we directly use the datasets released by BIBREF9 ijcai2017-250 which are based on WordNet11 BIBREF29 . Since they do not conduct experiments on the link prediction task, we construct the required datasets based on FB15K BIBREF11 following a similar protocol used in BIBREF9 ijcai2017-250 as follows. Sampling unseen entities. Firstly, we randomly sample INLINE",
            " text, it is fully saved in the pattern, otherwise the POS tags are used for matching. For example, the original sentence `We went shop on Saturday' and the corrected version `We went shopping on Saturday' would produce the following pattern: (VVD shop_VV0 II, VVD shopping_VVG II) After collecting statistics from the background corpus, errors can be inserted into error-free text. The learned patterns are now reversed, looking for the correct side of the tuple",
            " addition of artificial data to the training process was evaluated on three error detection annotations, using the FCE and CoNLL 2014 datasets. Making use of artificial data provided improvements for all data generation methods. By relaxing the type restrictions and generating all types of errors, our pattern-based method consistently outperformed the system by Felice2014a. The combination of the pattern-based method with the machine translation approach gave further substantial improvements and the best performance on all datasets.\n\n\n",
            " the system to learn the unique weighted combinations of the aforementioned 2,497 features that each decision tree applies to make a prediction. At test time, given a novel visual question, the trained system converts a 2,497 feature descriptor of the visual question into a final prediction that reflects the majority vote prediction from the ensemble of decision trees. The system returns the final prediction along with a probability indicating the system's confidence in that prediction. We employ the Matlab implementation of random forests, using 25 trees and the",
            " the template. As illustrated in Figure FIGREF6 , this module consists of a Convolution Encoder Block, a Similarity Matrix and a Pooling Layer. Convolution Encoder Block. This block maps the input article and its candidate templates into high-level representations. The popular ways to this are either by using recurrent neural network (RNN) or a stack of convolutional neural network (CNN), while none of them are suitable for our problem. This is because a source article is usually",
            " is the fact that two different class domains are involved. On the one hand, there is the class domain of authors, where the task is to distinguish INLINEFORM7 and INLINEFORM8 . On the other hand, there is the elevated or lifted domain of verification problem classes, which are Y and N. The training phase of binary-intrinsic approaches is used for learning to distinguish these two classes, and the verification task can be understood as putting the verification problem as a whole into",
            " high.  An approach widely used in the neural network community is FineTune. We first train the model with the source dataset and then it is used as the initial parameters for training the model with the target dataset. The training process is stopped in reference to the development set in order to avoid over-fitting. We could extend this method by posing a regularization term (e.g. $l_2$ regularization) in order not to deviate from the pre-trained parameter.",
            " than classifier-based models at embedding previously unseen words, our best embeddings are still much poorer for unseen words. Improvements in this direction may come from larger training sets, or may require new models that better model the shared structure between words. Other directions for future work include additional forms of supervision and training, as well as application to downstream tasks.\n\n\n",
            " and thus we had to build our own. To this end, we searched the web for publicly-available templates of property lease agreements and found 115 templates in total. For each template, we manually generated one, two, or three instances, using a fake data generator tool to fill in the missing content elements such as addresses. In total, we created (and annotated) 223 contracts by hand. This corpus was further split into training, validation, and test data with a 6:2:2"
        ]
    },
    {
        "title": "AttSum: Joint Learning of Focusing and Summarization with Neural Attention",
        "answer": "He BIBREF29 and Chisholm BIBREF6, NTEE BIBREF16, Local models: He BIBREF29 and Chisholm BIBREF6, three types of global models: Local models: He BIBREF29 and Chisholm BIBREF6, three types of global models: NTEE BIBREF16, Local models: He BIBREF29 and Chisholm BIBREF6, three types of global models: NTEE BIBREF16, Local models: He BIBREF29 and Chisholm BIBREF6, three types of global models: NTEE BIBREF16, Local models: He BIBREF29 and Chisholm BIBREF6, three types of global models: NTEE BIBREF16, Local models: He BIBREF29 and Chisholm BIBREF6, three types of global models: NTEE BIBREF16, Local models: He BIBREF29 and Chisholm BIBREF6, three types of global models: NTEE BIBREF16, Local models: He BIBREF29 and Chisholm BIBREF6, three types of global models:",
        "evidence": [
            " is roughly 3x faster to train than (S7) using the same GPU resources, so (E1) is also 1.5x faster to train than a single model (S7).\n\n\n",
            " products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\" (MOS) BIBREF23 , BIBREF24 to measure the effectiveness: how similar a response were comparing to the correct product's usage. The MOS score range is 1-5 (1-Bad, 2",
            "bert are not directly comparable, because these models have different coverage. In particular, Multi-SimLex contains some out-of-vocabulary (OOV) words whose static ft embeddings are not available. On the other hand, m-bert has perfect coverage. A general comparison between CC+Wiki and Wiki ft vectors, however, supports the intuition that larger corpora (such as CC+Wiki) yield higher correlations. Another finding is that a single massively multilingual model such as m",
            ", provide qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.\n\n\n",
            "Having observed an unideal performance on this task (see Experiments below), we turned our attention to building a model that can replicate the writing style of high popularity listing descriptions. To solve this task, we designed a framework for a general adversarial network. This model employs the standard set up of a generator and a discriminator, but extends the framework with the adoption of the Diehl-Martinez-Kamalu loss. The generator is designed as a feed-forward neural network with",
            " access to the gold-standards of SENTIPOLC'14, TASS'15 and SemEval 2015 & 2016, respectively. The authors also thank Elio Villaseñor for the helpful discussions in early stages of this research.\n\n\n",
            " $ Our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture Wiseman, reinforcing the crucial role of structure in data semantics and saliency. The analysis of RG metrics shows that Wiseman seems to be the more naturalistic in terms of number of factual mentions (RG#) since it is the closest scenario to the gold value (16.83 vs. 17.31 for resp. Wiseman and Hierarchical-k). However, Wiseman achieves only $75.",
            " key modules in NCEL and give qualitative results for comprehensive analysis.\n\n\nBaselines and Datasets\nWe compare NCEL with the following state-of-the-art EL methods including three local models and three types of global models: Local models: He BIBREF29 and Chisholm BIBREF6 beat many global models by using auto-encoders and web links, respectively, and NTEE BIBREF16 achieves the best performance based on joint embeddings",
            " model has much steeper slope and converges to a much better minima in both the tasks. It can also be noticed that in the training procedure for the model which has both attention and conflict, the updates are much smoother.\n\n\nQualitative Comparison\nWe also show qualitative results where we can observe that our model with attention and conflict combined does better on cases where pairs are non-duplicate and has very small difference. We have observed that the conflict model is very sensitive to even minor",
            "REF7 ), both baselines perform worse on questions that require the understanding deep semantics and userID&hashtags. The former kind of questions also appear in other benchmarks and is known to be challenging for many current models. The second kind of questions is tweet-specific and is related to specific properties of social media data. Since both models are designed for formal-text passages and there is no special treatment for understanding user IDs and hashtags, the performance is severely limited on the questions requiring such reasoning abilities",
            " areas under the ROC and PR curves for the three models of primary concern are presented in Figure FIGREF36 and Figure FIGREF37 , respectively. Furthermore, the dyad-wise and edge-wise shared partners and modularities are presented for all four models in Figure FIGREF38 and Figure FIGREF39 . Although the original models from BIBREF8 and our textual and multiplex models all exhibit impressive GOFs, the multiplex model exhibits the best in-sample GOF as measured by",
            " conditions with entailment/contradiction, we did not run all data size conditions given our limited computational resources. Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with the"
        ]
    },
    {
        "title": "A Simple Approach to Multilingual Polarity Classification in Twitter",
        "answer": "part-of-speech tagging, named entity recognition, dependency parsing, semantic role labeling, subject-verb-object triple extraction, and lexicon matching.  claim, premise, backing, rebuttal, and refutation.  users, Wizard of Oz (WOz) framework, MultiWOZ, transformers, Spacy, AllenNLP, Flair, fusional, agglutinative, isolating, introflexive, languages, language families, universal phoneset, FST decoder, language model, linear hidden unit contribution (LHUC), decoder, encoder, auxiliary input, hidden unit outputs, LHUC technique, multilingual CTC, language adaptive vectors, auxiliary input, decoder, encoder, auxiliary input, hidden unit outputs, LHUC technique, multilingual CTC, language adaptive vectors, auxiliary input, decoder, encoder, auxiliary input, hidden unit outputs, LHUC technique, multilingual CTC, language adaptive vectors, auxiliary input, decoder, encoder, auxiliary input, hidden unit outputs, LHUC technique, multilingual CTC, language adaptive vectors, auxiliary input, decoder, encoder, auxiliary input, hidden unit outputs, LHUC technique, multilingual CTC, language adaptive vectors, auxiliary input, decoder, encoder, auxiliary input, hidden unit outputs",
        "evidence": [
            " work.\n\n\nFramework for Processing Portuguese Text\nThe framework for processing Portuguese texts is depicted in Fig. FIGREF2, which illustrates how relevant pieces of information are extracted from the text. Namely, input files (Portuguese texts) go through a series of modules: part-of-speech tagging, named entity recognition, dependency parsing, semantic role labeling, subject-verb-object triple extraction, and lexicon matching. The main goal of all the modules except lexicon matching is",
            " in an analogous way to how language ID is tagged in multilingual g2p.\n\n\n",
            " the multilingual model is trained on a much smaller set of languages (17 versus 100), and further improvements can be achieved by training a dedicated bilingual model. Finally, leveraging bilingual parallel data seems to offer additional slight gains, but a tiny difference between xlm-2 and xlm-2++ also suggests that this rich bilingual information is not used in the optimal way within the xlm architecture for semantic similarity. In summary, these results indicate that, in order to improve performance in cross",
            " “weaker”). We call the model as a modified Toulmin's model. It contains five argument components, namely claim, premise, backing, rebuttal, and refutation. When annotating a document, any arbitrary token span can be labeled with an argument component; the components do not overlap. The spans are not known in advance and the annotator thus chooses the span and the component type at the same time. All components are optional (they do not have to be present",
            " BIBREF17, BIBREF14.\n\n\nRelated work ::: The Wizard of Oz (WOz) Approach and MultiWOZ\nThe WOz framework, first introduced by BIBREF12 as a methodology for iterative design of natural language interfaces, presents a more effective approach to human-human dialog collection. In this setup, users are led to believe they are interacting with an automated assistant but in fact it is a human behind the scenes that controls the system responses",
            " with a wrapper in several popular NLP frameworks such as Spacy BIBREF20, AllenNLP BIBREF21 or Flair BIBREF22. Transformers is an ongoing effort maintained by the team of engineers and research scientists at HuggingFace, with support from a vibrant community of more than 120 external contributors. We are committed to the twin efforts of developing the library and fostering positive interaction among its community members, with the ambition of creating the standard library for modern deep learning NLP.",
            " (by number of speakers of the language) for maximum impact of the resource, while simultaneously having a diverse suite of languages based on their typological features (such as morphological type and language family). Table TABREF10 summarizes key information about the languages currently included in Multi-SimLex. We have included a mixture of fusional, agglutinative, isolating, and introflexive languages that come from eight different language families. This includes languages that are very widely used such as",
            " a universal phoneset, FST decoder and language model. The authors also use linear hidden unit contribution (LHUC) BIBREF12 technique to rescale the hidden unit outputs for each language as a way to adapt to a particular language. Another work BIBREF13 on multilingual CTC shows the importance of language adaptive vectors as auxiliary input to the encoder in multilingual CTC model. The decoder used here is a simple INLINEFORM0 decoder. An extensive",
            " Section SECREF3 describes the datasets and contests used to test our approach; whereas, the experimental results, and, the discussion are presented on Section SECREF4 . Finally, Section SECREF5 concludes.\n\n\nOur Approach: Multilingual Polarity Classification\nWe propose a method for multilingual polarity classification that can serve as a baseline as well as a framework to build more complex sentiment analysis systems due to its simplicity and availability as an open source software. As we mentioned, this baseline algorithm for",
            " for computing cookie. In order to solve the ambiguity, we propose to take advantage of the built ontology tree. In detail, SimplerVoice uses the joint verb set of 3 types of nouns: the input object, the parents, and the neighbors as the 3 noun types are always in the same context of ontology. Equation EQREF8 shows the word-sense disambiguation mechanism with INLINEFORM0 (Object) indicates the verb set of an object generated by heur",
            " These steps reflect the recurring questions that arise when building an NLP pipeline: defining the model architecture, processing the text data and finally, training the model and performing inference in production. In the following section, we'll give an overview of the three base components of the library: configuration, model and tokenization classes. All of the components are compatible with PyTorch and TensorFlow (starting 2.0). For complete details, we refer the reader to the documentation available on https://hug",
            " as a distinct description of words. Our expectation is that the combination of these embedding versions, trained by different NNs on different corpora, should contain more information than each version individually. We want to leverage this diversity of different embedding versions to extract higher quality sentence features and thereby improve sentence classification performance. The letters “M” and “V” in the name “MVCNN” of our architecture denote the multichannel and variable-size convolution filters"
        ]
    },
    {
        "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing",
        "answer": "S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2",
        "evidence": [
            " years, (ii) the entity class specific performance, and (iii) the impact on entity profile expansion by suggesting missing sections to entities based on the pre-computed templates. Baselines. To the best of our knowledge, we are not aware of any comparable approach for this task. Therefore, the baselines we consider are the following: S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2 ",
            ":: Tweet-Level Models ::: Baseline GRU.\nOur baseline is a GRU network for each of the three tasks. We use the same network architecture across the 3 tasks. For each network, the network contains a layer unidirectional GRU, with 500 units and an output linear layer. The network is trained end-to-end. Our input embedding layer is initialized with a standard normal distribution, with $\\mu =0$, and $\\sigma =1$, i",
            " of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as",
            "kit.\n\n\nBaseline Decoder\nOur baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations:\n\n\nDecoder Speed Improvements\nThis section describes a number of speedups that can be made to a CPU-based attentional sequence-to-sequence beam decoder. Crucially, none of these speedups affect the actual mathematical computation of the decoder, so they can be applied to any network architecture with a guarantee that they will not affect the",
            " top images result from Google and Bing. We also provided the product title along with the images. SimplerVoice: We shown the generated key messages (Tab. TABREF19 ), and visual description including 2 components: photorealistics images and pictographs (Fig. FIGREF20 ) from SimplerVoice system. Intuitively, baseline 1 shows how much information a user would receive from the products' packages without prior knowledge of the products while baseline 2 might provide additional information by showing top images",
            " of randomly selected out-of-domain natural sentences, with the same architecture and training parameters, running validation every 2000 updates with a patience of 10. Since BPE units are selected based only on the out-of-domain statistics, fine-tuning is performed on sentences that are slightly longer (ie. they contain more units) than for the initial training. This system defines an upper-bound of the translation performance and is denoted below as natural. Our baseline and topline results are",
            " a baseline model. A pictorial representation of this ontology is given in Figure FIGREF16 Considering the criminal law domain case study, we made a few changes to the original SEM ontology. The entities of the model are: Actor: person involved with event Place: location of the event Time: time of the event Object: that actor act upon Organization: organization involved with event Currency: money involved with event The proposed ontology was designed in such a manner that it can incorporate information extracted from multiple",
            " out of the Internet Archive—we should be open about the limitations of our analyses, acknowledging the flaws in our data and drawing cautious and reasonable conclusions from them. In all cases, we should report the choices we have made when creating or re-using any dataset.\n\n\nCompiling data\nAfter identifying the data source(s), the next step is compiling the data. This step is fundamental: if the sources cannot support a convincing result, no result will be convincing. In many cases, this",
            "Labeled Question Types\nBesides the analysis on different reasoning types, we also look into the performance over questions with different first tokens in the development set, which provide us an automatic categorization of questions. According to the results in Table TABREF46 , the three neural baselines all perform the best on “Who” and “Where” questions, to which the answers are often named entities. Since the tweet contexts are short, there are only a small number of named entities to",
            " a higher KL loss in the inputless setting than the standard setting, as $\\mathbf {z}$ is required to encode more information about the input data for reconstruction. In terms of overall performance, our model outperforms all the baselines in both datasets (i.e., PTB and E2E). For instance, when comparing with the strongest baseline vMF-VAE in the standard setting, our model reduces NLL from 96 to 79 and PPL from 98 to 43 in",
            ". Unlike Baseline1 which avoids an explicit segmentation step, the Pointer-network can explicitly model which sentences are likely to be a boundary sentence. Moreover, the search space of the spans in the Pointer-network is $2n$ where $n$ is the number of sentences, because it selects only two boundary sentences. Note that the search space of Baseline1 is $n^2$. A much smaller search space might improve the accuracy by making the model consider fewer candidates.",
            " classification systems based on each classifier's predictions for the 121,512 visual questions in the test dataset. We first show performance of the baseline and our two prediction systems using precision-recall curves. The goals are to achieve a high precision, to minimize wasting crowd effort when their efforts will be redundant, and a high recall, to avoid missing out on collecting the diversity of accepted answers from a crowd. We also report the average precision (AP), which indicates the area under a precision-recall"
        ]
    },
    {
        "title": "Story Ending Generation with Incremental Encoding and Commonsense Knowledge",
        "answer": "seq2seq, mixed RNN model, LEAD, QUERY_SIM, MARN (no $\\mathcal {A}$ ), MARN, S1, S2, S4, S6, wFST.",
        "evidence": [
            " length due to the parsing labels; we still use a maximum output length of 50 subwords for these systems.\n\n\nBaselines\nThe proposed models are compared against two baselines. The first, referred to here as seq2seq, is the standard RNN-based neural machine translation system with attention BIBREF0 . This baseline does not use the parsed data. The second baseline we consider is a slight modification of the mixed RNN model proposed by li2017modeling. This uses an",
            " P=0.17, with around 70% of instances used for training and the remainder for evaluation. For the remaining years the total improvement is INLINEFORM3 P=0.18 in contrast to the performance at year 2009. On the other hand, the baseline S1 has an average precision of P=0.12. The performance across the years varies slightly, with the year 2011 having the highest average precision of P=0.13. Always picking the most frequent section as in S",
            "?\n\n\nAcknowledgments\nThis research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada. We would like to thank Google Colab for providing support in terms of computational resources.\n\n\n",
            " the source is minimal, so we only perform ablation on the target. For the source, we use a 3-layer 512-dim bidi GRU in all models (S1)-(S6). Model (S1) and (S2) are one and two layer baselines. Model (S4), which uses 7 intermediate FC layers, has similar decoding cost to (S2) while doubling the improvement over (S1) to 1.2 BLEU. We",
            "Sum, we implement rich extractive summarization methods. Above all, we introduce two common baselines. The first one just selects the leading sentences to form a summary. It is often used as an official baseline of DUC, and we name it “LEAD”. The other system is called “QUERY_SIM”, which directly ranks sentences according to its TF-IDF cosine similarity to the query. In addition, we implement two popular extractive query-",
            " of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as",
            " classification systems based on each classifier's predictions for the 121,512 visual questions in the test dataset. We first show performance of the baseline and our two prediction systems using precision-recall curves. The goals are to achieve a high precision, to minimize wasting crowd effort when their efforts will be redundant, and a high recall, to avoid missing out on collecting the diversity of accepted answers from a crowd. We also report the average precision (AP), which indicates the area under a precision-recall",
            " of the coefficients $a_t$ are crucial is an important research question. From the results tables, we notice that the MARN (no $\\mathcal {A}$ ) baseline severely under-performs compared to MARN. This supports the importance of the attentions in the MAB. Without these attentions, MARN is not able to accurately model the cross-view dynamics. RQ3: In our experiments the MARN with only one attention (like conventional attention models)",
            " high.  An approach widely used in the neural network community is FineTune. We first train the model with the source dataset and then it is used as the initial parameters for training the model with the target dataset. The training process is stopped in reference to the development set in order to avoid over-fitting. We could extend this method by posing a regularization term (e.g. $l_2$ regularization) in order not to deviate from the pre-trained parameter.",
            " answer length being 3.74 compared to 2.13 for any other types. Also, since all the answers are written by humans instead of just spans from the context, these abstractive answers can make it even harder for current models to handle. We also observe that when people write “Why” questions, they tend to copy word spans from the tweet, potentially making the task easier for the matching baseline.\n\n\n",
            "-tuned BERT. Our full comparison including the BiDAF performance and evaluation on more metrics can be found in Appendix SECREF7 . Following previous RC research, we also include analysis on automatically-labeled question types in Appendix SECREF8 . As indicated by the results on METEOR and ROUGE-L (also indicated by a third metric, BLEU-1, as shown in Appendix SECREF7 ), both baselines perform worse on questions that require the understanding deep",
            " languages often map the same grapheme to phonemes that are only subtly different from each other. However, these statistics have not been widely reported for other g2p systems, so we omit them here.\n\n\nBaseline\nResults on LangID and NoLangID are compared to the system presented by deri2016grapheme, which is identified in our results as wFST. Their results can be divided into two parts: High resource results, computed with wF"
        ]
    },
    {
        "title": "Multi-Source Syntactic Neural Machine Translation",
        "answer": "0.5",
        "evidence": [
            " a higher KL loss in the inputless setting than the standard setting, as $\\mathbf {z}$ is required to encode more information about the input data for reconstruction. In terms of overall performance, our model outperforms all the baselines in both datasets (i.e., PTB and E2E). For instance, when comparing with the strongest baseline vMF-VAE in the standard setting, our model reduces NLL from 96 to 79 and PPL from 98 to 43 in",
            ", they do not attempt to learn execution models for those predicates, nor can they answer questions using those predicates. Yao and Van Durme yao-2014-info-extraction-freebase-qa and Dong et al. dong-2015-freebase-qa-mccnn proposed question answering models that use similar features to those used in this work. However, they did not produce semantic parses of language, instead using methods that are non-compositional and do",
            "sequence NMT; this model took parse trees as input using a tree-LSTM BIBREF10 encoder. bastings2017graph used a graph convolutional encoder in order to take labeled dependency parses of the source sentences into account. hashimoto2017neural added a latent graph parser to the encoder, allowing it to learn soft dependency parses while simultaneously learning to translate.\n\n\nLinearized Parse Trees in NMT\nThe idea of incorporating linearized pars",
            " or unsupervised approaches, and the quality of supervised systems trained on little data can directly impact semi-supervised workflows, for instance for the back-translation of monolingual data.\n\n\nAcknowledgments\nRico Sennrich has received funding from the Swiss National Science Foundation in the project CoNTra (grant number 105212_169888). Biao Zhang acknowledges the support of the Baidu Scholarship.\n\n\nHyperparameters\nTable TABREF23 lists",
            " is not correct, it tends to assign more probability mass to guesses that are more similar to the correct one. Applying some sort of filtering or reranking of the system output might therefore lead to better performance.\n\n\nFuture Work\nBecause the language ID token is so beneficial to performance, it would be very interesting to find ways to extend a similar benefit to unseen languages. One possible way to do so is with tokens that identify something other than the language, such as typological features about the language",
            " parsing models. We presented a technique to do this conversion in a way that is scalable using graph-based feature extraction methods. Our combined model achieved relative gains of over 50% in mean average precision and mean reciprocal rank versus a purely distributional approach. We also introduced a better mapping from surface text to logical forms, and a simple method for using a KB to find candidate entities during inference. Taken together, the methods introduced in this paper improved mean average precision on our task from .163 to .370",
            " Majority performs majority voting for classification tasks, and predicts the expected label for regression tasks. This baseline is useful as a lower bound of model performance. Human performance is calculated for CMU-MOSI dataset which offers per annotator results. This is the accuracy of human performance in a one-vs-rest classification/regression. Finally, MARN indicates our proposed model. Additionally, the modified baseline MARN (no MAB) removes the MAB and learns no dense cross-view dynamics",
            " discarded. It also tells us how much a filter with specific size influences. The block “tricks” (28–29) shows the system performance when no mutual-learning or no pretraining is used. The block “layers” (30–33) demonstrates how the system performs when it has different numbers of convolution layers. From the “layers” block, we can see that our system performs best with two layers of convolution in Standard Sentiment",
            "2 and compilers-4 which have the lowest i.ratio among the 12 courses none of the neural models better the reported baseline BIBREF7 (course level not scores not shown in this paper). The effect is pronounced in compilers-4 course where none of the neural models were able to predict any intervened threads. This is due to the inherent weakness of standard neural models, which are unable to learn features well enough when faced with sparse data. The best performance of UPA indicates that",
            " comment information. UTCNN without user information shows results similar to SVMs — it does well for Sup and Neu but detects no Uns. Its best f-scores on both Sup and Neu among all methods show that with enough training data, content-based models can perform well; at the same time, the lack of user information results in too few clues for minor-class posts to either predict their stance directly or link them to other users and posts for improved performance. The 17.",
            " free research tool with a Tesla K80 GPU and 12G RAM. Based on our experiments, we trained our classifier with a batch size of 32 for 3 epochs. The dropout probability is set to 0.1 for all layers. Adam optimizer is used with a learning rate of 2e-5. As an input, we tokenized each tweet with the BERT tokenizer. It contains invalid characters removal, punctuation splitting, and lowercasing the words. Based on the",
            " implicitly capture the dialog context state changes. For fine-grained analyses of the model performance, we further compute the test set perplexity per POS tag and per dialog act tag. We selected the most frequent POS tags and dialog act tags in SwDA corpus, and report the tag based perplexity relative changes ( $\\%$ ) of the proposed models comparing to Single-Turn-RNNLM. A negative number indicates performance gain. Table 2 shows the model perplexity per POS tag. All the"
        ]
    },
    {
        "title": "Domain Adaptation for Neural Networks by Parameter Augmentation",
        "answer": "191",
        "evidence": [
            " training example from the target domain ( INLINEFORM0 ), INLINEFORM1 learns to assign a lower score to INLINEFORM2 and INLINEFORM3 in that example, that is, to minimize INLINEFORM4 and INLINEFORM5 . On the other hand, given a training example from the source domain ( INLINEFORM6 ), INLINEFORM7 learns to assign a larger value to INLINEFORM8 and INLINEFORM9 . Furthermore, we update the parameters of the embed",
            " eight different language families. This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili. We hope to further include additional languages and inspire other researchers to contribute to the effort over the lifetime of this project. The work on data collection can be divided into two crucial phases: 1) a translation phase where the extended English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages",
            " article abstracts with crowdsourced annotations indicating snippets (sequences) that describe the Participants (p), Interventions (i), and Outcome (o) elements of the respective RCT, and the test set is composed of 191 abstracts with p, i, o sequence annotations from three medical experts. Table 1 shows an example of difficult and easy examples according to our definition of difficulty. The underlined text demarcates the (consensus) reference label provided by domain experts. In the difficult",
            " negative examples for all verbs that appear as a target verb in the test set. Another is construction-level (-Pattern), by removing all negative examples occurring in a particular syntactic pattern. We ablate a single construction at a time for -Pattern, from four non-local subject-verb dependencies (across a prepositional phrase (PP), subject RC, object RC, and long verb phrase (VP)). We hypothesize that models are less affected by token-level ablation, as knowledge",
            " insightful comments. This work was supported by the European Research Council (ERC StG DeepSPIN 758969) and by the Fundação para a Ciência e Tecnologia through contracts UID/EEA/50008/2013, PTDC/EEI-SII/7092/2014 (LearnBig), and CMUPERI/TIC/0046/2014 (GoLocal).\n\n\nProof of Proposition \nWe provide here a detailed proof",
            " new specialized domains? Specifically, how much annotated data do we need to achieve some (reasonable) level of accuracy? This is an important question due to the heterogeneity of business documents; it would be onerous if organizations were required to engage in large annotation efforts for every type of document. Second, how would a BERT model pre-trained on general natural language corpora perform in specific, and potentially highly-specialized, domains? There are aspects of this task that make it both easier and",
            " the “core” and all annotations as a “cluster”. The data were extended by Ghosh2014, who annotated “targets” and “callouts” on the top of the units. Park.Cardie.2014 annotated about 10k sentences from 1,047 documents into four types of argument propositions with Cohen's INLINEFORM0 0.73 on 30% of the dataset. Only 7% of the sentences were found to",
            ",976 images for training and 38,749 for validation. The selected pictures from the food domain are typically a close-up of foods or people eating some foods. Table 1 shows some captions from the food and non-food domain datasets. Table 2 shows the top twenty frequent words in the two datasets except for the stop words. We observe that the frequent words are largely different, but still there are some words common in both datasets. To model the image captaining, we use L",
            "Abstract\nFor many text classification tasks, there is a major problem posed by the lack of labeled data in a target domain. Although classifiers for a target domain can be trained on labeled text data from a related source domain, the accuracy of such classifiers is usually lower in the cross-domain setting. Recently, string kernels have obtained state-of-the-art results in various text classification tasks such as native language identification or automatic essay scoring. Moreover, classifiers based on string kernels have been",
            " as the target token, as shown in Table TABREF2. In addition, we reformulate each example into a commonsense reasoning “template” with two statements: hypothesis (from _ to the end of the original problem statement) and premise (the remaining part of the original problem statement). Note that the bold and colored fonts are for clarity only; those tokens are not marked in any way in the model input. At inference (test) time, we also decompose the problem into",
            "domain ten-fold cross validation for each of the six domains. Third, in order to evaluate the domain portability of our approach, we train the system on five domains and test on the remaining one for all six domains (which we report as cross-domain validation).\n\n\nIdentification of argument components\nIn the following experiment, we focus on automatic identification of arguments in the discourse. Our approach is based on supervised and semi-supervised machine learning methods on the gold data Toulmin dataset",
            " {D}^t$ . Since we are considering supervised settings, each element of the datasets has a form of input output pair $(x,y)$ . The goal of domain adaptation is to learn a function $f : \\mathcal {X} \\rightarrow \\mathcal {Y}$ that models the input-output relation of $D^t$ . We implicitly assume that there is a connection between the source and target distributions and thus can leverage the information of the source domain"
        ]
    },
    {
        "title": "Interactive Machine Comprehension with Information Seeking Agents",
        "answer": "next, previous, and Ctrl+F.",
        "evidence": [
            " and INLINEFORM1 generated by the encoder layer are combined using an attention mechanism. We use one-way attention because the graph contains information about the whole environment, while the instruction has (potentially incomplete) local information about the route of interest. The use of attention provides our model with a two-step strategy to interpret commands. This resembles the way people find paths on a map: first, relevant parts on the map are selected according to their affinity to each of the words in the input",
            "izers implement additional useful features for the users, by offering values to be used with a model; these range from token type indices in the case of sequence classification to maximum length sequence truncating taking into account the added model-specific special tokens (most pretrained Transformers models have a maximum sequence length they can handle, defined during their pretraining step). Tokenizers can be instantiated from existing configurations available through Transformers originating from the pretrained models or created more generally by the user from user-specifications.",
            ", they do not attempt to learn execution models for those predicates, nor can they answer questions using those predicates. Yao and Van Durme yao-2014-info-extraction-freebase-qa and Dong et al. dong-2015-freebase-qa-mccnn proposed question answering models that use similar features to those used in this work. However, they did not produce semantic parses of language, instead using methods that are non-compositional and do",
            "\nSettings\nIn our experiments, we tune model parameters on the development set and report results on the test set. We design experiments from both ranking-based direction and question generation-based direction. The evaluation metric is precision @1 BIBREF17 , which indicates whether the top ranked result is the correct answer. We further report BLEU score BIBREF27 for the question generation approach. We also adapt BiDAF BIBREF10 , a top-performing reading comprehension model on the",
            " observe next, with such behavior tantamount to learning to do IR. As mentioned, our idea for reformulating existing MRC datasets as partially observable and interactive environments is straightforward and general. Almost all MRC datasets can be used to study interactive, information-seeking behavior through similar modifications. We hypothesize that such behavior can, in turn, help in solving real-world MRC problems involving search.\n\n\n",
            "theta $ stands for the model parameters and $P(\\hat{y}=y\\ |\\ s; \\theta )$ the probability of the model to generate the adequate description $y$ for table $s$. During inference, we generate the sequence $\\hat{y}^*$ with the maximum a posteriori probability conditioned on table $s$. Using the chain rule, we get:  This equation is intractable in practice, we approximate a solution using beam search,",
            " that when an agent must use Ctrl+F to navigate within documents, the set of question words is a much smaller action space in contrast to the other two settings. In the 4-action setting, an agent can rely on issuing next and previous actions to reach any sentence in a document. The effect of action space size on model performance is particularly clear when using a datasets' entire vocabulary as query candidates in the 2-action setting. From Figure FIGREF40 (and figures with sufficient information rewards in",
            " options for algorithms, including non-embedding-based methods (e.g., pattern-based methods). We can use more than one entity expansion model to find related entities. For instance, general embedding models, such as those built on Wikipedia, might be a good choice in early iterations, whereas more domain-specific models trained on domain-specific corpora might be helpful in later iterations. LUWAK is flexible to change and use more than one Expansion API. This design encourages us to",
            " trigger the appropriate functionality of the AV system. In our current explorations, we focused on AMIE scenarios describing usages around setting or changing the destination and route, updating driving behavior or speed, finishing the trip and other use-cases to support various natural commands. We collected a multi-modal in-cabin dataset with multi-turn dialogues between the passengers and AMIE using a Wizard-of-Oz scheme via a realistic scavenger hunt game activity. After exploring various recent Rec",
            " models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given model to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog",
            "'s shape as image, or (3) other forms, e.g. object's information from scanned barcode, speech from users, etc. Based on the captured input, the system, then, generates a query string/sequence of text which is the key message describing the object's usage. Due to low-literates' lack of reading capability, the generated text requires not only informativeness, but also simplicity, and clarity. Therefore, we propose to use the \"S-V",
            ". We use this unrealistic setup to observe the training over noisy and hardly informative source sentences. We then use the procedures described in § SECREF4 , except that the pseudo-source embeddings in the copy-marked setup are pretrained for three epochs on the in-domain data, while all remaining parameters are frozen. This prevents random parameters from hurting the already trained model.\n\n\nCopy+marking+noise is not so stupid\nWe observe that the copy setup has only a"
        ]
    },
    {
        "title": "Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models",
        "answer": "jointly trained hybrid model.",
        "evidence": [
            " simpler and more transparent model. We explore various combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text.\n\n\nIntroduction\nFollowing the recent progress in",
            "ank et al.'s (2016) bi-LSTM performs extremely well, surpassed by MarMoT on only 3 out of 16 datasets (Czech, French and Italian), and by MElt only once (Indonesian).\n\n\nModels enriched with external lexical information\nTable TABREF13 provides the results of four systems enriched with lexical information. The feature-based systems MElt and MarMoT, respectively based on MEMMs and CRFs, are extended with",
            "2Seq, HLSTM, and HLSTM+Copy) have predicted improper entities (cake), generated repetitive contents (her family), or copied wrong words (eat). The models equipped with incremental encoding or knowledge through MSA(GA/CA) perform better in this example. The ending by IE+MSA is more coherent in logic, and fluent in grammar. We can see that there may exist multiple reasonable endings for the same story context. In order to verify the ability of our",
            " LMs are needed to confirm this observation, and also to evaluate the complementarity of both strategies. More work is needed to better understand the impact of BT on subparts of the network (§ SECREF7 ). In future work, we plan to investigate other cheap ways to generate artificial data. The experimental setup we proposed may also benefit from a refining of the data selection strategies to focus on the most useful monolingual sentences.\n\n\n",
            " code and strong incentives to share state-of-the-art pretrained models. The combination of these factors has lead researchers to reproduce previous results more easily, investigate current approaches and test hypotheses without having to redevelop them first, and focus their efforts on formulating and testing new hypotheses. To bring Transfer Learning methods and large-scale pretrained Transformers back into the realm of these best practices, the authors (and the community of contributors) have developed Transformers, a library for state-of-the art",
            " translation and semi-supervised learning, as well as the creation of stepwise ensembles, increase the performance of our Spanish-language models. Strong support was found for the translation and semi-supervised learning approaches; our best models for all subtasks use either one of these approaches. These results suggest that both of these additional data resources are beneficial when determining emotion intensity (for Spanish). However, the creation of a stepwise ensemble from the best models did not result in better performance compared to",
            "abularies, but they only perform this well if the parallel corpus for training are sufficiently large. For many under-resourced languages, unfortunately, it hardly presents. In reality, although the monolingual data of Vietnamese and Japanese are immensely available due to the popularity of their speakers, the bilingual Japanese-Vietnamese corpora are very limited and often in low quality or in narrowly technical domains. Therefore, data augmentation methods to exploit monolingual data for NMT systems are necessary to",
            " perform slightly better than most of the SVM models but still, the content information is insufficient to achieve a good performance on the Uns posts. As to adding comment information to these models, since the commenters do not always hold the same stance as the author, simply adding comments and post contents together merely adds noise to the model. Among all UTCNN variations, we find that user information is most important, followed by topic and comment information. UTCNN without user information shows results similar to SVMs —",
            "IM reads the sentences independently using LSTM at first, and then applies attention to align/contrast the sentences. Another round of LSTM reading then produces the final representations, which are compared to make the prediction. Detailed description of ESIM can be found in the Appendix. Using the SNLI BIBREF4 data, we train two variants of ESIM, with dimensionality 50 and 300 respectively, referred to as ESIM-50 and ESIM-300 in the remainder of",
            "2). Our second method involves classical addition of chunk features to cwr-infused architectures for four different downstream tasks (§SECREF3). Shallow syntactic information is obtained automatically using a highly accurate model (97% $F_1$ on standard benchmarks). In both settings, we observe only modest gains on three of the four downstream tasks relative to ELMo-only baselines (§SECREF4). Recent work has probed the knowledge encoded in cwrs and found they capture",
            " an additional loss ($L_{add}$): where $\\beta $ is a relative weight for $L_{add}$. Given outputs of LSTMs, a linear and binary softmax layers predict whether the next token is singular or plural. $L_{add}$ is a loss for this classification, only defined for the contexts preceding a target token $x_{i}$: where $x_{1:i} = x_1 \\cdots x_{i}$ is a",
            "-layers LSTM network with a copy mechanism. In order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data-structure and we train the model to switch between freely generating words from the vocabulary and copying words from the input. We now describe the hierarchical encoder and the hierarchical attention.\n\n\nHierarchical Encoder Model for Data-to-Text ::: Hierarchical Encoding Model\nAs outlined in"
        ]
    },
    {
        "title": "BERT-Based Arabic Social Media Author Profiling",
        "answer": "BABEL speech corpus.",
        "evidence": [
            " conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script",
            "points needs to be taken into account in systems' design. Characteristics that are not considered as relevant in a given task can be encapsulated in data nonetheless, and lead to bias performance. Being aware of the demographic skews our data set might contain is a first step to track the life cycle of a training data set and a necessary step to control the tools we develop.\n\n\n",
            " conditions with entailment/contradiction, we did not run all data size conditions given our limited computational resources. Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with the",
            ", the choice falls on a binary-extrinsic method, a strategy has to be considered for collecting representative documents for the outlier class INLINEFORM0 . Several existing methods such as BIBREF32 , BIBREF1 , BIBREF2 rely on search engines for retrieving appropriate documents, but these search engines might refuse their service if a specified quota is exhausted. Additionally, the retrieved documents render these methods inherently non-deterministic. Moreover, such methods cause relatively high run",
            " but also which emoticons are used in a similar context. We take advantage of distant supervision by using Facebook reactions as proxies for emotion labels, which to the best of our knowledge hasn't been done yet, and we train a set of Support Vector Machine models for emotion recognition. Our models, differently from existing ones, exploit information which is acquired entirely automatically, and achieve competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. For explanatory purposes",
            " datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size influences.",
            " others, and what does this say about the collection? Similar questions arise with the use of born-digital data. For instance, when using the Internet Archive’s Wayback Machine to gather data from archived web pages, we need to consider what pages were captured, which are likely missing, and why. We must often repurpose born-digital data (e.g., Twitter was not designed to measure public opinion), but data biases may lead to spurious results and limit justification for generalization",
            " used in our approach.\n\n\nData details and experimental setup\nIn this work, the experiments are conducted using the BABEL speech corpus collected from the IARPA babel program. The corpus is mainly composed of conversational telephone speech (CTS) but some scripted recordings and far field recordings are presented as well. Table TABREF14 presents the details of the languages used in this work for training and evaluation. 80 dimensional Mel-filterbank (fbank) features are then extracted from",
            " privacy information BIBREF21, BIBREF22, BIBREF23 have not been widely adopted by industry. Recently, there have been significant research effort devoted to understanding privacy policies by leveraging NLP techniques BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, especially by identifying specific data practices within a privacy policy. We adopt a personalized approach to understanding privacy policies, that allows users to query a document and selectively explore content salient",
            "2010. The data augmentation methods has been applied only for the Japanese INLINEFORM0 Vietnamese direction. For Back Translation, we use Vietnamese monolingual data from VNESEcorpus of DongDu which includes 349578 sentences. We shuffle the lines of VNESEcorpus corpus and take out the first 106758 sentences (the same as the number of sentence pairs in the original parallel corpus). For Mix-Source, instead of using a subsampled monolingual corpus,",
            ", it seems like 60–90 documents are sufficient to achieve F$_1$ on par with using all available training data. Beyond this point, we hit rapidly diminishing returns. For a number of “easy” content elements (e.g., dates in the property lease agreements), it seems like 30 documents are sufficient to achieve good accuracy, and more does not appear to yield substantial improvements. Note that in a few cases, training on more data actually decreases F$_1$ slightly",
            " our extensive literature research, numerous measures (e. g., Accuracy, F INLINEFORM0 , c@1, AUC, AUC@1, INLINEFORM1 or EER) have been used so far to assess the performance of AV methods. In regard to our experiments, we decided to use c@1 and AUC for several reasons. First, Accuracy, F INLINEFORM2 and INLINEFORM3 are not applicable in cases where AV methods leave verification problems unanswered,"
        ]
    },
    {
        "title": "WiSeBE: Window-based Sentence Boundary Evaluation",
        "answer": "S1, S2",
        "evidence": [
            " WiSeBEWiSeBE\nTo exemplify the INLINEFORM0 score we evaluated and compared the performance of two different SBD systems over a set of YouTube videos in a multi-reference enviroment. The first system (S1) employs a Convolutional Neural Network to determine if the middle word of a sliding window corresponds to a SU boundary or not BIBREF30 . The second approach (S2) by contrast, introduces a bidirectional Recurrent Neural Network",
            " other as they are translated across coordinate regions. Positive words now appear in quadrant 1, whereas negative words appear in quadrant 3. Thus, in the VSM, words that are sentimentally similar to each other could be clustered more accurately. Besides clustering, we also employed the SVD method to perform dimensionality reduction on the unsupervised dictionary algorithm and used the newly generated matrix by combining it with other subapproaches. The number of dimensions is chosen as 200 again according to the $",
            " the applicability of the methods in real forensic settings. Then, we proposed several properties that enable a better characterization and by this a better comparison between AV methods. Among others, we explained that the performance measure AUC is meaningless in regard to unary or specific non-optimizable AV methods, which involve a fixed decision criterion (for example, NNCD). Additionally, we mentioned that determinism must be fulfilled such that an AV method can be rated as reliable. Moreover, we clarified a number",
            " access to the gold-standards of SENTIPOLC'14, TASS'15 and SemEval 2015 & 2016, respectively. The authors also thank Elio Villaseñor for the helpful discussions in early stages of this research.\n\n\n",
            "able condition. SVM accomplishes this same feat by taking the samples to a higher dimensional hyperplane where the samples are linearly separable. Gradient Boosting Classifier (GBC) and Random Forest (RF) both utilize a set of decision trees and achieve similar results (RF performs slightly without eliminating stop words). Naive Bayesian Classifier (NBC) shows performance on per with GBC and RF algorithms. The overall better performance of all the algorithms when provided with stop words show the",
            " the source is minimal, so we only perform ablation on the target. For the source, we use a 3-layer 512-dim bidi GRU in all models (S1)-(S6). Model (S1) and (S2) are one and two layer baselines. Model (S4), which uses 7 intermediate FC layers, has similar decoding cost to (S2) while doubling the improvement over (S1) to 1.2 BLEU. We",
            " when only two of our resources are specialized in Drugs, among 12 sub-collections overall. Accordingly, the assessors noticed that the performance of the QA systems was better on questions about diseases than on questions about drugs, which suggests a need for extending our medical QA collection with more information about drugs and associated question types. We also looked closely at the private websites used by the LiveQA-Med annotators to provide some of the reference answers for the test questions. For instance, the",
            "2seq baseline. In addition, the lexicalized multi-source systems yields slightly higher BLEU scores than the unlexicalized multi-source systems; this is surprising because the lexicalized systems have significantly longer sequences than the unlexicalized ones. Finally, it is interesting to compare the seq2seq and parse2seq baselines. Parse2seq outperforms seq2seq by only a small amount compared to multi-source; thus, while adding syntax to NMT",
            " of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as",
            " results on cross-domain classification proposing the structural correspondence learning (SCL) method, and its variant based on mutual information (SCL-MI). Pan et al. BIBREF1 proposed a spectral feature alignment (SFA) algorithm to align domain-specific words from different domains into unified clusters, using domain-independent words as a bridge. Bollegala et al. BIBREF31 used a cross-domain lexicon creation to generate a sentiment-sensitive thesaurus (SST",
            " the highest one: 4.82 (the stdev is 0.35) which means the most effective approach to provide users with products' usage. Additionally, a paired-samples t-test was conducted to compare the MOS scores of users' responses among all products using baseline 1 and SimplerVoice system. There was a significant difference in the scores for baseline 1 (Mean = 2.57, Stdev = 1.17) and SimplerVoice (Mean = 4.",
            " Here we adopt SVCCA to measure the linear similarity of two sets of representations in the same multi-BERT from different translated datasets, which are parallel to each other. The results are shown in Fig FIGREF28.\n\n\nSupplemental Material ::: Improve Transfering\nIn the paper, we show that internal representations of multi-BERT are linear-mappable to some extent between different languages. This implies that multi-BERT model might encode semantic and syntactic information in"
        ]
    },
    {
        "title": "Recognizing Musical Entities in User-generated Content",
        "answer": "AdaBoost, MLP, SVM, K-NN.  BIBREF28, BIBREF13, BIBREF14, BIBREF15, BIBREF8, BIBREF9, BIBREF16, BIBREF0.  BIBREF7, BIBREF2, BIBREF27.  BIBREF5, BIBREF1, BIBREF4, BIBREF12, BIBREF11, BIBREF10.  BIBREF6, BIBREF3.  BIBREF29.  BIBREF28, BIBREF27, BIBREF26, BIBREF25, BIBREF24, BIBREF23, BIBREF22, BIBREF21, BIBREF20, BIBREF19, BIBREF18, BIBREF17.  BIBREF16, BIBREF15, BIBREF14, BIBREF13, BIBREF12, BIBREF11, BIBREF10, BIBREF9, BIBREF8, BIBREF7, BIBREF6, BIBREF5, BIBREF4, BIBREF3, BIBREF2, BIBREF",
        "evidence": [
            " was then added to our original training set. Again, the machine translation platform Apertium BIBREF5 was used for the translation of the datasets.\n\n\nAlgorithms Used\nThree types of models were used in our system, a feed-forward neural network, an LSTM network and an SVM regressor. The neural nets were inspired by the work of Prayas BIBREF7 in the previous shared task. Different regression algorithms (e.g. AdaBoost,",
            " several ways. In particular, they provide neither a unified API across models nor standardized ways to access the internals of the models. Targeting a more general machine-learning community, these Hubs lack the NLP-specific user-facing features provided by Transformers like tokenizers, dedicated processing scripts for common downstream tasks and sensible default hyper-parameters for high performance on a range of language understanding and generation tasks. The last direction is related to machine learning research frameworks that are specifically used to test,",
            " observe consistent improvements brought by LAN. The effectiveness of LAN on link prediction validates LAN's superiority to other aggregators and the necessities to treat the neighbors differently in a permutation invariant way. To analyze whether LAN outperforms the others for expected reasons and generalizes to other configurations, we conduct the following studies. In this experiment, we would like to confirm that it's necessary for the aggregator to be aware of the query relation. Specifically, we investigate the attention neural network and design two degener",
            " and uses back-propagation for updating parameter weights using loss function calculated on training set into classification. K-NN has shown the weakest performance overall, as this algorithm has a bad reputation of not working well in high dimensional data BIBREF28. MLP and SVM have shown similar performance. MLP takes advantage of multiple hidden layers in order to take non-linearly separable samples in a linearly separable condition. SVM accomplishes this same feat by taking the samples to",
            " race, etc. Hate speech detection is not a stable or simple target because misclassification of regular conversation as hate speech can severely affect users’ freedom of expression and reputation, while misclassification of hateful conversations as unproblematic would maintain the status of online communities as unsafe environments BIBREF2. To detect online hate speech, a large number of scientific studies have been dedicated by using Natural Language Processing (NLP) in combination with Machine Learning (ML) and Deep Learning (DL)",
            "\nResearches on question classification, question taxonomies and QA system have been undertaken in recent years. There are two types of approaches for question classification according to Banerjee et al in BIBREF13 - by rules and by machine learning approach. Rule based approaches use some hard coded grammar rules to map the question to an appropriate answer type BIBREF14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in",
            " a single explanation. Other methods are more specific to deep convolutional neural networks used in computer vision: the authors of BIBREF8 proposed a network propagation technique based on deconvolutions to reconstruct input image patterns that are linked to a particular feature map activation or prediction. The work of BIBREF9 aimed at revealing salient structures within images related to a specific class by computing the corresponding prediction score derivative with respect to the input image. The latter method reveals the sensitivity of the classifier decision to",
            " applied to other classification tasks, such as topic classification and concept mining. The experiments show that even unsupervised approaches, as in the corpus-based approach, can outperform supervised approaches in classification tasks. Combining some approaches, which can compensate for what others lack, can help us build better vectors. Our word vectors are created by conventional machine learning algorithms; however, they, as in the corpus-based model, produce state-of-the-art results. Although we preferred to use a",
            " on 160 GB of text using 1024 32GB V100. On Amazon-Web-Services cloud computing (AWS), such a pretraining would cost approximately 100K USD. Contrary to this trend, the booming research in Machine Learning in general and Natural Language Processing in particular is arguably explained significantly by a strong focus on knowledge sharing and large-scale community efforts resulting in the development of standard libraries, an increased availability of published research code and strong incentives to share state-of-the-art pretrained",
            " subject of ongoing work.\n\n\nImplications\nCollectively, the success of large pretrained neural models, both encoder-only BERT-like architectures as well as encoder–decoder architectures like T5, raise interesting questions for the pursuit of commonsense reasoning. Researchers have discovered that previous models perform well on benchmark datasets because they pick up on incidental biases in the dataset that have nothing to do with the task; in contrast, the WinoGrande dataset has devoted considerable effort to",
            " As one of the first attempts in neural network models, Djuric et al. BIBREF16 proposed a two-step method including a continuous bag of words model to extract paragraph2vec embeddings and a binary classifier trained along with the embeddings to distinguish between hate speech and clean content. Badjatiya et al. BIBREF0 investigated three deep learning architectures, FastText, CNN, and LSTM, in which they initialized the word embeddings with",
            " use their GPUs to perform the experiments mentioned in the paper. We also thank the anonymous reviewers for their careful reading of our paper and insightful comments.\n\n\n"
        ]
    },
    {
        "title": "Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER",
        "answer": "NER",
        "evidence": [
            " achieving state-of-the-art zero-resource results. There are many recent approaches to zero-resource cross-lingual classification and NER, including adversarial learning BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , using a model pretrained on parallel text BIBREF8 , BIBREF9 , BIBREF10 and self-training BIBREF11 . Due to the newness of the subject matter, the definition of `",
            "NN full model), we achieved much greater improvements (4.8%). We attribute this result to the tendency of users to use different wording for different roles (for instance author vs commenter). This is observed when the user, acting as an author, attempts to support her argument against nuclear power by using improvements in solar power; when acting as a commenter, though, she interacts with post contents by criticizing past politicians who supported nuclear power or by arguing that the proposed evacuation plan in case of a nuclear accident is",
            " Indeed, both model types are much more successful for in-vocabulary words, and their performance improves the higher the training frequency of the words. However, performance increases more quickly for the Siamese network than for the classifier as training frequency increases. This may be due to the fact that, if a word type occurs at least INLINEFORM0 times in the classifier training set, then it occurs at least INLINEFORM1 times in the Siamese paired training data. \n\n",
            "Abstract\nIn this paper, we examine the use case of general adversarial networks (GANs) in the field of marketing. In particular, we analyze how GAN models can replicate text patterns from successful product listings on Airbnb, a peer-to-peer online market for short-term apartment rentals. To do so, we define the Diehl-Martinez-Kamalu (DMK) loss function as a new class of functions that forces the model's generated output to include a",
            " can see that the performances of most tasks can be improved with the help of multi-task learning. FS-MTL shows the minimum performance gain from multi-task learning since it puts all private and shared information into a unified space. SSP-MTL and PSP-MTL achieve similar performance and are outperformed by ASP-MTL which can better separate the task-specific and task-invariant features by using adversarial training. Our proposed models (SA-MTL and DA",
            "-error robust. Although the embedding encoder for both domains may implicitly learn some common latent representations, adversarial learning can provide a more direct training signal for aligning the output distribution of the embedding encoder from both domains. The embedding encoder takes in a sequence of word vectors and generates a sequence of hidden vectors with the same length. We use INLINEFORM0 and INLINEFORM1 ( INLINEFORM2 and INLINEFORM3 ) to represent the hidden vector sequence given",
            " from the machine learning techniques. If they do not return any entities, the entities predicted by the schedule matching are considered. Our strategy is justified by the poorer results obtained by the NER based only on the schedule matching, compared to the other models used in the experiments, to be presented in the next section.\n\n\nResults\nThe performances of the NER experiments are reported separately for three different parts of the system proposed. Table 6 presents the comparison of the various methods while performing NER on the",
            " better performance. Unlikelihood loss does not work unless we choose a huge weight parameter ($\\alpha =1000$), but it does not outperform ours, with a similar value of perplexity. The improvements by binary-classification loss are smaller, indicating that the signals are weaker than other methods with explicit negative examples. Sentence-level margin loss is conceptually advantageous in that it can deal with any types of negative examples defined in a sentence including NPIs. We see that it is often competitive",
            "Abstract\nWe report results of a comparison of the accuracy of crowdworkers and seven NaturalLanguage Processing (NLP) toolkits in solving two important NLP tasks, named-entity recognition (NER) and entity-level sentiment(ELS) analysis. We here focus on a challenging dataset, 1,000 political tweets that were collected during the U.S. presidential primary election in February 2016. Each tweet refers to at least one of four presidential candidates,i.e., four named entities.",
            " learning classifiers like support vector machines or neural networks, by extending the primary classification result with additional information linking the classifier's decision back to components of the input, in our case words in a document. Furthermore, based on LRP relevance, we have introduced a new way of condensing the semantic information contained in word embeddings (such as word2vec) into a document vector representation that can be used for nearest neighbors classification, and that leads to better performance than standard TFIDF weight",
            " classification. We have shown that UTCNN achieves promising and balanced results. In the future we plan to explore the effectiveness of the UTCNN user embeddings for author stance classification.\n\n\nAcknowledgements\nResearch of this paper was partially supported by Ministry of Science and Technology, Taiwan, under the contract MOST 104-2221-E-001-024-MY2.\n\n\n",
            "REF1 . This could be useful for understanding mistakes made by neural networks, which have human-level performance most of the time, but can perform very poorly on seemingly easy cases. For instance, convolutional networks can misclassify adversarial examples with very high confidence BIBREF2 , and made headlines in 2015 when the image tagging algorithm in Google Photos mislabeled African Americans as gorillas. It's reasonable to expect recurrent networks to fail in similar ways as well. It would thus be"
        ]
    }
]