[
    {
        "title": "Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations",
        "answer": "{\"name\": \"Model\", \"description\": \"A model is a mathematical representation of a system or a process.\"}\n     {\"name\": \"Unlabeled data\", \"description\": \"Unlabeled data is data that does not have a label or a target variable associated with it.\"}\n     {\"name\": \"Representation learning\", \"description\": \"Representation learning is the process of learning a representation of the input data that is useful for a particular task.\"}\n     {\"name\": \"Self-supervised learning\", \"name\": \"Self-supervised learning\", \"description\": \"Self-supervised learning is a type of unsupervised learning where the model is trained on unlabeled data and the model learns to predict the input data itself.\"}\n     {\"name\": \"Autoencoders\", \"description\": \"Autoencoders are a type of neural network that can learn to compress and reconstruct the input data.\"}\n     {\"name\": \"Contrastive learning\", \"description\": \"Contrastive learning is a type of self-supervised learning where the model is trained to distinguish between similar and dissimilar examples.\"}\n     {\"model\": \"They utilize unlabeled data to improve model representations by training the model on the unlabeled data using self-supervised learning techniques such as auto",
        "evidence": []
    },
    {
        "title": "Artificial Error Generation with Machine Translation and Syntactic Patterns",
        "answer": "{\"name\": \"Python\", \"name\": \"Java\", \"name\": \"C++\", \"name\": \"C\", \"name\": \"Rust\", \"name\": \"Go\", \"name\": \"JavaScript\", \"name\": \"Swift\", \"name\": \"PHP\", \"name\": \"TypeScript\", \"name\": \"Kotlin\", \"name\": \"R\", \"name\": \"MATLAB\", \"name\": \"Perl\", \"name\": \"Scala\", \"name\": \"Julia\", \"name\": \"Haskell\", \"name\": \"Lua\", \"name\": \"Ruby\", \"name\": \"SQL\", \"name\": \"MATLAB\", \"name\": \"Python\", \"name\": \"C\", \"name\": \"C++\", \"name\": \"Java\", \"name\": \"Rust\", \"name\": \"Go\", \"name\": \"JavaScript\", \"name\": \"Swift\", \"name\": \"PHP\", \"name\": \"TypeScript\", \"name\": \"Kotlin\", \"name\": \"R\", \"name\": \"MATLAB\", \"name\": \"Perl\", \"name\": \"Scala\", \"name\": \"Julia\", \"name\": \"Haskell\", \"name\": \"Lua\", \"name\": \"Ruby\", \"",
        "evidence": []
    },
    {
        "title": "Generating Word and Document Embeddings for Sentiment Analysis",
        "answer": "{\"name\": \"word\", \"score\": 0.5}\n     {\"name\": \"word\", \"score\": 0.7}\n     {\"name\": \"word\", \"score\": 0.3}\n     {\"name\": \"word\", \"score\": 0.9}\n     {\"name\": \"word\", \"score\": 0.1}",
        "evidence": []
    },
    {
        "title": "Using General Adversarial Networks for Marketing: A Case Study of Airbnb",
        "answer": "{\"name\": \"Airbnb\", \"size\": \"private\", \"type\": \"rental\"}",
        "evidence": []
    },
    {
        "title": "Question Answering for Privacy Policies: Combining Computational and Legal Perspectives",
        "answer": "{\"name\": \"baseline\", \"description\": \"The baseline is a simple model that predicts the same class for all instances.\", \"url\": \"https://en.wikipedia.org/wiki/Template:Machine learning\", \"type\": \"machine learning\"}",
        "evidence": []
    },
    {
        "title": "Dynamic Compositional Neural Networks over Tree Structure",
        "answer": "{\"name\": \"task\", \"type\": \"experiment\", \"description\": \"Experimenting with tasks\"}",
        "evidence": []
    },
    {
        "title": "Automatic Discourse Segmentation: an evaluation in French",
        "answer": "{\"name\": \"Segmentation\", \"description\": \"The process of dividing a dataset into smaller subsets based on specific criteria.\", \"evaluation\": \"Segmentation quality is evaluated based on the accuracy of the division, the number of segments, and the homogeneity of each segment.\"}",
        "evidence": []
    },
    {
        "title": "Multimodal Named Entity Recognition for Short Social Media Posts",
        "answer": "{\"name\": \"MNER\", \"size\": \"1.4M\", \"type\": \"captions\", \"url\": \"https://github.com/THUNUSAR/THUNUSAR-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTHU-THUWTH",
        "evidence": []
    },
    {
        "title": "Neural Collective Entity Linking",
        "answer": "{\"name\": \"NCEL\", \"description\": \"National Center for Education Law\", \"url\": \"https://www.nclb.gov/nclb/center_for_education_law.html\", \"type\": \"organization\"}",
        "evidence": []
    },
    {
        "title": "Distant supervision for emotion detection using Facebook reactions",
        "answer": "{\"name\": \"Facebook\", \"id\": \"1234567890\"}",
        "evidence": []
    },
    {
        "title": "MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge",
        "answer": "{\"name\": \"CrowdFlower\", \"description\": \"CrowdFlower is a crowdsourcing platform that allows users to create and manage crowdsourcing projects, including data annotation, content moderation, and transcription tasks.\"}",
        "evidence": []
    },
    {
        "title": "Stochastic Answer Networks for SQuAD 2.0",
        "answer": "{\"name\": \"SpanDetector\", \"layers\": [\"Embedding\", \"BiLSTM\", \"Attention\", \"Output\"]}\n\n    {\"name\": \"SpanDetector\", \"layers\": [\"Embedding\", \"BiLSTM\", \"Attention\", \"Output\"]}",
        "evidence": []
    },
    {
        "title": "Winograd Schemas and Machine Translation",
        "answer": "{\"name\": \"Python\", \"version\": \"3.9.7\"}",
        "evidence": []
    },
    {
        "title": "How we do things with words: Analyzing text as social and cultural data",
        "answer": "{\"name\": \"1. Sentiment analysis of text from non-English languages\", \"description\": \"The ability to analyze text from languages other than English is a significant challenge in NLP. This includes languages with non-Latin scripts, such as Chinese, Japanese, and Arabic, as well as languages with complex grammar and syntax, such as Russian and Arabic.\"}\n     {\"name\": \"2. Emotion detection in text\", \"description\": \"Emotion detection in text is a complex task that involves identifying the emotions expressed in a piece of text. This can be challenging, especially when the text is ambiguous or open to interpretation.\"}\n     {\"name\": \"3. Text summarization\", \"description\": \"Text summarization involves condensing a large amount of text into a shorter summary. This can be challenging, especially when the text is technical or contains complex concepts.\"}\n     {\"name\": \"4. Named entity recognition\", \"description\": \"Named entity recognition involves identifying and categorizing named entities in text, such as people, places, and organizations. This can be challenging, especially when the text is noisy or contains misspelled words.\"}\n     {\"name\": \"5. Text classification\", \"name\": \"6. Text classification\", \"description\": \"",
        "evidence": []
    },
    {
        "title": "Quasi-Recurrent Neural Networks",
        "answer": "{\"name\": \"pooling\", \"description\": \"A function that reduces the spatial dimensions of a feature map by taking the maximum or average value of neighboring pixels.\"}",
        "evidence": []
    },
    {
        "title": "Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction",
        "answer": "{\"name\": \"Instance\", \"type\": \"tuple\"}",
        "evidence": []
    },
    {
        "title": "TWEETQA: A Social Media Focused Question Answering Dataset",
        "answer": "{\"name\": \"Twitter\", \"description\": \"Twitter is a social media platform that allows users to share short messages, called tweets, with their followers.\", \"url\": \"https://www.twitter.com\", \"type\": \"social media\"}",
        "evidence": []
    },
    {
        "title": "Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation",
        "answer": "{\"name\": \"COCO\", \"url\": \"http://cocodataset.org/\", \"description\": \"Common Objects in Context\"}\n     {\"name\": \"PASCAL VOC\", \"url\": \"http://host.robots.ox.ac.uk/pascal/VOC/\", \"description\": \"Visual Object Classes\"}\n     {\"name\": \"KITTI\", \"url\": \"http://www.cvlibs.net/datasets/kitti/\", \"description\": \"KITTI Vision Benchmark Suite\"}",
        "evidence": []
    },
    {
        "title": "Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance",
        "answer": "{\"name\": \"C4\", \"description\": \"The Common Crawl corpus\", \"url\": \"https://commoncrawl.org/\"} \n     {\"name\": \"WikiText-103\", \"description\": \"A large corpus of text from Wikipedia\", \"url\": \"https://github.com/ethanjpiper/WikiText-103\"} \n     {\"name\": \"WikiText-2\", \"description\": \"A large corpus of text from Wikipedia\", \"url\": \"https://github.com/mmarley/wiki_text\"}",
        "evidence": []
    },
    {
        "title": "Knowledge Based Machine Reading Comprehension",
        "answer": "{\"name\": \"QG\", \"description\": \"Question Generation\", \"url\": \"https://en.wikipedia.org/wiki/Question_generation\"}",
        "evidence": []
    },
    {
        "title": "Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health",
        "answer": "{\"name\": \"dataset\", \"value\": \"CIFAR-10\"}",
        "evidence": []
    },
    {
        "title": "A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media",
        "answer": "{\"name\": \"Approach\", \"items\": [\"Naive Bayes\", \"Decision Trees\", \"Random Forest\", \"Gradient Boosting\", \"Support Vector Machines\", \"K-Nearest Neighbors\", \"Logistic Regression\", \"Neural Networks\", \"Gradient Boosting\", \"AdaBoost\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\", \"Gradient Boosting\",",
        "evidence": []
    },
    {
        "title": "External Lexical Information for Multilingual Part-of-Speech Tagging",
        "answer": "{\"name\": \"Python\", \"name\": \"Java\", \"name\": \"C++\", \"name\": \"JavaScript\", \"name\": \"C\", \"name\": \"Ruby\", \"name\": \"Swift\", \"name\": \"Go\", \"name\": \"Rust\", \"name\": \"Kotlin\", \"name\": \"PHP\", \"name\": \"TypeScript\", \"name\": \"Scala\", \"name\": \"R\", \"name\": \"Perl\", \"name\": \"Lua\", \"name\": \"MATLAB\", \"name\": \"Julia\", \"name\": \"Haskell\", \"name\": \"Fortran\", \"name\": \"Pascal\", \"name\": \"SQL\", \"name\": \"TeX\", \"name\": \"HTML\", \"name\": \"CSS\", \"name\": \"XML\", \"name\": \"JSON\", \"name\": \"YAML\", \"name\": \"Markdown\", \"name\": \"LaTeX\", \"name\": \"Dart\", \"name\": \"Erlang\", \"name\": \"OCaml\", \"name\": \"F#\", \"name\": \"Racket\", \"name\": \"Scheme\", \"name\": \"Common Lisp\", \"name\": \"Clojure\", \"name\": \"Tcl\", \"name\":",
        "evidence": []
    },
    {
        "title": "Disunited Nations? A Multiplex Network Approach to Detecting Preference Affinity Blocs using Texts and Votes",
        "answer": "assistant\n    IMDB",
        "evidence": []
    },
    {
        "title": "When to reply? Context Sensitive Models to Predict Instructor Interventions in MOOC Forums",
        "answer": "{\"name\": \"Latent Context\", \"type\": \"instructor intervention\", \"description\": \"The type of latent context used to predict instructor intervention is called 'instructor intervention context'.\"}",
        "evidence": []
    },
    {
        "title": "A Hierarchical Model for Data-to-Text Generation",
        "answer": "://{\"improvement\": 0.5}",
        "evidence": []
    },
    {
        "title": "Fully Convolutional Speech Recognition",
        "answer": "{\"name\": \"WSJ\", \"description\": \"Wall Street Journal\", \"url\": \"https://www.wsj.com/\", \"type\": \"newspaper\", \"language\": \"English\", \"country\": \"United States\", \"founded\": \"1889\", \"circulation\": \"2.5 million\", \"headquarters\": \"New York City\", \"owner\": \"News Corp\", \"frequency\": \"Daily\", \"format\": \"Print and online\", \"topics\": \"Business, finance, economics, politics, and more\"}",
        "evidence": []
    },
    {
        "title": "Massively Multilingual Neural Grapheme-to-Phoneme Conversion",
        "answer": "{\"name\": \"CIFAR-10\", \"url\": \"https://www.cs.toronto.edu/~kriz/cifar.html\"}, \n     {\"name\": \"CIFAR-100\", \"url\": \"https://www.cs.toronto.edu/~kriz/cifar.html\"}, \n     {\"name\": \"CIFAR-10-B\", \"url\": \"https://www.cs.toronto.edu/~kriz/cifar.html\"}, \n     {\"name\": \"CIFAR-100-B\", \"url\": \"https://www.cs.toronto.edu/~kriz/cifar.html\"}",
        "evidence": []
    },
    {
        "title": "A Neural Approach to Irony Generation",
        "answer": "{\"name\": \"The judges were the participants themselves.\"}",
        "evidence": []
    },
    {
        "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer",
        "answer": "{\"name\": \"BERT\", \"type\": \"transformer\", \"num_layers\": 12, \"hidden_size\": 768, \"attention_heads\": 12, \"feed_forward_size\": 3072}",
        "evidence": []
    },
    {
        "title": "From Textual Information Sources to Linked Data in the Agatha Project",
        "answer": "{\"name\": \"Pipeline\", \"description\": \"A pipeline is a series of processes that are connected in a linear fashion, allowing data to flow through each process in sequence.\"}\n     {\"name\": \"Evaluation\", \"description\": \"The process of assessing the performance of a system, model, or process to determine its effectiveness, efficiency, and accuracy.\"}\n     {\"name\": \"Metrics\", \"description\": \"Quantifiable measures used to evaluate the performance of a system, model, or process.\"}\n     {\"name\": \"Accuracy\", \"description\": \"A measure of how close a system's output is to its intended output.\"}\n     {\"name\": \"Precision\", \"description\": \"A measure of how accurate a system is in identifying the correct output.\"}\n     {\"name\": \"Recall\", \"description\": \"A measure of how many relevant items a system identifies out of all relevant items.\"}\n     {\"name\": \"F1-score\", \"description\": \"A measure of the balance between precision and recall.\"}\n     {\"name\": \"Mean Squared Error\", \"description\": \"A measure of the average squared difference between predicted and actual values.\"}\n     {\"name\": \"Mean Absolute Error\", \"description\": \"A measure of the average absolute difference between",
        "evidence": []
    },
    {
        "title": "Sparse and Constrained Attention for Neural Machine Translation",
        "answer": "{\"name\": \"English\", \"name\": \"French\", \"name\": \"Spanish\", \"name\": \"German\", \"name\": \"Italian\", \"name\": \"Portuguese\", \"name\": \"Dutch\", \"name\": \"Russian\", \"name\": \"Chinese\", \"name\": \"Japanese\", \"name\": \"Korean\", \"name\": \"Arabic\", \"name\": \"Hebrew\", \"name\": \"Greek\", \"name\": \"Turkish\", \"name\": \"Polish\", \"name\": \"Hungarian\", \"name\": \"Swedish\", \"name\": \"Finnish\", \"name\": \"Czech\", \"name\": \"Danish\", \"name\": \"Norwegian\", \"name\": \"Ukrainian\", \"name\": \"Romanian\", \"name\": \"Bulgarian\", \"name\": \"Croatian\", \"name\": \"Slovak\", \"name\": \"Slovenian\", \"name\": \"Lithuanian\", \"name\": \"Latvian\", \"name\": \"Estonian\", \"name\": \"Albanian\", \"name\": \"Serbian\", \"name\": \"Macedonian\", \"name\": \"Montenegrin\", \"name\": \"B",
        "evidence": []
    },
    {
        "title": "Dialog Context Language Modeling with Recurrent Neural Networks",
        "answer": "{\"name\": \"dialog_history\", \"value\": 1000}",
        "evidence": []
    },
    {
        "title": "Shallow Syntax in Deep Water",
        "answer": "://\n    {\"answer\": \"The syntactic features are obtained automatically on downstream task data.\"}",
        "evidence": []
    },
    {
        "title": "TTTTTackling WinoGrande Schemas",
        "answer": "{\"name\": \"previous state of the art\", \"description\": \"The previous state of the art refers to the current best practices, methods, or techniques used in a particular field or industry. It is the most advanced or up-to-date approach that has been widely accepted and adopted by experts in the field.\", \"url\": \"https://en.wikipedia.org/wiki/State_of_the_art\"}",
        "evidence": []
    },
    {
        "title": "A Question-Entailment Approach to Question Answering",
        "answer": "ul>\n    \t<li>Graph Convolutional Networks (GCNs)</li>\n    \t<li>Graph Attention Networks (GATs)</li>\n    \t<li>Graph Autoencoders</li>\n    \t<li>Graph Neural Networks (GNNs)</li>\n    \t<li>Graph Recurrent Neural Networks (GRNNs)</li>\n    \t<li>Graph Transformers</li>\n    \t<li>Message Passing Neural Networks (MPNNs)</li>\n    \t<li>Graph Convolutional Autoencoders</li>\n    \tli>Graph-based Recurrent Neural Networks (RNNs)</li>\n    \t<li>Graph-based Long Short-Term Memory (LSTM) Networks</li>\n    \t<li>Graph-based Gated Recurrent Unit (GRU) Networks</li>\n    \t</ul>",
        "evidence": []
    },
    {
        "title": "Argumentation Mining in User-Generated Web Discourse",
        "answer": "{\"name\": \"register\", \"description\": \"A register is a set of related words that have a similar grammatical function in a language.\"}\n     {\"name\": \"domain\", \"description\": \"A domain is a specific area of knowledge or activity.\"}\n     {\"name\": \"task\", \"description\": \"A task is a piece of work that needs to be done.\"}\n     {\"name\": \"register\", \"description\": \"A register is a set of related words that have a similar grammatical function in a language.\"}\n     {\"name\": \"domain\", \"description\": \"A domain is a specific area of knowledge or activity.\"}\n     {\"name\": \"task\", \"description\": \"A piece of work that needs to be done.\"}\n     {\"name\": \"register\", \"description\": \"A set of related words that have a similar grammatical function in a language.\"}\n     {\"name\": \"domain\", \"description\": \"A specific area of knowledge or activity.\"}\n     {\"name\": \"task\", \"description\": \"Work that needs to be done.\"}\n     {\"name\": \"register\", \"description\": \"Related words that have a similar grammatical function in a language.\"}\n     {\"name\": \"domain\", \"description\":",
        "evidence": []
    },
    {
        "title": "Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model",
        "answer": "{\"name\": \"Zero-shot learning\", \"description\": \"A type of machine learning where a model is trained on a small set of labeled data and then tested on a completely new, unseen task without any additional training.\"}",
        "evidence": []
    },
    {
        "title": "Automated News Suggestions for Populating Wikipedia Entity Pages",
        "answer": "{\"name\": \"salience\", \"description\": \"The degree to which an entity is relevant to the current context.\", \"type\": \"property\"}\n     {\"name\": \"authority\", \"description\": \"The degree to which an entity is considered trustworthy or credible.\", \"type\": \"property\"}",
        "evidence": []
    },
    {
        "title": "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
        "answer": "{\"name\": \"Python\", \"version\": \"3.8.10\", \"type\": \"scripting\"}",
        "evidence": []
    },
    {
        "title": "Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation",
        "answer": "://github.com/duyvuleo/Japanese-Vietnamese-Parallel-Corpus",
        "evidence": []
    },
    {
        "title": "Ancient-Modern Chinese Translation with a Large Training Dataset",
        "answer": "{\"name\": \"Ancient Chinese dataset\", \"description\": \"A dataset of ancient Chinese texts\", \"source\": \"Chinese Academy of Social Sciences\", \"url\": \"https://www.cssn.cn/\"}",
        "evidence": []
    },
    {
        "title": "UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish",
        "answer": "://>\n    Semi-supervised learning",
        "evidence": []
    },
    {
        "title": "Conflict as an Inverse of Attention in Sequence Relationship",
        "answer": "://\n    {\"answer\": \"They test their conflict method on tasks that require decision-making, problem-solving, and critical thinking.\"}",
        "evidence": []
    },
    {
        "title": "A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification",
        "answer": "{\"name\": \"Decision Trees\", \"description\": \"Decision Trees are a type of supervised learning algorithm that can be used for both classification and regression tasks. They work by recursively partitioning the data into smaller subsets based on the features of the data.\"}\n     {\"name\": \"Random Forest\", \"description\": \"Random Forest is an ensemble learning method that combines multiple decision trees to improve the accuracy and robustness of the model. It works by creating multiple decision trees on different subsets of the data and then combining their predictions to produce a final prediction.\"}\n     {\"name\": \"Gradient Boosting\", \"description\": \"Gradient Boosting is a type of ensemble learning algorithm that combines multiple weak models to create a strong predictive model. It works by iteratively adding new models to the ensemble, each of which aims to correct the errors of the previous model.\"}\n     {\"name\": \"Support Vector Machines\", \"description\": \"Support Vector Machines (SVMs) are a type of supervised learning algorithm that can be used for both classification and regression tasks. They work by finding the hyperplane that maximally separates the classes in the feature space.\"}\n     {\"name\": \"Neural Networks\", \"description\": \"Neural Networks are a type of machine learning model inspired by the",
        "evidence": []
    },
    {
        "title": "Extractive Summarization of Long Documents by Combining Global and Local Context",
        "answer": "{\n      \"globalContext\": \"The global context refers to the overall situation or environment in which a piece of text is being interpreted. It includes the speaker's intentions, the topic, and the situation in which the text is being used.\",\n      \"localContext\": \"The local context refers to the specific details or information that are relevant to a particular part of a text. It includes the words, phrases, and sentences that are immediately surrounding a particular word or phrase.\"\n    }",
        "evidence": []
    },
    {
        "title": "Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets",
        "answer": "{\"name\": \"evaluation measures\", \"description\": \"Metrics used to assess the performance of a model or system.\"}",
        "evidence": []
    },
    {
        "title": "Creative GANs for generating poems, lyrics, and metaphors",
        "answer": "{\"name\": \"dataset\", \"type\": \"list\", \"values\": [\"IMDB\", \"20 Newsgroups\", \"Reuters\", \"AG News\", \"SST-2\", \"Yelp\", \"Amazon Review\", \"DBPedia\", \"WikiQA\", \"SNLI\", \"SQuAD\", \"BoolQ\", \"OpenBookQA\", \"MultiNLI\", \"QuAC\", \"HotpotQA\", \"Natural Questions\", \"TriviaQA\", \"WebQuestions\", \"WikiHop\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBookQA\", \"OpenBook",
        "evidence": []
    },
    {
        "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text",
        "answer": "{\"name\": \"baselines\", \"description\": \"A baseline is a standard or reference point used as a basis for comparison.\"}",
        "evidence": []
    },
    {
        "title": "Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity",
        "answer": "{\"name\": \"dataset\", \"type\": \"text\", \"description\": \"The dataset used in the study.\"}\n     {\"name\": \"annotation\", \"type\": \"text\", \"description\": \"The process of annotating the dataset.\"}\n     {\"name\": \"manual\", \"type\": \"text\", \"description\": \"Manual annotation process.\"}\n     {\"name\": \"active learning\", \"type\": \"text\", \"description\": \"Active learning process.\"}\n     {\"name\": \"semi-supervised\", \"type\": \"text\", \"description\": \"Semi-supervised learning process.\"}\n     {\"name\": \"unsupervised\", \"text\": \"The dataset was annotated using a combination of manual and active learning processes.\"}",
        "evidence": []
    },
    {
        "title": "Multi-attention Recurrent Network for Human Communication Comprehension",
        "answer": "{\"name\": \"Long-short Term Hybrid Memory\", \"description\": \"A type of neural network architecture that combines the strengths of both long short-term memory (LSTM) and short-term memory (STM) networks.\", \"difference\": \"The main difference is that LSHM uses a hybrid approach, combining the strengths of both LSTMs and STMs, whereas LSTMs only use a single type of memory cell.\"}",
        "evidence": []
    },
    {
        "title": "Construction of a Japanese Word Similarity Dataset",
        "answer": "{\"name\": \"Data Sources\", \"description\": \"Data sources are the origin of the data used in a dataset. They can be primary, secondary, or tertiary sources. Primary sources are original data collected directly from the source, while secondary sources are derived from primary sources. Tertiary sources are compiled from secondary sources. Data sources can be internal or external. Internal sources are within the organization, while external sources are outside the organization. Data sources can be structured or unstructured. Structured data is organized in a specific format, while unstructured data is not. Data sources can be qualitative or quantitative. Qualitative data is descriptive, while quantitative data is numerical. Data sources can be primary, secondary, or tertiary. Primary sources are original data collected directly from the source, while secondary sources are derived from primary sources. Tertiary sources are compiled from secondary sources. Data sources can be internal or external. Internal sources are within the organization, while external sources are outside the organization. Data sources can be structured or unstructured. Structured data is organized in a specific format, while unstructured data is not. Data sources can be qualitative or quantitative. Qualitative data is descriptive, while quantitative data is numerical. Data sources can be primary, secondary, or tertiary. Primary sources are",
        "evidence": []
    },
    {
        "title": "Assessing the Applicability of Authorship Verification Methods",
        "answer": "A self-compiled corpus is a collection of texts that are gathered and compiled by an individual or organization for a specific purpose, such as research, education, or training.",
        "evidence": []
    },
    {
        "title": "Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach",
        "answer": "{\"name\": \"BERT\", \"description\": \"Bidirectional Encoder Representations from Transformers\", \"url\": \"https://huggingface.co/bert-base-uncased\"}\n     {\"name\": \"RoBERTa\", \"description\": \"Robustly optimized BERT pretraining approach\", \"url\": \"https://huggingface.co/roberta-base\"}\n     {\"name\": \"DistilBERT\", \"description\": \"Distilled version of BERT\", \"url\": \"https://huggingface.co/distilbert-base-uncased\"}\n     {\"models\": [\"BERT\", \"RoBERTa\", \"DistilBERT\"]}",
        "evidence": []
    },
    {
        "title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU",
        "answer": "{\"name\": \"Transformer\", \"type\": \"baseline\"}",
        "evidence": []
    },
    {
        "title": "A Stable Variational Autoencoder for Text Modelling",
        "answer": "{\"name\": \"Text Quality Evaluation\", \"description\": \"Evaluating the quality of generated text is a crucial step in natural language processing (NLP) and machine learning (ML) applications. There are several metrics and techniques used to assess the quality of generated text, including:\",\"url\": \"https://en.wikipedia.org/wiki/Text_quality_evaluation\", \"type\": \"definition\"}",
        "evidence": []
    },
    {
        "title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances",
        "answer": "{\"name\": \"joint_model\", \"description\": \"A joint model is a statistical model that combines the information from multiple sources or variables to make predictions or inferences. It is used in various fields such as machine learning, natural language processing, and computer vision.\"}",
        "evidence": []
    },
    {
        "title": "Is there Gender bias and stereotype in Portuguese Word Embeddings?",
        "answer": "{\"name\": \"word2vec\", \"description\": \"Word2vec is a group of techniques in natural language processing (NLP) used for word embedding, a type of word representation that allows words to be used in mathematical operations. It is a method for converting words or phrases into vectors of real numbers, which can be used in machine learning algorithms.\", \"url\": \"https://en.wikipedia.org/wiki/Word2vec\"}\n     {\"name\": \"GloVe\", \"description\": \"GloVe is a word embedding technique that is used to represent words as vectors in a high-dimensional space. It is based on the mathematical concept of matrix factorization and is often used in natural language processing (NLP) and information retrieval.\", \"url\": \"https://en.wikipedia.org/wiki/GloVe\"}\n     {\"name\": \"fastText\", \"description\": \"FastText is a library for efficient learning of word representations and sentence classification. It is based on the word2vec algorithm and is designed to be faster and more accurate than word2vec.\", \"url\": \"https://fasttext.cc/\"}\n     {\"name\": \"Bert\", \"description\": \"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google.",
        "evidence": []
    },
    {
        "title": "An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models",
        "answer": "{\"name\": \"BERT\", \"description\": \"Bidirectional Encoder Representations from Transformers\", \"url\": \"https://en.wikipedia.org/wiki/BERT_(language_model)\"}\n     {\"name\": \"RoBERTa\", \"description\": \"Robustly optimized BERT pretraining approach\", \"url\": \"https://en.wikipedia.org/wiki/RoBERTa\"}\n     {\"name\": \"T5\", \"description\": \"Text-to-Text Transfer Transformer\", \"url\": \"https://en.wikipedia.org/wiki/T5_(language_model)\"}\n     {\"name\": \"XLNet\", \"description\": \"Extreme Language Model\", \"url\": \"https://en.wikipedia.org/wiki/XLNet\"}",
        "evidence": []
    },
    {
        "title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset",
        "answer": "{\"name\": \"domain\", \"value\": \"Business\"} \n     {\"name\": \"domain\", \"value\": \"Finance\"} \n     {\"name\": \"domain\", \"value\": \"Healthcare\"} \n     {\"name\": \"domain\", \"value\": \"Technology\"} \n     {\"name\": \"domain\", \"value\": \"Education\"} \n     {\"name\": \"domain\", \"value\": \"Sports\"}",
        "evidence": []
    },
    {
        "title": "Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge",
        "answer": "{\"name\": \"Knowledge Graph\", \"description\": \"A knowledge graph is a knowledge base that stores and represents knowledge as a graph, where entities are nodes and relationships are edges.\"}",
        "evidence": []
    },
    {
        "title": "SimplerVoice: A Key Message&Visual Description Generator System for Illiteracy",
        "answer": "{\"name\": \"BERT\", \"description\": \"Bidirectional Encoder Representations from Transformers\", \"url\": \"https://en.wikipedia.org/wiki/BERT_(language_model)\"}\n     ]",
        "evidence": []
    },
    {
        "title": "A Parallel Corpus of Theses and Dissertations Abstracts",
        "answer": "{\"name\": \"Transformer\", \"id\": 1}, \n     {\"name\": \"Transformer-XL\", \"id\": 2}, \n     {\"name\": \"BERT\", \"id\": 3}, \n     {\"name\": \"XLNet\", \"id\": 4}, \n     {\"name\": \"RoBERTa\", \"id\": 5}, \n     {\"name\": \"DistilBERT\", \"id\": 6}, \n     {\"name\": \"ALBERT\", \"id\": 7}, \n     {\"name\": \"T5\", \"id\": 8}, \n     {\"name\": \"Longformer\", \"id\": 9}, \n     {\"name\": \"BigBird\", \"id\": 10}, \n     {\"name\": \"Reformer\", \"id\": 11}, \n     {\"name\": \"Sparse Transformer\", \"id\": 12}, \n     {\"name\": \"Switch Transformer\", \"id\": 13}, \n     {\"name\": \"Sparse Transformer\", \"id\": 14}, \n     {\"name\": \"Reformer\", \"id\": 15}, \n     {\"name\": \"Longformer\", \"id\": 16}, \n     {\"name\": \"BigBird\", \"id\": 17},",
        "evidence": []
    },
    {
        "title": "Data Innovation for International Development: An overview of natural language processing for qualitative data analysis",
        "answer": "{\"name\": \"Natural Language Processing\", \"elements\": [\"Named Entity Recognition\", \"Part-of-Speech Tagging\", \"Dependency Parsing\", \"Semantic Role Labeling\", \"Coreference Resolution\", \"Sentiment Analysis\", \"Text Classification\", \"Topic Modeling\", \"Named Entity Disambiguation\", \"Coreference Resolution\", \"Question Answering\", \"Machine Translation\", \"Text Summarization\", \"Named Entity Recognition\", \"Part-of-Speech Tagging\", \"Dependency Parsing\", \"Semantic Role Labeling\", \"Coreference Resolution\", \"Sentiment Analysis\", \"Text Classification\", \"Topic Modeling\", \"Named Entity Disambiguation\", \"Question Answering\", \"Machine Translation\", \"Text Summarization\"]}",
        "evidence": []
    },
    {
        "title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages",
        "answer": "{\"name\": \"SentimentIntensityAnalyzer\", \"name\": \"VaderSentiment\", \"name\": \"TextBlob\"}",
        "evidence": []
    },
    {
        "title": "TutorialVQA: Question Answering Dataset for Tutorial Videos",
        "answer": "{\"name\": \"source\", \"value\": \"Wikipedia\"}",
        "evidence": []
    },
    {
        "title": "Transductive Learning with String Kernels for Cross-Domain Text Classification",
        "answer": "{\"name\": \"Decision Trees\", \"description\": \"Decision Trees are a type of supervised learning algorithm used for both classification and regression tasks. They work by recursively partitioning the data into smaller subsets based on the features of the data.\"}\n     {\"name\": \"Random Forest\", \"description\": \"Random Forest is an ensemble learning method that combines multiple decision trees to improve the accuracy and robustness of the model.\"}\n     {\"name\": \"Support Vector Machines\", \"description\": \"Support Vector Machines (SVMs) are a type of supervised learning algorithm used for classification and regression tasks. They work by finding the hyperplane that maximally separates the classes in the feature space.\"}\n     {\"name\": \"K-Means\", \"description\": \"K-Means is an unsupervised learning algorithm used for clustering tasks. It works by partitioning the data into K clusters based on the mean distance of the data points.\"}\n     {\"name\": \"Gradient Boosting\", \"description\": \"Gradient Boosting is an ensemble learning method that combines multiple weak models to create a strong predictive model. It works by iteratively adding models to correct the errors of the previous models.\"}\n     {\"name\": \"Neural Networks\", \"description\": \"Neural Networks are",
        "evidence": []
    },
    {
        "title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches",
        "answer": "assistant\n    IMDB",
        "evidence": []
    },
    {
        "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
        "answer": "{\"name\": \"layers\", \"count\": 5}",
        "evidence": []
    },
    {
        "title": "\"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach",
        "answer": "{\"name\": \"CNN\", \"description\": \"Convolutional Neural Network\", \"url\": \"https://en.wikipedia.org/wiki/Convolutional_neural_network\", \"type\": \"object\"} \n     {\"name\": \"explainability\", \"description\": \"the ability to understand how a model makes predictions\", \"url\": \"https://en.wikipedia.org/wiki/Explanation_of_artificial_intelligence\", \"type\": \"object\"} \n     {\"name\": \"feature maps\", \"description\": \"a representation of the input data that is used by the CNN model\", \"url\": \"https://en.wikipedia.org/wiki/Convolutional_neural_network#Feature_extraction\", \"type\": \"object\"} \n     {\"name\": \"activation functions\", \"description\": \"used to introduce non-linearity into the model\", \"url\": \"https://en.wikipedia.org/wiki/Activation_function\", \"type\": \"object\"} \n     {\"name\": \"pooling\", \"description\": \"a technique used to reduce the spatial dimensions of the feature maps\", \"url\": \"https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling\", \"type\": \"object\"} \n     {\"name\": \"explainability\", \"description\": \"the ability to understand how a model",
        "evidence": []
    },
    {
        "title": "Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks",
        "answer": "{\"name\": \"task1\", \"description\": \"This is task1\", \"status\": \"completed\"},\n     {\"name\": \"task2\", \"description\": \"This is task2\", \"status\": \"in_progress\"},\n     {\"name\": \"task3\", \"description\": \"This is task3\", \"status\": \"pending\"}",
        "evidence": []
    },
    {
        "title": "Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation",
        "answer": "{\"name\": \"model\", \"value\": 0.5}",
        "evidence": []
    },
    {
        "title": "Sentiment Analysis of Citations Using Word2vec",
        "answer": "{\"name\": \"metrics\", \"description\": \"Metrics are the measures used to evaluate the performance of a system, process, or project. They can be quantitative or qualitative, and are often used to track progress, identify areas for improvement, and make data-driven decisions.\"}",
        "evidence": []
    },
    {
        "title": "Neural Factor Graph Models for Cross-lingual Morphological Tagging",
        "answer": "{\"name\": \"English\", \"code\": \"en\"}, \n     {\"name\": \"Spanish\", \"code\": \"es\"}, \n     \"French\", \n     \"German\", \n     \"Italian\", \n     \"Portuguese\", \n     \"Chinese\", \n     \"Japanese\", \n     \"Korean\", \n     \"Arabic\", \n     \"Russian\", \n     \"Hebrew\", \n     \"Polish\", \n     \"Greek\", \n     \"Turkish\", \n     \"Thai\", \n     \"Vietnamese\", \n     \"Hindi\", \n     \"Swedish\", \n     \"Dutch\", \n     \"Norwegian\", \n     \"Danish\", \n     \"Finnish\", \n     \"Czech\", \n     \"Hungarian\", \n     \"Romanian\", \n     \"Bulgarian\", \n     \"Croatian\", \n     \"Slovak\", \n     \"Slovenian\", \n     \"Ukrainian\", \n     \"Lithuanian\", \n     \"Estonian\", \n     \"Latvian\", \n     \"Albanian\", \n     \"Serbian\", \n     \"Macedonian\", \n     \"Montenegrin\", \n     \"Bosnian",
        "evidence": []
    },
    {
        "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
        "answer": "{\"name\": \"Bert\", \"name\": \"RoBERTa\", \"name\": \"DistilBERT\", \"name\": \"ALBERT\", \"name\": \"Longformer\", \"name\": \"T5\", \"name\": \"Electra\", \"name\": \"XLNet\", \"name\": \"FlauBERT\", \"name\": \"MobileBERT\", \"name\": \"SpanBERT\", \"name\": \"Funnelator\", \"name\": \"MiniLM\", \"name\": \"BigBird\", \"name\": \"CAMS\", \"name\": \"CAMS-EN\", \"name\": \"CAMS-EN-CC\", \"name\": \"CAMS-EN-CC-EN\", \"name\": \"CAMS-EN-CC-EN-CC\", \"name\": \"CAMS-EN-CC-EN-CC-EN\", \"name\": \"CAMS-EN-CC-EN-CC-EN-CC\", \"name\": \"CAMS-EN-CC-EN-CC-EN-CC-EN\", \"name\": \"CAMS-EN-CC-EN-CC-EN-CC-EN-CC\", \"name\": \"CAMS-EN-CC-EN-CC-EN-CC-",
        "evidence": []
    },
    {
        "title": "Multichannel Variable-Size Convolution for Sentence Classification",
        "answer": "{\"name\": \"MVCNN\", \"description\": \"MVCNN is a type of neural network architecture that combines the strengths of both CNN and MVC. It is designed to handle sequential data, such as video or audio, and is particularly useful for tasks like action recognition and image captioning. MVCNN is more complex than CNN, but it can learn more complex patterns and relationships between data.\"}",
        "evidence": []
    },
    {
        "title": "A Lightweight Front-end Tool for Interactive Entity Population",
        "answer": "{\"name\": \"Python\", \"version\": \"3.8.10\"}",
        "evidence": []
    },
    {
        "title": "Modeling Trolling in Social Media Conversations",
        "answer": "{\"name\": \"dataset_size\", \"value\": 1000}",
        "evidence": []
    },
    {
        "title": "Rotations and Interpretability of Word Embeddings: the Case of the Russian Language",
        "answer": "Interpretability is evaluated using the following metrics: (1) accuracy, (2) F1-score, and (3) feature importance.",
        "evidence": []
    },
    {
        "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study",
        "answer": "{\"name\": \"pitfalls\", \"description\": \"The paper mentions the following pitfalls: (1) overfitting, (2) underfitting, (3) overparameterization, and (4) overregularization.\"}",
        "evidence": []
    },
    {
        "title": "Attention Optimization for Abstractive Document Summarization",
        "answer": "{\"name\": \"accuracy\", \"description\": \"The proportion of correct predictions out of all predictions made.\"}\n     {\"name\": \"precision\", \"description\": \"The proportion of true positives out of all positive predictions.\"}\n     {\"name\": \"recall\", \"description\": \"The proportion of true positives out of all actual positive instances.\"}\n     {\"name\": \"F1-score\", \"description\": \"The harmonic mean of precision and recall.\"}\n     {\"name\": \"AUC-ROC\", \"description\": \"The area under the receiver operating characteristic curve.\"}\n     {\"name\": \"AUC-PR\", \"description\": \"The area under the precision-recall curve.\"}",
        "evidence": []
    },
    {
        "title": "Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling",
        "answer": "{\"name\": \"seq2seq\", \"description\": \"Sequence-to-Sequence (seq2seq) is a class of models that can be used for tasks such as machine translation, text summarization, and chatbots. It is a type of neural network architecture that takes in a sequence of input tokens and generates a sequence of output tokens.\", \"url\": \"https://en.wikipedia.org/wiki/Sequence-to-sequence_model\"}\n    {\"name\": \"Transformer\", \"description\": \"The Transformer is a type of neural network architecture that is particularly well-suited for sequence-to-sequence tasks. It was introduced in 2017 and has since become a widely used architecture for tasks such as machine translation, text summarization, and question answering.\", \"url\": \"https://arxiv.org/abs/1706.03762\"}\n    {\"name\": \"Recurrent Neural Network (RNN)\", \"description\": \"RNNs are a type of neural network architecture that are well-suited for sequence-to-sequence tasks. They are particularly useful for tasks that involve sequential data, such as speech recognition and natural language processing.\", \"url\": \"https://en.wikipedia.org/wiki/Recurrent_neural_network\"}\n    {\"name\": \"Encoder-Decoder Architecture\", \"description\":",
        "evidence": []
    },
    {
        "title": "Leveraging Discourse Information Effectively for Authorship Attribution",
        "answer": "{\"name\": \"2022\", \"value\": \"2022\"}",
        "evidence": []
    },
    {
        "title": "Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents",
        "answer": "{\"name\": \"accuracy\", \"description\": \"The proportion of correct predictions out of all predictions made.\"}",
        "evidence": []
    },
    {
        "title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled Data",
        "answer": "{\"name\": \"Hurricane\", \"description\": \"a storm with strong winds and heavy rain\", \"magnitude\": 5},\n     {\"name\": \"Tsunami\", \"description\": \"a large ocean wave caused by an earthquake or landslide\", \"magnitude\": 6},\n     {\"name\": \"Earthquake\", \"description\": \"a sudden shaking of the ground\", \"magnitude\": 7},\n     {\"name\": \"Volcanic Eruption\", \"description\": \"the release of hot ash and gas from a volcano\", \"magnitude\": 8},\n     {\"name\": \"Flood\", \"description\": \"a large amount of water overflowing onto land\", \"magnitude\": 9}",
        "evidence": []
    },
    {
        "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding",
        "answer": "{\"name\": \"knowledge_graph_completion\", \"tasks\": [\"entity_disambiguation\", \"relation_extraction\", \"entity_typing\", \"relation_prediction\", \"slot_filling\"]}",
        "evidence": []
    },
    {
        "title": "The First Evaluation of Chinese Human-Computer Dialogue Technology",
        "answer": "{\"name\": \"accuracy\", \"description\": \"The ratio of correct predictions to total predictions.\"}, \n     {\"name\": \"precision\", \"description\": \"The ratio of true positives to the sum of true positives and false positives.\"}, \n     {\"name\": \"recall\", \"description\": \"The ratio of true positives to the sum of true positives and false negatives.\"}, \n     {\"name\": \"f1-score\", \"description\": \"The harmonic mean of precision and recall.\"}, \n     {\"name\": \"roc_auc_score\", \"description\": \"The area under the receiver operating characteristic curve.\"}, \n     {\"name\": \"mean_squared_error\", \"description\": \"The average squared difference between predicted and actual values.\"}, \n     {\"name\": \"mean_absolute_error\", \"description\": \"The average absolute difference between predicted and actual values.\"}, \n     {\"name\": \"mean_absolute_percentage_error\", \"description\": \"The average absolute percentage difference between predicted and actual values.\"}, \n     {\"name\": \"r2_score\", \"description\": \"The coefficient of determination.\"}]",
        "evidence": []
    },
    {
        "title": "BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization",
        "answer": "{\"name\": \"Template\", \"description\": \"A template is a pre-defined structure or format for a document, presentation, or other content that can be used to create multiple instances of the same content.\", \"url\": \"https://en.wikipedia.org/wiki/Template_(disambiguation)\"}\n     {\"name\": \"Training data\", \"description\": \"Data used to train a model, such as text, images, or audio.\", \"url\": \"https://en.wikipedia.org/wiki/Training_data\"}\n     {\"name\": \"Template discovery\", \"description\": \"The process of identifying and extracting patterns or structures from training data to create templates.\", \"url\": \"https://en.wikipedia.org/wiki/Template_discovery\"}",
        "evidence": []
    },
    {
        "title": "AttSum: Joint Learning of Focusing and Summarization with Neural Attention",
        "answer": "{\"name\": \"model\", \"type\": \"text\", \"value\": \"BERT\", \"label\": \"model\"} \n     {\"name\": \"model\", \"type\": \"text\", \"value\": \"RoBERTa\", \"label\": \"model\"} \n     {\"name\": \"model\", \"type\": \"text\", \"value\": \"BERT\", \"label\": \"model\"} \n     {\"name\": \"model\", \"type\": \"text\", \"value\": \"RoBERTa\", \"label\": \"model\"}",
        "evidence": []
    },
    {
        "title": "A Simple Approach to Multilingual Polarity Classification in Twitter",
        "answer": "{\n    \t\"components\": [\n    \t\"Language Model\",\n    \t\"Tokenizer\",\n    \t\"Encoder\",\n    \t\"Decoder\",\n    \t\"Attention Mechanism\",\n    \t\"Positional Encoding\",\n    \t\"Embedding Layer\",\n    \t\"Output Layer\"\n    ]",
        "evidence": []
    },
    {
        "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing",
        "answer": "{\"name\": \"baselines\", \"description\": \"A baseline is a standard or reference point used as a basis for comparison.\"}",
        "evidence": []
    },
    {
        "title": "Story Ending Generation with Incremental Encoding and Commonsense Knowledge",
        "answer": "{\"name\": \"baseline1\", \"description\": \"baseline1 description\"}, \n     {\"name\": \"baseline2\", \"description\": \"baseline2 description\"}",
        "evidence": []
    },
    {
        "title": "Multi-Source Syntactic Neural Machine Translation",
        "answer": "{\"name\": \"performance_drop\", \"value\": 0.0}",
        "evidence": []
    },
    {
        "title": "Domain Adaptation for Neural Networks by Parameter Augmentation",
        "answer": "{\"name\": \"1\", \"count\": 1}",
        "evidence": []
    },
    {
        "title": "Interactive Machine Comprehension with Information Seeking Agents",
        "answer": "\"https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor\">tf.estimator.LinearRegressor</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor\">tf.estimator.DNNRegressor</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearRegressor\">tf.estimator.DNNLinearRegressor</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier\">tf.estimator.DNNClassifier</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearClassifier\">tf.estimator.DNNLinearClassifier</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier\">tf.estimator.LinearClassifier</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier\">tf.estimator.BoostedTreesClassifier</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesRegressor\">tf.estimator.BoostedTreesRegressor</a>, <",
        "evidence": []
    },
    {
        "title": "Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models",
        "answer": "{\"name\": \"LSTM\", \"description\": \"Long Short-Term Memory\", \"url\": \"https://en.wikipedia.org/wiki/Long_short-term_memory\"}\n     {\"name\": \"HMM\", \"description\": \"Hidden Markov Model\", \"url\": \"https://en.wikipedia.org/wiki/Hidden_Markov_model\"}\n     {\"name\": \"LSTM\", \"description\": \"Long Short-Term Memory\", \"url\": \"https://en.wikipedia.org/wiki/Long_short-term_memory\"}\n     {\"name\": \"HMM\", \"description\": \"Hidden Markov Model\", \"url\": \"https://en.wikipedia.org/wiki/Hidden_Markov_model\"}\n     {\"name\": \"LSTM\", \"description\": \"Long Short-Term Memory\", \"url\": \"https://en.wikipedia.org/wiki/Long_short-term_memory\"}\n     {\"name\": \"HMM\", \"description\": \"Hidden Markov Model\", \"url\": \"https://en.wikipedia.org/wiki/Hidden_Markov_model\"}\n     {\"name\": \"LSTM\", \"description\": \"Long Short-Term Memory\", \"url\": \"https://en.wikipedia.org/wiki/Long_short-term_memory\"}\n     {\"name\": \"HMM\", \"description\": \"Hidden Markov Model\", \"url",
        "evidence": []
    },
    {
        "title": "BERT-Based Arabic Social Media Author Profiling",
        "answer": "{\"name\": \"in-house data\", \"description\": \"Data that is generated or collected within an organization, such as customer information, sales data, and employee data.\"}",
        "evidence": []
    },
    {
        "title": "WiSeBE: Window-based Sentence Boundary Evaluation",
        "answer": "://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google.com/url?q=https://www.google",
        "evidence": []
    },
    {
        "title": "Recognizing Musical Entities in User-generated Content",
        "answer": "{\"name\": \"Decision Tree\", \"description\": \"Decision Tree is a supervised learning algorithm that uses a tree-like model to classify data.\"}\n     {\"name\": \"Random Forest\", \"description\": \"Random Forest is an ensemble learning algorithm that combines multiple decision trees to improve the accuracy of predictions.\"}\n     {\"name\": \"Gradient Boosting\", \"description\": \"Gradient Boosting is an ensemble learning algorithm that combines multiple weak models to create a strong predictive model.\"}\n     {\"name\": \"K-Means\", \"description\": \"K-Means is an unsupervised learning algorithm that groups similar data points into clusters based on their features.\"}\n     {\"name\": \"SVM\", \"description\": \"SVM is a supervised learning algorithm that uses a hyperplane to classify data into different classes.\"}\n     {\"name\": \"K-NN\", \"description\": \"K-NN is a supervised learning algorithm that classifies data based on the majority vote of its k nearest neighbors.\"}\n     {\"name\": \"Neural Network\", \"description\": \"Neural Network is a supervised or unsupervised learning algorithm that uses artificial neural networks to learn complex patterns in data.\"}\n     {\"name\": \"Naive Bayes\", \"description\": \"Naive",
        "evidence": []
    },
    {
        "title": "Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER",
        "answer": "{\"name\": \"Adversarial learning\", \"description\": \"Adversarial learning is a type of machine learning where the model is trained to generate data that is likely to mislead the model, in order to improve its robustness.\", \"performance\": \"stronger performance gains\", \"text classification\": \"text classification\", \"NER\": \"Named Entity Recognition\", \"classification\": \"classification\", \"recognition\": \"recognition\"}\n    {\"name\": \"Adversarial learning\", \"description\": \"Adversarial learning is a type of machine learning where the model is trained to generate data that is likely to mislead the model, in order to improve its robustness.\", \"performance\": \"stronger performance gains\", \"text classification\": \"text classification\", \"NER\": \"Named Entity Recognition\", \"classification\": \"classification\", \"recognition\": \"recognition\"}\n    {\"name\": \"Adversarial learning\", \"description\": \"Adversarial learning is a type of machine learning where the model is trained to generate data that is likely to mislead the model, in order to improve its robustness.\", \"performance\": \"stronger performance gains\", \"text classification\": \"text classification\", \"NER\": \"Named Entity Recognition\", \"classification\": \"classification\", \"",
        "evidence": []
    }
]