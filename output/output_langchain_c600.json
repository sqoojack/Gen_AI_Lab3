[
    {
        "title": "Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations",
        "answer": "They utilize unlabeled data to improve model representations by incorporating it into the training process, using it to fine-tune the model, and assigning a new dropout rate to the local representation vectors.",
        "evidence": [
            "learn models with open-data or test-agnostic data. In this way, the learned models behave more like humans.",
            "improves the F1 score slightly, from 88.8% (row (g)) to 88.9% (row (h)) on the UGWC dataset. This result occurs because both the labeled and unlabeled data in the UGWC dataset are drawn from the same finance domain. The average number of new words found in a new unlabeled data passage is only 0.650, as shown in Table TABREF32 . Therefore, there is little additional information to be learned from unlabeled data. CVT also improved the model on the IWSLT dataset, from an overall F1 score of 64.5% (row (g)) to 65.3% (row (h)) and from a 2-class F1 score of 81.7% to 82.7%. Because both the labeled",
            "Experiments ::: Label-fully-unseen evaluation",
            "Model Improvements",
            "The second is the number of unlabeled mini-batches INLINEFORM0 used for training between supervised mini-batches. Third, rather than using the same dropout rate for the local representation vectors, a new dropout rate is assigned. The details of hyperparameters such as the hidden size of each layer and dropout rate are given in Section SECREF7 .",
            "more works remains to be done in this field, and recent developments in unsupervised generative models already promise possible extensions to our analysis. For example, since the introduction of GANs, research groups have studied how variational autoencoders can be incorporated into these models, to increase the clarity and accuracy of the models’ outputs. Wang et al. in particular demonstrate how using a variational autoencoder in the generator model can increase the accuracy of generated text samples [11]. Most promisingly, the data used by the group was a large data set of Amazon customer",
            "can be encoded in the model through graph representation techniques, and therefore, be used to facilitate understanding story context and inferring coherent endings. Integrating the context clues and commonsense knowledge, the model can generate more reasonable endings than state-of-the-art baselines. Our contributions are as follows:",
            "in neural models is a less studied question. Improvements over the state of the art might be achieved by integrating lexical information both from an external lexicon and from word vector representations into tagging models. In that regard, further work will be required to understand which class of models perform the best. An option would be to integrate feature-based models such as a CRF with an LSTM-based layer, following recent proposals such as the one proposed by BIBREF45 for named entity recognition.",
            "appropriate role of unsupervised methods in political science research, because they require human interpretation and lack well-defined criteria for accuracy and performance assessment. Both the word embeddings and multilayer community detection procedure are unsupervised methods, and so we posit that in order to validate that these located affinity blocs really do capture meaningful preference similarity in international politics, they should be able to improve upon our current ability to predict conflict onset out-of-sample. This would enable us to more adequately explain observed state",
            "There are several recent studies applying domain adaptation methods to deep neural networks. However, few studies have focused on improving the fine tuning and dual outputs methods in the supervised setting. sun2015return have proposed an unsupervised domain adaptation method and apply it to the features from deep neural networks. Their idea is to minimize the domain shift by aligning the second-order statistics of source and target distributions. In our setting, it is not necessarily true that there is a correspondence between the source and target input distributions, and therefore we",
            "as in machine reading comprehension, rather than selects key information from the two to form an enhanced article representation.",
            "To further improve the performance of our models, we introduce in-house labeled data that we use to fine-tune BERT. For the gender classification task, we manually label an in-house dataset of 1,100 users with gender tags, including 550 female users, 550 male users. We obtain 162,829 tweets by crawling the 1,100 users' timelines. We combine this new gender dataset with the gender TRAIN data (from shared task) to obtain an extended dataset, to which we refer as EXTENDED_Gender. For the dialect identification task, we randomly sample 20,000 tweets for each class from an in-house dataset gold"
        ]
    },
    {
        "title": "Artificial Error Generation with Machine Translation and Syntactic Patterns",
        "answer": "English, Vietnamese, Chinese, French, Europe, Spanish, German, Italian, Portuguese, Dutch, Greek, Russian, Japanese, Korean, Arabic, Hebrew, Hindi, Polish, Turkish, Swedish, Danish, Norwegian, Finnish, Romanian, Hungarian, Czech, Slovak, Bulgarian, Croatian, Serbian, Ukrainian, Estonian, Latvian, Lithuanian, Icelandic, Azerbaijani, Georgian, Belarusian, Albanian, Bosnian, Macedonian, Montenegrin, Kazakh, Uzbek, Azerbaijani, Kazakh, Uzbek, Mongolian, Thai, Khmer, Lao, Burmese, Indonesian, Malay, Swahili, Zulu, Yoruba, Igbo, Shona, Swahili, Yoruba, Igbo, Shona, Amharic, Tigrinya, Oromo, Somali, Hausa, Yiddish, Welsh, Breton, Cornish, Irish, Scottish Gaelic, Manx, Frisian, Occitan, Catalan, Basque, Galician, Asturian, Occitan, Catalan, Basque, Galician, Asturian, Welsh, Breton, Cornish, Irish, Scottish Gaelic, Manx, Frisian, Y",
        "evidence": [
            "Results on Unseen Languages",
            "that can be used in testing protocols such as the one we proposed. Despite this evolution, these baselines (or their improved versions) can be used in applications such as automatic document summarisation (e.g., BIBREF12, BIBREF13), or sentences compression BIBREF14. The main feature of the proposed baseline system is its flexibility with respect to the language considered. In fact, it only uses a list of language markers and the grammatical category of words. The first resource, although dependent on each language, is relatively easy to obtain. We have found that, even with lists of moderate",
            "Paper Overview",
            "before and were also not familiar with the packaged products including the chosen 20 products; hence, they were \"illiterate\" in terms of comprehending English and in terms of having used any of the products although they might be literate in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this",
            "this work, we propose contextual RNN language models that specially track the interactions between speakers. We expect such models to generate better representations of the dialog context. The remainder of the paper is organized as follows. In section 2, we introduce the background on contextual language modeling. In section 3, we describe the proposed dialog context language models. Section 4 discusses the evaluation procedures and results. Section 5 concludes the work.",
            "poses challenges, in terms of high costs, time, or expertise required. The application of natural language processing (NLP) and machine learning (ML) can make feasible the speedy analysis of qualitative data on a large scale. We start with a brief description of the main approaches to NLP and how they can be augmented by human coding. We then move on to the issue of working with multiple languages and different document formats. We then provide an overview of the recent application of these techniques in a United Nations Development Program (UNDP) study.",
            "Data Collection. To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. Paragraph Alignment. To further ensure the quality of the new dataset, the work of paragraph alignment is manually",
            "especially interesting for evaluating a multilingual system, because different languages often map the same grapheme to phonemes that are only subtly different from each other. However, these statistics have not been widely reported for other g2p systems, so we omit them here.",
            "Experiments ::: Linguistic Probes ::: Probing Tasks",
            "Acknowledgments\nWe would like to acknowledge the support of CHIST-ERA for funding this work through the Access Multilingual Information opinionS (AMIS), (France - Europe) project. We also like to acknowledge the support given by the Prof. Hanifa Boucheneb from VERIFORM Laboratory (École Polytechnique de Montréal).",
            "of the language. Our approach achieves those performance's levels since it optimizes a set of parameters carefully selected to work on a variety of languages and being robust to informal writing. The latter problem is not properly tackled in many cases.",
            "Analysis of the convolutional language model"
        ]
    },
    {
        "title": "Generating Word and Document Embeddings for Sentiment Analysis",
        "answer": "using (DISPLAY_FORM4) and obtained from the training data.",
        "evidence": [
            "Our last component is a simple metric that uses four supervised scores for each word in the corpus. We extract these scores as follows. For a target word in the corpus, we scan through all of its contexts. In addition to the target word's polarity score (the self score), out of all the polarity scores of words occurring in the same contexts as the target word, minimum, maximum, and average scores are taken into consideration. The word polarity scores are computed using (DISPLAY_FORM4). Here, we obtain those scores from the training data. The intuition behind this method is that those four",
            "Supervised models",
            "Word Error Rate results",
            "After a long period of research, learning-based models like Logistic Regression BIBREF1 etc. have become growingly popular in this area. However, most current supervised summarization systems often perform the two tasks in isolation. Usually, they design query-dependent features (e.g., query word overlap) to learn the relevance ranking, and query-independent features (e.g., term frequency) to learn the saliency ranking. Then, the two types of features are combined to train an overall ranking model. Note that the only supervision available is the reference summaries. Humans write summaries",
            "and 11.9K turns (22.2K utterances) respectively. Maximum turn length is set to 160. The vocabulary is defined with the top frequent 10K words.",
            "will be deleted from the original clauses. However, some ancient characters do not appear in its corresponding modern Chinese words. An ancient Chinese dictionary is employed to address this issue. We preprocess the ancient Chinese dictionary and remove the stop words. In this dictionary matching step, we retrieve the dictionary definition of each unmatched ancient character and use it to match the remaining modern Chinese words. To reduce the impact of universal word matching, we use Inverse Document Frequency (IDF) to weight the matching words. The lexical matching score is calculated as:",
            "indicate that additional post-processing methods such as abtt and uncovec on top of mean-centered vector spaces can lead to further gains in most languages. The gains are even visible for languages which start from high correlation scores: for instance., cmn with CC+Wiki ft increases from 0.534 to 0.583, from 0.315 to 0.526 with Wiki ft, and from 0.408 to 0.487 with m-bert. Similarly, for rus with CC+Wiki ft we can improve from 0.422 to 0.500, and for fra the scores improve from 0.578 to 0.613. There are additional similar cases reported in Table TABREF43. Overall, the unsupervised",
            "Multi-SimLex: Translation and Annotation ::: Guidelines and Word Pair Scoring",
            "they cannot be determined uniquely with high confidence. Normalized interpretibility measures for different components (calculated for 50 top/bottom words) for the source and the rotated models are shown in Fig. 4.",
            "Dictionaries are frequently used to code texts in content analyses BIBREF37 . Dictionaries consist of one or more categories (i.e. word lists). Sometimes the output is simply the number of category occurrences (e.g., positive sentiment), thus weighting words within a category equally. In some other cases, words are assigned continuous scores. The high transparency of dictionaries makes them sometimes more suitable than supervised machine learning models. However, dictionaries should only be used if the scores assigned to words match how the words are used in the data (see BIBREF38 for a",
            "via majority vote (at least 3 out of 5). Consequently, for each question with a majority vote on either text-based or script-based, there are 3 to 5 correct and incorrect answer candidates, one from each participant who agreed on the category. Questions without a clear majority vote or with ties were not included in the dataset. We performed four post-processing steps on the collected data. We manually filtered out texts that were instructional rather than narrative. All texts, questions and answers were spellchecked by running aSpell and manually inspecting all corrections proposed by the",
            "how frequently they appear across multiple documents. Therefore, common words such as the and in, which appear in many documents, will have a low score, and words that appear frequently in a single document will have high score. This weighting scheme selects the terms that represent a document."
        ]
    },
    {
        "title": "Using General Adversarial Networks for Marketing: A Case Study of Airbnb",
        "answer": "30 hours",
        "evidence": [
            "For each listing, we were given information of the amenities of the listing (number of bathrooms, number of bedrooms …), the listing’s zip code, the host’s description of the listing, the price of the listing, and the occupancy rate of the listing. Airbnb defines a home's occupancy rate, as the percentage of time that a listing is occupied over the time period that the listing is available. This gives us a reasonable metric for defining popular versus less popular listings.",
            "How Many Expert Annotations?",
            "Experiment Setup",
            "The development of online peer-to-peer markets in the 1990s, galvanized by the launch of sites like eBay, fundamentally shifted the way buyers and sellers could connect [4]. These new markets not only leveraged technology to allow for faster transaction speeds, but in the process also exposed a variety of unprecedented market-designs [4]. Today, many of the most well-known peer-to-peer markets like Uber and Instacart use a centralized system that matches workers with assigned tasks via a series of complex algorithms [4]. Still, a number of other websites like Airbnb and eBay rely on sellers",
            "vocabulary size (depending on different query type settings, we mask out words in the vocabulary that are not query candidates). The overall Q-value is simply the sum of the two components:",
            "Our AV in-cabin dataset includes around 30 hours of multi-modal data collected from 30 passengers (15 female, 15 male) in a total of 20 rides/sessions. In 10 sessions, single passenger was present (i.e., singletons), whereas the remaining 10 sessions include two passengers (i.e., dyads) interacting with the vehicle. The data is collected \"in the wild\" on the streets of Richmond, British Columbia, Canada. Each ride lasted about 1 hour or more. The vehicle is modified to hide the operator and the human acting as in-cabin agent from the passengers, using a variation of WoZ approach BIBREF20 .",
            "Other languages, other ambiguities",
            "Supplemental Material ::: Hyperparameters ::: Shallow Syntax\nThe size of the shallow syntactic feature embedding was 50 across all experiments, initialized uniform randomly. All model implementations are based on the AllenNLP library BIBREF33.",
            "samples [11]. Most promisingly, the data used by the group was a large data set of Amazon customer reviews, which in many ways parallels the tone and semantic structure of Airbnb listing descriptions. More generally, Bowman et al. demonstrate how the use of variational autoencoders presents a more accurate model for text generation, as compared to standard recurrent neural network language models [1]. For future work, we may also consider experimenting with different forms of word embeddings, such as improved word vectors (IWV) which were suggested by Rezaeinia et al [10]. These IMV word",
            "WiSeBEWiSeBE",
            "Introduction ::: Sharing is caring",
            "the data while making it easy for workers to apply labels consistently.  Size: The total of 13,215 dialogs in this corpus is on par with similar, recently released datasets such as MultiWOZ BIBREF13."
        ]
    },
    {
        "title": "Question Answering for Privacy Policies: Combining Computational and Legal Perspectives",
        "answer": "Yes.",
        "evidence": [
            "Comparison with baseline models",
            "Emotion labels were not considered as mututally exclusive, and each emotion was assigned a score from 0 to 100. Training/developing data amounted to 250 annotated headlines (Affective development), while systems were evaluated on another 1000 (Affective test). Evaluation was done using two different methods: a fine-grained evaluation using Pearson's r to measure the correlation between the system scores and the gold standard; and a coarse-grained method where each emotion score was converted to a binary label, and precision, recall, and f-score were computed to assess performance. As it is",
            "are significant, as in BIBREF21 , albeit about half as good as with BT, and result in small improvements for the in-domain and for the out-of-domain tests. Experiments combining forward and backward translation (backfwdtrans-nmt), each using a half of the available artificial data, do not outperform the best BT results. We finally note the large remaining difference between BT data and natural data, even though they only differ in their source side. This shows that at least in our domain-adaptation settings, BT does not really act as a regularizer, contrarily to the findings of BIBREF4 ,",
            "sleep. For the classes higher in the hierarchy - no evidence of depression, evidence of depression, and depressive symptoms - the classifier produced consistent F1-scores, even slightly above the baseline for depressive symptoms and minor fluctuations of change in recall and precision when removing other feature groups suggesting that the contribution of non-lexical features to classification performance was limited. However, notable changes in F1-score were observed for the classes lower in the hierarchy including disturbed sleep and fatigue or loss of energy. For instance, changes in",
            "This system parallels the deep learning architecture we adapt. However, it predicts the system's uncertainty in its own answer, whereas we are interested in the humans' collective disagreement on the answer. Still, it is a useful baseline to see if an existing algorithm could serve our purpose. We evaluate the predictive power of the classification systems based on each classifier's predictions for the 121,512 visual questions in the test dataset. We first show performance of the baseline and our two prediction systems using precision-recall curves. The goals are to achieve a high precision,",
            "other hyperparameters were equal to their values for the QRNN and the same beam search procedure was applied. Table TABREF17 shows that the QRNN outperformed the character-level LSTM, almost matching the performance of a word-level attentional baseline.",
            "system INLINEFORM0 INLINEFORM1 , we normalize the associated score by dividing it by the maximum score among the INLINEFORM2 candidate questions retrieved by INLINEFORM3 for INLINEFORM4 :  INLINEFORM0   INLINEFORM0 INLINEFORM1  In our experiments, we fixed the value of INLINEFORM0 to 100. This threshold value was selected as a safe value for this task for the following reasons: Our collection of 47,457 question-answer pairs was collected from only 12 NIH institutes and is unlikely to contain more than 100 occurrences of the same focus-type pair. Each question was indexed with additional",
            "using the RTE-1, RTE-2 and RTE-3 open-domain datasets provided by the PASCAL challenge and the results were similar. We have also tested the SemEval-cQA-2016 model and had a similar drop in performance on RQE data. This could be explained by the different types of data leading to wrong internal conceptualizations of medical terms and questions in the deep neural layers. This performance drop could also be caused by the complexity of the test consumer health questions that are often composed of several subquestions, contain contextual information, and may contain misspellings and ungrammatical",
            "when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells",
            "packages without prior knowledge of the products while baseline 2 might provide additional information by showing top images from search engines. With the baseline 2, we attempt to measure whether merely adding \"relevant\" or \"similar\" products' images would be sufficient to improve the end-users' ability to comprehend the product's intended use. Moreover, with SimplerVoice, we test if our system could provide users with the proper visual components to help them understand the products' usage based on the proposed techniques, and measure the usefulness of SimplerVoice's generated description.",
            "when the source lexicons did not contain any. We also performed experiments in which we retained the full original tags provided by the lexicons, with all morphological features included. On average, results were slightly better than those presented in the paper, although not statistically significantly. Moreover, the granularity of tag inventories in the lexicons is diverse, which makes it difficult to draw general conclusions about results based on full tags. This is why we only report results based on (coarse) PoS extracted from the original lexicons.",
            "Besides the analysis on different reasoning types, we also look into the performance over questions with different first tokens in the development set, which provide us an automatic categorization of questions. According to the results in Table TABREF46 , the three neural baselines all perform the best on “Who” and “Where” questions, to which the answers are often named entities. Since the tweet contexts are short, there are only a small number of named entities to choose from, which could make the answer pattern easy to learn. On the other hand, the neural models fail to perform well on the"
        ]
    },
    {
        "title": "Dynamic Compositional Neural Networks over Tree Structure",
        "answer": "Task 1, Task 2, Task 3, Task 4, Task-oriented Dialogue, iMRC, MRC, RL, SimLex, Mult-SimLex, Online Testing, Offline Testing.  # Corrected Answer",
        "evidence": [
            "Translation experiments",
            "interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment to many users, we maintain a consistent set of video segments and extend the possibility to generate a diverse set of question for the same segment.",
            "Task Definition",
            "Experiments\nWe test the network on four classification tasks. We begin by specifying aspects of the implementation and the training of the network. We then report the results of the experiments.",
            "Task 2: Online Testing of Task-oriented Dialogue",
            "n-grams and GloVe features are the only group of features that contribute to more than a class type for the different tasks. Now, the “All Features” experiment shows how the interaction between feature sets perform than any of the other features groups in isolation. The accuracy metric for each trolling task is meant to provide an overall performance for all the classes within a particular task, and allow comparison between different experiments. In particular, we observe that GloVe vectors are the most powerful feature set, accuracy-wise, even better than the experiments with all features",
            "Experimenting with Transformers ::: Ecosystem",
            "at the same time also investigating other important questions such as performance of static versus contextualized word embeddings, or multilingual versus language-specific pretraining. Another purpose of the experiments is to outline the wide potential and applicability of the Multi-SimLex datasets for multilingual and cross-lingual representation learning evaluation.",
            "Experiments ::: How do the generated hypotheses influence",
            "and formulate the new task as an RL problem. We develop a baseline agent that combines a top performing MRC model and a state-of-the-art RL optimization algorithm and test it on our iMRC tasks. We conduct experiments on several variants of iMRC and discuss the significant challenges posed by our setting.",
            "Data details and experimental setup",
            "a more direct signal to learn a clear contrast between correct and incorrect words. This is an empirical problem we pursue in the experiments."
        ]
    },
    {
        "title": "Automatic Discourse Segmentation: an evaluation in French",
        "answer": "Automatic Evaluation, Human Evaluation, Precision, Recall, F1-score, Classification Error Rate and Slot Error Rate (SER) and informativity.",
        "evidence": [
            "In this section we will compare the results of the different segmentation systems through automatic evaluations. First of all, the human segmentation, from the subcorpus $D$ composed of common documents. The results are presented in the table tab:humains. The first row shows the performance of the $I$ segments, taking the experts as a reference, while the second presents the process in the opposite direction. We have found that segmentation by experts and naive produces two subcorpus $E$ and $N$ with very similar characteristics. This surprised us, as we expected a more important difference",
            "Sentence alignment quality",
            "cleaning”). This is achieved by segmenting words into subword units using segmentation techniques in the preprocessing phase prior to translation. There are several segmentation methods; Some are the complicate ones which require linguistic resources or human-crafted rules. Thus, they are not language-independent and expensive to obtain for low-resourced languages. Byte-Pair Encoding, otherwise, is a simple but robust technique to do subword segmentation. Since it is an unsupervised and fast technique, it has great effects when applied to build NMT systems for morphologically rich languages.",
            "iMRC: Making MRC Interactive ::: Evaluation Metric",
            "Description of segmentation strategies ::: Segmentation with explicit use of a marker",
            "dataset, our CVT model (row (h)) achieves an F1 score of 88.9%, which is higher than the F1 score of both the baselines (CRF-ngram and Bi-LSTM-CRF (rows d and e, respectively)). Thus, our model is now the state-of-the-art model for Thai sentence segmentation on both the Orchid and UGWC datasets. Our model outperforms all the sequence tagging models. T-BRNN-pre (row (c)) is the current state-of-the-art model, as shown in Table TABREF47 . The CVT model improves the overall F1 score from the 64.4% of T-BRNN-pre to 65.3% (row (h)), despite the fact that T-BRNN-pre integrates a pretrained word",
            ". Despite their differences in features and/or methodology, almost all previous cited research share a common element; the evaluation methodology. Metrics as Precision, Recall, F1-score, Classification Error Rate and Slot Error Rate (SER) are used to evaluate the proposed system against one reference. As discussed in Section SECREF1 , further NLP tasks rely on the result of SBD, meaning that is crucial to have a good segmentation. But comparing the output of a system against a unique reference will provide a reliable score to decide if the system is good or bad? Bohac et al. BIBREF24 compared",
            "from an ASR system will difficult in a large degree the manual segmentation process. The number of words per transcript oscilate between 271 and 1,602 with a total number of 8,080. We gave clear instructions to three evaluators ( INLINEFORM0 ) of how segmentation was needed to be perform, including the SU concept and how punctuation marks were going to be taken into account. Periods (.), question marks (?), exclamation marks (!) and semicolons (;) were considered SU delimiters (boundaries) while colons (:) and commas (,) were considered as internal SU marks. The number of segments per",
            "Automatic Evaluation",
            "The aim of this work was twofold: to design a discursive segmenter using a minimum of resources and to establish an evaluation protocol to measure the performance of segmenters. The results show that we can build a simple version of the baseline, which employs only a list of markers and presents a very encouraging performance. Of course, the quality of the list is a preponderant factor for a correct segmentation. We have studied the impact of the marker which, even though it may seem fringe-worthy, contributes to improving the performance of our segmenters. Thus, it is an interesting marker",
            "Baselines ::: Baseline2: Segment retrieval",
            "We then carried out a human evaluation to evaluate the generated summaries from another perspective. Our evaluators include 8 graduate students and 4 senior undergraduates, while the dataset is 100 randomly-selected articles from the test set. Each sample in this dataset also includes: 1 reference summary, 5 summaries generated by Open-NMT BIBREF21 , INLINEFORM0 BIBREF11 and BiSET under three settings, respectively, and 3 randomly-selected summaries for trapping. We asked the evaluators to independently rate each summary on a scale of 1 to 5, with respect to its quality in informativity,"
        ]
    },
    {
        "title": "Multimodal Named Entity Recognition for Short Social Media Posts",
        "answer": "13,215",
        "evidence": [
            "SnapCaptions Dataset",
            "We proposed a new multimodal NER (MNER: image + text) task on short social media posts. We demonstrated for the first time an effective MNER system, where visual information is combined with textual information to outperform traditional text-based NER baselines. Our work can be applied to myriads of social media posts or other articles across multiple platforms which often include both text and accompanying images. In addition, we proposed the modality attention module, a new neural mechanism which learns optimal integration of different modes of correlated information. In essence, the",
            "For the PTB dataset, we used the train-test split following BIBREF0, BIBREF5. For the E2E dataset, we used the train-test split from the original dataset BIBREF10 and indexed the words with a frequency higher than 3. We represent input data with 512-dimensional word2vec embeddings BIBREF13. We set the dimension of the hidden layers of both encoder and decoder to 256. The Adam optimiser BIBREF14 was used for training with an initial learning rate of 0.0001. Each utterance in a mini-batch was padded to the maximum length for that batch, and the maximum batch-size allowed is 128.",
            "pizza, creating auto repair appointments, setting up rides for hire, ordering movie tickets, ordering coffee drinks and making restaurant reservations.  Two collection methods: The two-person dialogs and self-dialogs each have pros and cons, revealing interesting contrasts.  Multiple turns: The average number of utterances per dialog is about 23 which ensures context-rich language behaviors.  API-based annotation: The dataset uses a simple annotation schema providing sufficient grounding for the data while making it easy for workers to apply labels consistently.  Size: The total of 13,215",
            "year of Reuters English newswire from August 1996 to August 1997. (ii) Huang. huang2012improving incorporated global context to deal with challenges raised by words with multiple meanings; size: 100,232 word embeddings; training corpus: April 2010 snapshot of Wikipedia. (iii) GloVe. Size: 1,193,514 word embeddings; training corpus: a Twitter corpus of 2B tweets with 27B tokens. (iv) SENNA. Size: 130,000 word embeddings; training corpus: Wikipedia. Note that we use their 50-dimensional embeddings. (v) Word2Vec. It has no 50-dimensional embeddings available online. We use released code to train",
            "in particular have proven extremely valuable to visual questions-answering tasks BIBREF9, the most similar being MovieQA BIBREF1 and VideoQA BIBREF0. Similar to how our data is generated from video tutorials, the MovieQA and VideoQA corpus is generated from movie scripts and news transcipts, respectively. MovieQA's answers have a shorter span than the answers collected in our corpus, because questions and answer pairs were generated after each paragraph in a movie's plot synopsis BIBREF1. The MovieQA dataset also contains specific annotated answers with incorrect examples for each question.",
            "without having to train an indefinitely large word embeddings matrix for arbitrarily noisy social media text datasets.",
            "is difficult. Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures BIBREF6 , BIBREF5 , BIBREF7 . Current CNN systems usually implement a convolution layer with fixed-size filters (i.e., feature detectors), in which the concrete filter size is a hyperparameter. They essentially split a sentence into multiple sub-sentences by a sliding window, then determine the sentence label by using the dominant label across all sub-sentences. The underlying assumption is that the sub-sentence",
            "In this experiment, we explore the benefit of adaptation from both sides of the domains. Flickr30K is another captioning dataset, consisting of 30K images, and each image has five captions BIBREF16 . Although the formats of the datasets are almost the same, the model trained by the MS COCO dataset does not work well for the Flickr 30K dataset and vice versa. The word distributions of the captions are considerably different. If we ignore words with less than 30 counts, MS COCO has 3,655 words and Flicker30K has 2732 words; and only 1,486 words are shared. Also, the average lengths of captions",
            "at 200-word intervals, and evaluate our CNNs in the multi-class condition. Generalization-dataset experiments. To confirm that our models generalize, we pick the best models from the baseline-dataset experiments and evaluate on the novel-50 and IMDB62 datasets. For novel-50, the chunking size applied is 2000-word as per the baseline-dataset experiment results, and for IMDB62, texts are not chunked (i.e., we feed the models with the original reviews directly). For model comparison, we also run the SVMs (i.e., SVM2 and SVM2-PV) used in the baseline-dataset experiment. All the experiments",
            "We evaluate the RQE methods (i.e. deep learning model and logistic regression classifier) using two datasets of sentence pairs (SNLI and multiNLI), and three datasets of question pairs (Quora, Clinical-QE, and SemEval-cQA). The Stanford Natural Language Inference corpus (SNLI) BIBREF13 contains 569,037 sentence pairs written by humans based on image captioning. The training set of the MultiNLI corpus BIBREF14 consists of 393,000 pairs of sentences from five genres of written and spoken English (e.g. Travel, Government). Two other \"matched\" and \"mismatched\" sets are also available for",
            "public events, venues, etc. All snaps were posted between year 2016 and 2017, and do not contain raw images or other associated information (only textual captions and obfuscated visual descriptor features extracted from the pre-trained InceptionNet are available). We split the dataset into train (70%), validation (15%), and test sets (15%). The captions data have average length of 30.7 characters (5.81 words) with vocabulary size 15,733, where 6,612 are considered unknown tokens from Stanford GloVE embeddings BIBREF22 . Named entities annotated in the SnapCaptions dataset include many of new"
        ]
    },
    {
        "title": "Neural Collective Entity Linking",
        "answer": "0.72",
        "evidence": [
            "methods, we evaluate NCEL on the following five datasets: (1) CoNLL-YAGO BIBREF22 : the CoNLL 2003 shared task including testa of 4791 mentions in 216 documents, and testb of 4485 mentions in 213 documents. (2) TAC2010 BIBREF39 : constructed for the Text Analysis Conference that comprises 676 mentions in 352 documents for testing. (3) ACE2004 BIBREF23 : a subset of ACE2004 co-reference documents including 248 mentions in 35 documents, which is annotated by Amazon Mechanical Turk. (4) AQUAINT BIBREF40 : 50 news articles including 699 mentions from three different news agencies. (5) WW BIBREF19",
            "Stage 0 - Naive approach",
            "a very effective method to address this issue and maybe we should further explore stronger models to learn ironic styles better.",
            "Results and Evaluation",
            "to the prior probability. Collectively, NCEL correctly identifies England cricket team with a probability of 0.72 as compared with 0.20 in NCEL-local with the help of its neighbor mention Essex.",
            "of all decoding steps from the global perspective. The performances on the CNN/Daily Mail dataset verify the effectiveness of our methods.",
            "NLP Toolkits",
            "in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index from the comments and queried it for comments containing the word “troll” with an edit distance of 1 in order to include close variations of this word, hypothesizing that such comments would be reasonable candidates of real trolling attempts. We did observe, however, that sometimes people use the word",
            "In particular, they apply a hierarchical encoder at the word and section levels. Then, in the decoding step, they combine the word attention and section attention to obtain a context vector. This approach to capture discourse structure is however quite limited both in general and especially when you consider its application to extractive summarization. First, their hierarchical method has a large number of parameters and it is therefore slow to train and likely prone to overfitting. Secondly, it does not take the global context of the whole document into account, which may arguably be",
            "Neural Collective Entity Linking\nNCEL incorporates GCN into a deep neural network to utilize structured graph information for collectively feature abstraction, while differs from conventional GCN in the way of applying the graph. Instead of the entire graph, only a subset of nodes is “visible\" to each node in our proposed method, and then the overall structured information shall be reached in a chain-like way. Fixing the size of the subset, NCEL is further speeded up by batch techniques and GPUs, and is efficient to large-scale data.",
            "\"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\" (MOS) BIBREF23 , BIBREF24 to measure the effectiveness: how similar a response were comparing to the correct product's usage. The MOS score range is 1-5 (1-Bad, 2-Poor, 3-Fair, 4-Good, 5-Excellent) with 1 means incorrect product usage interpretability - the lowest level of effectiveness and 5 means correct product usage interpretability - the highest effectiveness level. The assigned scores corresponding to responses were aggregated over all participated",
            "Acknowledgments\nThis work was funded by the Amazon Academic Research Awards program."
        ]
    },
    {
        "title": "Distant supervision for emotion detection using Facebook reactions",
        "answer": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.",
        "evidence": [
            "Selecting Facebook pages",
            "a list of words that distinguished the text in the treatment group (subreddits that were closed by Reddit) from text in the control group (similar subreddits that were not closed). The researchers then returned to the hate speech definition provided by the European Court of Human Rights, and manually filtered the top SAGE words based on this definition. Not all identified words fitted the definition. The others included: the names of the subreddits themselves, names of related subreddits, community-specific jargon that was not directly related to hate speech, and terms such as IQ and welfare,",
            "Weibo, RenRen, and Chinese microblogs",
            "We exploit the Facebook reaction feature in a distant supervised fashion to train a support vector machine classifier for emotion detection, using several feature combinations and combining different Facebook pages. We test our models on existing benchmarks for emotion detection and show that employing only information that is derived completely automatically, thus without relying on any handcrafted lexicon as it's usually done, we can achieve competitive results. The results also show that there is large room for improvement, especially by gearing the collection of Facebook pages, with a",
            "Research questions",
            "Facebook\nSocial media has been used to estimate preferences as well. The advantage of social media compared to speeches or any other preference indicator is coverage. BIBREF31 use endorsement of official pages on Facebook to scale ideological positions of politicians from different levels of government and the public into a common space. Their method extends to other social media such as Twitter where endorsements and likes could be leveraged.",
            "They used a convolutional neural network (CNN) model to classify a question into a restricted set of 10 question types and crawled \"relevant\" online web pages to find the answers. However, the results were lower than those achieved by the systems relying on finding similar answered questions. These results support the relevance of similar question matching for the end-to-end QA task as a new way of approaching QA instead of the classical QA approaches based on Question Analysis and Answer Retrieval.",
            "described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. Note that thankful was only available during specific time spans related to certain events, as Mother's Day in May 2016. For each page, we downloaded the latest 1000 posts, or the maximum available if there are fewer, from February 2016, retrieving the counts of reactions for each post. The output is a JSON file containing a list of dictionaries with a",
            "In this section, we describe how we build our dataset with tweets. First, we crawl over 2M tweets from twitter using GetOldTweets-python. We crawl English tweets from 04/09/2012 to /12/18/2018. We first remove all re-tweets and use langdetect to remove all non-English sentences. Then, we remove hashtags attached at the end of the tweets because they are usually not parts of sentences and will confuse our language model. After that, we utilize Ekphrasis to process tweets. We remove URLs and restore remaining hashtags, elongated words, repeated words, and all-capitalized words. To simplify our",
            "the assumption of this paper – posts attract users who hold the same stance to like them – is reliable, we examine the likes from authors of different stances. Posts in FBFans dataset are used for this analysis. We calculate the like statistics of each distinct author from these 32,595 posts. As the numbers of authors in the Sup, Neu and Uns stances are largely imbalanced, these numbers are normalized by the number of users of each stance. Table TABREF13 shows the results. Posts with stances (i.e., not neutral) attract users of the same stance. Neutral posts also attract both supportive and",
            "Results and Discussions",
            "there is large room for improvement, especially by gearing the collection of Facebook pages, with a view to the target domain."
        ]
    },
    {
        "title": "MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge",
        "answer": "CrowdFlower, Amazon Mechanical Turk, Reddit, Google Play Store, ICT-MMMO, YouTube, MOUD Datasets, Cloud Platform, Weibo, RenRen, Chinese microblogs, Reddit, Google Play Store, ICT-MMMO, YouTube, MOUD Datasets, Cloud Platform, Weibo, RenRen, Chinese microblogs.    (Note: This answer is not correct, I made a mistake. I will correct it now)",
        "evidence": [
            "Algorithms Used",
            "devised a crowdsourcing scheme with variable crowdworker numbers based on the difficulty of the annotation task. They provided a dataset of tweets where the sentiments towards political candidates were labeled both by experts in political communication and by crowdworkers who were likely not domain experts. BIBREF2 revealed that crowdworkers can match expert performance relatively accurately and in a budget-efficient manner. Given this result, the authors envisioned future work in which groundtruth labels would be crowdsourced for a large number of tweets and then used to design an automated",
            "The Taskmaster Corpus ::: Two-person, spoken dataset\nIn order to replicate a two-participant, automated digital assistant experience, we built a WOz platform that pairs agents playing the digital assistant with crowdsourced workers playing the user in task-based conversational scenarios. An example dialog from this dataset is given in Figure FIGREF5.",
            "Reddit is popular website that allows registered users (without identity verification) to participate in fora grouped by topic or interest. Participation consists of posting stories that can be seen by other users, voting stories and comments, and comments in the story's comment section, in the form of a forum. The forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of",
            "Weibo, RenRen, and Chinese microblogs",
            "and Igor Shalyminov for helpful discussions related to this work, and to the three anonymous reviewers of this draft for their constructive feedback. Finally, the authors would like to thank all crowdworkers who consented to participate in this study.",
            "crowdworkers are presented with public information about a mobile application available on the Google Play Store including its name, description and navigable screenshots. Figure FIGREF9 shows an example of our user interface. Crowdworkers are asked to imagine they have access to a trusted third-party privacy assistant, to whom they can ask any privacy question about a given mobile application. We use the Amazon Mechanical Turk platform and recruit crowdworkers who have been conferred “master” status and are located within the United States of America. Turkers are asked to provide five",
            "next proposed a system that automatically predicts whether a visual question will lead to a single versus multiple answers from a crowd. Our method outperforms a strong existing VQA system limited to estimating system uncertainty rather than crowd disagreement. Finally, we demonstrated how to employ the prediction system to accelerate the collection of diverse answers from a crowd by typically at least 20% over today's status quo of fixed redundancy allocation.",
            "Cloud Platform",
            "collected tweets as Hate, Offensive, or Neither, they randomly sampled $25k$ tweets and asked users of CrowdFlower crowdsourcing platform to label them. In detail, the distribution of different classes in both datasets will be provided in Subsection SECREF15.",
            "Routing To Experts or Crowd",
            "Results on ICT-MMMO, YouTube, MOUD Datasets"
        ]
    },
    {
        "title": "Stochastic Answer Networks for SQuAD 2.0",
        "answer": "SAN answer module.",
        "evidence": [
            "to the lower layers of SAN, which has a lexicon encoding layer, a contextual layer and a memory generation layer. On top of it, there are different answer modules for different tasks. We employ the SAN answer module for the span detector and a one-layer feed forward neural network for the binary classification task. It can also be viewed as a multi-task learning BIBREF8 , BIBREF5 , BIBREF9 . We will briefly describe the model from ground up as follows. Detailed descriptions can be found in BIBREF1 . Lexicon Encoding Layer. We map the symbolic/surface feature of INLINEFORM0 and INLINEFORM1",
            "Model architecture",
            "Beam-search decoder",
            "Transductive String Kernels",
            "both view-specific and cross-view dynamics in the network architecture and continuously models both dynamics through time. In MARN, view-specific dynamics within each modality are modeled using a Long-short Term Hybrid Memory (LSTHM) assigned to that modality. The hybrid memory allows each modality's LSTHM to store important cross-view dynamics related to that modality. Cross-view dynamics are discovered at each recurrence time-step using a specific neural component called the Multi-attention Block (MAB). The MAB is capable of simultaneously finding multiple cross-view dynamics in each",
            "compared to the Felice2014a system, on all three datasets. While Rei2016 also report separate experiments that achieve even higher performance, these models were trained on a considerably larger proprietary corpus. In this paper we compare error detection frameworks trained on the same publicly available FCE dataset, thereby removing the confounding factor of dataset size and only focusing on the model architectures. The error generation methods can generate alternative versions of the same input text – the pattern-based method randomly samples the error locations, and the SMT system can",
            "Stance detection",
            "Error Detection Model",
            "All the models follow the same philosophy of abstraction enabling a unified API in the library. Configuration - A configuration class instance (usually inheriting from a base class `PretrainedConfig`) stores the model and tokenizer parameters (such as the vocabulary size, the hidden dimensions, dropout rate, etc.). This configuration object can be saved and loaded for reproducibility or simply modified for architecture search. The configuration defines the architecture of the model but also architecture optimizations like the heads to prune. Configurations are agnostic to the deep learning",
            "LSTM Gating Signals",
            "improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount. Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder. Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
            "images and coherent text samples [5]. The framework that GANs use for generating new data points employs an end-to-end neural network comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being directly exposed to the real samples. Instead, this model learns to reproduce the general patterns of the input via its interaction with the discriminator. The role of the discriminator, in turn, is to tell apart which data points are ‘real’ and which have been created by the generator. On each run"
        ]
    },
    {
        "title": "Winograd Schemas and Machine Translation",
        "answer": "English, French, Italian, Spanish, Hebrew, Arabic, Portuguese, Vietnamese, Polish, Russian, Estonian, Finnish, Chinese, Japanese, Korean, German, Dutch, Greek, Turkish, Hungarian, Romanian, Czech, Swedish, Danish, Norwegian, Icelandic, Latvian, Lithuanian, Bulgarian, Croatian, Serbian, Slovak, Ukrainian, Belarusian, Azerbaijani, Kazakh, Georgian, Armenian, Mongolian, Uzbek, Tajik, Pashto, Persian, Azerbaijani, Kazakh, Georgian, Armenian, Mongolian, Uzbek, Tajik, Pashto, Persian, Azerbaijani, Kazakh, Georgian, Armenian, Mongolian, Uzbek, Tajik, Pashto, Persian, Azerbaijani, Kazakh, Georgian, Armenian, Mongolian, Uzbek, Tajik, Pashto, Persian, Azerbaijani, Kazakh, Georgian, Armenian, Mongolian, Uzbek, Tajik, Pashto, Persian, Azerbaijani, Kazakh, Georgian, Armenian, Mongolian, Uzbek, Tajik, Pashto, Persian, Azerbaijani, Kazakh, Georgian, Armenian, Mongolian, Uzbek, Tajik, Pashto, Persian, Azerbaijani, Kazakh,",
        "evidence": [
            "Translation experiments",
            "In many cases, the identification of the referent of the prounoun in a Winograd schema is critical for finding the correct translation of that pronoun in a different language. Therefore, Winograd schemas can be used as a very difficult challenge for the depth of understanding achieved by a machine translation program. The third person plural pronoun `they' has no gender in English and most other languages (unlike the third person singular pronouns `he', `she', and `it'). However Romance languages such as French, Italian, and Spanish, and Semitic languages such as Hebrew and Arabic distinguish",
            "Another axe of innovation in this research is the development, for the Portuguese language, of a pipeline of Natural Language Processing (NLP) processes, that allows us to fully process sentences and represent their content in an ontology. Although there are several tools for the processing of the Portuguese language, the combination of all these steps in a integrated tool is a new contribution. Moreover, we have already explored other related research path, namely author profiling BIBREF2, aggression identification BIBREF3 and hate-speech detection BIBREF4 over social media, plus statute law",
            "before and were also not familiar with the packaged products including the chosen 20 products; hence, they were \"illiterate\" in terms of comprehending English and in terms of having used any of the products although they might be literate in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this",
            "figuring out their pronunciations. The etymology of a word could be tagged in an analogous way to how language ID is tagged in multilingual g2p.",
            "training and developing corpus. Hence, there are two sub tasks for the task 1. One is closed test, which means the participants can only use the released data for training and developing. The other is open test, which allows the participants to explore external corpus for training and developing. Note that there is a same test set for both the closed test and the open test. For task 2, we release 11 examples of the complete user intent and 3 data file, which includes about one month of flight, hotel and train information, for participants to build their dialogue systems. The current date for",
            "Several efforts focused on recognizing similar questions. Jeon et al. BIBREF35 showed that a retrieval model based on translation probabilities learned from a question and answer archive can recognize semantically similar questions. Duan et al. BIBREF36 proposed a dedicated language modeling approach for question search, using question topic (user's interest) and question focus (certain aspect of the topic). Lately, these efforts were supported by a task on Question-Question similarity introduced in the community QA challenge at SemEval (task 3B) BIBREF3 . Given a new question, the task",
            "Language Resource References\nlrec lrec2018",
            "while Polish has the largest ($\\sigma =1.62$). All of the languages are strongly correlated with each other, as shown in Figure FIGREF20, where all of the Spearman's correlation coefficients are greater than 0.6 for all language pairs. Languages that share the same language family are highly correlated (e.g, cmn-yue, rus-pol, est-fin). In addition, we observe high correlations between English and most other languages, as expected. This is due to the effect of using English as the base/anchor language to create the dataset. In simple words, if one translates to two languages $L_1$ and $L_2$",
            "answers were spellchecked by running aSpell and manually inspecting all corrections proposed by the spellchecker. We found that some participants did not use “they” when referring to the protagonist. We identified “I”, “you”, “he”, “she”, “my”, “your”, “his”, “her” and “the person” as most common alternatives and replaced each appearance manually with “they” or “their”, if appropriate. We manually filtered out invalid questions, e.g. questions that are suggestive (“Should you ask an adult before using a knife?”) or that ask for the personal opinion of the reader (“Do you think going to the",
            "were sampled as to span all the 34 domains available as part of BabelDomains BIBREF93, which roughly correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample.  3) Source: CARD-660 BIBREF78. 67 word pairs are taken from this dataset focused on rare word similarity, applying the same selection criteria a) to e) employed for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus BIBREF94. CARD-660 contains some words that are very rare (logboat), domain-specific",
            "Other languages, other ambiguities"
        ]
    },
    {
        "title": "How we do things with words: Analyzing text as social and cultural data",
        "answer": "thorny issues not always at the forefront of discussions about computational text analysis methods.  Second, they hope to provide a set of best practices for  and third, they aim to shed light on the challenges of textual analysis.  First, they aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods.  First, they aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods.  First, they aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods.  First, they aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods.  First, they aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods.  First, they aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods.  First, they aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods.  First, they aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods.  First, they aim to shed light on thorny",
        "evidence": [
            "questions, which may continue to evolve for much of the project. At each stage, our discussion is critically informed by insights from the humanities and social sciences, fields that have focused on, and worked to tackle, the challenges of textual analysis—albeit at smaller scales—since their inception. In describing our experiences with computational text analysis, we hope to achieve three primary goals. First, we aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods. Second, we hope to provide a set of best practices for",
            "of dataset are much more important than whether the training and testing are in the same language or not.",
            "Results and Discussion ::: What makes Questions Unanswerable?",
            "Bahdanau et al. BIBREF2 introduced attention first in neural machine translation. It used a feed-forward network over addition of encoder and decoder states to compute alignment score. Our work is very similar to this except we use element wise difference instead of addition to build our conflict function. BIBREF3 came up with a scaled dot-product attention in their Transformer model which is fast and memory-efficient. Due to the scaling factor, it didn't have the issue of gradients zeroing out. On the other hand, BIBREF4 has experimented with global and local attention based on the how many",
            "A number of real-world problems related to text data have been studied under the framework of natural language processing (NLP). Example of such problems include topic categorization, sentiment analysis, machine translation, structured information extraction, or automatic summarization. Due to the overwhelming amount of text data available on the Internet from various sources such as user-generated content or digitized books, methods to automatically and intelligently process large collections of text documents are in high demand. For several text applications, machine learning (ML) models",
            "The text for example UID28 does not contain the information that breakfast is eaten in the morning, but it could still be inferred from many pointers in the text (e.g. phrases such as I woke up), or from commonsense knowledge. These few examples illustrate that our dataset covers questions with a wide spectrum of difficulty, from rather simple questions that can be answered from the text to challenging inference problems.",
            "Language-specific issues",
            "the performance of string kernels in cross-domain text classification, particularly in English polarity classification.",
            "the characteristic words within each topic. Some of the topics deal with fighting, excommunication, prayer, or Ramadan. While the topics are relatively distinct, there is some overlap, i.e. the distance between them varies. This information can be leveraged to map out rhetorical network.",
            "aspects such as topic, emotion, etc. Existing $\\textsc {0shot-tc}$ studies have mainly the following three problems.",
            "The goal of this study was to select documents suitable for a fine-grained analysis of arguments. In a preliminary study on annotating argumentation using a small sample (50 random documents) of forum posts and comments from the raw corpus, we found that many documents convey no argumentation at all, even in discussions about controversies. We observed that such contributions do not intend to persuade; these documents typically contain story-sharing, personal worries, user interaction (asking questions, expressing agreement), off-topic comments, and others. Such characteristics are typical to",
            "For example, in a landmark study, BIBREF1 analyzed the authorship of The Federalist Papers using a statistical text analysis by focusing on style, based on the distribution of function words, rather than content. As another example, BIBREF2 studied what defines English haiku and showed how computational analysis and close reading can complement each other. Computational approaches are valuable precisely because they help us identify patterns that would not otherwise be discernible. Yet these approaches are not a panacea. Examining thick social and cultural questions using computational text"
        ]
    },
    {
        "title": "Quasi-Recurrent Neural Networks",
        "answer": "max-over-time pooling.",
        "evidence": [
            "Pooling Layer",
            "The Retrieve module involves an unsupervised process with traditional indexing and retrieval techniques. For Fast Rerank, since there is no ground truth available, we use ROUGE-1 BIBREF16 to evaluate the saliency of a candidate template with respect to the gold summary of current source article. Therefore, the loss function is defined as: DISPLAYFORM0  where INLINEFORM0 is a score predicted by Equation EQREF16 , and INLINEFORM1 is the product of the training set size, INLINEFORM2 , and the number of retrieved templates for each article. For the BiSET module, the loss function is chosen as the",
            "the performance of string kernels in cross-domain text classification, particularly in English polarity classification.",
            "that the output vectors of each transformer encoder are concatenated, and a matrix is produced. The convolutional operation is performed with a window of size (3, hidden size of BERT which is 768 in BERTbase model) and the maximum value is generated for each transformer encoder by applying max pooling on the convolution output. By concatenating these values, a vector is generated which is given as input to a fully connected network. By applying softmax on the input, the classification operation is performed.",
            "Feature selection",
            "recurrent connections across timesteps, so a single RNN pass uses a single stochastic subset of the recurrent weights. Zoneout stochastically chooses a new subset of channels to “zone out” at each timestep; for these channels the network copies states from one timestep to the next without modification. As QRNNs lack recurrent weights, the variational inference approach does not apply. Thus we extended zoneout to the QRNN architecture by modifying the pooling function to keep the previous pooling state for a stochastic subset of channels. Conveniently, this is equivalent to stochastically",
            "Standard and baseline methods",
            "a sentence and the document cluster could only measure the sentence saliency. To include the query information, we supplement the common hand-crafted feature TF-IDF cosine similarity to the query. This query-dependent feature, together with the embedding similarity, are used in sentence ranking. ISOLATION removes the attention mechanism, and mixtures hand-crafted and automatically learned features. All these methods adopt the same sentence selection process illustrated in Section \"Sentence Selection\" for a fair comparison.",
            "Summarization Performance",
            "the words happy and unhappy are assigned to the same cluster, since they have the same words, such as feeling, in their dictionary definitions. However, they are of opposite polarities and should be discerned from each other. Therefore, we utilise a metric to move such words away from each other in the VSM, even though they have common words in their dictionary definitions. We multiply each value in a row with the corresponding row word's raw supervised score, thereby having more meaningful clusters. Using the training data only, the supervised polarity score per word is calculated as in",
            "Mix-Source Approach",
            "\\in \\mathbb {R}^l$ of the filter. Here we use the max-over-time pooling BIBREF10 .  $$\\hat{\\bf {c}}^h=\\max \\lbrace {\\bf {c}}_1^h, {\\bf {c}}_2^h,\\cdots \\rbrace $$   (Eq. 10)  The idea behind it is to capture the most important features in a feature map. $\\hat{\\bf {c}}^h$ is the output of CNN Layer, i.e., the embeddings of sentences and queries."
        ]
    },
    {
        "title": "Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction",
        "answer": "0.5",
        "evidence": [
            "Sentence Selection",
            "$c(e)$ or relation instance $r(e_1, e_2)$ is given by: $ p(c(e)) &= \\sigma ( \\theta _c^T \\phi _e ) \\\\ p(r(e_1, e_2)) &= \\sigma (",
            "different Information Retrieval (IR) methods. In another definition, the Stanford Natural Language Inference corpus SNLI BIBREF13 , used three classification labels for the relations between two sentences: entailment, neutral and contradiction. For the entailment label, the annotators who built the corpus were presented with an image and asked to write a caption “that is a definitely true description of the photo”. For the neutral label, they were asked to provide a caption “that might be a true description of the label”. They were asked for a caption that “is definitely a false description",
            "Expert annotations of Random and Difficult Instances",
            "A couple of examples: The word `sie' in German serves as both the formal second person prounoun (always capitalized), the third person feminine singular, and the third person plural. Therefore, it can be translated into English as either “you”, “she”, “it”, or “they”; and into French as either `vous', `il', `elle', `ils', or `elles'. (The feminine third-person singular German `sie' can be translated as neuter in English and as masculine in French because the three languages do not slice up the worlds into genders in the same way.) Likewise, the possessive pronoun `ihr' in all its declensions",
            "achieve substantial improvement on IE dataset, which covers the richness of compositionality (idiomaticity). We attribute the success on IE to its power in modeling more complicated compositionality.",
            "Given the question “When is the deadline of AAAI?”, as a human, one might try searching “AAAI” on a search engine, follow the link to the official AAAI website, then search for keywords “deadline” or “due date” on the website to jump to a specific paragraph. Humans have a deep understanding of questions because of their significant background knowledge. As a result, the keywords they use to search are not limited to what appears in the question. Inspired by this observation, we study 3 query types for the Ctrl+F $<$query$>$ command. One token from the question: the setting with smallest",
            "Different sentence classification tasks are crucial for many Natural Language Processing (NLP) applications. Natural language sentences have complicated structures, both sequential and hierarchical, that are essential for understanding them. In addition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision BIBREF0 , speech recognition BIBREF1 and NLP BIBREF2 . A problem largely specific to NLP is",
            "component. From the remaining ones, only 50 sentences (1%) have more than one argument component. Although in 19 cases (0.5%) the sentence contains a Claim-Premise pair which is an important distinction from the argumentation perspective, given the overall small number of such occurrences, we simplify the task by treating each sentence as if it has either one argument component or none. The approximation with sentence-level units is explained in the example in Figure FIGREF112 . In order to evaluate the expected performance loss using this approximation, we used an oracle that always predicts",
            "however, via standard semantic relatedness information (chicken and hotdogs are meat; water, soda and juice are drinks). The following cases require commonsense inference to be decided. In all these cases, the answers are not overtly contained nor easily derivable from the respective texts. We do not show the full texts, but only the scenario names for each question. Example UID23 refers to a library setting. Script knowledge helps in assessing that usually, paying is not an event when borrowing a book, which answers the question. Similarly, event information helps in answering the questions",
            "sentence, it is the figurative language me infla la vena (it inflates my vein) that the system is not able to understand. The first two error-categories might be solved by including smart features regarding capitalization and named entity recognition. However, the last two categories are problems of natural language understanding and will be very difficult to fix.",
            "Inference Without Parsed Sentences"
        ]
    },
    {
        "title": "TWEETQA: A Social Media Focused Question Answering Dataset",
        "answer": "By crawling thousands of news articles that include tweet quotations.",
        "evidence": [
            "media data. Rather than naively obtaining tweets from Twitter using the Twitter API which can yield irrelevant tweets with no valuable information, we restrict ourselves only to tweets which have been used by journalists in news articles thus implicitly implying that such tweets contain useful and relevant information. To obtain such relevant tweets, we crawled thousands of news articles that include tweet quotations and then employed crowd-sourcing to elicit questions and answers based on these event-aligned tweets. Table TABREF3 gives an example from our TweetQA dataset. It shows that QA",
            "plan is to use state-of-the-art deep neural networks and compare their performance for entity-level sentiment analysis of political tweets.",
            "extra delimiting characters are removed, but we keep all stop words because our model trains the sequence of words in a text directly. We also convert all tweets to lower case.",
            "additional problems related to provenance and contextualisation. It may not always be possible to determine the criteria applied during the creation process. For example, why were certain newspapers digitized but not others, and what does this say about the collection? Similar questions arise with the use of born-digital data. For instance, when using the Internet Archive’s Wayback Machine to gather data from archived web pages, we need to consider what pages were captured, which are likely missing, and why. We must often repurpose born-digital data (e.g., Twitter was not designed to measure",
            "positive samples. We form our test set by manually labeling a subset of 500 tweets from the the event-related tweets (randomly chosen across all five natural disasters), of which 50.0% are positive samples.",
            "different reactions from readers if it was posted on Fox News or The Late Night Show, as the target audience is likely to feel very differently about the same issue. This also brings up theoretical issues related more generally to the definition of the emotion detection task, as it's strongly dependent on personal traits of the audience. Also, in this work, pages initially selected on availability and intuition were further grouped into sets to make training data according to performance on development data, and label distribution. Another criterion to be exploited would be vocabulary overlap",
            "the 2018 blizzard and wildfires studied, implying that Twitter users' opinions on climate change are fairly ingrained on this subset of natural disasters.",
            "Tweets often contains user IDs and hashtags, which are single special tokens. Understanding these special tokens is important to answer person- or event-related questions.",
            "Experiments ::: Tweet-Level Models ::: Baseline GRU.",
            "by leveraging knowledge-aware language understanding that it has and it can be the main reason for high misclassifications of hate samples as offensive (in reality they are more similar to offensive rather than hate by considering social context, geolocation, and dialect of tweeters).",
            "From our annotated dataset of Twitter tweets (n=9,300 tweets), we conducted two feature studies to better understand the predictive power of several feature groups for classifying whether or not a tweet contains no evidence of depression (n=6,829 tweets) or evidence of depression (n=2,644 tweets). If there was evidence of depression, we determined whether the tweet contained one or more depressive symptoms (n=1,656 tweets) and further classified the symptom subtype of depressed mood (n=1,010 tweets), disturbed sleep (n=98 tweets), or fatigue or loss of energy (n=427 tweets) using support",
            "is possible that she is referring to a track which has been played a relatively short time before. In this cases, we want to show that knowing the radio schedule can help improving the results when detecting entities. Once assigned a list of entities to each track, we perform two types of matching. Firstly, within the tracks we identify the ones which have been played in a fixed range of time (t) before and after the generation of the user's tweet. Using the resulting tracks, we create a list of candidates entities on which performing string similarity. The score of the matching based on"
        ]
    },
    {
        "title": "Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation",
        "answer": "CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW, Affective Text, Fairy Tales, ISEAR, ClueWeb, RW, YouTube videos, TweetQA.",
        "evidence": [
            "Models Used in the Evaluation",
            "before calculating the true similarity. All the datasets except for RW simply calculated the average of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator.",
            "We focused evaluation over a small but diversified dataset composed by 10 YouTube videos in the English language in the news context. The selected videos cover different topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 minutes. To encourage the diversity of content format we included newscasts, interviews, reports and round tables. During the transcription phase we opted for a manual transcription process because we observed that using transcripts from an ASR system will difficult in a large degree the manual segmentation process. The",
            "Datasets",
            "All experiments were programmed using scikit-learn 0.18. The initial matrices of almost 17,000 features were reduced by eliminating features that only occurred once in the full dataset, resulting in 5,761 features. We applied Chi-Square feature selection and plotted the top-ranked subset of features for each percentile (at 5 percent intervals cumulatively added) and evaluated their predictive contribution using the support vector machine with linear kernel and stratified, 5-fold cross validation. In Figure 2, we observed optimal F1-score performance using the following top feature counts: no",
            "training data from a separate section of ClueWeb. However, as they did not release a development set with their data, we used this set as a development set. For a final evaluation, we generated another, similar test set from a different held out section of ClueWeb, in the same fashion as done by Krishnamurthy and Mitchell. This final test set contains 307 queries.",
            "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7 . A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role",
            "An evaluation of distributed word representation is generally conducted using a word similarity task and/or a word analogy task. There are many datasets readily available for these tasks in English. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult. Therefore, as a first step toward evaluating distributed representations in Japanese, we constructed a Japanese word similarity dataset. To the best of our knowledge, our dataset is the first resource that can be used to evaluate distributed representations in Japanese.",
            "methods, we evaluate NCEL on the following five datasets: (1) CoNLL-YAGO BIBREF22 : the CoNLL 2003 shared task including testa of 4791 mentions in 216 documents, and testb of 4485 mentions in 213 documents. (2) TAC2010 BIBREF39 : constructed for the Text Analysis Conference that comprises 676 mentions in 352 documents for testing. (3) ACE2004 BIBREF23 : a subset of ACE2004 co-reference documents including 248 mentions in 35 documents, which is annotated by Amazon Mechanical Turk. (4) AQUAINT BIBREF40 : 50 news articles including 699 mentions from three different news agencies. (5) WW BIBREF19",
            "Experiments\nIn this section, we introduce our evaluations on a standard dataset.",
            "As described in the question-answer writing process, the answers in our dataset are different from those in some existing extractive datasets. Thus we consider the task of answer generation for TweetQA and we use several standard metrics for natural language generation to evaluate QA systems on our dataset, namely we consider BLEU-1 BIBREF16 , Meteor BIBREF17 and Rouge-L BIBREF18 in this paper. To evaluate machine systems, we compute the scores using both the original answer and validation answer as references. For human performance, we use the validation answers as generated ones and the",
            "Cross-Lingual Evaluation ::: Results and Discussion"
        ]
    },
    {
        "title": "Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance",
        "answer": "Theses and Dissertations Catalog (TDC), CAPES, four state-of-the-art corpora of French broadcast, the Taskmaster Corpus, the raw corpus.  The corpora were noted using Glozz. Annodis aims at building an annotated corpus. The proposed annotations are on two levels of analysis, that is, two perspectives: Ascendant: part of EDU are used in the construction of more complex structures, through the relations of discourse; Descending: approaches the text in its entirety and relies on the various shallow indices to identify high-level discursive.  The corpora were noted using Glozz. Annodis aims at building an annotated corpus. The proposed annotations are on two levels of analysis, that is, two perspectives: Ascendant: part of EDU are used in the construction of more complex structures, through the relations of discourse; Descending: approaches the text in its entirety and relies on the various shallow indices to identify high-level discursive.  The corpora were noted using Glozz. Annodis aims at building an annotated corpus. The proposed annotations are on two levels of analysis, that is, two perspectives: Ascendant: part of EDU are used in the construction of more complex structures, through the relations of discourse",
        "evidence": [
            "which contain documents that are much longer than in previously used corpora.[4] purpleOur model not only achieves state-of-the-art on these two datasets, but in an additional experiment, in which we consider documents with increasing length, it becomes more competitive for longer documents.[5] purpleWe also ran an ablation study to assess the relative contribution of the global and local components of our approach. [1] Rather surprisingly, it appears that the benefits of our model come only from modeling the local context. For future work, we initially intend to investigate neural methods to",
            "Corpora",
            "Annotation studies and corpus creation",
            "Analysis of the convolutional language model",
            "used corpora, examined AV methods, selected performance measures and experiments. Finally, Section SECREF5 concludes the work and outlines future work.",
            "of the conference Traitement Automatique des Langues Naturelles (TALN) 2008 (25 articles, 169 000 words); Reports from Institut Français de Relations Internationales (32 raports, 266 000 words). The corpora were noted using Glozz. Annodis aims at building an annotated corpus. The proposed annotations are on two levels of analysis, that is, two perspectives: Ascendant: part of EDU are used in the construction of more complex structures, through the relations of discourse; Descending: approaches the text in its entirety and relies on the various shallow indices to identify high-level discursive",
            "The Taskmaster Corpus ::: Overview",
            "and test sets. Corpus statistics are presented in Table TABREF10 . In addition to the raw IPA transcriptions extracted from Wiktionary, the corpus provides an automatically cleaned version of transcriptions. Cleaning is a necessary step because web-scraped data is often noisy and may be transcribed at an inconsistent level of detail. The data cleaning used here attempts to make the transcriptions consistent with the phonemic inventories used in Phoible BIBREF4 . When a transcription contains a phoneme that is not in its language's inventory in Phoible, that phoneme is replaced by the phoneme",
            "In Brazil, the governmental body responsible for overseeing and coordinating post-graduate programs, CAPES, keeps records of all theses and dissertations presented in the country. Information regarding such documents can be accessed online in the Theses and Dissertations Catalog (TDC), which contains abstracts in Portuguese and English, and additional metadata. Thus, this database can be a potential source of parallel corpora for the Portuguese and English languages. In this article, we present the development of a parallel corpus from TDC, which is made available by CAPES under the open data",
            "impact it can have, the following content of this paper is structured as this: we first describe statistically the gender representation of a data set composed of four state-of-the-art corpora of French broadcast, widely used within the speech community, introducing the notion of speaker's role to refine our analysis in terms of voice professionalism. We then question the impact of such a representation on a ASR system trained on these data. BIBREF25",
            "Acknowledgements\nThis work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. Dong Nguyen is supported with an Alan Turing Institute Fellowship (TU/A/000006). Maria Liakata is a Turing fellow at 40%. We would also like to thank the participants of the “Bridging disciplines in analysing text as social and cultural data” workshop held at the Turing Institute (2017) for insightful discussions. The workshop was funded by a Turing Institute seed funding award to Nguyen and Liakata.",
            "Given the six controversial topics and four different registers, we compiled a collection of plain-text documents, which we call the raw corpus. It contains 694,110 tokens in 5,444 documents. As a coarse-grained analysis of the data, we examined the lengths and the number of paragraphs (see Figure FIGREF43 ). Comments and forum posts follow a similar distribution, being shorter than 300 tokens on average. By contrast, articles and blogs are longer than 400 tokens and have 9.2 paragraphs on average. The process of compiling the raw corpus and its further statistics are described in detail in"
        ]
    },
    {
        "title": "Knowledge Based Machine Reading Comprehension",
        "answer": "§ \"Semantic Parsing using Paraphrasing\".",
        "evidence": [
            "Question Answering Model",
            "concatenate a pre-trained 600-dimensional CoVe vectors BIBREF12 trained on German-English machine translation dataset, with the aforementioned lexicon embeddings as the final input of the contextual encoding layer, and also with the output of the first contextual encoding layer as the input of its second encoding layer. Thus, we obtain the final representation of the contextual encoding layer by a concatenation of the outputs of two BiLSTM: INLINEFORM0 for questions and INLINEFORM1 for passages. Memory Generation Layer. In this layer, we generate a working memory by fusing information from",
            "Researches on question classification, question taxonomies and QA system have been undertaken in recent years. There are two types of approaches for question classification according to Banerjee et al in BIBREF13 - by rules and by machine learning approach. Rule based approaches use some hard coded grammar rules to map the question to an appropriate answer type BIBREF14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in BIBREF16 and BIBREF0. Many classifiers have been used in machine learning for QC such as Support Vector Machine (SVM)",
            "Candidate Generation",
            ", ours is the first implementation of CFG that uses only monolingual data. Second, we show that generated paraphrases can be used to improve semantic parsing of questions into Freebase logical forms (§ \"Semantic Parsing using Paraphrasing\" ). We build on a strong baseline of reddylargescale2014 and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources.",
            "commands (defined in previous sections) to interact with iMRC. If the generated command is stop or the agent is forced to stop, the question answerer takes the current information at game step $t$ to generate head and tail pointers for answering the question; otherwise, the information gathering procedure continues. In this section, we describe the high-level model structure and training strategies of QA-DQN. We refer readers to BIBREF18 for detailed information. We will release datasets and code in the near future.",
            "with the ease in identifying a region of interest. Moreover, we hypothesize this feature will capture our observation from the previous study that counting problems typically leads to disagreement for images showing many objects, and agreement otherwise. We employ a 2,492-dimensional feature vector to represent the question-based features. One feature is the number of words in the question. Intuitively, a longer question offers more information and we hypothesize additional information makes a question more precise. The remaining features come from two one-hot vectors describing each of the",
            "The IR models and the RQE Logistic Regression model bring different perspectives to the search for relevant candidate questions. In particular, question entailment allows understanding the relations between the important terms, whereas the traditional IR methods identify the important terms, but will not notice if the relations are opposite. Moreover, some of the question types that the RQE classifier learns will not be deemed important terms by traditional IR and the most relevant questions will not be ranked at the top of the list. Therefore, in our approach, when a question is submitted to",
            "What is the Model Learning?",
            "KB, the formal component of the model can always give a score to any entity pair in the KB. This allows our model to considerably improve question answering performance on rare entities. It would be computationally intractable to consider all Freebase entities as answers to queries, and so we use a simple candidate entity generation technique to consider only a small set of likely entities for a given query. We first find all entities in the query, and consider as candidates any entity that has either been seen at training time with a query entity or is directly connected to a query entity in",
            "The Question Generation Model\nIn this section, we present the generation model which generates a question based on the semantics of a candidate answer. Afterward, we introduce how our paraphrasing model, which measures the semantic relevance between the generated question and the original question, is pretrained.",
            "the workers were also required to provide a justification as to why they made their questions. We manually checked this justification to filter out the questions with poor quality by removing those questions which were unrelated to the video. One initial challenge worth mentioning is that at first some workers input questions they had about the video and not questions which the video could answer. This was solved by providing them with an unrelated example. The second part of the question collection framework consisted of a paraphrasing task. In this task we presented workers with the"
        ]
    },
    {
        "title": "Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health",
        "answer": "IWSLT 2014 corpus, KFTT corpus, WMT 2016 dataset, TutorialVQA Dataset, SnapCaptions Dataset, Ancient-Modern Chinese Dataset, Code-switching Dataset, CHQ dataset.",
        "evidence": [
            "Datasets Used for the RQE Study",
            "What Does Zero-shot Transfer Model Learn? ::: Code-switching Dataset",
            "Acknowledgments\nThe authors would like to thank COMPETE 2020, PORTUGAL 2020 Program, the European Union, and ALENTEJO 2020 for supporting this research as part of Agatha Project SI & IDT number 18022 (Intelligent analysis system of open of sources information for surveillance/crime control). The authors would also like to thank LISP - Laboratory of Informatics, Systems and Parallelism.",
            "labeled to display positive, negative or neutral sentiment. Out of 79 videos in the dataset, 49 are used for training, 10 for validation and 20 for testing.",
            "to their similarity to the original question. The same dataset was used for SemEval 2017 Task 3 BIBREF4 . To construct our test dataset, we used a publicly shared set of Consumer Health Questions (CHQs) received by the U.S. National Library of Medicine (NLM), and annotated with named entities, question types, and focus BIBREF46 , BIBREF47 . The CHQ dataset consists of 1,721 consumer information requests manually annotated with subquestions, each identified by a question type and a focus. First, we selected automatically harvested FAQs, from U.S. National Institutes of Health (NIH) websites,",
            "Data Set",
            "Ancient-Modern Chinese Dataset",
            "All experiments were programmed using scikit-learn 0.18. The initial matrices of almost 17,000 features were reduced by eliminating features that only occurred once in the full dataset, resulting in 5,761 features. We applied Chi-Square feature selection and plotted the top-ranked subset of features for each percentile (at 5 percent intervals cumulatively added) and evaluated their predictive contribution using the support vector machine with linear kernel and stratified, 5-fold cross validation. In Figure 2, we observed optimal F1-score performance using the following top feature counts: no",
            "We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19 , and the WMT 2016 dataset for Ro-En. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively. Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with",
            "TutorialVQA Dataset ::: Overview",
            "SnapCaptions Dataset",
            "Algorithms Used"
        ]
    },
    {
        "title": "A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media",
        "answer": "BackTrans, Unpaired, CrossAlign, CPTG, $\\textsc {0shot-tc}$ studies.",
        "evidence": [
            "Proposed Approach",
            "We highlighted the problem that underlying characteristics of authorship verification approaches have not been paid much attention in the past research and that these affect the applicability of the methods in real forensic settings. Then, we proposed several properties that enable a better characterization and by this a better comparison between AV methods. Among others, we explained that the performance measure AUC is meaningless in regard to unary or specific non-optimizable AV methods, which involve a fixed decision criterion (for example, NNCD). Additionally, we mentioned that",
            "Qualitative analysis shows the advantages and limitations of our approaches, as well as the remaining challenges.",
            "Experiments on Additional Losses ::: Which additional loss works better?",
            "Comparison to Other Datasets",
            "We compare our model with the following state-of-art generative models: BackTrans BIBREF7 : In BIBREF7 , authors propose a model using machine translation in order to preserve the meaning of the sentence while reducing stylistic properties. Unpaired BIBREF10 : In BIBREF10 , researchers implement a method to remove emotional words and add desired sentiment controlled by reinforcement learning. CrossAlign BIBREF6 : In BIBREF6 , authors leverage refined alignment of latent representations to perform style transfer and a cross-aligned auto-encoder is implemented. CPTG BIBREF24 : An interpolated",
            "Hybrid models",
            "Our main results (see § SECREF4 and § SECREF5 ) suggest promising directions for efficient domain adaptation with cheaper techniques than conventional BT.",
            "Methodology ::: Fine-Tuning Strategies",
            "aspects such as topic, emotion, etc. Existing $\\textsc {0shot-tc}$ studies have mainly the following three problems.",
            "Conclusions, discussion and perspectives",
            "In this paper we focus on the supervised setting, and compare a variety of RNN-based structures trained with different losses."
        ]
    },
    {
        "title": "External Lexical Information for Multilingual Part-of-Speech Tagging",
        "answer": "English, Russian, Polish, Finnish, Estonian, Cantonese, Mandarin Chinese, Spanish, French, German, Italian, Portuguese, Dutch, Swedish, Danish, Norwegian, Greek, Turkish, Romanian, Hungarian, Arabic, Hebrew, Hindi, Japanese, Korean, Vietnamese, Thai, Indonesian, Swahili, Swazi, Zulu, Xhosa, Yoruba, Igbo, Shona, Shona, Yiddish, Welsh, Ukrainian, Urdu, Uzbek, Wolof, Yucatec Maya, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi, Yuchi",
        "evidence": [
            "Translation experiments",
            "through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "patterns in the scores. In particular, Russian and Polish (both Slavic), Finnish and Estonian (both Uralic), Cantonese and Mandarin Chinese (both Sinitic), and Spanish and French (both Romance) are all neighbors. In order to quantify exactly the effect of language affinity on the similarity scores, we run correlation analyses between these and language features. In particular, we extract feature vectors from URIEL BIBREF99, a massively multilingual typological database that collects and normalizes information compiled by grammarians and field linguists about the world's languages. In",
            "with broad implications for systems seeking to serve users' information-seeking intent. By releasing this resource, we hope to provide an impetus to develop systems capable of language understanding in this increasingly important domain.",
            "Open-ended survey questions",
            "Global Features",
            "It has been shown that extractive QA tasks like SQuAD may be tackled by some language independent strategies, for example, matching words in questions and context BIBREF20. Is zero-shot learning feasible because the model simply learns this kind of language independent strategies on one language and apply to the other? To verify whether multi-BERT largely counts on a language independent strategy, we test the model on the languages unseen during pre-training. To make sure the languages have never been seen before, we artificially make unseen languages by permuting the whole vocabulary of",
            "The final adapted results they reported include most of the 85 languages with high resource results, as well as the various languages they were able to adapt them for, for a total of 229 languages. This test set omits 23 of the high resource languages that are written in unique scripts or for which language distance metrics could not be computed.",
            "Other languages, other ambiguities",
            "We now detail the development of the final Multi-SimLex resource, describing our language selection process, as well as translation and annotation of the resource, including the steps taken to ensure and measure the quality of this resource. We also provide key data statistics and preliminary cross-lingual comparative analyses.  Language Selection. Multi-SimLex comprises eleven languages in addition to English. The main objective for our inclusion criteria has been to balance language prominence (by number of speakers of the language) for maximum impact of the resource, while simultaneously",
            "Experiments ::: Linguistic Probes\nWe further analyze whether awareness of shallow syntax carries over to other linguistic tasks, via probes from BIBREF1. Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases. Unlike §SECREF11, there is minimal downstream task architecture, bringing into focus the transferability of cwrs, as opposed to task-specific adaptation.",
            "Cross-Lingual Evaluation ::: Models in Comparison"
        ]
    },
    {
        "title": "Disunited Nations? A Multiplex Network Approach to Detecting Preference Affinity Blocs using Texts and Votes",
        "answer": "System-T BIBREF11, TutorialVQA Dataset, SnapCaptions Dataset, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW",
        "evidence": [
            "Datasets",
            "data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to ensure they match the original author implementations' performances on various benchmarks.",
            "annotated. We used the dataset from System-T BIBREF11, which has SRL tags, as well as, the other preceding tags. It was necessary to do some pre-processing and tag mapping in order to make it viable to train a Portuguese model. We made 589 tag conversions over 14 different categories. The breakdown of tag conversions per category is given by table TABREF7. These rules can be further seen in the corresponding Github repository BIBREF12. The modified training and development datasets are also available on another Github repositorypage BIBREF13 for further research and comparison purposes.",
            "Benchmark the dataset ::: Situation detection",
            "before calculating the true similarity. All the datasets except for RW simply calculated the average of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator.",
            "of dataset are much more important than whether the training and testing are in the same language or not.",
            "Data details and experimental setup",
            "TutorialVQA Dataset ::: Overview",
            "Datasets Used for the RQE Study",
            "dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.",
            "the following context. As we discuss in Section method, this property makes it difficult to apply training with negative examples for NPIs for most of the methods studied in this work. All examples above (UNKREF1–UNKREF11) are actual test sentences, and we can see that since they are synthetic some may sound somewhat unnatural. The main argument behind using this dataset is that even not very natural, they are still strictly grammatical, and an LM equipped with robust syntactic abilities should be able to handle them as human would do.",
            "Results: SnapCaptions Dataset"
        ]
    },
    {
        "title": "When to reply? Context Sensitive Models to Predict Instructor Interventions in MOOC Forums",
        "answer": "discussion thread.",
        "evidence": [
            "infer states that triggered intervention. Their model, however, requires a hyperparameter for the number of latent states. It is likely that their empirically reported setting will not generalise due to their weak evaluation BIBREF7 . In this paper, we propose models to infer the context that triggers instructor intervention that does not require context lengths to be set apriori. All our proposed models generalise over modelling assumptions made by BIBREF0 . For the purpose of comparison against a state-of-the-art and competing baselines we choose BIBREF7 since BIBREF0 's system and data are",
            "posts to either predict their stance directly or link them to other users and posts for improved performance. The 17.5% improvement when adding user information suggests that user information is especially useful when the dataset is highly imbalanced. All models that consider user information predict the minority class successfully. UCTNN without topic information works well but achieves lower performance than the full UTCNN model. The 4.9% performance gain brought by LDA shows that although it is satisfactory for single topic datasets, adding that latent topics still benefits performance:",
            "Interactive Dialog Context LM",
            "Instructor Intervention in MOOC forums",
            "self-attention components focus on modeling global interactions. The context-query attention layer generates the question-document similarity matrix and computes the question-aware vector representations of the context words. After that, a model encoder layer containing seven encoder blocks captures the interactions among the context words conditioned on the question. Finally, the output layer predicts a start position and an end position in the document to extract the answer span from the document.",
            "Pretraining with Shallow Syntactic Annotations ::: Pretraining Model Architecture ::: Staged parameter updates",
            "Do models generalize explicit supervision, or just memorize it?",
            "more substantively intuitive model, as well as increased predictive performance. Although this is a relatively large gain in predictive accuracy, these substantively small quantities confirm that the prediction of violent conflict onset remains an enduring challenge for scholars of IR. Nonetheless, the ability to exploit revealed preference information in speeches and votes in tandem appears to promise fruitful potential gains in terms of methodological capability and theoretical soundness.",
            "Link prediction in the inductive setting aims at reasoning the missing part “?” in a triplet when given INLINEFORM0 or INLINEFORM1 with emerging entities INLINEFORM2 or INLINEFORM3 respectively. To tackle the task, we firstly hide the object (subject) of each testing triplet in Subject-R (Object-R) to produce a missing part. Then we replace the missing part with all entities in the entity set INLINEFORM4 to construct candidate triplets. We compute the scoring function INLINEFORM5 defined in Eq. ( EQREF20 ) for all candidate triplets, and rank them in descending order. Finally, we evaluate",
            "`Golden State Warriors') and suppresses word-based contexts (word embeddings for unknown tokens `WaRriOoOrs'), leading to correct predictions. This result is significant because it shows performance of the model, with an almost identical architecture, can still improve without having to scale the word embeddings matrix indefinitely. Figure FIGREF19 (b) shows the cases where the modality attention led to incorrect predictions. For example, the model predicts missing tokens HUUUGE and Shampooer incorrectly as named entities by amplifying misleading character-based contexts (capitalized first",
            "or late. Our three proposed attentive model variants to infer the latent context improve over the state-of-the-art by a significant, large margin of 11% in F1 and 10% in recall, on average. Further, introspection of attention help us better understand what aspects of a discussion post propagate through the discussion thread that prompts instructor intervention.",
            "sub-forums with other non-standard names (e.g., Assignments instead of Homework) into of the four said sub-forums. Threads on general discussion, meet and greet and other custom sub-forums for social chitchat are omitted as our focus is to aid instructors on intervening on discussion on the subject matter. We also exclude announcement threads and other threads started by instructors since they are not interventions. We preprocess each thread by replacing URLs, equations and other mathematical formulae and references to timestamps in lecture videos by tokens INLINEFORM0 URL INLINEFORM1 ,"
        ]
    },
    {
        "title": "A Hierarchical Model for Data-to-Text Generation",
        "answer": "0.718 - 0.716 = 0.002.",
        "evidence": [
            "for each method are summarized in Table 8 . Since the questions have four choices, all methods should perform better than 25%. TgtOnly is close to the baseline because the model is trained with only 400 samples. As in the previous experiments, FineTune and Dual are better than All and Proposed is better than the other methods.",
            "Proposed Approach",
            "Baseline",
            "that can be used in testing protocols such as the one we proposed. Despite this evolution, these baselines (or their improved versions) can be used in applications such as automatic document summarisation (e.g., BIBREF12, BIBREF13), or sentences compression BIBREF14. The main feature of the proposed baseline system is its flexibility with respect to the language considered. In fact, it only uses a list of language markers and the grammatical category of words. The first resource, although dependent on each language, is relatively easy to obtain. We have found that, even with lists of moderate",
            "3 furthermore suggests that LRP provides semantically more meaningful semantic extraction than the baseline methods. In the next section we will confirm these observations quantitatively.",
            "the LR method with the IR baseline provided in the context of SemEval-cQA. The hybrid method combines the score provided by the Logistic Regression model and the reciprocal rank from the IR baseline using a weight-based combination:  INLINEFORM0  The weight INLINEFORM0 was set empirically through several tests on the cQA-2016 development set ( INLINEFORM1 ). Table TABREF30 presents the results on the cQA-2016 and cQA-2017 test datasets. The hybrid method (LR+IR) provided the best results on both datasets. On the 2016 test data, the LR+IR method outperformed the best system in all measures,",
            "improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount. Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder. Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
            "is the highest one: 4.82 (the stdev is 0.35) which means the most effective approach to provide users with products' usage. Additionally, a paired-samples t-test was conducted to compare the MOS scores of users' responses among all products using baseline 1 and SimplerVoice system. There was a significant difference in the scores for baseline 1 (Mean = 2.57, Stdev = 1.17) and SimplerVoice (Mean = 4.82, Stdev = 0.35); t= -8.18224, p =1.19747e-07. These results show that there is a statistically significant difference in the MOS means between baseline 1 and SimplerVoice and that SimplerVoice",
            "The results of the answerability baselines are presented in Table TABREF31, and on answer sentence selection in Table TABREF32. We observe that bert exhibits the best performance on a binary answerability identification task. However, most baselines considerably exceed the performance of a majority-class baseline. This suggests considerable information in the question, indicating it's possible answerability within this domain. Table.TABREF32 describes the performance of our baselines on the answer sentence selection task. The No-answer (NA) baseline performs at 28 F1, providing a lower bound",
            "Quantitative Evaluation",
            "the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation. The code for our model is available online.",
            "manner if the removal increased the average score. This was done based on their original scores, i.e. starting out by trying to remove the worst individual model and working our way up to the best model. We only consider it an increase in score if the difference is larger than 0.002 (i.e. the difference between 0.716 and 0.718). If at some point the score does not increase and we are therefore unable to remove a model, the process is stopped and our best ensemble of models has been found. This process uses the scores on the development set of different combinations of models. Note that this"
        ]
    },
    {
        "title": "Fully Convolutional Speech Recognition",
        "answer": "Librispeech",
        "evidence": [
            "State-of-the-art",
            "WSJ and Librispeech. We keep all the words (162K) in WSJ training corpus. For Librispeech, we only use the most frequent 200K tokens (out of 900K). Hyperparameter tuning The acoustic models are trained following BIBREF11 , BIBREF17 , using SGD with a decreasing learning rate, weight normalization and gradient clipping at 0.2 and a momentum of 0.9. The language models are trained with Nesterov accelerated gradient BIBREF27 . Following BIBREF0 , we also use weight normalization and gradient clipping. The parameters of the beam search (see Section SECREF9 ) INLINEFORM0 , INLINEFORM1 and",
            "Results on TAC2010 and WW",
            "Benchmark the dataset ::: Situation detection",
            "F1 mean F1_{mean} vs. WiSeBEWiSeBE",
            "that our model achieves a high irony accuracy with well-preserved sentiment and content. The contributions of our work are as follows:",
            "to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We",
            "Raw corpus statistics",
            "trait recognition and emotion recognition. MARN shows state-of-the-art performance on all the datasets.",
            "Multi-Source Attention (MSA)",
            "We use the large-scale Yahoo dataset released by DBLPZhangZL15. Yahoo has 10 classes: {“Society & Culture”, “Science & Mathematics”, “Health”, “Education & Reference”, “Computers & Internet”, “Sports”, “Business & Finance”, “Entertainment & Music”, “Family & Relationships”, “Politics & Government”}, with original split: 1.4M/60k in train/test (all labels are balanced distributed). We reorganize the dataset by first fixing the dev and test sets as follows: for dev, all 10 labels are included, with 6k labeled instances for each; For test, all 10 labels are included, with 10k instances for each.",
            "Acknowledgments\nThis work was funded by the Amazon Academic Research Awards program."
        ]
    },
    {
        "title": "Massively Multilingual Neural Grapheme-to-Phoneme Conversion",
        "answer": "ICT-MMMO, YouTube, MOUD, PTB, E2E, SnapCaptions, RW, FCE, CoNLL 2014, Ancient-Modern Chinese Dataset, Distant Supervision, Random Search, INLINEFORM0, RW, FCE, CoNLL 2014, PTB, E2E, SnapCaptions, RW, FCE, CoNLL 2014, Ancient-Modern Chinese Dataset, Distant Supervision, Random Search, INLINEFORM0.  Also, 290 million tweets, 150 million tweets, 13,939 questions, 3,827 questions, 9,731 questions, 1,411 questions, 2,797 questions, 196.0 words, 7.8 words, 3.6 words.  Also, 5 scenarios, 1,470 texts, 219 texts, 430 texts.  Also, 512-dimensional word2vec embeddings, 256, 0.0001, 128.  Also, 27.4%, 27.4%.  Also, 290 million tweets, 150 million tweets, 13,939 questions, 3,827 questions, 9,731 questions, 1,",
        "evidence": [
            "Datasets Used for the RQE Study",
            "ensemble of two subsystems based on convolutional neural networks, the first subsystem is created using 290 million tweets, and the second one is feeded with 150 million tweets. All these tweets were selected from a very large unlabeled dataset through distant supervision techniques. Table TABREF23 shows the multilingual set of techniques and the set with language-dependent techniques; for each, we optimized the set of parameters through Random Search and INLINEFORM0 (see Subsection SECREF14 ). The reached performance is reported using both cross-validation and the official gold-standard.",
            "Feature Sets",
            "before calculating the true similarity. All the datasets except for RW simply calculated the average of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator.",
            "the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions. We split the dataset into training (9,731 questions on 1,470 texts), development (1,411 questions on 219 texts), and test set (2,797 questions on 430 texts). Each text appears only in one of the three sets. The complete set of texts for 5 scenarios was held out for the test set. The average text, question, and answer length is 196.0 words, 7.8 words, and 3.6 words, respectively. On average, there are",
            "Results and Analysis",
            "Ancient-Modern Chinese Dataset",
            "Results on ICT-MMMO, YouTube, MOUD Datasets",
            "Approach ::: Datasets",
            "SnapCaptions Dataset",
            "using the FCE and CoNLL 2014 datasets. Making use of artificial data provided improvements for all data generation methods. By relaxing the type restrictions and generating all types of errors, our pattern-based method consistently outperformed the system by Felice2014a. The combination of the pattern-based method with the machine translation approach gave further substantial improvements and the best performance on all datasets.",
            "For the PTB dataset, we used the train-test split following BIBREF0, BIBREF5. For the E2E dataset, we used the train-test split from the original dataset BIBREF10 and indexed the words with a frequency higher than 3. We represent input data with 512-dimensional word2vec embeddings BIBREF13. We set the dimension of the hidden layers of both encoder and decoder to 256. The Adam optimiser BIBREF14 was used for training with an initial learning rate of 0.0001. Each utterance in a mini-batch was padded to the maximum length for that batch, and the maximum batch-size allowed is 128."
        ]
    },
    {
        "title": "A Neural Approach to Irony Generation",
        "answer": "human evaluators.",
        "evidence": [
            "the output sentences is calculated to evaluate the content preservation performance. In order to evaluate the overall performance of different models, we also report the geometric mean (G2) and harmonic mean (H2) of the sentiment accuracy and the BLEU score. As for the irony accuracy, we only report it in human evaluation results because it is more accurate for the human to evaluate the quality of irony as it is very complicated. We first sample 50 non-ironic input sentences and their corresponding output sentences of different models. Then, we ask four annotators who are proficient in",
            "are laid out without examining examples first. These are then tested (and possibly adjusted) based on a sample of the data. In line with Adcock and Collier's notion of “content validity” BIBREF13 , the goal is to assess whether the codebook adequately captures the systematized concept. By looking at the data themselves, she gains a better sense of whether some things have been left out of the coding rules and whether anything is superfluous, misleading, or confusing. Adjustments are made and the process is repeated, often with another researcher involved. The final annotations can be",
            "a list of words that distinguished the text in the treatment group (subreddits that were closed by Reddit) from text in the control group (similar subreddits that were not closed). The researchers then returned to the hate speech definition provided by the European Court of Human Rights, and manually filtered the top SAGE words based on this definition. Not all identified words fitted the definition. The others included: the names of the subreddits themselves, names of related subreddits, community-specific jargon that was not directly related to hate speech, and terms such as IQ and welfare,",
            "0.780, Interpretation: 0.797 and Response 0.776. In the end, the annotators discussed their discrepancies and managed to resolve all of them.",
            "are good both in grammar and logicality (2-2). The proportion of good logicality (score=2.0) from IE+MSA is much larger than that from IE (45.0%+5.0%/41.0%+4.0% vs. 36.0%+2.0% for GA/CA, respectively), and also remarkable larger than those from other baselines. Further, HLSTM equipped with MSA is better than those without MSA, indicating that commonsense knowledge is helpful. And the kappa measuring inter-rater agreement is 0.29 for three annotators, which implies a fair agreement.",
            "they express different degrees of feeling. Compared with “request”, “eager” indicates a stronger feeling. There were two annotators who emphasized the similarity of the act itself rather than the different degrees of feeling, and vice versa. In this case, Annotator A assigned a rating of 9, but Annotator B assigned a rating of 2. Although it was necessary to distinguish similarity and semantic relatedness BIBREF7 and we asked annotators to rate the pairs based on semantic similarity, it was not straightforward to put paraphrase candidates onto a single scale considering all the attributes of",
            "children who do just fine,] but [refutation: that hasn't been my experience.] To the best of our knowledge, these context-dependent dialogical properties of argument components using Toulmin's model have not been solved in the literature on argumentation theory and we suggest that these observations should be taken into account in the future research in monological argumentation. Appeal to emotion, sarcasm, irony, or jokes are common in argumentation in user-generated Web content. We also observed documents in our data that were purely sarcastic (the pathos dimension), therefore logical",
            "documents, 524 (53%) were labeled as on-topic persuasive. We will refer to this corpus as gold data persuasive. We examined all disagreements between annotators and discovered some typical problems, such as implicitness or topic relevance. First, the authors often express their stance towards the topic implicitly, so it must be inferred by the reader. To do so, certain common-ground knowledge is required. However, such knowledge heavily depends on many aspects, such as the reader's familiarity with the topic or her cultural background, as well as the context of the source website or the",
            "Note the 13-point spread between validation and test accuracies achieved. Ideally, the training, validation, and test datasets have the same underlying distribution of tweet sentiments; the assumption made with our labeling technique is that the influential accounts chosen are representative of all Twitter accounts. Critically, when choosing the influential Twitter users who believe in climate change, we highlighted primarily politicians or news sources (i.e., verifiably affirming or denying climate change); these tweets rarely make spelling errors or use sarcasm. Due to this skew, the model",
            "\"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\" (MOS) BIBREF23 , BIBREF24 to measure the effectiveness: how similar a response were comparing to the correct product's usage. The MOS score range is 1-5 (1-Bad, 2-Poor, 3-Fair, 4-Good, 5-Excellent) with 1 means incorrect product usage interpretability - the lowest level of effectiveness and 5 means correct product usage interpretability - the highest effectiveness level. The assigned scores corresponding to responses were aggregated over all participated",
            "then selected based on how many other answers it was merged with. Only if there was no majority, we selected the candidate with the highest overlap with the text as a fallback. Due to annotation mistakes, we found a small number of chosen correct and incorrect answers to be inappropriate, that is, some “correct” answers were actually incorrect and vice versa. Therefore, we manually validated the complete dataset in a final step. We asked annotators to read all texts, questions, and answers, and to mark for each question whether the correct and incorrect answers were appropriate. If an answer",
            "to be a coherent discourse unit. For example, if a particular occurrence of a premise cannot be summarized/rephrased into one statement, this may require further splitting into two or more premises. For the actual annotations, we developed a custom-made web-based application that allowed users to switch between different granularity of argument components (tokens or sentences), to annotate the same document in different argument “dimensions” (logos and pathos), and to write summary for each annotated argument component. As a measure of annotation reliability, we rely on Krippendorff's"
        ]
    },
    {
        "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer",
        "answer": "Hybrid HMM-DNN architecture.",
        "evidence": [
            "Model architecture",
            "The ASR system used in this work is described in BIBREF25. It uses the KALDI toolkit BIBREF26, following a standard Kaldi recipe. The acoustic model is based on a hybrid HMM-DNN architecture and trained on the data summarized in Table . Acoustic training data correspond to 100h of non-spontaneous speech type (mostly broadcast news) coming from both radio and TV shows. A 5-gram language model is trained from several French corpora (3,323M words in total) using SRILM toolkit BIBREF27. The pronunciation model is developed using the lexical resource BDLEX BIBREF28 as well as automatic",
            "to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of the base class, adding a specific head on top of the base model hidden states. Examples of these heads are language modeling or sequence classification heads. These classes follow similar naming pattern: XXXForSequenceClassification or XXXForMaskedLM where XXX is the name of the model and can be used for adaptation (fine-tuning) or pre-training. All models are available both in PyTorch and TensorFlow",
            "model but both heads are different. Our conflict head explicitly captures difference between the inputs.",
            "Pretraining with Shallow Syntactic Annotations ::: Pretraining Model Architecture",
            "In this section, we will describe the general architecture of NMT as a kind of sequence-to-sequence modeling framework. In this kind of sequence-to-sequence modeling framework, often there is an encoder trying to encode context information from the input sequence and a decoder to generate one item of the output sequence at a time based on the context of both input and output sequences. Besides, an additional component, named attention, exists in between, deciding which parts of the input sequence the decoder should pay attention in order to choose which to output next. In other words, this",
            "QA Model 2: KVMemNet",
            "data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to ensure they match the original author implementations' performances on various benchmarks.",
            "Models Used in the Evaluation",
            "What is the Model Learning?",
            "Hybrid models",
            "Our main hybrid model is put together sequentially, as shown in Figure 1 . We first run the discrete HMM on the data, outputting the hidden state distributions obtained by the HMM's forward pass, and then add this information to the architecture in parallel with a 1-layer LSTM. The linear layer between the LSTM and the prediction layer is augmented with an extra column for each HMM state. The LSTM component of this architecture can be smaller than a standalone LSTM, since it only needs to fill in the gaps in the HMM's predictions. The HMM is written in Python, and the rest of the architecture"
        ]
    },
    {
        "title": "From Textual Information Sources to Linked Data in the Agatha Project",
        "answer": "MOS scores.",
        "evidence": [
            "Results and Evaluation",
            "Furthermore, this advantage might be beneficial to continuously improve the whole pipeline of interactive entity population system.",
            "which involve a fixed decision criterion (for example, NNCD). Additionally, we mentioned that determinism must be fulfilled such that an AV method can be rated as reliable. Moreover, we clarified a number of misunderstandings in previous research works and proposed three clear criteria that allow to classify the model category of an AV method, which in turn influences its design and the way how it should be evaluated. In regard to binary-extrinsic AV approaches, we explained which challenges exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV",
            "packages without prior knowledge of the products while baseline 2 might provide additional information by showing top images from search engines. With the baseline 2, we attempt to measure whether merely adding \"relevant\" or \"similar\" products' images would be sufficient to improve the end-users' ability to comprehend the product's intended use. Moreover, with SimplerVoice, we test if our system could provide users with the proper visual components to help them understand the products' usage based on the proposed techniques, and measure the usefulness of SimplerVoice's generated description.",
            "Baselines ::: Baseline3: Pipeline Segment retrieval",
            "Methodology ::: Gender bias evaluation procedure of an ASR system performance ::: Evaluation",
            "RQE-based QA Approach",
            "In this section we outline the evaluation plan to verify the effectiveness of our learning approaches. To evaluate the news suggestion problem we are faced with two challenges. What comprises the ground truth for such a task ? How do we construct training and test splits given that entity pages consists of text added at different points in time ? Consider the ground truth challenge. Evaluating if an arbitrary news article should be included in Wikipedia is both subjective and difficult for a human if she is not an expert. An invasive approach, which was proposed by Barzilay and Sauper BIBREF8",
            "as qualitative results demonstrates its effectiveness. In the future, we will extend our method into cross-lingual settings to help link entities in low-resourced languages by exploiting rich knowledge from high-resourced languages, and deal with NIL entities to facilitate specific applications.",
            "level. The assigned scores corresponding to responses were aggregated over all participated subjects and over the 3 experts. The result of the score is reported in the next section Result. Table TABREF21 shows the MOS scores indicating the performance of 3 approaches. The mean of MOS scores of baseline 1 is the lowest one: 2.57 (the standard deviation (stdev) is 1.17), the baseline 2 mean score is slightly higher than the baseline 1's: 2.86 (the stdev is 1.27), while SimplerVoice evaluation score is the highest one: 4.82 (the stdev is 0.35) which means the most effective approach to provide",
            "The aim of this work was twofold: to design a discursive segmenter using a minimum of resources and to establish an evaluation protocol to measure the performance of segmenters. The results show that we can build a simple version of the baseline, which employs only a list of markers and presents a very encouraging performance. Of course, the quality of the list is a preponderant factor for a correct segmentation. We have studied the impact of the marker which, even though it may seem fringe-worthy, contributes to improving the performance of our segmenters. Thus, it is an interesting marker",
            "Comparison with baseline models"
        ]
    },
    {
        "title": "Sparse and Constrained Attention for Neural Machine Translation",
        "answer": "cmn-spa, heb-est, rus-fin, cmn-heb, cmn-est, cmn-pol, cmn-rus, cmn-fra, cmn-deu, cmn-ita, cmn-jpn, cmn-vie, cmn-yue, cmn-kor, cmn-tha, cmn-hin, cmn-urd, cmn-ara, cmn-ell, cmn-swa, cmn-wel, cmn-zho, cmn-sin, cmn-tur, cmn-ukr, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze, cmn-geo, cmn-aze",
        "evidence": [
            "languages such as Welsh and Kiswahili. It also seems that the lack of monolingual data is a larger problem than typological dissimilarity between language pairs, as we do observe reasonably high correlation scores with vecmap for language pairs such as cmn-spa, heb-est, and rus-fin. However, typological differences (e.g., morphological richness) still play an important role as we observe very low scores when pairing cmn with morphologically rich languages such fin, est, pol, and rus. Similar to prior work of Vulic:2019we and doval2019onthe, given the fact that unsupervised vecmap is the most",
            "Analysis of the convolutional language model",
            "Multi-SimLex: Translation and Annotation ::: Word Pair Translation",
            "doktor is used as a translation of doctor to generate a valid pair. We measure the quality of the translated pairs by using a random sample set of 100 pairs (from the 1,888 pairs) to be translated by an independent translator for each target language. The sample is proportionally stratified according to the part-of-speech categories. The independent translator is given identical instructions to the main translator; we then measure the percentage of matched translated words between the two translations of the sample set. Table TABREF12 summarizes the inter-translator agreement results for all",
            "We use a linear model to map ungrounded graphs to grounded ones. The parameters of the model are learned from question-answer pairs. For example, the question What language do people in Czech Republic speak? paired with its answer $\\lbrace \\textsc {CzechLanguage}\\rbrace $ . In line with most work on question answering against Freebase, we do not rely on annotated logical forms associated with the question for training and treat the mapping of a question to its grounded graph as latent. Let $q$ be a question, let $p$ be a paraphrase, let $u$ be an ungrounded graph for $p$ , and let $g$ be a",
            "Our results show that the parameters learned by the shared encoder–decoder are able to exploit the orthographic and phonemic similarities between the various languages in our data.",
            "system learned correlate closely to the genetic relationships between languages. However, neither of these models was applied to g2p.",
            "Data Collection. To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. Paragraph Alignment. To further ensure the quality of the new dataset, the work of paragraph alignment is manually",
            "range. For example, the 10K-20K segment of the bars represents the proportion of words in the dataset that occur in the list of most frequent words between the frequency rank of 10,000 and 20,000 in that language; likewise with other intervals. Frequency lists for the presented languages are derived from Wikipedia and Common Crawl corpora. While many concept pairs are direct or approximate translations of English pairs, we can see that the frequency distribution does vary across different languages, and is also related to inherent language properties. For instance, in Finnish and Russian,",
            "SMT the best approach in many translation directions but also hard to apply to new domains or to other language pairs. In this paper we attempt to build NMT systems for such a low-resourced language pair: Japanese INLINEFORM0 Vietnamese. Our aim is to set the first and reasonable NMT systems that can be reproducible in order to serve as the baseline for further researches in the direction. Furthermore, we conduct experiments using some advanced methods to improve the quality of the systems. An important criteria for those methods is that they must be scalable and language-independent as much",
            "We apply this protocol to a set of 12 languages, including a mixture of major languages (e.g., Mandarin, Russian, and French) as well as several low-resource ones (e.g., Kiswahili, Welsh, and Yue Chinese). We demonstrate that our proposed dataset creation procedure yields data with high inter-annotator agreement rates (e.g., the average mean inter-annotator agreement for Welsh is 0.742). The unified construction protocol and alignment between concept pairs enables a series of quantitative analyses. Preliminary studies on the influence that polysemy and cross-lingual variation in lexical",
            "Results on Unseen Languages"
        ]
    },
    {
        "title": "Dialog Context Language Modeling with Recurrent Neural Networks",
        "answer": "128 turns.",
        "evidence": [
            "We use the Switchboard Dialog Act Corpus (SwDA) in evaluating our contextual langauge models. The SwDA corpus extends the Switchboard-1 Telephone Speech Corpus with turn and utterance-level dialog act tags. The utterances are also tagged with part-of-speech (POS) tags. We split the data in folder sw00 to sw09 as training set, folder sw10 as test set, and folder sw11 to sw13 as validation set. The training, validation, and test sets contain 98.7K turns (190.0K utterances), 5.7K turns (11.3K utterances), and 11.9K turns (22.2K utterances) respectively. Maximum turn length is set to 160. The",
            "Conversation Excerpts",
            "dialog analysts and given two hours of training on the system interface as well as on how to handle specific scenarios such as uncooperative users and technical glitches. Uncooperative users typically involve those who either ignored agent input or who rushed through the conversation with short phrases. Technical issues involved dropped sessions (e.g. WebRTC connections failed) or cases in which the user could not hear the agent or vice-versa. In addition, weekly meetings were held with the agents to answer questions and gather feedback on their experiences. Agents typically work four hours",
            "low redundancy in the questions posed by crowdworkers over each policy, with each policy receiving ~49.94 unique questions despite crowdworkers independently posing questions. Questions are on average 8.4 words long. As declining to answer a question can be a legally sound response but is seldom practically useful, answers to questions where a minority of experts abstain to answer are filtered from the dataset. Privacy policies are ~3000 words long on average. The answers to the question asked by the users typically have ~100 words of evidence in the privacy policy document.",
            "The self-dialog technique renders quality data and avoids some of the challenges seen with the two-person approach. To begin, since the same person is writing both sides of the conversation, we never see misunderstandings that lead to frustration as is sometimes experienced between interlocutors in the two-person approach. In addition, all the self-dialogs follow a reasonable path even when the user is constructing conversations that include understanding errors or other types of dialog glitches such as when a particular choice is not available. As it turns out, crowdsourced workers are quite",
            "are able to cope with very challenging verification cases such as 250 characters long informal chat conversations (72.7% accuracy) or cases in which two scientific documents were written at different times with an average difference of 15.6 years (>75% accuracy). However, we also identified that all involved methods are prone to cross-topic verification cases.",
            "Discourse Segmenter Overall Description",
            "controversy) and one article was an interview, which the current model cannot capture (a dialogical argumentation model would be required). For each of the 340 documents, the gold standard annotations were obtained using the majority vote. If simple majority voting was not possible (different boundaries of the argument component together with a different component label), the gold standard was set after discussion among the annotators. We will refer to this corpus as the gold standard Toulmin corpus. The distribution of topics and registers in this corpus in shown in Table TABREF71 , and",
            "systems are often carefully programmed for very narrow and specific cases BIBREF6, BIBREF7. General understanding of natural spoken behaviors across multiple dialog turns, even in single task-oriented situations, is by most accounts still a long way off. In this way, most of these products are very much hand crafted, with inherent constraints on what users can say, how the system responds and the order in which the various subtasks can be completed. They are high precision but relatively low coverage. Not only are such systems unscalable, but they lack the flexibility to engage in truly",
            "by a small margin. This indicates that the proposed model may implicitly capture the dialog context state for language modeling.",
            "the data while making it easy for workers to apply labels consistently.  Size: The total of 13,215 dialogs in this corpus is on par with similar, recently released datasets such as MultiWOZ BIBREF13.",
            "to start the dialogue, is the same. The tester then begin to converse to each system. A dialogue is finished if the system successfully returns the information which the user inquires or the number of dialogue turns is larger than 30 for a single task. For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There"
        ]
    },
    {
        "title": "Shallow Syntax in Deep Water",
        "answer": "None mentioned.",
        "evidence": [
            "syntactic features obtained automatically on downstream task data. Neither approach leads to a significant gain on any of the four downstream tasks we considered relative to ELMo-only baselines. Further analysis using black-box probes confirms that our shallow-syntax-aware contextual embeddings do not transfer to linguistic tasks any more easily than ELMo's embeddings. We take these findings as evidence that ELMo-style pretraining discovers representations which make additional awareness of shallow syntax redundant.",
            "Language Dependent Features",
            "BIBREF43 or time series data BIBREF44 could be used. The features (sometimes called variables or predictors) are used by the model to make the predictions. They may vary from content-based features such as single words, sequences of words, or information about their syntactic structure, to meta-information such as user or network information. Deciding on the features requires experimentation and expert insight and is often called feature engineering. For insight-driven analysis, we are often interested in why a prediction has been made and features that can be interpreted by humans may be",
            "The Taskmaster Corpus ::: Overview",
            "Part-of-speech tagging is now a classic task in natural language processing, for which many systems have been developed or adapted for a large variety of languages. Its aim is to associate each “word” with a morphosyntactic tag, whose granularity can range from a simple morphosyntactic category, or part-of-speech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, etc.). The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms",
            "across a number of sentences (n rows at a time) or across the entire document (one entire column at a time), denoted as local and global reading respectively. For the GR discourse features, in the case of local reading, we process the entity roles one sentence pair at a time (Figure FIGREF18 , left). For example, in processing the pair INLINEFORM0 , we find the first non-empty role INLINEFORM1 for entity INLINEFORM2 in INLINEFORM3 . If INLINEFORM4 also has a non-empty role INLINEFORM5 in the INLINEFORM6 , we collect the entity role transition INLINEFORM7 . We then proceed to the following",
            "There are several key attributes that make Taskmaster-1 both unique and effective for data-driven approaches to building dialog systems and for other research.  Spoken and written dialogs: While the spoken sources more closely reflect conversational language BIBREF20, written dialogs are significantly cheaper and easier to gather. This allows for a significant increase in the size of the corpus and in speaker diversity.  Goal-oriented dialogs: All dialogs are based on one of six tasks: ordering pizza, creating auto repair appointments, setting up rides for hire, ordering movie tickets,",
            "Different from the static task-attentive sentence encoding model, the query vectors of the dynamic task-attentive sentence encoding model are generated dynamically. When each task belongs to a different domain, we can introduce an auxiliary domain classifier to predict the domain (or task) of the specific sentence. Thus, the domain information is also included in the shared sentence representation, which can be used to generate the task-specific query vector of attention. The original tasks and the auxiliary task of domain classification (DC) are joint learned in our multi-task learning",
            "which match the meaning of both query and documents. The working mechanism of AttSum is consistent with the way how humans read when having a particular query in their minds. Naturally, they pay more attention to the sentences that meet the query need. It is noted that, unlike most previous summarization systems, our model is totally data-driven, i.e., all the features are learned automatically. We verify AttSum on the widely-used DUC 2005 $\\sim $ 2007 query-focused summarization benchmark datasets. AttSum outperforms widely-used summarization systems which rely on rich hand-crafted features.",
            "on each language, is relatively easy to obtain. We have found that, even with lists of moderate size, the results are quite significant. The grammatical categories were obtained with the help of the TreeTagger statistics tool. However, TreeTagger could be replaced by any other tool producing similar results.",
            "from unpunctuated text to punctuated text. However, our main task, sentence segmentation, is an upstream task in text processing, unlike punctuation restoration, which is considered a downstream task. Therefore, the task needs to operate rapidly; consequently, we focus only on the sequence tagging model, which is less complex than the machine translation model. In addition to those machine translation tasks, both traditional approaches and deep learning approaches must solve a word sequence tagging problem. Of the traditional approaches, contextual features around the considered word were",
            "is used as constraints at tagging time. These lexical features can also be divided into local lexical features (for example the list of possible tags known to the external lexicon for the current word) and contextual lexical features (for example the list of possible tags known to the external lexicon for surrounding words). In particular, lexical contextual features provide a means to model the right context of the current word, made of words that have not yet been tagged by the system but for which the lexicon often provides a list of possible tags. Moreover, tagging accuracy for"
        ]
    },
    {
        "title": "TTTTTackling WinoGrande Schemas",
        "answer": "baseline from prior work.",
        "evidence": [
            "Current state of the art",
            "baseline from prior work. All of the data and code used in these experiments is available at http://github.com/allenai/open_vocab_semparse.",
            "Background",
            "Original Toulmin's model",
            "Discussion and Future Work",
            "Pre-Compute Attention",
            "Properties of back-translated data",
            "Traditional Methodologies",
            "Merge Recurrent States",
            "Data details and experimental setup",
            "LSTM Gating Signals",
            "directly inferred from the tweets. After we retrieve the QA pairs from all HITs, we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script to download the original tweets and detailed"
        ]
    },
    {
        "title": "A Question-Entailment Approach to Question Answering",
        "answer": "Support Vector Machine (SVM), Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM), Recurrent Neural Networks (RNN), Universal Sentence Encoder (USE), GloVe, Stochastic Gradient Descent (SGD), Naive Bayes (NB), Kernel Naive Bayes (KNB), Decision Tree (DT), Rule Induction (RI), Maximum Entropy Model.  Also, deep learning approaches such as Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM), Recurrent Neural Networks (RNN), Universal Sentence Encoder (USE).  Additionally, ensemble models combining RNN and USE.  Furthermore, pre-trained GloVe vectors.  Also, pre-trained common crawl version with 840B tokens and 300d vectors.  And, Support Vector Regression (SVR) model.  Also, Stochastic Gradient Descent (SGD) model.  And, Naive Bayes (NB) model.  And, Kernel Naive Bayes (KNB) model.  And, Decision Tree (DT) model.  And, Rule Induction (RI) model.  And, Maximum Entropy Model.  And, Support Vector Machine (SVM) model.",
        "evidence": [
            "Datasets Used for the RQE Study",
            "supervised machine learning-based approaches have used different text mining-based features such as surface features, sentiment analysis, lexical resources, linguistic features, knowledge-based features or user-based and platform-based metadata BIBREF8, BIBREF9, BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be changing direction, with deep learning models being used for both feature extraction and the training of classifiers. These newer models are applying deep learning approaches such as Convolutional Neural Networks (CNNs), Long Short-Term",
            "Deep Learning Model",
            "sources in the medical domain, consumers are more and more faced with a similar challenge, one that needs dedicated solutions that can adapt to the heterogeneity and specifics of health-related information. Dedicated Question Answering (QA) systems are one of the viable solutions to this problem as they are designed to understand natural language questions without relying on external information on the users. In the context of QA, the goal of Recognizing Question Entailment (RQE) is to retrieve answers to a premise question ( INLINEFORM0 ) by retrieving inferred or entailed questions, called",
            "the ensemble performance. Columns correspond to label type. Results from all models outperform the baseline SVR model: Pearson's correlation coefficients range from 0.550 to 0.622. The regression correlations are the lowest. The RNN model realizes the strongest performance among the stand-alone (non-ensemble) models, outperforming variants that exploit CNN and USE representations. Combining the RNN and USE further improves results. We hypothesize that this is due to complementary sentence information encoded in universal representations. For all models, correlations for Intervention and",
            "with pretrained GloVe vectors. This adaptation provided the best performance in previous experiments with RQE data. GloVe is an unsupervised learning algorithm to generate vector representations for words BIBREF42 . Training is performed on aggregated word co-occurrence statistics from a large corpus, and the resulting representations show interesting linear substructures of the word vector space. We use the pretrained common crawl version with 840B tokens and 300d vectors, which are not updated during training.",
            "be guaranteed. In the past two decades, researchers from different fields including linguistics, psychology, computer science and mathematics proposed numerous techniques and concepts that aim to solve the AV task. Probably due to the interdisciplinary nature of this research field, AV approaches were becoming more and more diverse, as can be seen in the respective literature. In 2013, for example, Veenman and Li BIBREF2 presented an AV method based on compression, which has its roots in the field of information theory. In 2015, Bagnall BIBREF3 introduced the first deep learning approach that",
            "Many classifiers have been used in machine learning for QC such as Support Vector Machine (SVM) BIBREF16 BIBREF17, Support Vector Machines and Maximum Entropy Model BIBREF18, Naive Bayes (NB), Kernel Naive Bayes (KNB), Decision Tree (DT) and Rule Induction (RI) BIBREF13. In BIBREF0, they claimed to achieve average precision of 0.95562 for coarse class and 0.87646 for finer class using Stochastic Gradient Descent (SGD).",
            "the MAP@10 and MRR@10 of our QA methods. The IR+RQE system outperforms the IR-based QA system with 0.311 MAP@10 and 0.333 MRR@10.",
            "Reading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries. In recent years, many large-scale RC datasets in English BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11. The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance. Nonetheless, RC on languages other than English has been limited due to the absence of",
            "Following the recent progress in deep learning, researchers and practitioners of machine learning are recognizing the importance of understanding and interpreting what goes on inside these black box models. Recurrent neural networks have recently revolutionized speech recognition and translation, and these powerful models could be very useful in other applications involving sequential data. However, adoption has been slow in applications such as health care, where practitioners are reluctant to let an opaque expert system make crucial decisions. If we can make the inner workings of RNNs more",
            "Background\nIn this section we define the RQE task and describe related work at the intersection of question answering, question similarity and textual inference."
        ]
    },
    {
        "title": "Argumentation Mining in User-Generated Web Discourse",
        "answer": "lexical and semantic challenges of domain-specific data, the higher sparseness of relevant information sources, and the more pronounced lack of similarities between users' searches. Properly detecting boundaries of argument components, the distinction between premise and backing, and labeling on clause level. annotation quality, the annotation strategy, and the value of difficult data. domain shift, the lack of labeled data in a target domain, and the challenges of cross-domain settings. the granularity annotation difficulties, the approximation in the system, and the level of detail required. the annotation quality, the annotation strategy, and the value of difficult data. the annotation quality, the annotation strategy, and the value of difficult data. the annotation quality, the annotation strategy, and the value of difficult data. the annotation quality, the annotation strategy, and the value of difficult data. the annotation quality, the annotation strategy, and the value of difficult data. the annotation quality, the annotation strategy, and the value of difficult data. the annotation quality, the annotation strategy, and the value of difficult data. the annotation quality, the annotation strategy, and the value of difficult data. the annotation quality, the annotation strategy, and the value of difficult data. the annotation quality, the annotation strategy, and the value of difficult data. the",
        "evidence": [
            "Topics and registers",
            "Domain shift is a fundamental problem in machine learning, that has attracted a lot of attention in the natural language processing and vision communities BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . To understand and address this problem, generated by the lack of labeled data in a target domain, researchers have studied the behavior of machine learning methods in cross-domain settings BIBREF2 , BIBREF11 , BIBREF10 and came up with various domain adaptation techniques BIBREF12 , BIBREF5 , BIBREF6 , BIBREF9 . In cross-domain",
            "Each presents challenges. And especially when working across disciplines, the research often involves a fair amount of discussion—even negotiation—about what means of operationalization and approaches to analysis are appropriate and feasible. And yet, with a bit of perseverance and mutual understanding, conceptually sound and meaningful work results so that we can truly make use of the exciting opportunities rich textual data offers.",
            "With the availability of rich data on users' locations, profiles and search history, personalization has become the leading trend in large-scale information retrieval. However, efficiency through personalization is not yet the most suitable model when tackling domain-specific searches. This is due to several factors, such as the lexical and semantic challenges of domain-specific data that often include advanced argumentation and complex contextual information, the higher sparseness of relevant information sources, and the more pronounced lack of similarities between users' searches. A recent",
            "for specific entity classes. The approach is trained on already-populated entity pages of a given class (e.g. `Diseases') by learning templates about the entity page structure (e.g. diseases have a treatment section). For a new entity page, first, they extract documents via Web search using the entity title and the section title as a query, for example `Lung Cancer'+`Treatment'. As already discussed in the introduction, this has problems with reproducibility and maintainability. However, their main focus is on identifying the best paragraphs extracted from the collected documents. They rank",
            "We compare it with a standard MRC model BiDAF, and also provide the difficulty of the dataset and lay out remaining challenges.",
            "and are of interest to us in segmenting the videos. Each segment can be seen as a subtask within a larger video dictating an example. We thus chose these videos because of the amount of procedural information stored in each video for which the user may ask. Though there is only one domain, each video corresponds to a different overall goal.",
            "pose biggest challenges. Properly detecting boundaries of argument components caused problems, as shown in Figure FIGREF146 (a). This goes in line with the granularity annotation difficulties discussed in section UID86 . The next example in Figure FIGREF146 (b) shows that even if boundaries of components were detected precisely, the distinction between premise and backing fails. The example also shows that in some cases, labeling on clause level is required (left-hand side claim and premise) but the approximation in the system cannot cope with this level of detail (as explained in section",
            "Cross-Domain Classification",
            "that a strong neural baseline underperforms human performance by almost 0.3 F1 on PrivacyQA, suggesting considerable room for improvement for future systems. Further, we use this dataset to shed light on challenges to question answerability, with domain-general implications for any question answering system. The PrivacyQA corpus offers a challenging corpus for question answering, with genuine real-world utility.",
            "report on larger comparison, carried out in a larger multilingual setting and comparing different tagging models. Using different 16 datasets, we compare the performances of two feature-based models enriched with external lexicons and of two LSTM-based models enriched with word vector representations. A secondary goal of our work is to compare the relative improvements linked to the use of external lexical information in the two feature-based models, which use different models (MEMM vs. CRF) and feature sets. More specifically, our starting point is the MElt system BIBREF12 , an MEMM tagging",
            "The preceding experiments demonstrate that re-weighting difficult sentences annotated by the crowd generally improves the extraction models. Presumably the performance is influenced by the annotation quality. We now examine the possibility that the higher quality and more consistent annotations of domain experts on the difficult instances will benefit the extraction model. This simulates an annotation strategy in which we route difficult instances to domain experts and easier ones to crowd annotators. We also contrast the value of difficult data to that of an i.i.d. random sample of the same"
        ]
    },
    {
        "title": "Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model",
        "answer": "general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with the discriminator.  general patterns of the input via its interaction with",
        "evidence": [
            "and even degrades the performance. We further explore what does the model learn in zero-shot setting.",
            "images and coherent text samples [5]. The framework that GANs use for generating new data points employs an end-to-end neural network comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being directly exposed to the real samples. Instead, this model learns to reproduce the general patterns of the input via its interaction with the discriminator. The role of the discriminator, in turn, is to tell apart which data points are ‘real’ and which have been created by the generator. On each run",
            "Model Training",
            "in INLINEFORM13 . The values in a row of INLINEFORM14 are considered similarity scores, and we can retrieve the desired PMF for each row by normalizing by its sum. At the start of each epoch, we initialize INLINEFORM0 with 0's along the diagonal and 1's elsewhere (which reduces to uniform sampling). For each training pair INLINEFORM1 , we update INLINEFORM2 for both INLINEFORM3 and INLINEFORM4 :  INLINEFORM0  The PMFs INLINEFORM0 are updated after the forward pass of an entire mini-batch. The constant INLINEFORM1 enforces a potentially stronger constraint than is used in the INLINEFORM2 loss,",
            "Do models generalize explicit supervision, or just memorize it?",
            "un-annotated sentences (multi-BERT), in cross-lingual zero-shot RC. We fine-tune multi-BERT on the training set in source language, then test the model in target language, with a number of combinations of source-target language pair to explore the cross-lingual ability of multi-BERT. Surprisingly, we find that the models have the ability to transfer between low lexical similarity language pair, such as English and Chinese. Recent studies BIBREF17, BIBREF12, BIBREF18 show that cross-lingual language models have the ability to enable preliminary zero-shot transfer on simple natural language",
            "An entailment model for @!START@$\\textsc {0shot-tc}$@!END@ ::: Entailment model learning.",
            "6k labeled instances for each; For test, all 10 labels are included, with 10k instances for each. Then training sets are created on remaining instances as follows. For label-partially-unseen, we create two versions of Yahoo train for $\\textsc {0shot-tc}$: Train-v0: 5 classes: {“Society & Culture”, “Health”, “Computers & Internet”, “Business & Finance”, “Family & Relationships”} are included; each is equipped with 130k labeled instances. Train-v1: 5 classes: { “Science & Mathematics”, “Education & Reference”, “Sports”, “Entertainment & Music”, “Politics & Government”} are included; each is",
            "the definition of `zero-resource' varies somewhat from author to author. For the experiments that follow, `zero-resource' means that, during model training, we do not use labels from non-English data, nor do we use human or machine-generated parallel text. Only labeled English text and unlabeled non-English text are used during training, and hyperparameters are selected using English evaluation sets. Our contributions are the following:",
            "set; (2) at a construction level, by not giving negative examples about a particular construction, e.g., verbs after a subject RC. We observe no huge score drops by both. This suggests that our learning signals at a lexical level (negative words) strengthen the abstract syntactic knowledge about the target constructions, and also that the models can generalize the knowledge acquired by negative examples to similar constructions for which negative examples are not explicitly given. The result also implies that negative examples do not have to be complete and can be noisy, which will be",
            "200 characters. The input sequences are segmented by the BERT tokenizer, with the special [CLS] token inserted at the beginning and the special [SEP] token added at the end. All inputs are then padded to a length of 256 tokens. After feeding through BERT, we obtain the hidden state of the final layer, denoted as ($h_{1}$, $h_{2}$, ... $h_{N}$) where $N$ is the max length setting. We add a fully-connected layer and softmax on top, and the final prediction is formulated as: where ${W}$ represents the parameter of the fully-connected layer and ${b}$ is the bias. The learning objective is to",
            "for 15 epochs. The learning rate is initialized to 0.001 and dropped every 3 epochs until no improvement is seen on the dev set. The final model is taken from the epoch with the highest dev set AP. All models were implemented in Torch BIBREF37 and used the rnn library of BIBREF38 ."
        ]
    },
    {
        "title": "Automated News Suggestions for Populating Wikipedia Entity Pages",
        "answer": "salience feature, authority and novelty features.",
        "evidence": [
            "performance in comparison to the baselines. Relative entity frequency from the salience feature, models the entity salience as an exponentially decaying function based on the positional index of the paragraph where the entity appears. The performance of INLINEFORM0 with relative entity frequency from the salience feature group is close to that of all the features combined. The authority and novelty features account to a further improvement in terms of precision, by adding roughly a 7%-10% increase. However, if both feature groups are considered separately, they significantly outperform the",
            "Properties of Attentions",
            "Word Relevance and Vector-Based Document Representation",
            "example has no features in common, and a BOW representation would assign low similarity scores or high distances. When results are projected onto a two dimensional space, language relationships surface, such as the clustering of synonyms, antonyms, scales (e.g. democracy to authoritarianism), hyponym-hypernyms (e.g. democracy is a type of regime), co-hyponyms (e.g. atom bombs and ballistic missiles are types of weapons), and groups of words which tend to appear in similar contexts like diplomat, envoy, and embassy. Mikolov and collaborators introduce an evaluation scheme based on word",
            "associated focus (topic of the web page) as well as the question type identified with the designed patterns (cf. Section SECREF36 ). To provide additional information about the questions that could be used for diverse IR and NLP tasks, we automatically annotated the questions with the focus, its UMLS Concept Unique Identifier (CUI) and Semantic Type. We combined two methods to recognize named entities from the titles of the crawled articles and their associated UMLS CUIs: (i) exact string matching to the UMLS Metathesaurus, and (ii) MetaMap Lite BIBREF49 . We then used the UMLS Semantic",
            "click the + or - buttons to assign positive or negative labels to the entity candidates. The score column stores the similarity score, which is calculated by the Expansion API as reference information for users. The user can also see how these entities are generated by looking at the original entities in the original column. The original entity information can be used to detect semantic drift. For instance, if the user finds the original entity of some entity candidates has negative labels, the user might consider inactivating the entity to prevent semantic drift. In the next step, the user",
            "attention can be easily concentrated to a single, undisputed region in an image and 2) a lay person would find the requested task easy to address. We employ five image-based features coming from the salient object subitizing BIBREF22 (SOS) method, which produces five probabilities that indicate whether an image contains 0, 1, 2, 3, or 4+ salient objects. Intuitively, the number of salient objects shows how many regions in an image are competing for an observer's attention, and so may correlate with the ease in identifying a region of interest. Moreover, we hypothesize this feature will",
            "For h2, the alignment of “kid” and “her family” seems to be the most salient for the decision of Neutral. Finally, for h3, the alignment between “is having fun” and “kid is playing” have the strongest impact toward the decision of Entailment. From this example, we can see that by inspecting the attention saliency, we effectively pinpoint which part of the alignments contribute most critically to the final prediction whereas simply visualizing the attention itself reveals little information. In the previous examples, we study the behavior of the same model on different inputs. Now we use the",
            "model. Note that the only supervision available is the reference summaries. Humans write summaries with the trade-off between relevance and saliency. Some salient content may not appear in reference summaries if it fails to respond to the query. Likewise, the content relevant to the query but not representative of documents will be excluded either. As a result, in an isolated model, weights for neither query-dependent nor query-independent features could be learned well from reference summaries. In addition, when measuring the query relevance, most summarization systems merely make use of",
            "and (ii) integrate discourse features into the best text classifier (i.e., CNN-based models), in the expectation of achieving state-of-the-art results in AA.  BIBREF1 (henceforth F&H14) made the first comprehensive attempt at using discourse information for AA. They employ an entity-grid model, an approach introduced by BIBREF6 for the task of ordering sentences. This model tracks how the grammatical relations of salient entities (e.g., subj, obj, etc.) change between pairs of sentences in a document, thus capturing a form of discourse coherence. The grid is summarized into a vector of",
            "part of our model. However, these features are document-internal — we will show that they are not sufficient to predict news inclusion into an entity page and add features of entity authority, news authority and novelty that measure the relations between several entities, between entity and news article as well as between several competing news articles.",
            "in an end-to-end fashion. Particularly, we introduce attention mechanism to robustly model local contextual information by selecting informative words and filtering out the noise. On the other hand, we apply GCNs to improve discriminative signals of candidate entities by exploiting the rich structure underlying the correct entities. To alleviate the global computations, we propose to convolute on the subgraph of adjacent mentions. Thus, the overall coherence shall be achieved in a chain-like way via a sliding window over the document. To the best of our knowledge, this is the first effort to"
        ]
    },
    {
        "title": "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
        "answer": "English.",
        "evidence": [
            "Data",
            "on each language, is relatively easy to obtain. We have found that, even with lists of moderate size, the results are quite significant. The grammatical categories were obtained with the help of the TreeTagger statistics tool. However, TreeTagger could be replaced by any other tool producing similar results.",
            "Multilingual RNNLM",
            "less sophisticated. Thus, setting 5 million installs as a threshold, we ensure each category includes applications with installs on both sides of this threshold. All policies included in the corpus are in English, and were collected before April 1, 2018, predating many companies' GDPR-focused BIBREF41 updates. We leave it to future studies BIBREF42 to look at the impact of the GDPR (e.g., to what extent GDPR requirements contribute to making it possible to provide users with more informative answers, and to what extent their disclosures continue to omit issues that matter to users).",
            "Results and Analysis",
            "of dataset are much more important than whether the training and testing are in the same language or not.",
            "Properties of back-translated data",
            "Methodology ::: Data presentation",
            "Our Dataset",
            "of resources in different languages. Further, concept pairs for different languages are sourced from different corpora (e.g., direct translation of the English data versus sampling from scratch in the target language). Moreover, the previous SimLex-based multilingual datasets inherit the main deficiencies of the English original version, such as the focus on nouns and highly frequent concepts. Finally, prior work mostly focused on languages that are widely spoken and do not account for the variety of the world's languages. Our long-term goal is devising a standardized methodology to extend",
            "Word embeddings",
            "Data Collection and Annotation"
        ]
    },
    {
        "title": "Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation",
        "answer": "Japanese-Vietnamese parallel corpus.  Japanese-Vietnamese MT dataset.  Japanese-Vietnamese dataset.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese parallel corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.  Japanese-Vietnamese bilingual corpus.  Japanese-Vietnamese corpus.",
        "evidence": [
            "the lines of VNESEcorpus corpus and take out the first 106758 sentences (the same as the number of sentence pairs in the original parallel corpus). For Mix-Source, instead of using a subsampled monolingual corpus, we use the Vietnamese part of the Japanese-Vietnamese parallel corpus in order to learn the multilingual information in the same domain. Our datasets are listed in Table TABREF14 .",
            "Training Dataset",
            "Construction of a Japanese Word Similarity Dataset",
            "Vietnamese Tokenization",
            "Benchmark the dataset ::: Topic detection ::: Yahoo.",
            "although the monolingual data of Vietnamese and Japanese are immensely available due to the popularity of their speakers, the bilingual Japanese-Vietnamese corpora are very limited and often in low quality or in narrowly technical domains. Therefore, data augmentation methods to exploit monolingual data for NMT systems are necessary to obtain more bilingual data, thus upgrading the translating quality.",
            "using the FCE and CoNLL 2014 datasets. Making use of artificial data provided improvements for all data generation methods. By relaxing the type restrictions and generating all types of errors, our pattern-based method consistently outperformed the system by Felice2014a. The combination of the pattern-based method with the machine translation approach gave further substantial improvements and the best performance on all datasets.",
            "dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.",
            "Japanese-Vietnamese MT is firstly mentioned in 2005 BIBREF18 . The authors focused on the difference from embedding structures between Japanese and Vietnamese, and then proposed rules for MT system and experiment on very small dataset (714 Japanese embedding sentences). This approach is suitable for small system applied in a specific domain or language, but it is not easily extendable to other domains or languages due to the expensiveness of building such rules. The other previous work for Japanese INLINEFORM0 Vietnamese uses SMT BIBREF19 . They also conducted the experiments on parallel",
            "directly inferred from the tweets. After we retrieve the QA pairs from all HITs, we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script to download the original tweets and detailed",
            "Datasets Used for the RQE Study",
            "In both tasks, we need datasets whose test sets contain new entities unseen during training. For the task of triplet classification, we directly use the datasets released by BIBREF9 ijcai2017-250 which are based on WordNet11 BIBREF29 . Since they do not conduct experiments on the link prediction task, we construct the required datasets based on FB15K BIBREF11 following a similar protocol used in BIBREF9 ijcai2017-250 as follows. Sampling unseen entities. Firstly, we randomly sample INLINEFORM0 of the original testing triplets to form a new test set INLINEFORM1 for our inductive scenario ("
        ]
    },
    {
        "title": "Ancient-Modern Chinese Translation with a Large Training Dataset",
        "answer": "TDC datasets are available in the CAPES open data website divided by years, from 2013 to 2016 in CSV and XLSX formats. We downloaded all CSV files from the respective website and loaded them into an SQL database for better manipulation. The database was then filtered to remove documents without both Portuguese and English abstracts, and additional metadata selected. After the initial filtering, the resulting documents were processed for language checking to make sure that there was no misplacing of English abstracts in the Portuguese field, or the other way around, removing the documents.",
        "evidence": [
            "Ancient-Modern Chinese Dataset",
            "understanding, but they do not explicitly address commonsense knowledge. Two notable exceptions are the NewsQA and TriviaQA datasets. NewsQA BIBREF20 is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection with respect to the method of data acquisition. As for our data collection, full texts were not shown to workers as a basis for question formulation, but only the text's title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background",
            "The TDC datasets are available in the CAPES open data website divided by years, from 2013 to 2016 in CSV and XLSX formats. We downloaded all CSV files from the respective website and loaded them into an SQL database for better manipulation. The database was then filtered to remove documents without both Portuguese and English abstracts, and additional metadata selected. After the initial filtering, the resulting documents were processed for language checking to make sure that there was no misplacing of English abstracts in the Portuguese field, or the other way around, removing the documents",
            "From gender representation in data to gender bias in AI ::: On the importance of data",
            "will be deleted from the original clauses. However, some ancient characters do not appear in its corresponding modern Chinese words. An ancient Chinese dictionary is employed to address this issue. We preprocess the ancient Chinese dictionary and remove the stop words. In this dictionary matching step, we retrieve the dictionary definition of each unmatched ancient character and use it to match the remaining modern Chinese words. To reduce the impact of universal word matching, we use Inverse Document Frequency (IDF) to weight the matching words. The lexical matching score is calculated as:",
            "of data. There are some exceptions when testing on Korean. Translating the English training data into Chinese, Japanese and Korean still improve the performance on Korean. We also found that when translated into the same language, the English training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language",
            "Weibo, RenRen, and Chinese microblogs",
            "Data Set",
            "Cross-Lingual Multi-SimLex Datasets",
            "In this section, we describe how we build our dataset with tweets. First, we crawl over 2M tweets from twitter using GetOldTweets-python. We crawl English tweets from 04/09/2012 to /12/18/2018. We first remove all re-tweets and use langdetect to remove all non-English sentences. Then, we remove hashtags attached at the end of the tweets because they are usually not parts of sentences and will confuse our language model. After that, we utilize Ekphrasis to process tweets. We remove URLs and restore remaining hashtags, elongated words, repeated words, and all-capitalized words. To simplify our",
            "Similar to BIBREF11 and BIBREF6 , the statistical information contains alignment mode and length information. There are many alignment modes between ancient and modern Chinese languages. If one ancient Chinese clause aligns two adjacent modern Chinese clauses, we call this alignment as 1-2 alignment mode. We show some examples of different alignment modes in Figure FIGREF9 . In this paper, we only consider 1-0, 0-1, 1-1, 1-2, 2-1 and 2-2 alignment modes which account for INLINEFORM0 of the Dev set. We estimate the probability Pr INLINEFORM1 n-m INLINEFORM2 of each alignment mode n-m on the",
            "In both tasks, we need datasets whose test sets contain new entities unseen during training. For the task of triplet classification, we directly use the datasets released by BIBREF9 ijcai2017-250 which are based on WordNet11 BIBREF29 . Since they do not conduct experiments on the link prediction task, we construct the required datasets based on FB15K BIBREF11 following a similar protocol used in BIBREF9 ijcai2017-250 as follows. Sampling unseen entities. Firstly, we randomly sample INLINEFORM0 of the original testing triplets to form a new test set INLINEFORM1 for our inductive scenario ("
        ]
    },
    {
        "title": "UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish",
        "answer": "CVT BIBREF20, pseudo-supervised learning, semi-supervised learning, unsupervised learning, word2vec, GloVe, LSA, CVT BIBREF20, pseudo-supervised learning, semi-supervised learning, unsupervised learning, word2vec, GloVe, LSA, CVT BIBREF20, pseudo-supervised learning, semi-supervised learning, unsupervised learning, word2vec, GloVe, LSA, CVT BIBREF20, pseudo-supervised learning, semi-supervised learning, unsupervised learning, word2vec, GloVe, LSA, CVT BIBREF20, pseudo-supervised learning, semi-supervised learning, unsupervised learning, word2vec, GloVe, LSA, CVT BIBREF20, pseudo-supervised learning, semi-supervised learning, unsupervised learning, word2vec, GloVe, LSA, CVT BIBREF20, pseudo-supervised learning, semi-supervised learning, unsupervised learning, word2vec, GloVe, LSA, CVT BIBREF20, pseudo-supervised learning, semi-supervised learning, unsupervised learning, word2vec, GloVe, LSA,",
        "evidence": [
            "Semi-supervised Learning",
            "supervised machine learning-based approaches have used different text mining-based features such as surface features, sentiment analysis, lexical resources, linguistic features, knowledge-based features or user-based and platform-based metadata BIBREF8, BIBREF9, BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be changing direction, with deep learning models being used for both feature extraction and the training of classifiers. These newer models are applying deep learning approaches such as Convolutional Neural Networks (CNNs), Long Short-Term",
            "presents the results of this experiment. Logistic Regression trained on the clinical-RQE data outperformed DL models trained on all datasets, with 73.18% Accuracy. To validate further the performance of the LR method, we evaluated it on question similarity detection. A typical approach to this task is to use an IR method to find similar question candidates, then a more sophisticated method to select and re-rank the similar questions. We followed a similar approach for this evaluation by combining the LR method with the IR baseline provided in the context of SemEval-cQA. The hybrid method",
            "There are several recent studies applying domain adaptation methods to deep neural networks. However, few studies have focused on improving the fine tuning and dual outputs methods in the supervised setting. sun2015return have proposed an unsupervised domain adaptation method and apply it to the features from deep neural networks. Their idea is to minimize the domain shift by aligning the second-order statistics of source and target distributions. In our setting, it is not necessarily true that there is a correspondence between the source and target input distributions, and therefore we",
            "Do models generalize explicit supervision, or just memorize it?",
            "is back-propagated down to the real and pseudo-word embeddings. Pseudo-encoder and discriminator parameters are pre-trained for 10k updates. At test time, the pseudo-encoder is ignored and inference is run as usual.",
            "CVT BIBREF20 is a semi-supervised learning technique whose goal is to improve the model representation using a combination of labeled and unlabeled data. During training, the model is trained alternately with one mini-batch of labeled data and INLINEFORM0 mini-batches of unlabeled data. Labeled data are input into the model to calculate the standard supervised loss for each mini-batch and the model weights are updated regularly. Meanwhile, each mini-batch of unlabeled data is selected randomly from the pool of all unlabeled data; the model computes the loss for CVT from the mini-batch of",
            "all text documents (or interview data from each respondent) prior to analyzing them. Rather, a sufficiently large set of documents can be labelled to train an algorithm and then used to classify the remaining documents. In unsupervised learning, the outcome variable is unknown. The exercise is, therefore, of a more exploratory nature. Here the purpose is to reveal patterns in the data that allow us to distinguish distinct groups whose differences are small, while variations across groups are large. In a set of text documents, we may be interested in the main topics about which respondents are",
            "systems trained on little data can directly impact semi-supervised workflows, for instance for the back-translation of monolingual data.",
            "be obtained through latent semantic analysis (LSA) used to factorize the feature matrices, but two recently developed models have been introduced which rely on different logic: word2vec from Mikolov and the research group at Google, and the global vectors for word representations (GloVe) unsupervised learning algorithm from the Stanford Natural Language Processing group. We use GloVe introduced by BIBREF15 because while LSA tends to maximize the statistical information used, it does not perform well on analogy tasks. Word2vec does better on the analogy test but does not utilize statistics of",
            "To conclude, the present study described our submission for the Semeval 2018 Shared Task on Affect in Tweets. We participated in four Spanish subtasks and our submissions ranked second, second, fourth and fifth place. Our study aimed to investigate whether the automatic generation of additional training data through translation and semi-supervised learning, as well as the creation of stepwise ensembles, increase the performance of our Spanish-language models. Strong support was found for the translation and semi-supervised learning approaches; our best models for all subtasks use either one",
            "training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance ."
        ]
    },
    {
        "title": "Conflict as an Inverse of Attention in Sequence Relationship",
        "answer": "Task 1: Offline Testing of Task-oriented Dialogue, Task 2: Online Testing of Task-oriented Dialogue, Task 3: Argumentation Mining, Task 4: Conflict Analysis.",
        "evidence": [
            "provide a theoretical limitation to it and propose another technique called conflict that looks for contrasting relationship between words in two sequences. We empirically verify that our proposed conflict mechanism combined with attention can outperform the performance of attention working solely.",
            "Therefore, a Y-result is correct with relatively high certainty (i. e., the method has high precision compared to other approaches with a similar c@1 score), as NNCD decided that author INLINEFORM6 fits best to INLINEFORM7 among INLINEFORM8 candidates. In contrast to Caravel, NNCD only retrieves the impostors from the given corpus, but it does not exploit background knowledge about the distribution of problems in the corpus. Overall, the results indicate that it is possible to recognize writing styles across large time spans. To gain more insights regarding the question which features led to",
            "style.  Conversation and user quality control: Once the task is completed, the agents tag each conversation as either successful or problematic depending on whether the session had technical glitches or user behavioral issues. We are also then able to root out problematic users based on this logging.  Agent quality control: Agents are required to login to the system which allows us to monitor performance including the number and length of each session as well as their averages.  User queuing: When there are more users trying to connect to the system than available agents, a queuing mechanism",
            "Traditional Methodologies",
            "standpoint in a given controversy (which is homeschooling in this case). In general, the output of automatic argument analysis performed on the large scale in Web data can provide users with analyzed arguments to a given topic of interest, find the evidence for the given controversial standpoint, or help to reveal flaws in argumentation of others. Satisfying the above-mentioned information needs cannot be directly tackled by current methods for, e.g., opinion mining, questions answering, or summarization and requires novel approaches within the argumentation mining field. Although",
            "Conflict model",
            "(10 folds) technique is used to test the classifier. The performance of the classifier is presented using different metrics depending the competition. SemEval uses the average of score INLINEFORM0 of positive and negative labels, TASS uses the accuracy and SENTIPOLC uses a custom metric (see BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 ).",
            "When do experts disagree? We would like to analyze the reasons for potential disagreement on the annotation task, to ensure disagreements arise due to valid differences in opinion rather than lack of adequate specification in annotation guidelines. It is important to note that the annotators are experts rather than crowdworkers. Accordingly, their judgements can be considered valid, legally-informed opinions even when their perspectives differ. For the sake of this question we randomly sample 100 instances in the test data and analyze them for likely reasons for disagreements. We consider a",
            "Task 2: Online Testing of Task-oriented Dialogue",
            "and direction of the coefficients on the covariates do not substantially differ. We find that the signs remain the same and the coefficients do not change dramatically. These results are reported in [table:tergm]Table 1 as Model 1 and 2, respectively. We then use speech and multiplex blocs as substitutes for the original vote clusters and report these as Model 3 and 4, respectively. The results of Model 3 in [table:tergm]Table 1 indicate that the text-based clusters exhibit a noticeably larger effect of preference similarity on conflict avoidance compared to the vote-based clusters.",
            "thresholds provide reliable and statistically significant estimates of conflict onset. In order to choose the final model, we select the model with the lowest variance in out-of-sample prediction capability (i.e. the area under the precision recall curve), that is, we select the model which yields the most consistent predictions over time. Because out-of-sample predictions on sparse networks (such as the conflict onset network) is a challenging task, this selection approach helps to eliminate models with predictions that vary widely from one year to the next. As previously mentioned, a",
            "incorrect, such as (UNKREF5) above. Since these negative examples provide a direct learning signal on the task at test time it may not be very surprising if the task performance goes up. We acknowledge this, and argue that our motivation for this setup is to deepen understanding, in particular the limitation or the capacity of the current architectures, which we expect can be reached with such strong supervision. Another motivation is engineering: we could exploit negative examples in different ways, and establishing a better way will be of practical importance toward building an LM or"
        ]
    },
    {
        "title": "A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification",
        "answer": "SVM, NNCD, MEMM models, Loopy Belief Propagation, AIDA, Ganea, Globerson, PBoH, NTEE, He, Chisholm, NCEL, LBP, LR+IR, Discrimination-Aware Ensemble Classification, Reject Option Classification, themis-ml, MElt, NNCD, MEMM models, Loopy Belief Propagation, AIDA, Ganea, Globerson, PBoH, NTEE, He, Chisholm, NCEL, LBP, LR+IR, Discrimination-Aware Ensemble Classification, Reject Option Classification, themis-ml, MElt, NNCD, MEMM models, Loopy Belief Propagation, AIDA, Ganea, Globerson, PBoH, NTEE, He, Chisholm, NCEL, LBP, LR+IR, Discrimination-Aware Ensemble Classification, Reject Option Classification, themis-ml, MElt, NNCD, MEMM models, Loopy Belief Propagation, AIDA, Ganea, Globerson, PBoH, NTEE, He, Chisholm, NCEL, LBP, LR+IR, Discrimination-Aware Ensemble",
        "evidence": [
            "Dictionary-based approaches",
            "Although intrinsic validation can be used to compare relevance decomposition methods for a given ML model, this approach is not suited to compare the explanatory power of different ML models, since the latter requires a common evaluation basis. Furthermore, even if we would track the classification performance changes induced by different ML models using an external classifier, it would not necessarily increase comparability, because removing words from a document may affect different classifiers very differently, so that their graphs $f(\\tilde{x})$ are not comparable. Therefore, we propose a",
            "MARN Model",
            "ML interface called themis-ml. In this interface, the main idea is to pick up a data set from a modified dataset. Themis-ml implements two methods for training fairness-aware models. The tool relies on two methods to make agnostic model type predictions: Reject Option Classification and Discrimination-Aware Ensemble Classification, these procedures being used to post-process predictions in a way that reduces potentially discriminatory predictions. According to the authors, it is possible to perceive the potential use of the method as a means of reducing bias in the use of ML algorithms. In",
            "datasets. On the 2016 test data, the LR+IR method outperformed the best system in all measures, with 80.57% Accuracy and 77.47% MAP (official system ranking measure in SemEval-cQA). On the cQA-2017 test data, the LR+IR method obtained 44.66% MAP and outperformed the cQA-2017 best system in Accuracy with 67.27%.",
            "Researches on question classification, question taxonomies and QA system have been undertaken in recent years. There are two types of approaches for question classification according to Banerjee et al in BIBREF13 - by rules and by machine learning approach. Rule based approaches use some hard coded grammar rules to map the question to an appropriate answer type BIBREF14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in BIBREF16 and BIBREF0. Many classifiers have been used in machine learning for QC such as Support Vector Machine (SVM)",
            "which involve a fixed decision criterion (for example, NNCD). Additionally, we mentioned that determinism must be fulfilled such that an AV method can be rated as reliable. Moreover, we clarified a number of misunderstandings in previous research works and proposed three clear criteria that allow to classify the model category of an AV method, which in turn influences its design and the way how it should be evaluated. In regard to binary-extrinsic AV approaches, we explained which challenges exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV",
            "models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, feature-based approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods.",
            "Results ::: Comparison w.r.t. baselines.",
            "We compare NCEL with the following state-of-the-art EL methods including three local models and three types of global models: Local models: He BIBREF29 and Chisholm BIBREF6 beat many global models by using auto-encoders and web links, respectively, and NTEE BIBREF16 achieves the best performance based on joint embeddings of words and entities. Iterative model: AIDA BIBREF22 links entities by iteratively finding a dense subgraph. Loopy Belief Propagation: Globerson BIBREF18 and PBoH BIBREF30 introduce LBP BIBREF31 techniques for collective inference, and Ganea BIBREF24 solves the global",
            "Principal components of different models",
            "associated focus (topic of the web page) as well as the question type identified with the designed patterns (cf. Section SECREF36 ). To provide additional information about the questions that could be used for diverse IR and NLP tasks, we automatically annotated the questions with the focus, its UMLS Concept Unique Identifier (CUI) and Semantic Type. We combined two methods to recognize named entities from the titles of the crawled articles and their associated UMLS CUIs: (i) exact string matching to the UMLS Metathesaurus, and (ii) MetaMap Lite BIBREF49 . We then used the UMLS Semantic"
        ]
    },
    {
        "title": "Extractive Summarization of Long Documents by Combining Global and Local Context",
        "answer": "global context, the whole document; local context, e.g., the section/topic.",
        "evidence": [
            "of both the global (the whole document) and the local context (e.g., the section/topic) when deciding if a sentence should be included in the summary Our main contributions are as follows: (i) In order to capture the local context, we are the first to apply LSTM-minus to text summarization. LSTM-minus is a method for learning embeddings of text spans, which has achieved good performance in dependency parsing BIBREF6 , in constituency parsing BIBREF7 , as well as in discourse parsing BIBREF8 . With respect to more traditional methods for capturing local context, which rely on hierarchical",
            "Local Features",
            "all scholars broadly seeking to measure political preferences and polarization in multidimensional contexts.",
            "External State Interactive Dialog Context LM",
            "Conclusions, discussion and perspectives",
            "Do models generalize explicit supervision, or just memorize it?",
            "knowledge that could enhance our understanding of micro-level processes and enable us to make better context-tailored decisions.",
            "the latent context, i.e., the series of posts that trigger an intervention. Earlier studies on MOOC forum intervention either model the entire context or require the context size to be specified explicitly.",
            "of all decoding steps from the global perspective. The performances on the CNN/Daily Mail dataset verify the effectiveness of our methods.",
            "Methodology ::: Supervised Contextual 4-scores",
            "through incremental encoding and use of commonsense knowledge. The chain of context clues, such as ${be\\_cooking}\\rightarrow {want\\_everything\\_be\\_right}\\rightarrow {perfect\\_everything}\\rightarrow {lay\\_down}\\rightarrow {get\\_back}$ , and the commonsense knowledge, such as $<$ cook, AtLocation, kitchen $>$ and $<$ oven, UsedFor, burn $>$ , are useful for generating reasonable story endings.",
            "were sampled as to span all the 34 domains available as part of BabelDomains BIBREF93, which roughly correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample.  3) Source: CARD-660 BIBREF78. 67 word pairs are taken from this dataset focused on rare word similarity, applying the same selection criteria a) to e) employed for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus BIBREF94. CARD-660 contains some words that are very rare (logboat), domain-specific"
        ]
    },
    {
        "title": "Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets",
        "answer": "Accuracy, F1 score, c@1, AUC, AUC@1, EER, PER, WER, WER 100.",
        "evidence": [
            "Evaluation Metrics",
            "Emotion labels were not considered as mututally exclusive, and each emotion was assigned a score from 0 to 100. Training/developing data amounted to 250 annotated headlines (Affective development), while systems were evaluated on another 1000 (Affective test). Evaluation was done using two different methods: a fine-grained evaluation using Pearson's r to measure the correlation between the system scores and the gold standard; and a coarse-grained method where each emotion score was converted to a binary label, and precision, recall, and f-score were computed to assess performance. As it is",
            "output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth. Besides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human",
            "According to our extensive literature research, numerous measures (e. g., Accuracy, F INLINEFORM0 , c@1, AUC, AUC@1, INLINEFORM1 or EER) have been used so far to assess the performance of AV methods. In regard to our experiments, we decided to use c@1 and AUC for several reasons. First, Accuracy, F INLINEFORM2 and INLINEFORM3 are not applicable in cases where AV methods leave verification problems unanswered, which concerns some of our examined AV approaches. Second, using AUC alone is meaningless for non-optimizable AV methods, as explained in Section UID17 . Third, both have been used in",
            "A human evaluation further validates the effectiveness of our model in generating informative, concise and readable summaries. 1.0 The contributions of this work include:",
            "We use the following three evaluation metrics: Phoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences. Word Error Rate (WER) is the percentage of words in which the predicted phoneme sequence does not exactly match the gold standard phoneme sequence. Word Error Rate 100 (WER 100) is the percentage of words in the test set for which the correct guess is not in the first 100 guesses of the system. In system evaluations, WER, WER 100, and PER numbers",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\", “iol\"). In this plan, “R-1\",“C-1\", “C-0\", and “O-3\" are symbols for locations (nodes) in the graph. We compare the performance of translation approaches based on four metrics:",
            "Experimental Setup\nBelow, we give details on the evaluation dataset and baselines used for comparison. We also describe the model features and provide implementation details.",
            "Cross-Lingual Evaluation ::: Models in Comparison",
            "Benchmark the evaluation ::: Label-fully-unseen.",
            "We developed an interface to perform the manual evaluation of the retrieved answers. Figure 5 presents the evaluation interface showing, for each test question, the top-10 answers of the evaluated QA method and the reference answer(s) used by LiveQA assessors to help judging the retrieved answers by the participating systems. We used the test questions of the medical task at TREC-2017 LiveQA BIBREF11 . These questions are randomly selected from the consumer health questions that the NLM receives daily from all over the world. The test questions cover different medical entities and have a wide",
            "During the evaluation, each task is assessed using different metrics based on previous works. For Thai sentence segmentation, three metrics are used in the evaluation: sentence boundary F1 score, non-sentence boundary F1 score, and space correct BIBREF8 . In this work, we mainly focus on the performance of sentence boundary prediction and not non-sentence boundary prediction or space prediction. Therefore, we make comparisons with other models regarding only their sentence boundary F1 scores. The equation for the sentence boundary F1 score metric is shown in f1sb. In calculating the F1 score,"
        ]
    },
    {
        "title": "Creative GANs for generating poems, lyrics, and metaphors",
        "answer": "Affective Text dataset, Fairy Tales dataset, ISEAR dataset, TutorialVQA Dataset, SnapCaptions Dataset, System-T dataset.  APDA shared task dataset, Portuguese dataset.  FCE dataset, CoNLL 2014 dataset.  Japanese dataset.  Facebook posts dataset.  Twitter dataset.  Github repository dataset.  Fairy Tales dataset.  ISEAR dataset.  Affective Text dataset.  TutorialVQA Dataset.  SnapCaptions Dataset.  System-T dataset.  APDA shared task dataset.  Portuguese dataset.  FCE dataset.  CoNLL 2014 dataset.  Japanese dataset.  Facebook posts dataset.  Twitter dataset.  Github repository dataset.  Fairy Tales dataset.  ISEAR dataset.  Affective Text dataset.  TutorialVQA Dataset.  SnapCaptions Dataset.  System-T dataset.  APDA shared task dataset.  Portuguese dataset.  FCE dataset.  CoNLL 2014 dataset.  Japanese dataset.  Facebook posts dataset.  Twitter dataset.  Github repository dataset.  Fairy Tales dataset.  ISEAR dataset.  Affective Text dataset.  TutorialVQA Dataset.  SnapCaptions Dataset.  System-T dataset",
        "evidence": [
            "Datasets",
            "using the FCE and CoNLL 2014 datasets. Making use of artificial data provided improvements for all data generation methods. By relaxing the type restrictions and generating all types of errors, our pattern-based method consistently outperformed the system by Felice2014a. The combination of the pattern-based method with the machine translation approach gave further substantial improvements and the best performance on all datasets.",
            "Models Used in the Evaluation",
            "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7 . A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role",
            "GAN setups",
            "TutorialVQA Dataset ::: Overview",
            "For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in",
            "Dictionary-based approaches",
            "dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.",
            "Datasets Used for the RQE Study",
            "Results: SnapCaptions Dataset",
            "annotated. We used the dataset from System-T BIBREF11, which has SRL tags, as well as, the other preceding tags. It was necessary to do some pre-processing and tag mapping in order to make it viable to train a Portuguese model. We made 589 tag conversions over 14 different categories. The breakdown of tag conversions per category is given by table TABREF7. These rules can be further seen in the corresponding Github repository BIBREF12. The modified training and development datasets are also available on another Github repositorypage BIBREF13 for further research and comparison purposes."
        ]
    },
    {
        "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text",
        "answer": "1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN, and baseline decoder. Also, parse2seq and baseline agent.  Additionally, dictionary-based approaches and baseline agent with training strategy and action generation.  Furthermore, baseline decoder with beam search decoder and parse2seq with linearized parses.  Also, baseline agent with parse2seq and baseline decoder with parse2seq.  Lastly, baseline agent with parse2seq and baseline decoder with beam search decoder.  Also, baseline agent with parse2seq and baseline decoder with beam search decoder and parse2seq with linearized parses.  Furthermore, baseline agent with parse2seq and baseline decoder with beam search decoder and parse2seq with linearized parses and baseline decoder with beam search decoder and parse2seq with linearized parses.  Lastly, baseline agent with parse2seq and baseline decoder with beam search decoder and parse2seq with linearized parses and baseline decoder with beam search decoder and parse2seq with linearized parses.  Also, baseline agent with parse2seq and baseline decoder with beam search decoder and parse2seq with",
        "evidence": [
            "Baseline",
            "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN",
            "Preliminaries and Framework",
            "Experimental Setup ::: Answerability Identification Baselines",
            "latter is much preferable for the overall performance. This might explain that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "the gold reference. The average of the maximum F1 across all reference answers is computed as the human baseline.",
            "Baseline Decoder\nOur baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations:",
            "Dictionary-based approaches",
            "contained in both a formal KB and a large corpus, and (3) is not limited to the schema of the underlying KB. We demonstrate significantly improved performance over state-of-the-art baselines on an open-domain natural language question answering task.",
            "Baseline Agent ::: Training Strategy ::: Action Generation",
            "side, the linearized parses are used. We allow the system to attend equally to words and node labels on the source side, rather than restricting the attention to words. We refer to this baseline as parse2seq.",
            "directly inferred from the tweets. After we retrieve the QA pairs from all HITs, we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script to download the original tweets and detailed"
        ]
    },
    {
        "title": "Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity",
        "answer": "Manually, by expert human annotators.",
        "evidence": [
            "Data Collection and Annotation",
            "INLINEFORM0 was 0.70. The research has also been active on non-English datasets. Goudas.et.al.2014 focused on user-generated Greek texts. They selected 204 documents and manually annotated sentences that contained an argument (760 out of 16,000). They distinguished claims and premises, but the claims were always implicit. However, the annotation agreement was not reported, neither was the number of annotators or the guidelines. A study on annotation of arguments was conducted by Peldszus.Stede.2013, who evaluate agreement among 26 “naive\" annotators (annotators with very little training).",
            "The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC). These captions are collected exclusively from snaps submitted to public and crowd-sourced stories (aka Snapchat Live Stories or Our Stories). Examples of such public crowd-sourced stories are “New York Story” or “Thanksgiving Story”, which comprise snaps that are aggregated for various public events, venues, etc. All snaps were posted between year 2016 and 2017, and do not contain",
            "expert annotations are available on the level of individual posts, spurious features may remain. BIBREF45 produced expert annotations of hate speech on Twitter. They found that one of the strongest features for sexism is the name of an Australian TV show, because people like to post sexist comments about the contestants. If we are trying to make claims about what inhibits or encourages hate speech, we would not want those claims to be tied to the TV show's popularity. Such problems are inevitable when datasets are not well-balanced over time, across genres, topics, etc. Especially with social",
            "more than 100 occurrences of the same focus-type pair. Each question was indexed with additional annotations for the question focus, its synonyms and the question type synonyms.",
            "200,222 excluded (x), 282 positive (p), 419 negative (n) and 2,880 objective (o) annotated sentences. Every sentence which does not contain any direct or indirect mention of the citation is labeled as being excluded (x) . The third dataset (dataset-pn) is a subset of dataset-basic, containing 828 positive and 280 negative citations. Dataset-pn was used for the purposes of (1) evaluating binary classification (positive versus negative) performance using sent2vec; (2) Comparing the sentiment classification ability of PS-Embeddings with other embeddings.",
            "Datasets Used for the RQE Study",
            "before calculating the true similarity. All the datasets except for RW simply calculated the average of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator.",
            "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7 . A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role",
            "The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF19 in the column “Size”. Due to the subjective nature of the task we did not expect perfect agreement. However, on the 100 doubly-annotated snippets, we obtained substantial inter-annotator agreement according to Cohen's kappa statistic BIBREF4 for each of the four aspects: Intention: 0.788, Intention Disclosure: 0.780, Interpretation: 0.797 and Response 0.776. In the end, the annotators discussed their",
            "How Many Expert Annotations?",
            "Dataset"
        ]
    },
    {
        "title": "Multi-attention Recurrent Network for Human Communication Comprehension",
        "answer": "Hybrid memory is a combination of LSTM and HMM, while LSTMs are standalone.",
        "evidence": [
            "Long-short Term Hybrid Memory",
            "Experiments on Additional Losses ::: Naive LSTM-LMs perform well\nThe main accuracy comparison across target constructions for different settings is presented in Table main. We first notice that our baseline LSTM-LMs (Section lm) perform much better than BIBREF0's LM. A similar observation is recently made by BIBREF6. This suggests that the original work underestimates the true syntactic ability induced by LSTM-LMs. The table also shows the results by their distilled LSTMs from RNNGs (Section intro).",
            "Intuitively, many aspects of the semantics of long sequences are context-invariant and can be computed in parallel (e.g., convolutionally), but some aspects require long-distance context and must be computed recurrently. Many existing neural network architectures either fail to take advantage of the contextual information or fail to take advantage of the parallelism. QRNNs exploit both parallelism and context, exhibiting advantages from both convolutional and recurrent neural networks. QRNNs have better predictive accuracy than LSTM-based models of equal hidden size, even though they use",
            "conversations are longer in the two-person conversations. We also report metrics by training a single model on both the datasets together.",
            "are assigned to the concatenated LSTHM memories using a deep neural network $\\mathcal {A} : \\mathbb {R}^{{d_{mem}}} \\mapsto \\mathbb {R}^{K \\times {d_{mem}}}$ with ${d_{mem}} = \\sum _{m \\in M} {d^{m}_{mem}}$ . At each timestep $t$ , the output of LSTHM is the set $\\lbrace h^m_t : m \\in M, h^m_t \\in \\mathbb {R}^{{d^m_{mem}}}\\rbrace $ . $h^m_t$0 takes the concatenation of LSTHM outputs $h^m_t$1 as input and outputs a set of $h^m_t$2 attentions $h^m_t$3 with $h^m_t$4 , $h^m_t$5 . $h^m_t$6 has a softmax layer at top of the network which takes the softmax activation along each one of the $h^m_t$7",
            "better than the standalone LSTM with the same LSTM state dimension. This effect gets smaller as we increase the LSTM size and the HMM makes less difference to the prediction (though it can still make a difference in terms of interpretability). The hybrid algorithm with 20 HMM states does better than the one with 10 HMM states. The joint hybrid algorithm outperforms the sequential hybrid on Shakespeare data, but does worse on PTB and Linux data, which suggests that the joint hybrid is more helpful for smaller data sets. The joint hybrid is an order of magnitude slower than the sequential",
            "Limitations of LSTM-LMs ::: Setup",
            "code for the hybrid memory of LSTHM. MARN achieves state-of-the-art results in 6 publicly available datasets and across 16 different attributes related to understanding human communication.",
            "The details of 5 groups of models and their variations that we experimented with for utterance-level intent recognition are summarized in this section. Instead of purely relying on machine learning (ML) or deep learning (DL) system, hybrid models leverage both ML/DL and rule-based systems. In this model, we defined our hybrid approach as using RNNs first for detecting/extracting intent keywords and slots; then applying rule-based mapping mechanisms to identify utterance-level intents (using the predicted intent keywords and slots). A sample network architecture can be seen in Fig. FIGREF18",
            "for tasks with larger vocabularies, the softmax would likely dominate computation time. It is also important to note that the cuDNN library's RNN primitives do not natively support any form of recurrent dropout. That is, running an LSTM that uses a state-of-the-art regularization scheme at cuDNN-like speeds would likely require an entirely custom kernel.",
            "A sequence of word tokens is input into the low-level module. The input tokens pass through two structures. The first structure generates a sequence of local representation vectors INLINEFORM0 , and the second structure generates low-level distant representation vectors INLINEFORM1 . After obtaining both sequences of representation vectors, the local representation vectors are fed to the Bi-LSTM to obtain the recurrent representation vectors INLINEFORM2 , as shown in recurrentrep. Then, the recurrent and distant representation vectors are concatenated to form the low-level representation",
            "LSTM-Minus"
        ]
    },
    {
        "title": "Construction of a Japanese Word Similarity Dataset",
        "answer": "tweets, articles, and crowdsourced question-answer pairs.",
        "evidence": [
            "Data Statistics",
            "directly inferred from the tweets. After we retrieve the QA pairs from all HITs, we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script to download the original tweets and detailed",
            "From gender representation in data to gender bias in AI ::: On the importance of data",
            "Overview of datasets and emotions",
            "In-domain and out-of-domain data",
            "TutorialVQA Dataset ::: Data Collection",
            "Datasets Used for the RQE Study",
            "Data Collection and Annotation",
            "Research questions",
            "Data details and experimental setup",
            "Our Dataset",
            "Methodology ::: Data presentation"
        ]
    },
    {
        "title": "Assessing the Applicability of Authorship Verification Methods",
        "answer": "A corpus compiled by the researchers themselves.",
        "evidence": [
            "Corpus statistics",
            "and crowdsourced workers interact to complete the task while the second is \"self-dialog\" in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective",
            "In addition to using the three approaches independently, we also combined all the matrices generated in the previous approaches. That is, we concatenate the reduced forms (SVD - U) of corpus-based, dictionary-based, and the whole of 4-score vectors of each word, horizontally. Accordingly, each corpus word is represented by a 404-dimensional vector, since corpus-based and dictionary-based vector components are each composed of 200 dimensions, whereas the 4-score vector component is formed by four values. The main intuition behind the ensemble method is that some approaches compensate for what",
            "The availability of cross-language parallel corpora is one of the basis of current Statistical and Neural Machine Translation systems (e.g. SMT and NMT). Acquiring a high-quality parallel corpus that is large enough to train MT systems, specially NMT ones, is not a trivial task, since it usually demands human curating and correct alignment. In light of that, the automated creation of parallel corpora from freely available resources is extremely important in Natural Language Processing (NLP), enabling the development of accurate MT solutions. Many parallel corpora are already available, some",
            "We create a new corpus which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation mining field to date. We choose several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming. A novel aspect of the corpus is its coverage of different registers of user-generated Web content, such as comments to articles, discussion forum posts, blog posts, as well as professional newswire articles. Since the data come from a variety of sources and no assumptions about its actual content with respect to",
            "exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus was to focus on a different aspect of the methods applicability. Our findings regarding the examined approaches can be summarized as follows: Despite of the good performance of the five AV methods GenIM, ImpGI, Unmasking, Caravel and SPATIUM, none of them can be truly considered as reliable and therefore applicable in real forensic cases. The reason for this is not only the non-deterministic behavior of the methods",
            "Answer Collection System",
            "Pre-Compute Embeddings",
            "the lines of VNESEcorpus corpus and take out the first 106758 sentences (the same as the number of sentence pairs in the original parallel corpus). For Mix-Source, instead of using a subsampled monolingual corpus, we use the Vietnamese part of the Japanese-Vietnamese parallel corpus in order to learn the multilingual information in the same domain. Our datasets are listed in Table TABREF14 .",
            "main focus is on identifying the best paragraphs extracted from the collected documents. They rank the paragraphs via an optimized supervised perceptron model for finding the most representative paragraph that is the least similar to paragraphs in other sections. This paragraph is then included in the newly generated entity page. Taneva and Weikum BIBREF12 propose an approach that constructs short summaries for the long tail. The summaries are called `gems' and the size of a `gem' can be user defined. They focus on generating summaries that are novel and diverse. However, they do not consider",
            "to generate additional ungrounded graphs, with the aim that one of those paraphrases will have a structure isomorphic to the correct grounding. Figure 3 and Figure 3 are two such paraphrases which can be converted to Figure 3 as described in sec:groundedGraphs. For a given input question, first we build ungrounded graphs from its paraphrases. We convert these graphs to Freebase graphs. To learn this mapping, we rely on manually assembled question-answer pairs. For each training question, we first find the set of oracle grounded graphs—Freebase subgraphs which when executed yield the correct",
            "a corpus that contains automatically collected question-answer pairs from 14 trivia and quiz-league websites, together with web-crawled evidence documents from Wikipedia and Bing. While a majority of questions require world knowledge for finding the correct answer, it is mostly factual knowledge."
        ]
    },
    {
        "title": "Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach",
        "answer": "LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal components of different models.  Baseline Models, LSTM, Transformer, Baseline Decoder, Principal",
        "evidence": [
            "Baseline Models",
            "and default hyperparameters from the fairseq BIBREF25 framework. We train the network with ADAM optimizer BIBREF26 with learning rate of 0.25 and dropout probability set to 0.2. LSTM: We consider LSTM models BIBREF27 with and without attention BIBREF28 and use the tensor2tensor BIBREF29 framework for the LSTM baselines. We use a two-layer LSTM network for both the encoder and the decoder with 128 dimensional hidden vectors. Transformer: As with LSTMs, we use the tensor2tensor framework for the Transformer model. Our Transformer BIBREF21 model uses 256 dimensions for both input embedding and",
            "What is the Model Learning?",
            "the model is able to use the syntactic information encoded in the parse while falling back to the sequential sentence when necessary. Our proposed model improves over both standard and parsed NMT baselines.",
            "for each method are summarized in Table 8 . Since the questions have four choices, all methods should perform better than 25%. TgtOnly is close to the baseline because the model is trained with only 400 samples. As in the previous experiments, FineTune and Dual are better than All and Proposed is better than the other methods.",
            "Table TABREF15 shows performance of all our proposed models and the neural baseline over our 12 MOOC dataset. Our models of UPA, PPA individually better the baseline by 5 and 2% on INLINEFORM0 and 3 and 6% on recall respectively. UPA performs the best in terms of INLINEFORM1 on average while PPA performs the best in terms of recall on average. At the individual course level, however, the results are mixed. UPA performs the best on INLINEFORM2 on 5 out of 12 courses, PPA on 3 out 12 courses, APA 1 out of 12 courses and the baseline hLSTM on 1. PPA performs the best on recall on 7 out of the 12",
            "directly inferred from the tweets. After we retrieve the QA pairs from all HITs, we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script to download the original tweets and detailed",
            "estado), the baseline model predicted {POS: Det, Gender: Masc, Number: Sing} INLINEFORM0 , {POS: Noun, Gender: Fem (X), Number: Sing} INLINEFORM1 , whereas our model was able to get the gender correct because of the transition factors in our model.",
            "Inductive Embedding Models",
            "TutorialVQA Dataset ::: Basis",
            "Baseline Decoder\nOur baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations:",
            "Principal components of different models"
        ]
    },
    {
        "title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU",
        "answer": "standard beam search decoder",
        "evidence": [
            "Baseline Decoder\nOur baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations:",
            "Learning and Decoding",
            "In our setups, we use a marked target copy, viewed as a fake source, which a generator encodes so as to fool a discriminator trained to distinguish a fake from a natural source. Our architecture contains two distinct encoders, one for the natural source and one for the pseudo-source. The latter acts as the generator ( INLINEFORM0 ) in the GAN framework, computing a representation of the pseudo-source that is then input to a discriminator ( INLINEFORM1 ), which has to sort natural from artificial encodings. INLINEFORM2 assigns a probability of a sentence being natural. During training, the",
            "Baseline models",
            "latter is much preferable for the overall performance. This might explain that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount. Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder. Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
            "LSTM Gating Signals",
            "side, the linearized parses are used. We allow the system to attend equally to words and node labels on the source side, rather than restricting the attention to words. We refer to this baseline as parse2seq.",
            "In particular, they apply a hierarchical encoder at the word and section levels. Then, in the decoding step, they combine the word attention and section attention to obtain a context vector. This approach to capture discourse structure is however quite limited both in general and especially when you consider its application to extractive summarization. First, their hierarchical method has a large number of parameters and it is therefore slow to train and likely prone to overfitting. Secondly, it does not take the global context of the whole document into account, which may arguably be",
            "Experimental Setup ::: Baselines\nWe compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base: A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue BIBREF0; VAE-CNN: A variational autoencoder model with a LSTM encoder and a dilated CNN decoder BIBREF7; vMF-VAE: A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution BIBREF5.",
            "Document Encoder",
            "Stage 1 - Retraining decoder only"
        ]
    },
    {
        "title": "A Stable Variational Autoencoder for Text Modelling",
        "answer": "informativity, coherence, and fluency.",
        "evidence": [
            "Sentence alignment quality",
            "tokens BIBREF8. When it comes to poetry generation using generative text models, Zhang and Lapata BIBREF9, Yi et al. BIBREF10 and Wang et al. BIBREF11 use language modeling to generate Chinese poems. However, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding. For the task of text generation, MaskGAN BIBREF12 uses a Reinforcement Learning signal from the discriminator, FMD-GAN BIBREF13 uses an optimal transport mechanism as an objective function. GumbelGAN BIBREF14 uses Gumbel-Softmax",
            "only about 5% of the whole amount of tokens. In the case of the automatically generated tweets, the percentage is significantly greater and entities represent about the 50%.",
            "Shortage of available training data is holding back progress in the area of automated error detection. This paper investigates two alternative methods for artificially generating writing errors, in order to create additional resources. We propose treating error generation as a machine translation task, where grammatically correct text is translated to contain errors. In addition, we explore a system for extracting textual patterns from an annotated corpus, which can then be used to insert errors into grammatically correct sentences. Our experiments show that the inclusion of artificially",
            "We then carried out a human evaluation to evaluate the generated summaries from another perspective. Our evaluators include 8 graduate students and 4 senior undergraduates, while the dataset is 100 randomly-selected articles from the test set. Each sample in this dataset also includes: 1 reference summary, 5 summaries generated by Open-NMT BIBREF21 , INLINEFORM0 BIBREF11 and BiSET under three settings, respectively, and 3 randomly-selected summaries for trapping. We asked the evaluators to independently rate each summary on a scale of 1 to 5, with respect to its quality in informativity,",
            "algorithm with the extracted features and class labels. During validation, the system extracts the features of the question and passes it into the model created during training and predicts the answer type.",
            "Methodology ::: Generating Document Vectors",
            "best scores is language-specific and hence needs to be tuned.  2) Similarity rankings obtained from word embeddings for nouns are better aligned with human judgments than all the other part-of-speech classes considered here (verbs, adjectives, and, for the first time, adverbs). This confirms previous generalizations based on experiments on English.  3) The factor having the greatest impact on the quality of word representations is the availability of raw texts to train them in the first place, rather than language properties (such as family, geographical area, typological features).  4)",
            "example, in the sentence Isaiah Thomas led the team in scoring, totaling 23 points [...]., an IE tool will extract the pair (Isaiah Thomas, 23, PTS). Second, we compute three metrics on the extracted information: $\\bullet $ Relation Generation (RG) estimates how well the system is able to generate text containing factual (i.e., correct) records. We measure the precision and absolute number (denoted respectively RG-P% and RG-#) of unique relations $r$ extracted from $\\hat{y}_{1:T}$ that also appear in $s$. $\\bullet $ Content Selection (CS) measures how well the generated document matches the",
            "GANs for creative text generation",
            "Automatic Evaluation",
            "main focus is on identifying the best paragraphs extracted from the collected documents. They rank the paragraphs via an optimized supervised perceptron model for finding the most representative paragraph that is the least similar to paragraphs in other sections. This paragraph is then included in the newly generated entity page. Taneva and Weikum BIBREF12 propose an approach that constructs short summaries for the long tail. The summaries are called `gems' and the size of a `gem' can be user defined. They focus on generating summaries that are novel and diverse. However, they do not consider"
        ]
    },
    {
        "title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances",
        "answer": "architecture",
        "evidence": [
            "We would like to investigate effectiveness the proposed joint model. To do so, the same shared layer/architecture is employed in the following variants of the proposed model: The results in terms of EM and F1 is summarized in Table TABREF20 . We observe that Joint SAN outperforms the SAN baseline with a large margin, e.g., 67.89 vs 69.27 (+1.38) and 70.68 vs 72.20 (+1.52) in terms of EM and F1 scores respectively, so it demonstrates the effectiveness of the joint optimization. By incorporating the output information of classifier into Joint SAN, it obtains a slight improvement, e.g., 72.2 vs",
            "Conflict model",
            "Do models generalize explicit supervision, or just memorize it?",
            "Exp II: Transferability of Shared Sentence Representation",
            "Modeling multimodal human communication has been studied previously. Past approaches can be categorized as follows: Non-temporal Models: Studies have focused on simplifying the temporal aspect of cross-view dynamics BIBREF5 , BIBREF6 , BIBREF7 in order to model co-occurrences of information across the modalities. In these models, each modality is summarized in a representation by collapsing the time dimension, such as averaging the modality information through time BIBREF8 . While these models are successful in understanding co-occurrences, the lack of temporal modeling is a major flaw as",
            "Principal components of different models",
            "UTCNN Model Description",
            "and speeches-cluster coefficients provides further indication of underlying heterogeneity in the network graphs. Both indicate that membership in affinity communities is associated with a decrease in the likelihood of conflict onset but appear to capture different manifestations of latent preferences. The multiplex model displays coefficients closer to Models 1 and 2. The multiplex bloc indicates that membership in affinity communities as located across vote and speech graphs is associated with a decrease in the likelihood that a given pair of states will engage in armed conflict. To increase",
            "Likewise, for topic information, a maximum pooling layer is added after the topic matrix embedding layer and topic vector embedding layer to form a joint topic matrix embedding INLINEFORM6 and a joint topic vector embedding INLINEFORM7 for topic INLINEFORM8 respectively, where INLINEFORM9 models the semantic transformation of topic INLINEFORM10 as in users and INLINEFORM11 models the topic stance tendency. The latent topic stance is also modeled by INLINEFORM12 for each topic. As for comments, we view them as short documents with authors only but without likers nor their own comments.",
            "The model\nWe create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11) . The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like.",
            "We compare three models in our experiments: (1) the distributional model of Krishnamurthy and Mitchell, described in Section \"Subgraph feature extraction\" , which is the current state-of-the-art method for open vocabulary semantic parsing; (2) a formal model (new to this work), where the distributional parameters $\\theta $ and $\\phi $ in Section \"Combined predicate models\" are fixed at zero; and (3) the combined model described in Section \"Combined predicate models\" (also new to this work). In each of these models, we used vectors of size 300 for all embeddings. Except where noted, all",
            "in a vector space common to both domains where each dimension reflects its distributional correspondence to a highly predictive term that behaves similarly across domains. A graph-based approach for sentiment classification that models the relatedness of different domains based on shared users and keywords is proposed in BIBREF30 ."
        ]
    },
    {
        "title": "Is there Gender bias and stereotype in Portuguese Word Embeddings?",
        "answer": "word2vec, fastText, BOW, LSTM, Siamese, sent2vec, PS-Embeddings, ACL-Embeddings, DBLPYogatamaDLB17, DBLPRiosK18, DBLPXiaZYCY18, DBLPYogatamaDLB17, DBLPRiosK18, DBLPXiaZYCY18, DBLPYogatamaDLB17, DBLPRiosK18, DBLPXiaZYCY18, DBLPYogatamaDLB17, DBLPRiosK18, DBLPXiaZYCY18, DBLPYogatamaDLB17, DBLPRiosK18, DBLPXiaZYCY18, DBLPYogatamaDLB17, DBLPRiosK18, DBLPXiaZYCY18, DBLPYogatamaDLB17, DBLPRiosK18, DBLPXiaZYCY18, DBLPYogatamaDLB17, DBLPRiosK18, DBLPXiaZYCY18, DBLPYogatamaDLB17, DBLPRiosK18, DBLPXiaZYCY18, DB",
        "evidence": [
            "Word embeddings",
            "BIBREF3 aiming to reduce sexist analogies previous generated. Thus, all the results are analyzed by comparing the accuracies. Using the word2vec model available in a public repository BIBREF14 , the proposal involves the analysis of the most similar analogies generated before and after the application of the BIBREF3 . The work is focused on the analysis of gender bias associated with professions in word embeddings. So therefore into the evaluation of the accuracy of the associations generated, aiming at achieving results as good as possible without prejudicing the evaluation metrics.",
            "features and found that (1) the largest feature set (including n-grams, structural features, syntactic features, topic distribution, sentiment distribution, semantic features, coreference feaures, discourse features, and features based on word embeddings) performs best in both in-domain and all-data cross validation, while (2) features based only on word embeddings yield best results in cross-domain evaluation. Since there is no one-size-fits-all argumentation theory to be applied to actual data on the Web, the argumentation model and an annotation scheme for argumentation mining is a",
            "In the literature, the main consensus is that the use of dense word embeddings outperform the sparse embeddings in many tasks. Latent semantic analysis (LSA) used to be the most popular method in generating word embeddings before the invention of the word2vec and other word vector algorithms which are mostly created by shallow neural network models. Although many studies have been employed on generating word vectors including both semantic and sentimental components, generating and analysing the effects of different types of embeddings on different tasks is an emerging field for Turkish.",
            "The encoder consists of an embedding layer, two stacks of transformer blocks (denoted as encoder transformer blocks and aggregation transformer blocks), and an attention layer. In the embedding layer, we aggregate both word- and character-level embeddings. Word embeddings are initialized by the 300-dimension fastText BIBREF20 vectors trained on Common Crawl (600B tokens), and are fixed during training. Character embeddings are initialized by 200-dimension random vectors. A convolutional layer with 96 kernels of size 5 is used to aggregate the sequence of characters. We use a max pooling layer",
            "Before proceeding to the description of our word embeddings approach, it is useful to first explore the corpus through commonly used measures of disagreement, namely Wordscore and Euclidean distance. Further, the network polarization measure known as modularity is also used to explore the levels of polarization exhibited in roll call data versus speeches. First, a bag-of-words (BOW) representation of the speeches is obtained through tokenization, stemming, removal of stop words, conversion to lower case, and the removal of punctuation, symbols, and numbers. We keep only the features which",
            "at the same time also investigating other important questions such as performance of static versus contextualized word embeddings, or multilingual versus language-specific pretraining. Another purpose of the experiments is to outline the wide potential and applicability of the Multi-SimLex datasets for multilingual and cross-lingual representation learning evaluation.",
            "of word embeddings, more and more work adopts pretrained word embeddings to represent the meaning of words, so as to provide the models with the knowledge of labels BIBREF10, BIBREF2, BIBREF11, BIBREF12. DBLPYogatamaDLB17 build generative LSTM to generate text given the embedded labels. DBLPRiosK18 use label embedding to attend the text representation in the developing of a multi-label classifier. But they report R@K, so it is unclear whether the system can really predict unseen labels. DBLPXiaZYCY18 study the zero-shot intent detection problem. The learned representations of intents are",
            "data and word types, is needed to confirm such patterns in general. Improvements in unseen word embeddings from the classifier embedding space to the Siamese embedding space (such as for democracy, morning, and basketball) are a likely result of optimizing the model for relative distances between words.",
            "method constructed sentence vectors (sent2vec) by averaging the word embeddings, which were learned from Anthology Collections (ACL-Embeddings). I also investigated polarity-specific word embeddings (PS-Embeddings) for classifying positive and negative citations. The sentence vectors formed a feature space, to which the examined citation sentence was mapped to. Those features were input into classifiers (support vector machines) for supervised classification. Using 10-cross-validation scheme, evaluation was conducted on a set of annotated citations. The results showed that word embeddings are",
            "Acoustic word embeddings --- fixed-dimensional vector representations of variable-length spoken word segments --- have begun to be considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and",
            "be the number of embedding versions in consideration, $V_1, V_2, \\ldots , V_i, \\ldots , V_c$ their vocabularies, $V^*=\\cup ^c_{i=1} V_i$ their union, and $V_i^-=V^*\\backslash V_i$ ( $i=1, \\ldots , c$ ) the vocabulary of unknown words for embedding version $i$ . Our goal is to learn embeddings for the words in $V_i^-$ by knowledge from the other $c-1$ embedding versions. We use the overlapping vocabulary between $V_i$ and $V_j$ , denoted as $V_{ij}$ , as training set, formalizing a projection $f_{ij}$ from space $V_i$ to space $V_j$ ( $i\\ne j; i,"
        ]
    },
    {
        "title": "An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models",
        "answer": "MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, FastText, CNN, LSTM, Transformers, EL, NN, RNN, LSTM, MEMM, Fast",
        "evidence": [
            "RNN Language Model",
            "As one of the first attempts in neural network models, Djuric et al. BIBREF16 proposed a two-step method including a continuous bag of words model to extract paragraph2vec embeddings and a binary classifier trained along with the embeddings to distinguish between hate speech and clean content. Badjatiya et al. BIBREF0 investigated three deep learning architectures, FastText, CNN, and LSTM, in which they initialized the word embeddings with either random or GloVe embeddings. Gambäck et al. BIBREF6 proposed a hate speech classifier based on CNN model trained on different feature embeddings such",
            "models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, feature-based approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods.",
            "through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "is to explore training end-to-end dialogue agents for our in-cabin use-cases. We are planning to exploit other modalities for improved understanding of the in-cabin dialogue as well.",
            "Deep learning has achieved tremendous success for many NLP tasks. However, unlike traditional methods that provide optimized weights for human understandable features, the behavior of deep learning models is much harder to interpret. Due to the high dimensionality of word embeddings, and the complex, typically recurrent architectures used for textual data, it is often unclear how and why a deep learning model reaches its decisions. There are a few attempts toward explaining/interpreting deep learning-based models, mostly by visualizing the representation of words and/or hidden states, and",
            "find sufficient disambiguation clues for the mention England from its surrounding words, unless it utilizes the coherence information of consistent topic “cricket\" among adjacent mentions England, Hussain, and Essex. Although the global model has achieved significant improvements, its limitation is threefold: To mitigate the first limitation, recent EL studies introduce neural network (NN) models due to its amazing feature abstraction and generalization ability. In such models, words/entities are represented by low dimensional vectors in a continuous space, and features for mention as well as",
            "The language models provided in Transformers are pretrained with a general purpose training objective, usually a variant of language modeling like standard (sometime called causal) language modeling as used for instance in BIBREF9 or masked language modeling as introduced in BIBREF13. A pretrained model is often evaluated using wide-range language understanding benchmarks. Transformers includes several tools and scripts to evaluate models on GLUE (BIBREF6) and SuperGLUE (BIBREF7). These two benchmarks gather a variety of datasets to evaluate natural language understanding systems. Details of",
            "Much prior work has exploited deep neural networks to model sentences. blacoe2012comparison represented a sentence by element-wise addition, multiplication, or recursive autoencoder over embeddings of component single words. yin2014exploration extended this approach by composing on words and phrases instead of only single words. collobert2008unified and yu2014deep used one layer of convolution over phrases detected by a sliding window on a target sentence, then used max- or average-pooling to form a sentence representation. blunsom2014convolutional stacked multiple layers of one-dimensional",
            "What is the Model Learning?",
            "In recent years, there has been an increasing interest in Machine reading comprehension (MRC), which plays a vital role in the assessment of how well a machine could understand natural language. Several datasets BIBREF0 , BIBREF1 , BIBREF2 for machine reading comprehension have been released in recent years and have driven the evolution of powerful neural models. However, much of the research up to now has been dominated by answering questions that can be well solved solved using superficial information, yet struggles to do accurate natural language understanding and reasoning. For example,",
            "We use a linear model to map ungrounded graphs to grounded ones. The parameters of the model are learned from question-answer pairs. For example, the question What language do people in Czech Republic speak? paired with its answer $\\lbrace \\textsc {CzechLanguage}\\rbrace $ . In line with most work on question answering against Freebase, we do not rely on annotated logical forms associated with the question for training and treat the mapping of a question to its grounded graph as latent. Let $q$ be a question, let $p$ be a paraphrase, let $u$ be an ungrounded graph for $p$ , and let $g$ be a"
        ]
    },
    {
        "title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset",
        "answer": "six domains.",
        "evidence": [
            "Datasets",
            "has been successfully applied on computer vision BIBREF9 and speech recognition BIBREF10 . It is also widely studied on NLP tasks such as sequence tagging and parsing BIBREF11 , BIBREF12 , BIBREF13 . Recently, adversarial domain adaptation has already been explored on spoken language understanding (SLU). Liu and Lane learned domain-general features to benefit from multiple dialogue datasets BIBREF14 ; Zhu et al. learned to transfer the model from the transcripts side to the ASR hypotheses side BIBREF15 ; Lan et al. constructed a shared space for slot tagging and language model BIBREF16 . This",
            "data set. Second, we deal with in-domain ten-fold cross validation for each of the six domains. Third, in order to evaluate the domain portability of our approach, we train the system on five domains and test on the remaining one for all six domains (which we report as cross-domain validation).",
            "the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions. We split the dataset into training (9,731 questions on 1,470 texts), development (1,411 questions on 219 texts), and test set (2,797 questions on 430 texts). Each text appears only in one of the three sets. The complete set of texts for 5 scenarios was held out for the test set. The average text, question, and answer length is 196.0 words, 7.8 words, and 3.6 words, respectively. On average, there are",
            "In-domain and out-of-domain data",
            "set. In detail, we search learning rate INLINEFORM0 in INLINEFORM1 , embedding dimension for neighbors INLINEFORM2 in INLINEFORM3 , and margin INLINEFORM4 in INLINEFORM5 . The optimal configurations are INLINEFORM6 for all the datasets.",
            "nature of TweetQA by comparing the collected data with traditional QA datasets collected primarily from formal domains. In particular, we demonstrate empirically that three strong neural models which achieve good performance on formal data do not generalize well to social media data, bringing out challenges to developing QA systems that work well on social media domains. In summary, our contributions are:",
            "What Does Zero-shot Transfer Model Learn? ::: Code-switching Dataset",
            "most relevant work to consider here is the recently released MultiWOZ dataset BIBREF13, since it is similar in size, content and collection methodologies. MultiWOZ has roughly 10,000 dialogs which feature several domains and topics. The dialogs are annotated with both dialog states and dialog acts. MultiWOZ is an entirely written corpus and uses crowdsourced workers for both assistant and user roles. In contrast, Taskmaster-1 has roughly 13,000 dialogs spanning six domains and annotated with API arguments. The two-person spoken dialogs in Taskmaster-1 use crowdsourcing for the user role but",
            "TutorialVQA Dataset ::: Overview",
            "The results for the in-domain cross validation scenario are shown in Table TABREF140 . Similarly to the cross-validation scenario, the overall best results were achieved using the largest feature set (01234). For mainstreaming and red-shirting, the best results were achieved using only the feature set 4 (embeddings). These two domains contain also fewer documents, compared to other domains (refer to Table TABREF71 ). We suspect that embeddings-based features convey important information when not enough in-domain data are available. This observation will become apparent in the next experiment.",
            "This experiment highlights a typical scenario in which domain adaptation is useful. Suppose that we have a large dataset of captioned images, which are taken from daily lives, but we would like to generate high quality captions for more specialized domain images such as minor sports and exotic food. However, captioned images for those domains are quite limited due to the annotation cost. We use domain adaptation methods to improve the captions of the target domain. To simulate the scenario, we split the Microsoft COCO dataset into food and non-food domain datasets. The MS COCO dataset"
        ]
    },
    {
        "title": "Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge",
        "answer": "Freebase",
        "evidence": [
            "The Behavioral Graph: A Knowledge Base For Navigation",
            "and the process is repeated, often with another researcher involved. The final annotations can be collected using a crowdsourcing platform, a smaller number of highly-trained annotators, or a group of experts. Which type of annotator to use should be informed by the complexity and specificity of the concept. For more complex concepts, highly-trained or expert annotators tend to produce more reliable results. However, complex concepts can sometimes be broken down into micro-tasks that can be performed independently in parallel by crowdsourced annotators. Concepts from highly specialized",
            "In the computer science field, an ontology can be defined has: a formal specification of a conceptualization; shared vocabulary and taxonomy which models a domain with the definition of objects and/or concepts and their properties and relations; the representation of entities, ideas, and events, along with their properties and relations, according to a system of categories. A knowledge base is one kind of repository typically used to store answers to questions or solutions to problems enabling rapid search, retrieval, and reuse, either by an ontology or directly by those requesting support.",
            "as references. For human performance, we use the validation answers as generated ones and the original answers as references to calculate the scores.",
            "Dictionary-based approaches",
            "KBs because the document KB is consistent with the representation of facts in external KBs. KBMRC also relates to knowledge-base question answering (KBQA) BIBREF23 , which aims to answer questions based on an external large-scale KB such as Freebase or ProBase. KBMRC differs from KBQA in that the original KB comes from the content of a document. External KB is used in this work to enhance the document KB. Moreover, existing benchmark datasets for KBQA such as WebQuestions BIBREF24 are typically limited to simple questions. The KBMRC task requires reasoning over two facts from the document KB.",
            "were sampled as to span all the 34 domains available as part of BabelDomains BIBREF93, which roughly correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample.  3) Source: CARD-660 BIBREF78. 67 word pairs are taken from this dataset focused on rare word similarity, applying the same selection criteria a) to e) employed for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus BIBREF94. CARD-660 contains some words that are very rare (logboat), domain-specific",
            "Incorporating External Knowledge",
            "Converting Freebase queries to features",
            "a corpus that contains automatically collected question-answer pairs from 14 trivia and quiz-league websites, together with web-crawled evidence documents from Wikipedia and Bing. While a majority of questions require world knowledge for finding the correct answer, it is mostly factual knowledge.",
            "TutorialVQA Dataset ::: Basis",
            "Datasets and contests"
        ]
    },
    {
        "title": "SimplerVoice: A Key Message&Visual Description Generator System for Illiteracy",
        "answer": "SimplerVoice",
        "evidence": [
            "not exploit negative examples explicitly; essentially a model is only informed of a key position (target word) that determines the grammaticality. This is rather an indirect learning signal, and we expect that it does not outperform the other approaches.",
            "Candidate entity generation",
            "into natural language. Wiseman et al. BIBREF10 show the limits of traditional NMT systems on larger structured-data, where NMT systems fail to accurately extract salient elements. To improve these models, a number of work BIBREF28, BIBREF12, BIBREF29 proposed innovating decoding modules based on planning and templates, to ensure factual and coherent mentions of records in generated descriptions. For example, Puduppully et al. BIBREF12 propose a two-step decoder which first targets specific records and then use them as a plan for the actual text generation. Similarly, Li et al. BIBREF28",
            "Principal components of different models",
            "This section discusses the process of generating key message from the object's input. Based on the retrieved input, we can easily obtain the object's title through searching in database, or using search engine; hence, we assume that the input of object2text is the object's title. The workflow of object2text is provided in Figure FIGREF4 . S-V-O query is constructed by the 3 steps below. In order to find the object type, SimplerVoice, first, builds an ontology-based knowledge tree. Then, the system maps the object with a tree's leaf node based on the object's title. For instance, given the",
            "images and coherent text samples [5]. The framework that GANs use for generating new data points employs an end-to-end neural network comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being directly exposed to the real samples. Instead, this model learns to reproduce the general patterns of the input via its interaction with the discriminator. The role of the discriminator, in turn, is to tell apart which data points are ‘real’ and which have been created by the generator. On each run",
            "can be encoded in the model through graph representation techniques, and therefore, be used to facilitate understanding story context and inferring coherent endings. Integrating the context clues and commonsense knowledge, the model can generate more reasonable endings than state-of-the-art baselines. Our contributions are as follows:",
            "What is the Model Learning?",
            "LSTM Gating Signals",
            "$0.0005$ , the model tends towards overweighting the representation of the keywords in the model output.",
            "In order to build up our language model and preserve the content, we apply the auto-encoder model. To prevent the model from simply copying the input sentence, we randomly add some noises in the input sentence. Specifically, for every word in the input sentence, there is 10% chance that we delete it, 10 % chance that we duplicate it, 10% chance that we swap it with the next word, or it remains unchanged. We first encode the input sentence INLINEFORM0 or INLINEFORM1 with respective encoder INLINEFORM2 or INLINEFORM3 to obtain its latent representation INLINEFORM4 or INLINEFORM5 and reconstruct",
            "created by any of the described methods. When comparing the error generation system by Felice2014a (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently improve performance. While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection. Combining the pattern-based approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets. The two frameworks learn to generate different"
        ]
    },
    {
        "title": "A Parallel Corpus of Theses and Dissertations Abstracts",
        "answer": "RNN-based NMT, Transformer, SMT.",
        "evidence": [
            "RNN-based NMT model",
            "—were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum.",
            "Second we asked AMT workers to provide question annotations for the videos. Our AMT experiment consisted of two parts. In the first part, we presented the workers with the video content of a segment. For each segment, we asked workers to generate questions that can be answered by the presented segment. We did not limit the number of questions a worker can input to a corresponding segment and encouraged them to input a diverse set of questions which the span can answer. Along with the questions, the workers were also required to provide a justification as to why they made their questions. We",
            "This situation sharply contrasts with the previous generation of statistical MT engines BIBREF1 , which could seamlessly integrate very large amounts of non-parallel documents, usually with a large positive effect on translation quality. Such observations have been made repeatedly and have led to many innovative techniques to integrate monolingual data in NMT, that we review shortly. The most successful approach to date is the proposal of BIBREF2 , who use monolingual target texts to generate artificial parallel data via backward translation (BT). This technique has since proven effective in",
            "Experiments ::: Effect of Training Scheme",
            "In the first experiment, we evaluated the DL and ML methods on SNLI, multi-NLI, Quora, and Clinical-QE. For the datasets that did not have a development and test sets, we randomly selected two sets, each amounting to 10% of the data, for test and development, and used the remaining 80% for training. For MultiNLI, we used the dev1-matched set for validation and the dev2-mismatched set for testing. Table TABREF28 presents the results of the first experiment. The DL model with GloVe word embeddings achieved better results on three datasets, with 82.80% Accuracy on SNLI, 78.52% Accuracy on",
            "Speech-to-Text Experiments for AMIE: Training and Testing Models on ASR Outputs",
            "latter is much preferable for the overall performance. This might explain that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "Models for Comparison",
            "when compared with the other two NMT models (RNN-based NMT and Transformer). For example, the translations of SMT are usually lack of auxiliary words, conjunctions and function words, which is not consistent with human translation habits. To further confirm this conclusion, the average length of the translation results of the three models are measured (RNN-based NMT:17.12, SMT:15.50, Transformer:16.78, Reference:16.47). We can see that the average length of the SMT outputs is shortest, and the length gaps between the SMT outputs and the references are largest. Meanwhile, the average length of",
            "Experiments ::: Tweet-Level Models ::: Baseline GRU.",
            "Translation experiments"
        ]
    },
    {
        "title": "Data Innovation for International Development: An overview of natural language processing for qualitative data analysis",
        "answer": "NLP, machine learning, human coding.",
        "evidence": [
            "poses challenges, in terms of high costs, time, or expertise required. The application of natural language processing (NLP) and machine learning (ML) can make feasible the speedy analysis of qualitative data on a large scale. We start with a brief description of the main approaches to NLP and how they can be augmented by human coding. We then move on to the issue of working with multiple languages and different document formats. We then provide an overview of the recent application of these techniques in a United Nations Development Program (UNDP) study.",
            "articles and proposed the CopyNet correspondingly. With a similar purpose, Gulcehre2016Pointing proposed to use a switch gate to control when to copy from the source article and when to generate from the vocabulary. Zhou2017Selective employed a selective gate to filter out unimportant information when encoding. Some other work attempts to incorporate external knowledge for abstractive summarization. For example, Nallapati2016Abstractive proposed to enrich their encoder with handcrafted features such as named entities and part-of-speech (POS) tags. guu2018generating also attempted to encode",
            "Qualitative Evaluation",
            "standpoint in a given controversy (which is homeschooling in this case). In general, the output of automatic argument analysis performed on the large scale in Web data can provide users with analyzed arguments to a given topic of interest, find the evidence for the given controversial standpoint, or help to reveal flaws in argumentation of others. Satisfying the above-mentioned information needs cannot be directly tackled by current methods for, e.g., opinion mining, questions answering, or summarization and requires novel approaches within the argumentation mining field. Although",
            "Morphological analysis (hajivc1998tagging, oflazer1994tagging, inter alia) is the task of predicting fine-grained annotations about the syntactic properties of tokens in a language such as part-of-speech, case, or tense. For instance, in Figure FIGREF2 , the given Portuguese sentence is labeled with the respective morphological tags such as Gender and its label value Masculine. The accuracy of morphological analyzers is paramount, because their results are often a first step in the NLP pipeline for tasks such as translation BIBREF1 , BIBREF2 and parsing BIBREF3 , and errors in the upstream",
            "experimental setup we proposed may also benefit from a refining of the data selection strategies to focus on the most useful monolingual sentences.",
            "Analysis of the convolutional language model",
            "Classical QA systems face two main challenges related to question analysis and answer extraction. Several QA approaches were proposed in the literature for the open domain BIBREF16 , BIBREF17 and the medical domain BIBREF18 , BIBREF19 , BIBREF20 . A variety of methods were developed for question analysis, focus (topic) recognition and question type identification BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 . Similarly, many different approaches tackled document or passage retrieval and answer selection and (re)ranking BIBREF25 , BIBREF26 , BIBREF27 . An alternative approach consists in finding",
            "E. Davis, “Qualitative Spatial Reasoning in Interpreting Text and Narrative” Spatial Cognition and Computation, 13:4, 2013, 264-294. H. Levesque, E. Davis, and L. Morgenstern, “The Winograd Schema Challenge,” KR 2012. L. Morgenstern, E. Davis, and C. Ortiz, “Report on the Winograd Schema Challenge, 2016”, in preparation. T. Winograd, “Procedures as a Representation for Data in a Computer Program for Understanding Natural Language,\" Ph.D. thesis, Department of Mathematics, MIT, August 1970. Published as MIT AITR-235, January 1971. T. Winograd, Understanding Natural Language, Academic Press,",
            "Entity extraction is one of the most major NLP components. Most NLP tools (e.g., NLTK, Stanford CoreNLP, etc.), including commercial services (e.g., Google Cloud API, Alchemy API, etc.), provide entity extraction functions to recognize named entities (e.g., PERSON, LOCATION, ORGANIZATION, etc.) from texts. Some studies have defined fine-grained entity types and developed extraction methods BIBREF0 based on these types. However, these methods cannot comprehensively cover domain-specific entities. For instance, a real estate search engine needs housing equipment names to index these terms for",
            "human annotations to another round of content validation. That is, take a stratified random sample, selecting observations from the full range of scores, and ask: Do these make sense in light of the systematized concept? If not, what seems to be missing? Or is something extraneous being captured? This is primarily a qualitative process that requires returning to theory and interrogating the systematized concept, indicators, and scores together. This type of validation is rarely done in NLP, but it is especially important when it is difficult to assess what drives a given machine learning",
            "Knowledge and/or data is often modeled in a structure, such as indexes, tables, key-value pairs, or triplets. These data, by their nature (e.g., raw data or long time-series data), are not easily usable by humans; outlining their crucial need to be synthesized. Recently, numerous works have focused on leveraging structured data in various applications, such as question answering BIBREF0, BIBREF1 or table retrieval BIBREF2, BIBREF3. One emerging research field consists in transcribing data-structures into natural language in order to ease their understandablity and their usablity. This field"
        ]
    },
    {
        "title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages",
        "answer": "Rosette Text Analytics, Google Cloud, TensiStrength, crowdworkers.",
        "evidence": [
            "as good as hand-crafted features, such as n-grams and sentence structure features. I may conclude from this experiment that word2vec technique has the potential to capture sentiment information in the citations, but hand-crafted features have better performance.",
            "Multi-Source",
            "Comparison with baseline models",
            "(joy, fear, anger, sadness, disgust, shame and guilt). In each case, the questions covered the way they had appraised a given situation and how they reacted. The final dataset contains reports by approximately 3000 respondents from all over the world, for a total of 7665 sentences labelled with an emotion, making this the largest dataset out of the three we use.",
            "to implement and easy to use multilingual framework, that can serve as a baseline for sentiment analysis contests, and as starting point to build new sentiment analysis systems. We compare our approach in eight different languages, three of them have important international contests, namely, SemEval (English), TASS (Spanish), and SENTIPOLC (Italian). Within the competitions our approach reaches from medium to high positions in the rankings; whereas in the remaining languages our approach outperforms the reported results.",
            "were sampled as to span all the 34 domains available as part of BabelDomains BIBREF93, which roughly correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample.  3) Source: CARD-660 BIBREF78. 67 word pairs are taken from this dataset focused on rare word similarity, applying the same selection criteria a) to e) employed for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus BIBREF94. CARD-660 contains some words that are very rare (logboat), domain-specific",
            "Sentiment Classification",
            "before calculating the true similarity. All the datasets except for RW simply calculated the average of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator.",
            "the sentiment reward with scores applied by two classifiers. To alleviate this problem and standardize the prediction results of two classifiers, we set a threshold for each classifier and subtract the respective threshold from scores applied by the classifier to obtain the comparative sentiment polarity score. We get the optimal threshold by maximizing the ability of the classifier according to the distribution of our training data. We denote the threshold of ironic sentiment classifier as INLINEFORM0 and the threshold of non-ironic sentiment classifier as INLINEFORM1 . The standardized",
            "to be 31.7% for Rosette Text Analytics, 43.2% for Google Cloud, 44.2% for TensiStrength, and 74.7% for the crowdworkers. This means the difference between the performance of the tools and the crowdworkers is significant – more than 30 percent points. Crowdworkers correctly identified 62% of the neutral, 85% of the positive, and 92% of the negative sentiments. Google Cloud correctly identified 88% of the neutral sentiments, but only 3% of the positive, and 19% of the negative sentiments. TensiStrength correctly identified 87.2% of the neutral sentiments, but 10.5% of the positive, and 8.1% of",
            "Datasets and contests",
            "to include information about emotions, so that emotion-similar words can get closer in space. Both the Google as well as our Facebook embeddings were retrofitted with lexical information obtained from the NRC10 Lexicon mentioned above, which provides emotion-similarity for each token. Note that differently from the previous two types of embeddings, the retrofitted ones do rely on handcrafted information in the form of a lexical resource."
        ]
    },
    {
        "title": "TutorialVQA: Question Answering Dataset for Tutorial Videos",
        "answer": "BIBREF9 ijcai2017-250",
        "evidence": [
            "Multi-Source",
            "Experiments on Triplet Classification\nTriplet classification aims at classifying a fact triplet INLINEFORM0 as true or false. In the dataset of BIBREF9 ijcai2017-250, triplets in the validation and testing sets are labeled as true or false, while triplets in the training set are all true ones. To tackle this task, we preset a threshold INLINEFORM0 for each relation r. If INLINEFORM1 , the triplet is classified as positive, otherwise it is negative. We determine the optimal INLINEFORM2 by maximizing classification accuracy on the validation set.",
            "Acknowledgements\nThe author is grateful to Mikhail Dektyarev, Mikhail Nokel, Anna Potapenko and Daniil Tararukhin for valuable and fruitful discussions.",
            "We use the following cheap ways to generate pseudo-source texts: copy: in this setting, the source side is a mere copy of the target-side data. Since the source vocabulary of the NMT is fixed, copying the target sentences can cause the occurrence of OOVs. To avoid this situation, BIBREF24 decompose the target words into source-side units to make the copy look like source sentences. Each OOV found in the copy is split into smaller units until all the resulting chunks are in the source vocabulary. copy-marked: another way to integrate copies without having to deal with OOVs is to augment the",
            "Effect of Cross-View Training (CVT)",
            "models that were provided a rearranged graph, beginning with the starting location of the robot, as models with “Ordered Triplets”.",
            "How Many Expert Annotations?",
            "Models enriched with external lexical information",
            "GLUE and SuperGLUE",
            "Theoretical background",
            "Raw corpus statistics",
            "Most relevant to our proposed work is the reading comprehension task, which is a question answering task involving a piece of text such as a paragraph or article. Such datasets for the reading comprehension task, such as SQuAD BIBREF6 based on Wikipedia, TriviaQA BIBREF7 constructed from trivia questions with answer evidence from Wikipedia, or those from Hermann et al. based on CNN and Daily Mail articles BIBREF8 are factoid-based, meaning the answers typically involve a single entity. Differing from video transcripts, the structures of these data sources, namely paragraphs from Wikipedia and"
        ]
    },
    {
        "title": "Transductive Learning with String Kernels for Cross-Domain Text Classification",
        "answer": "MLP, LBFGS, LSTM, CNN, Random Forest, SVM, Naive Bayes, Kernel Naive Bayes, Decision Tree, Rule Induction, Stochastic Gradient Descent.  Transformers library.  Convolutional Neural Networks.  Long Short-Term Memory.  Limited-memory Broyden–Fletcher–Goldfarb–Shanno.  PyTorch model.  TorchScript.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional Neural Networks.  Convolutional",
        "evidence": [
            "Algorithms Used",
            "What is the Model Learning?",
            "supervised machine learning-based approaches have used different text mining-based features such as surface features, sentiment analysis, lexical resources, linguistic features, knowledge-based features or user-based and platform-based metadata BIBREF8, BIBREF9, BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be changing direction, with deep learning models being used for both feature extraction and the training of classifiers. These newer models are applying deep learning approaches such as Convolutional Neural Networks (CNNs), Long Short-Term",
            "Implementation of the System ::: Classification Algorithms ::: Multi Layer Perceptron (MLP)\nMLP contains three layers - an input layer, an output layer and some hidden layers. Input layer receives the signal, the output layer gives a decision or prediction about the input and the computation of the MLP is conducted in the hidden layers. In our system, we use 100 layers. For weight optimization, we use Limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS) optimization algorithm.",
            "and the code and pretrained models, based on the Transformers library, are available online. Using in production To facilitate the transition from research to production, all the models in the library are compatible with TorchScript, an intermediate representation of a PyTorch model that can then be run either in Python in a more efficient way, or in a high-performance environment such as C++. Fine-tuned models can thus be exported to production-friendly environment. Optimizing large machine learning models for production is an ongoing effort in the community and there are many current",
            "plan is to use state-of-the-art deep neural networks and compare their performance for entity-level sentiment analysis of political tweets.",
            "Limitations of LSTM-LMs",
            "but it is especially important when it is difficult to assess what drives a given machine learning model. If there is a mismatch between the scores and systematized concept at this stage, the codebook may need to be adjusted, human coders retrained, more training data prepared, algorithms adjusted, or in some instances, even a new analytical method adopted. Other types of validation are also possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can",
            "poses challenges, in terms of high costs, time, or expertise required. The application of natural language processing (NLP) and machine learning (ML) can make feasible the speedy analysis of qualitative data on a large scale. We start with a brief description of the main approaches to NLP and how they can be augmented by human coding. We then move on to the issue of working with multiple languages and different document formats. We then provide an overview of the recent application of these techniques in a United Nations Development Program (UNDP) study.",
            "question. Despite the significant variety of questions and image content, and despite the variety of reasons for which the crowd can disagree, our learned model is able to produce quite accurate results. We observe our Random Forest classifier outperforms our deep learning classifier; e.g., Ours: RF yields a three percentage point improvement with respect to AP while consistently yielding improved precision-recall values over Ours: LSTM-CNN (Figure FIGREF8 a). In general, deep learning systems hold promise to replace handcrafted features to pick out the discriminative features. Our baselines",
            "use models that are already somewhat interpretable, for example models that use a small number of human-interpretable features. Rather than imposing such restrictions, there is also work on generating post-hoc explanations for individual predictions (e.g., BIBREF46 ), even when the underlying model itself is very complex.",
            "Many classifiers have been used in machine learning for QC such as Support Vector Machine (SVM) BIBREF16 BIBREF17, Support Vector Machines and Maximum Entropy Model BIBREF18, Naive Bayes (NB), Kernel Naive Bayes (KNB), Decision Tree (DT) and Rule Induction (RI) BIBREF13. In BIBREF0, they claimed to achieve average precision of 0.95562 for coarse class and 0.87646 for finer class using Stochastic Gradient Descent (SGD)."
        ]
    },
    {
        "title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches",
        "answer": "System-T BIBREF11, TutorialVQA Dataset, SnapCaptions Dataset, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW, RW",
        "evidence": [
            "Datasets",
            "data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to ensure they match the original author implementations' performances on various benchmarks.",
            "annotated. We used the dataset from System-T BIBREF11, which has SRL tags, as well as, the other preceding tags. It was necessary to do some pre-processing and tag mapping in order to make it viable to train a Portuguese model. We made 589 tag conversions over 14 different categories. The breakdown of tag conversions per category is given by table TABREF7. These rules can be further seen in the corresponding Github repository BIBREF12. The modified training and development datasets are also available on another Github repositorypage BIBREF13 for further research and comparison purposes.",
            "Benchmark the dataset ::: Situation detection",
            "before calculating the true similarity. All the datasets except for RW simply calculated the average of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator.",
            "of dataset are much more important than whether the training and testing are in the same language or not.",
            "Data details and experimental setup",
            "TutorialVQA Dataset ::: Overview",
            "Datasets Used for the RQE Study",
            "dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.",
            "the following context. As we discuss in Section method, this property makes it difficult to apply training with negative examples for NPIs for most of the methods studied in this work. All examples above (UNKREF1–UNKREF11) are actual test sentences, and we can see that since they are synthetic some may sound somewhat unnatural. The main argument behind using this dataset is that even not very natural, they are still strictly grammatical, and an LM equipped with robust syntactic abilities should be able to handle them as human would do.",
            "Results: SnapCaptions Dataset"
        ]
    },
    {
        "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
        "answer": "100",
        "evidence": [
            "What is the Model Learning?",
            "the number of fully-connected layers at INLINEFORM0 . There is clear utility in stacking additional layers; however, even with 4 stacked layers the RNNs still underperform the CNN-based embeddings of BIBREF13 until we begin adding fully connected layers. After exploring a variety of stacked RNNs, we fixed the stack to 3 layers and varied the number of fully connected layers. The value of each additional fully connected layer is clearly greater than that of adding stacked layers. All networks trained with 2 or 3 fully connected layers obtain more than 0.4 AP on the development set, while",
            "How Many Expert Annotations?",
            "self-attention components focus on modeling global interactions. The context-query attention layer generates the question-document similarity matrix and computes the question-aware vector representations of the context words. After that, a model encoder layer containing seven encoder blocks captures the interactions among the context words conditioned on the question. Finally, the output layer predicts a start position and an end position in the document to extract the answer span from the document.",
            "Implementation of the System ::: Classification Algorithms ::: Multi Layer Perceptron (MLP)\nMLP contains three layers - an input layer, an output layer and some hidden layers. Input layer receives the signal, the output layer gives a decision or prediction about the input and the computation of the MLP is conducted in the hidden layers. In our system, we use 100 layers. For weight optimization, we use Limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS) optimization algorithm.",
            "Principal components of different models",
            "paraphrasing model. The first dataset is from Quora dataset, which is built for detecting whether or not a pair of questions are semantically equivalent. 345,989 positive question pairs and 255,027 negative pairs are included in the first dataset. The second dataset includes web queries from query logs, which are obtained by clustering the web queries that click the same web page. In this way we obtain 6,118,023 positive query pairs. We implement a heuristic rule to get 6,118,023 negative instances for the query dataset. For each pair of query text, we clamp the first query and retrieve a",
            "Sequence-to-Sequence Model",
            "Having observed an unideal performance on this task (see Experiments below), we turned our attention to building a model that can replicate the writing style of high popularity listing descriptions. To solve this task, we designed a framework for a general adversarial network. This model employs the standard set up of a generator and a discriminator, but extends the framework with the adoption of the Diehl-Martinez-Kamalu loss. The generator is designed as a feed-forward neural network with three layers of depth. The input to the generator is simply a vector of random noise. This input is",
            "Experiments ::: User-Level Models",
            "Neural Factor Graph Model",
            "Models"
        ]
    },
    {
        "title": "\"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach",
        "answer": "The CNN model outperforms the BoW/SVM classifier in terms of explanatory power.",
        "evidence": [
            "explanations produced by LRP are semantically more meaningful than the latter. Finally, we confirm quantitatively the observations made before, namely that (1) the LRP decomposition method provides better explanations than SA and that (2) the CNN model outperforms the BoW/SVM classifier in terms of explanatory power.",
            "In NMT, like in many other deep learning tasks, accuracy can be greatly improved by adding more hidden layers, but training and decoding time increase significantly BIBREF11 , BIBREF12 , BIBREF2 . Several past works have noted that convolutional neural networks (CNNs) are significantly less expensive than RNNs, and replaced the source and/or target side with a CNN-based architecture BIBREF13 , BIBREF14 . However, these works have found it is difficult to replace the target side of the model with CNN layers while maintaining high accuracy. The use of a recurrent target is especially important",
            "Having observed an unideal performance on this task (see Experiments below), we turned our attention to building a model that can replicate the writing style of high popularity listing descriptions. To solve this task, we designed a framework for a general adversarial network. This model employs the standard set up of a generator and a discriminator, but extends the framework with the adoption of the Diehl-Martinez-Kamalu loss. The generator is designed as a feed-forward neural network with three layers of depth. The input to the generator is simply a vector of random noise. This input is",
            "as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.",
            "long documents. The word “document\" can therefore be misleading. But it is so ingrained in the common NLP lexicon that we use it anyway in this article. For insight-driven text analysis, it is often critical that high-level patterns can be communicated. Furthermore, interpretable models make it easier to find spurious features, to do error analysis, and to support interpretation of results. Some approaches are effective for prediction, but harder to interpret. The value we place on interpretability can therefore influence the approach we choose. There is an increasing interest in developing",
            "Comparison of CNN and n-gram models for local representation\nJacovi A. et al. BIBREF13 proposed that a CNN can be used as an n-gram detector to capture local text features. Therefore, we also performed an experiment to compare a CNN and n-gram embedded as local structures. The results in Table TABREF56 show that the model using the embedded n-gram yields greater improvement than the one using an embedded CNN on the Orchid and UGWC datasets.",
            "extensive grid search on the number of parameters in MARN with one attention. Increasing the number of parameters further (by increasing dense layers, LSTHM cellsizes etc.) did not improve performance. This indicates that the better performance of MARN with multiple attentions is not due to the higher number of parameters but rather due to better modeling of cross-view dynamics. RQ4: Different tasks and datasets require different number of attentions. This is highly dependent on each dataset's nature and the underlying interconnections between modalities.",
            "was not able to make up for the degradation observed in French, but allowed the German system to slightly outperform backtrans-nmt. Even though this setup is unrealistic and overly costly, it shows that GANs are actually helping even good systems.",
            "are visible even for languages such as cmn which showed reasonable performance with m-bert and are substantial on all test languages. This further confirms the validity of language-specific pretraining in lieu of multilingual training, if sufficient monolingual data are available. Moreover, a comparison of pretrained English encoders in Figure FIGREF49 largely follows the intuition: the larger bert-large model yields slight improvements over bert-base, and we can improve a bit more by relying on word-level (i.e., lexical-level) masking.Finally, light-weight albert model variants are quite",
            "any post-processing, changes in the hidden state are visible and interpretable in regards to the input. This is a consequence of the elementwise nature of the recurrent pooling function, which delays direct interaction between different channels of the hidden state until the computation of the next QRNN layer.",
            "use models that are already somewhat interpretable, for example models that use a small number of human-interpretable features. Rather than imposing such restrictions, there is also work on generating post-hoc explanations for individual predictions (e.g., BIBREF46 ), even when the underlying model itself is very complex.",
            "by a vector in two ways. The incremental encoder not only attends to the hidden states of $X_{i-1}$ , but also to the graph vectors at each position of $X_{i-1}$ . By this means, our model can generate more reasonable endings by representing context clues and encoding commonsense knowledge."
        ]
    },
    {
        "title": "Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks",
        "answer": "Slot Filling and Intent Keyword Extraction, topic categorization, and Search and Hyperlinking.  Also, they experimented with the Search and Hyperlinking Task at MediaEval BIBREF12.  Additionally, they experimented with the Multi-SimLex datasets for multilingual and cross-lingual representation learning evaluation.  Furthermore, they experimented with the 20Newsgroups dataset for topic categorization.  They also experimented with the training and developing corpus for the task 1.  Also, they experimented with the 20news-bydate version for topic categorization.  They experimented with the freely available 20Newsgroups dataset for topic categorization.  They experimented with the 20news-bydate version.  They experimented with the topic categorization task.  They experimented with the 20Newsgroups dataset.  They experimented with the 20news-bydate version.  They experimented with the topic categorization task.  They experimented with the 20Newsgroups dataset.  They experimented with the 20news-bydate version.  They experimented with the topic categorization task.  They experimented with the 20Newsgroups dataset.  They experimented with the 20news-bydate version.  They experimented with the topic categorization",
        "evidence": [
            "Translation experiments",
            "Task Definition",
            "and Hyperlinking Task at MediaEval BIBREF12. In the Search and Hyperlinking Task, 30 users were tasked to first browse the collection of videos, select interesting segments with start and end times, and then asked to conjecture questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions",
            "Experiments\nWe test the network on four classification tasks. We begin by specifying aspects of the implementation and the training of the network. We then report the results of the experiments.",
            "n-grams and GloVe features are the only group of features that contribute to more than a class type for the different tasks. Now, the “All Features” experiment shows how the interaction between feature sets perform than any of the other features groups in isolation. The accuracy metric for each trolling task is meant to provide an overall performance for all the classes within a particular task, and allow comparison between different experiments. In particular, we observe that GloVe vectors are the most powerful feature set, accuracy-wise, even better than the experiments with all features",
            "Experimenting with Transformers ::: Ecosystem",
            "Experiments on Additional Losses ::: Which additional loss works better?",
            "training and developing corpus. Hence, there are two sub tasks for the task 1. One is closed test, which means the participants can only use the released data for training and developing. The other is open test, which allows the participants to explore external corpus for training and developing. Note that there is a same test set for both the closed test and the open test. For task 2, we release 11 examples of the complete user intent and 3 data file, which includes about one month of flight, hotel and train information, for participants to build their dialogue systems. The current date for",
            "at the same time also investigating other important questions such as performance of static versus contextualized word embeddings, or multilingual versus language-specific pretraining. Another purpose of the experiments is to outline the wide potential and applicability of the Multi-SimLex datasets for multilingual and cross-lingual representation learning evaluation.",
            "Slot Filling and Intent Keyword Extraction Experiments",
            "For our experiments we consider a topic categorization task, and employ the freely available 20Newsgroups dataset consisting of newsgroup posts evenly distributed among twenty fine-grained categories. More precisely we use the 20news-bydate version, which is already partitioned into 11314 training and 7532 test documents corresponding to different periods in time. As a first preprocessing step, we remove the headers from the documents (by splitting at the first blank line) and tokenize the text with NLTK. Then, we filter the tokenized data by retaining only tokens composed of the following",
            "Experimental Results ::: Mastering Training Games"
        ]
    },
    {
        "title": "Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation",
        "answer": "4.4x",
        "evidence": [
            "Comparison with baseline models",
            "a decrease in the likelihood that a given pair of states will engage in armed conflict. To increase confidence in these results, however, we follow BIBREF8 in the assessment of out-of-sample predictive accuracy by training models on five year windows and then assessing predictions on the next year. The areas under the precision recall curves are then summed over the entire date range. The predictive capability of Model 3 outperforms their paper's baseline model with no preference networks but underperforms their date-adjusted model as measured by area under the precision recall curve (0.081",
            "However, if both feature groups are considered separately, they significantly outperform the baseline B1.",
            "using the FCE and CoNLL 2014 datasets. Making use of artificial data provided improvements for all data generation methods. By relaxing the type restrictions and generating all types of errors, our pattern-based method consistently outperformed the system by Felice2014a. The combination of the pattern-based method with the machine translation approach gave further substantial improvements and the best performance on all datasets.",
            "$0.0005$ , the model tends towards overweighting the representation of the keywords in the model output.",
            "significantly increases computation complexity. Combined with their RG-# score of $30.11$, we argue that our model is simpler, and obtains fluent description with accurate mentions in a more human-like fashion. We would also like to draw attention to the number of parameters used by those architectures. We note that our scenarios relies on a lower number of parameters (14 millions) compared to all baselines (ranging from 23 to 45 millions). This outlines the effectiveness in the design of our model relying on a structure encoding, in contrast to other approach that try to learn the structure",
            "other hyperparameters were equal to their values for the QRNN and the same beam search procedure was applied. Table TABREF17 shows that the QRNN outperformed the character-level LSTM, almost matching the performance of a word-level attentional baseline.",
            "manner if the removal increased the average score. This was done based on their original scores, i.e. starting out by trying to remove the worst individual model and working our way up to the best model. We only consider it an increase in score if the difference is larger than 0.002 (i.e. the difference between 0.716 and 0.718). If at some point the score does not increase and we are therefore unable to remove a model, the process is stopped and our best ensemble of models has been found. This process uses the scores on the development set of different combinations of models. Note that this",
            "estado), the baseline model predicted {POS: Det, Gender: Masc, Number: Sing} INLINEFORM0 , {POS: Noun, Gender: Fem (X), Number: Sing} INLINEFORM1 , whereas our model was able to get the gender correct because of the transition factors in our model.",
            "the ensemble performance. Columns correspond to label type. Results from all models outperform the baseline SVR model: Pearson's correlation coefficients range from 0.550 to 0.622. The regression correlations are the lowest. The RNN model realizes the strongest performance among the stand-alone (non-ensemble) models, outperforming variants that exploit CNN and USE representations. Combining the RNN and USE further improves results. We hypothesize that this is due to complementary sentence information encoded in universal representations. For all models, correlations for Intervention and",
            "improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount. Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder. Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
            "achieved better results on three datasets, with 82.80% Accuracy on SNLI, 78.52% Accuracy on MultiNLI, and 83.62% Accuracy on Quora. Logistic Regression achieved the best Accuracy of 98.60% on Clinical-RQE. We also performed a 10-fold cross-validation on the full Clinical-QE data of 8,588 question pairs, which gave 98.61% Accuracy. In the second experiment, we used these datasets for training only and compared their performance on our test set of 850 consumer health questions. Table TABREF29 presents the results of this experiment. Logistic Regression trained on the clinical-RQE data"
        ]
    },
    {
        "title": "Sentiment Analysis of Citations Using Word2vec",
        "answer": "accuracy, score, recall, F1 score, Macro-F, Weighted-F, perplexity, MRR, PMI.",
        "evidence": [
            "Evaluation Metrics",
            "best configuration found in the random search. By using H+M, sample size can be set to 32 or 64, as rule of thumb, and even reach improvements in most cases (see § SECREF4 ). Nonetheless, this simplification and performance boosting comes along with possible higher optimization times. Finally, the performance of each configuration is obtained using a cross-validation technique on the training data, and the metrics are usually used in classification such as: accuracy, score INLINEFORM0 , and recall, among others.",
            "One of traditional measures of interpretability in topic modeling looks as follows BIBREF29 , BIBREF19 . For each component, $n$ most probable words are selected. Then for each pair of selected words some co-occurrence measure such as PMI is calculated. These values are averaged over all pairs of selected words and all components. The other approaches use human markup. Such measures need additional data, and it is difficult to study them algebraically. Also, unlike topic modeling, word embeddings are not probabilistic: both positive and negative values of coordinates should be considered. Let",
            "match our conceptual definition? To ensure validity we must recognize gaps between what is important and what is easy to measure. We first discuss modeling considerations. Next, we describe several frequently used computational approaches and their limitations and strengths.",
            "not be considerable. This is due to the flaw of the MR metric since it is more sensitive to lower positions of the ranking, which is actually of less importance. The MRR metric is proposed for this reason, where we could observe consistent improvements brought by LAN. The effectiveness of LAN on link prediction validates LAN's superiority to other aggregators and the necessities to treat the neighbors differently in a permutation invariant way. To analyze whether LAN outperforms the others for expected reasons and generalizes to other configurations, we conduct the following studies. In this",
            "calculates metrics for each label, and find their unweighted mean. Macro-F does not take label imbalance into account. Weighted-F calculates metrics for each label, and find their average, weighted by support (the number of true instances for each label). Weighted-F alters macro-F to account for label imbalance.",
            "data. We follow madnani2012, who used MT metrics for paraphrase identification, and experiment with 8 MT metrics as features for our binary classifier. In addition, we experiment with a binary feature which checks if the sampled paraphrase preserves named entities from the input sentence. We use WEKA BIBREF32 to replicate the classifier of madnani2012 with our new feature. We tune the feature set for our classifier on the development data.",
            "Dataset and Analysis Methodology",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\", “iol\"). In this plan, “R-1\",“C-1\", “C-0\", and “O-3\" are symbols for locations (nodes) in the graph. We compare the performance of translation approaches based on four metrics:",
            "and semi-supervised models are trained until the validation metrics stop improving; the metrics are (1) sentence boundary F1 score and (2) overall F1 score for Thai sentence segmentation and English punctuation restoration, respectively. CVT has three main parameters that impact model accuracy. The first is the drop rate of the masked language model, which determines the number of tokens that are dropped and used for learning auxiliary prediction modules as described in Section SECREF21 . The second is the number of unlabeled mini-batches INLINEFORM0 used for training between supervised",
            "Target Task and Setup\nThe most common evaluation metric of an LM is perplexity. Although neural LMs achieve impressive perplexity BIBREF9, it is an average score across all tokens and does not inform the models' behaviors on linguistically challenging structures, which are rare in the corpus. This is the main motivation to separately evaluate the models' syntactic robustness by a different task.",
            "Taking context into account"
        ]
    },
    {
        "title": "Neural Factor Graph Models for Cross-lingual Morphological Tagging",
        "answer": "English, Vietnamese, French, Spanish, Arabic, German, Portuguese, Russian, Swedish, Welsh, Kiswahili, Slavic languages, Chinese, Japanese, Korean, Italian, Dutch, Greek, Hebrew, Hindi, Indonesian, Swahili, Turkish, Ukrainian, Thai, Polish, Hungarian, Czech, Romanian, Bulgarian, Croatian, Serbian, Slovak, Slovenian, Latvian, Lithuanian, Estonian, Georgian, Armenian, Azerbaijani, Belarusian, Kazakh, Kyrgyz, Tajik, Uzbek, Mongolian, Tibetan, Burmese, Khmer, Lao, Nepali, Pashto, Sinhala, Tamil, Telugu, Urdu, Yoruba, Zulu, Albanian, Bosnian, Catalan, Danish, Finnish, Galician, Icelandic, Irish, Malay, Norwegian, Occitan, Scottish Gaelic, Welsh, Yiddish, Afrikaans, Amharic, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Corsican, Croatian, Czech, Danish, Dutch, English, Estonian, Faroese, Finnish, French, Frisian, Galician, Georgian, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Irish,",
        "evidence": [
            "Translation experiments",
            "representation remains unscathed. This effectively means that m-bert's subword vocabulary contains plenty of cmn-specific and yue-specific subwords which are exploited by the encoder when producing m-bert-based representations. Simultaneously, higher scores with m-bert (and xlm in Table TABREF46) are reported for resource-rich languages such as French, Spanish, and English, which are better represented in m-bert's training data. We also observe lower absolute scores (and a larger number of OOVs) for languages with very rich and productive morphological systems such as the two Slavic languages",
            "In this overview, our aim has been to demonstrate how new forms of data can be leveraged to complement the work of practitioners in international development. We have demonstrated that a wide variety of questions can be asked. Exploratory work can be performed to systematize large quantities of text. Additionally, we can learn about the sentiment that specific groups express towards specific topics. Networks can be uncovered, the spread of information or ideas can be traced, and influential actors identified. We can classify documents based on human coding of a subset of documents and",
            "through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "before and were also not familiar with the packaged products including the chosen 20 products; hence, they were \"illiterate\" in terms of comprehending English and in terms of having used any of the products although they might be literate in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this",
            "Analysis of the convolutional language model",
            "figuring out their pronunciations. The etymology of a word could be tagged in an analogous way to how language ID is tagged in multilingual g2p.",
            "The literature devoted to the use of monolingual data is large, and quickly expanding. We already alluded to several possible ways to use such data: using back- or forward-translation or using a target language model. The former approach is mostly documented in BIBREF2 , and recently analyzed in BIBREF19 , which focus on fully artificial settings as well as pivot-based artificial data; and BIBREF4 , which studies the effects of increasing the size of BT data. The studies of BIBREF20 , BIBREF19 also consider forward translation and BIBREF21 expand these results to domain adaptation scenarios.",
            "considered as well as the other languages tested. It can be observed, from the table, the number of examples as well as the number of instances for each polarity level, namely, positive, neutral, negative and none. The training and development (only in SemEval) sets are used to train the sentiment classifier, and the gold set is used to test the classifier. In the case there dataset was not split in training and gold (Arabic, German, Portuguese, Russian, and Swedish) then a cross-validation (10 folds) technique is used to test the classifier. The performance of the classifier is presented",
            "is to explore training end-to-end dialogue agents for our in-cabin use-cases. We are planning to exploit other modalities for improved understanding of the in-cabin dialogue as well.",
            "Language Resource References\nlrec lrec2018",
            "of the world's languages. Our long-term goal is devising a standardized methodology to extend the coverage also to languages that are resource-lean and/or typologically diverse (e.g., Welsh, Kiswahili as in this work).  Multilingual Datasets for Natural Language Understanding. The Multi-SimLex initiative and corresponding datasets are also aligned with the recent efforts on procuring multilingual benchmarks that can help advance computational modeling of natural language understanding across different languages. For instance, pretrained multilingual language models such as multilingual bert"
        ]
    },
    {
        "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
        "answer": "BERT, RoBERTa, DistilBERT, ALBERT, XLNet, and others.",
        "evidence": [
            "All the models follow the same philosophy of abstraction enabling a unified API in the library. Configuration - A configuration class instance (usually inheriting from a base class `PretrainedConfig`) stores the model and tokenizer parameters (such as the vocabulary size, the hidden dimensions, dropout rate, etc.). This configuration object can be saved and loaded for reproducibility or simply modified for architecture search. The configuration defines the architecture of the model but also architecture optimizations like the heads to prune. Configurations are agnostic to the deep learning",
            "Neural models for NER have been recently proposed, producing state-of-the-art performance on standard NER tasks. For example, some of the end-to-end NER systems BIBREF4 , BIBREF2 , BIBREF3 , BIBREF0 , BIBREF1 use a recurrent neural network usually with a CRF BIBREF5 , BIBREF6 for sequence labeling, accompanied with feature extractors for words and characters (CNN, LSTMs, etc.), and achieve the state-of-the-art performance mostly without any use of gazetteers information. Note that most of these work aggregate textual contexts via concatenation of word embeddings and character embeddings.",
            "Pre-Compute Embeddings",
            "through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "Fortunately, we believe that the introduction of unsupervised generative language models presents a way in which to tackle this particular shortcoming of peer-to-peer markets. In 2014, Ian Goodfellow et. al proposed the general adversarial network (GAN) [5]. The group showcased how this generative model could learn to artificially replicate data patterns to an unprecedented realistic degree [5]. Since then, these models have shown tremendous potential in their ability to generate photo-realistic images and coherent text samples [5]. The framework that GANs use for generating new data points",
            "Recent advances in modern Natural Language Processing (NLP) research have been dominated by the combination of Transfer Learning methods with large-scale language models, in particular based on the Transformer architecture. With them came a paradigm shift in NLP with the starting point for training a model on a downstream task moving from a blank specific model to a general-purpose pretrained architecture. Still, creating these general-purpose models remains an expensive and time-consuming process restricting the use of these methods to a small sub-set of the wider NLP community. In this",
            "baseline from prior work. All of the data and code used in these experiments is available at http://github.com/allenai/open_vocab_semparse.",
            "analysis of pretrained encoders on word-level semantic similarity across diverse languages. In addition, the experiments now enabled by Multi-SimLex aim to answer several important questions. (Q1) Is it viable to extract high-quality word-level representations from pretrained encoders receiving subword-level tokens as input? Are such representations competitive with standard static word-level embeddings? (Q2) What are the implications of monolingual pretraining versus (massively) multilingual pretraining for performance? (Q3) Do lightweight unsupervised post-processing techniques improve word",
            "Supplemental Material ::: Hyperparameters ::: ELMo-transformer\nOur baseline pretraining model was a reimplementation of that given in BIBREF0. Hyperparameters were generally identical, but we trained on only 2 GPUs with (up to) 4,000 tokens per batch. This difference in batch size meant we used 6,000 warm up steps with the learning rate schedule of BIBREF16.",
            "describe a number of techniques for improving the speed of the decoder, and obtain a 4.4x speedup over a highly efficient baseline. These speedups do not affect decoding results, so they can be applied universally. In Section \"Model Improvements\" , we describe a simple but powerful network architecture which uses a single RNN (GRU/LSTM) layer at the bottom with a large number of fully-connected (FC) layers on top, and obtains improvements similar to a deep RNN model at a fraction of the training and decoding cost.",
            "provided architecture and ontology are language independent. Although some of the NLP modules are language dependent, they can be built using adequate AI methodologies.",
            "and the code and pretrained models, based on the Transformers library, are available online. Using in production To facilitate the transition from research to production, all the models in the library are compatible with TorchScript, an intermediate representation of a PyTorch model that can then be run either in Python in a more efficient way, or in a high-performance environment such as C++. Fine-tuned models can thus be exported to production-friendly environment. Optimizing large machine learning models for production is an ongoing effort in the community and there are many current"
        ]
    },
    {
        "title": "Multichannel Variable-Size Convolution for Sentence Classification",
        "answer": "It combines multichannel initialization and variable-size filters.",
        "evidence": [
            "Conclusion\nThis work presented MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization – diverse versions of pretrained word embeddings are used – and variable-size filters – features of multigranular phrases are extracted with variable-size convolution filters. We demonstrated that multichannel initialization and variable-size filters enhance system performance on sentiment classification and subjectivity classification tasks.",
            "Qualitative Comparison",
            "Wiki ft, and m-bert are not directly comparable, because these models have different coverage. In particular, Multi-SimLex contains some out-of-vocabulary (OOV) words whose static ft embeddings are not available. On the other hand, m-bert has perfect coverage. A general comparison between CC+Wiki and Wiki ft vectors, however, supports the intuition that larger corpora (such as CC+Wiki) yield higher correlations. Another finding is that a single massively multilingual model such as m-bert cannot produce semantically rich word-level representations. Whether this actually happens because the",
            "UTCNN Model Description",
            "process also automatically filters out most of the short tweets. For the tweets collected from CNN, INLINEFORM0 of them were filtered via semantic role labeling. For tweets from NBC, INLINEFORM1 of the tweets were filtered. We then use Amazon Mechanical Turk to collect question-answer pairs for the filtered tweets. For each Human Intelligence Task (HIT), we ask the worker to read three tweets and write two question-answer pairs for each tweet. To ensure the quality, we require the workers to be located in major English speaking countries (i.e. Canada, US, and UK) and have an acceptance rate",
            "certainly has a negative impact on all models. However, the performance of LAN does not decrease as drastically as that of MEAN and LSTM, indicating that LAN is more robust on sparse KGs.",
            "Effect of Cross-View Training (CVT)",
            "Each layer of a quasi-recurrent neural network consists of two kinds of subcomponents, analogous to convolution and pooling layers in CNNs. The convolutional component, like convolutional layers in CNNs, allows fully parallel computation across both minibatches and spatial dimensions, in this case the sequence dimension. The pooling component, like pooling layers in CNNs, lacks trainable parameters and allows fully parallel computation across minibatch and feature dimensions. Given an input sequence INLINEFORM0 of INLINEFORM1 INLINEFORM2 -dimensional vectors INLINEFORM3 , the convolutional",
            "a lot from the CNN1 model in the first deletion experiment. In order to quantitatively evaluate and compare the ML models in combination with a relevance decomposition or explanation technique, we apply the evaluation method described in section \"Measuring Model Explanatory Power through Extrinsic Validation\" . That is, we compute the accuracy of an external classifier (here KNN) on the classification of document summary vectors (obtained with the ML model's predicted class). For these experiments we remove test documents which are empty or contain only one word after preprocessing (this",
            "as in machine reading comprehension, rather than selects key information from the two to form an enhanced article representation.",
            "Graph Convolutional Network",
            "in particular have proven extremely valuable to visual questions-answering tasks BIBREF9, the most similar being MovieQA BIBREF1 and VideoQA BIBREF0. Similar to how our data is generated from video tutorials, the MovieQA and VideoQA corpus is generated from movie scripts and news transcipts, respectively. MovieQA's answers have a shorter span than the answers collected in our corpus, because questions and answer pairs were generated after each paragraph in a movie's plot synopsis BIBREF1. The MovieQA dataset also contains specific annotated answers with incorrect examples for each question."
        ]
    },
    {
        "title": "A Lightweight Front-end Tool for Interactive Entity Population",
        "answer": "JavaScript",
        "evidence": [
            "NLP Toolkits",
            "an original entity, which was used to find the entity, and the score, which denotes the confidence score. A user can directly edit the table by adding, renaming, and deleting entities. Moreover, the entity inactivation function allows a user to manually inactivate entities, so that entity expansion algorithms do not use the inactivated entities. The table implements a page switching function, a search function, and a sorting function to ensure visibility even when there is a large number of entities in the table.",
            "and the code and pretrained models, based on the Transformers library, are available online. Using in production To facilitate the transition from research to production, all the models in the library are compatible with TorchScript, an intermediate representation of a PyTorch model that can then be run either in Python in a more efficient way, or in a high-performance environment such as C++. Fine-tuned models can thus be exported to production-friendly environment. Optimizing large machine learning models for production is an ongoing effort in the community and there are many current",
            "before and were also not familiar with the packaged products including the chosen 20 products; hence, they were \"illiterate\" in terms of comprehending English and in terms of having used any of the products although they might be literate in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this",
            "LUWAK is implemented in pure JavaScript code, and it uses the LocalStorage of a web browser. A user does not have to install any packages except for a web browser. The only thing the user must do is to download the LUWAK software and put it in a local directory. We believe the cost of installing the tool will keep away a good amount of potential users. This philosophy follows our empirical feeling of the curse of installing required packages/libraries. Moreover, LUWAK does not need users to consider the usage and maintenance of these additional packages. That is why we are deeply committed to",
            "tagging use Hidden Markov Models BIBREF7, Maximum Entropy Markov Models BIBREF7, BIBREF8, or Conditional Random Fields BIBREF9. Recent works BIBREF10, BIBREF11 have used recurrent neural networks with attention modules for NER. Sentiment detection tools like SentiStrength BIBREF12 and TensiStrength BIBREF13 are rule-based tools, relying on various dictionaries of emoticons, slangs, idioms, and ironic phrases, and set of rules that can detect the sentiment of a sentence overall or a targeted sentiment. Given a list of keywords, TensiStrength (similar to SentiStrength) reports the sentiment",
            "Data details and experimental setup",
            "How Many Expert Annotations?",
            "less sophisticated. Thus, setting 5 million installs as a threshold, we ensure each category includes applications with installs on both sides of this threshold. All policies included in the corpus are in English, and were collected before April 1, 2018, predating many companies' GDPR-focused BIBREF41 updates. We leave it to future studies BIBREF42 to look at the impact of the GDPR (e.g., to what extent GDPR requirements contribute to making it possible to provide users with more informative answers, and to what extent their disclosures continue to omit issues that matter to users).",
            "development, value chain, female entrepreneurship and empowerment, and youth unemployment issues. The micro-narratives were collected using SenseMaker(r), a commercial tool for collecting qualitative and quantitative data. The micro-narratives were individual responses to context-tailored questions. An example of such a question is: “Share a recent example of an event that made it easier or harder to support how your family lives.” While the analysis and visualization of quantitative data was not problematic, systematic analysis and visualization of qualitative data, collected in a format of",
            "State-of-the-art",
            "For sentence alignment, we used the LF aligner tool, a wrapper around the Hunalign tool BIBREF7 , which provides an easy to use and complete solution for sentence alignment, including pre-loaded dictionaries for several languages. Hunalign uses Gale-Church sentence-length information to first automatically build a dictionary based on this alignment. Once the dictionary is built, the algorithm realigns the input text in a second iteration, this time combining sentence-length information with the dictionary. When a dictionary is supplied to the algorithm, the first step is skipped. A drawback"
        ]
    },
    {
        "title": "Modeling Trolling in Social Media Conversations",
        "answer": "13,215",
        "evidence": [
            "Datasets and Experimental Setup",
            "the data while making it easy for workers to apply labels consistently.  Size: The total of 13,215 dialogs in this corpus is on par with similar, recently released datasets such as MultiWOZ BIBREF13.",
            "Results: SnapCaptions Dataset",
            "shows the basic information of the three datasets. We can find that the data sizes of DUC are quite different. The sentence number of DUC 2007 is only about a half of DUC 2005's. For each cluster, a summarization system is requested to generate a summary with the length limit of 250 words. We conduct a 3-fold cross-validation on DUC datasets, with two years of data as the training set and one year of data as the test set.",
            "Data\nThe next step involves deciding on the data sources, collecting and compiling the dataset, and inspecting its metadata.",
            "We train our model using the Adam optimizer BIBREF25 with learning rate INLINEFORM0 and a drop out rate of 0.3. We use a mini-batch with a batch size of 32 documents, and the size of the GRU hidden states is 300. For word embeddings, we use GloVe BIBREF26 with dimension 300, pre-trained on the Wikipedia and Gigaword. The vocabulary size of our model is 50000. All the above parameters were set based on BIBREF18 without any fine-tuning. Again following BIBREF18 , we train each model for 50 epochs, and the best model is selected with early stopping on the validation set according to Rouge-2",
            "Ancient-Modern Chinese Dataset",
            "(joy, fear, anger, sadness, disgust, shame and guilt). In each case, the questions covered the way they had appraised a given situation and how they reacted. The final dataset contains reports by approximately 3000 respondents from all over the world, for a total of 7665 sentences labelled with an emotion, making this the largest dataset out of the three we use.",
            "TutorialVQA Dataset ::: Overview",
            "Data Analysis",
            "the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions. We split the dataset into training (9,731 questions on 1,470 texts), development (1,411 questions on 219 texts), and test set (2,797 questions on 430 texts). Each text appears only in one of the three sets. The complete set of texts for 5 scenarios was held out for the test set. The average text, question, and answer length is 196.0 words, 7.8 words, and 3.6 words, respectively. On average, there are",
            "Dataset"
        ]
    },
    {
        "title": "Rotations and Interpretability of Word Embeddings: the Case of the Russian Language",
        "answer": "INLINEFORM6, which scales the result depending of the agreement between references. Cross-Lingual Evaluation ::: Models in Comparison. INLINEFORM6, which scales the result depending of the agreement between references. Cross-Lingual Evaluation ::: Models in Comparison. INLINEFORM6, which scales the result depending of the agreement between references. Cross-Lingual Evaluation ::: Models in Comparison. INLINEFORM6, which scales the result depending of the agreement between references. Cross-Lingual Evaluation ::: Models in Comparison. INLINEFORM6, which scales the result depending of the agreement between references. Cross-Lingual Evaluation ::: Models in Comparison. INLINEFORM6, which scales the result depending of the agreement between references. Cross-Lingual Evaluation ::: Models in Comparison. INLINEFORM6, which scales the result depending of the agreement between references. Cross-Lingual Evaluation ::: Models in Comparison. INLINEFORM6, which scales the result depending of the agreement between references. Cross-Lingual Evaluation ::: Models in Comparison. INLINEFORM6, which scales the result depending of the agreement between references. Cross-Lingual Evaluation ::: Models in Comparison. INLINEFORM6, which scales the result depending of the agreement between references. Cross-Lingual Evaluation ::: Models in Comparison. INLINEFORM6, which scales",
        "evidence": [
            "Interpretability measures",
            "In particular, they apply a hierarchical encoder at the word and section levels. Then, in the decoding step, they combine the word attention and section attention to obtain a context vector. This approach to capture discourse structure is however quite limited both in general and especially when you consider its application to extractive summarization. First, their hierarchical method has a large number of parameters and it is therefore slow to train and likely prone to overfitting. Secondly, it does not take the global context of the whole document into account, which may arguably be",
            "be evaluated and then, the computation of INLINEFORM6 , which scales the result depending of the agreement between references.",
            "Cross-Lingual Evaluation ::: Models in Comparison",
            "scales on the other, are becoming increasingly common. Indeed, computational analysis is opening new possibilities for exploring challenging questions at the heart of some of the most pressing contemporary cultural and social issues. While a human reader is better equipped to make logical inferences, resolve ambiguities, and apply cultural knowledge than a computer, human time and attention are limited. Moreover, many patterns are not obvious in any specific context, but only stand out in the aggregate. For example, in a landmark study, BIBREF1 analyzed the authorship of The Federalist Papers",
            "(Q^T W^T W W^T W Q)_{1, 1} = \\left(q^T W^T W q\\right)^2\n$  is maximized when $q$ is the eigenvector of $W^T W$ with the largest singular value, i. e., the first right singular vector of $W$ BIBREF28 . Let's fix this vector and choose other vectors to be orthogonal to the selected ones and to maximize the interpretability. We arrive at $Q = V$ , where $V$ is the right orthogonal factor in SVD $W = U \\Sigma V^T$ .",
            "was assigned to questions that can be answered from the text directly. If the answer could only be inferred by using commonsense knowledge, the category script-based was assigned. Making this distinction is interesting for evaluation purposes, since it enables us to estimate the number of commonsense inference questions. For questions that did not make sense at all given a text, unfitting was assigned. If a question made sense for a text, but it was impossible to find an answer, the label unknown was used. In a second step, we told participants to formulate a plausible correct and a plausible",
            "the discussed controversy. We observed that this standpoint is not always explicitly expressed, but remains implicit and must be inferred by the reader. Therefore, we allow the claim to be implicit. In such a case, the annotators must explicitly write down the (inferred) stance of the author. By definition, the Toulmin's model is intended to model single argument, with the claim in its center. However, we observed in our data, that some authors elaborate on both sides of the controversy equally and put forward an argument for each side (by argument here we mean the claim and its premises,",
            "abstract evaluation measures, we could also assess the models in task-based settings using human experts. Furthermore, for insight-driven analyses, it can be more useful to focus on improving explanatory power than making small improvements in predictive performance.",
            "These metrics estimate the ability of our model to integrate elements from the table in its descriptions. Particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ. To do so, we follow the protocol presented in BIBREF10. First, we apply an information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset. Entity-value pairs are extracted from the descriptions. For example, in the sentence Isaiah Thomas led the team in scoring, totaling 23 points [...]., an IE",
            "Manual evaluations are indispensable to evaluate the coherence and logic of generated endings. For manual evaluation, we randomly sampled 200 stories from the test set and obtained 1,600 endings from the eight models. Then, we resorted to Amazon Mechanical Turk (MTurk) for annotation. Each ending will be scored by three annotators and majority voting is used to select the final label. We defined two metrics - grammar and logicality for manual evaluation. Score 0/1/2 is applied to each metric during annotation. Whether an ending is natural and fluent. Score 2 is for endings without any grammar",
            "0.780, Interpretation: 0.797 and Response 0.776. In the end, the annotators discussed their discrepancies and managed to resolve all of them."
        ]
    },
    {
        "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study",
        "answer": "MLE and relying on target sequences as a reference.  Also, repetition of the same answer from different websites.  Wrong focus.  Randomness in weight initialization.  Additional parameters.  Duplicates.  Near-duplicates.  Drugs.  Inter-answer comparisons.  Answer selection.  Citation contexts.  Shallow syntax.  Contextualization.  Downstream architectures.  Dataset statistics.  QA performance.  Precision.  Focus recognition module.  Answer selection.  Inter-answer comparisons.  Removal of near-duplicates.  Assessors.  Performance.  Task 1.  Attention-conflict model.  Weight initialization.  Additional parameters.  Examples.  Incorrectly marked.  Correctly marked.  Duplicate.  Non-duplicate.  Combined model.  Previous model.  Randomness.  Initialization.  Parameters.  Duplicates.  Near-duplicates.  Drugs.  Inter-answer comparisons.  Answer selection.  Citation contexts.  Shallow syntax.  Contextualization.  Downstream architectures.  Dataset statistics.  QA performance.  Precision.  Focus recognition module.  Answer selection.  Inter-answer comparisons.  Removal of near-duplicates.  Assessors.  Performance.  Task",
        "evidence": [
            "Paper Overview",
            "Particularly, since these scores use target sequences as a reference, it has the same pitfalls as relying on MLE. The advantages in this approach lie in the discriminator's ability to influence the generator to explore other possibilities. Sample outputs for our model can be found on our website .",
            "How Many Expert Annotations?",
            "questions, which may continue to evolve for much of the project. At each stage, our discussion is critically informed by insights from the humanities and social sciences, fields that have focused on, and worked to tackle, the challenges of textual analysis—albeit at smaller scales—since their inception. In describing our experiences with computational text analysis, we hope to achieve three primary goals. First, we aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods. Second, we hope to provide a set of best practices for",
            "Acknowledgements\nIn addition to the anonymous reviewers, we want to thank Lucia Passaro and Barbara Plank for insightful discussions, and for providing comments on draft versions of this paper.",
            "Traditional Methodologies",
            "Introduction ::: First problem.",
            "of shallow syntax or contextualization. Details of downstream architectures are provided below, and overall dataset statistics for all tasks is shown in the Appendix, Table TABREF26.",
            "Qualitative analysis shows the advantages and limitations of our approaches, as well as the remaining challenges.",
            "information on how the target paper is cited in other papers. Currently, we do not use any information from citation contexts.",
            "but a wrong focus, which indicates that including a focus recognition module to filter such wrong answers can improve further the QA performance in terms of precision. Another aspect that was reported is the repetition of the same (or similar) answer from different websites, which could be addressed by improving answer selection with inter-answer comparisons and removal of near-duplicates. Also, half of the LiveQA test questions are about Drugs, when only two of our resources are specialized in Drugs, among 12 sub-collections overall. Accordingly, the assessors noticed that the performance of",
            "Analyzing the gains\nWe analyzed the gains in Task 1 which we get from the attention-conflict model in order to ensure that they are not due to randomness in weight initialization or simply additional parameters. We particularly focused on the examples which were incorrectly marked in attention model but correctly in attention-conflict model. We saw that 70% of those cases are the ones where the pair was incorrectly marked as duplicate in the previous model but our combined model correctly marked them as non-duplicate."
        ]
    },
    {
        "title": "Attention Optimization for Abstractive Document Summarization",
        "answer": "F1 score, BLEU, METEOR, PER, WER, WER 100, Phoneme Error Rate, Word Error Rate, Word Error Rate 100, sentence boundary F1 score, non-sentence boundary F1 score, space correct, tokenized BLEU, METEOR, and sentence boundary F1 score.",
        "evidence": [
            "Evaluation Metrics",
            "We then carried out a human evaluation to evaluate the generated summaries from another perspective. Our evaluators include 8 graduate students and 4 senior undergraduates, while the dataset is 100 randomly-selected articles from the test set. Each sample in this dataset also includes: 1 reference summary, 5 summaries generated by Open-NMT BIBREF21 , INLINEFORM0 BIBREF11 and BiSET under three settings, respectively, and 3 randomly-selected summaries for trapping. We asked the evaluators to independently rate each summary on a scale of 1 to 5, with respect to its quality in informativity,",
            "ability. We also conduct an extensive analysis of time complexity, the impact of key modules, and qualitative results, which demonstrate the effectiveness and efficiency of our proposed method.",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\", “iol\"). In this plan, “R-1\",“C-1\", “C-0\", and “O-3\" are symbols for locations (nodes) in the graph. We compare the performance of translation approaches based on four metrics:",
            "We use the following three evaluation metrics: Phoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences. Word Error Rate (WER) is the percentage of words in which the predicted phoneme sequence does not exactly match the gold standard phoneme sequence. Word Error Rate 100 (WER 100) is the percentage of words in the test set for which the correct guess is not in the first 100 guesses of the system. In system evaluations, WER, WER 100, and PER numbers",
            "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised",
            "Benchmark the evaluation ::: Label-fully-unseen.",
            "Cross-Lingual Evaluation ::: Results and Discussion",
            "models including state-of-the-art neural seq2seq architectures together with perplexity and BLEU scores. We also provide qualitative human performance evaluations for these models and find that automatic evaluation metrics correlate well with human judgments. We will publicly release our corpus containing conversations, API call and argument annotations, and also the human judgments.",
            "During the evaluation, each task is assessed using different metrics based on previous works. For Thai sentence segmentation, three metrics are used in the evaluation: sentence boundary F1 score, non-sentence boundary F1 score, and space correct BIBREF8 . In this work, we mainly focus on the performance of sentence boundary prediction and not non-sentence boundary prediction or space prediction. Therefore, we make comparisons with other models regarding only their sentence boundary F1 scores. The equation for the sentence boundary F1 score metric is shown in f1sb. In calculating the F1 score,",
            "noticeable slowdown using multiple devices. As baselines, we use softmax attention, as well as two recently proposed coverage models: We also experimented combining the strategies above with the sparsemax transformation. As evaluation metrics, we report tokenized BLEU, METEOR ( BIBREF22 , as well as two new metrics that we describe next to account for over and under-translation.",
            "language and the possible disagreements that a task like SBD could provoke.  INLINEFORM0 shows to be correlated with standard SBD metrics, however we want to measure its correlation with extrinsic evaluations techniques like automatic summarization and machine translation."
        ]
    },
    {
        "title": "Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling",
        "answer": "seq2seq, parse2seq, multi-source, CTC, attention, QRNN, CNN, RNN, CRF, LSTM, CNN2-DE, RoBERTa.",
        "evidence": [
            "The sequence-to-sequence (seq2seq) model proposed in BIBREF0 , BIBREF1 , BIBREF2 is a neural architecture for performing sequence classification and later adopted to perform speech recognition in BIBREF3 , BIBREF4 , BIBREF5 . The model allows to integrate the main blocks of ASR such as acoustic model, alignment model and language model into a single framework. The recent ASR advancements in connectionist temporal classification (CTC) BIBREF5 , BIBREF4 and attention BIBREF3 , BIBREF6 based approaches has created larger interest in speech community to use seq2seq models. To leverage performance",
            "to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of the base class, adding a specific head on top of the base model hidden states. Examples of these heads are language modeling or sequence classification heads. These classes follow similar naming pattern: XXXForSequenceClassification or XXXForMaskedLM where XXX is the name of the model and can be used for adaptation (fine-tuning) or pre-training. All models are available both in PyTorch and TensorFlow",
            "systems have significantly longer sequences than the unlexicalized ones. Finally, it is interesting to compare the seq2seq and parse2seq baselines. Parse2seq outperforms seq2seq by only a small amount compared to multi-source; thus, while adding syntax to NMT can be helpful, some ways of doing so are more effective than others.",
            "improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount. Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder. Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
            "in neural models is a less studied question. Improvements over the state of the art might be achieved by integrating lexical information both from an external lexicon and from word vector representations into tagging models. In that regard, further work will be required to understand which class of models perform the best. An option would be to integrate feature-based models such as a CRF with an LSTM-based layer, following recent proposals such as the one proposed by BIBREF45 for named entity recognition.",
            "on coreference chains of entities (Section SECREF2 ), and only the global reading scheme takes advantage of the coreference pattern whereas the local reading breaks the chains. To find out whether coreference pattern is the key to the performance difference, we further ran a probe experiment where we read RST discourse relations in the order in which EDUs are arranged in the RST tree (i.e., left-to-right), and evaluated this model on novel-50 and IMDB62 with the same hyperparameter setting. The F1 scores turned out to be very close to the CNN2-DE (local) model, at 97.5 and 90.9. Based on this",
            "and that a tuba is usually too big to fit in a backpack—is not written down anywhere (unlike, say, factual knowledge, which can be modeled in a knowledge graph). As a result—the reasoning goes—data-driven techniques (even neural models) will be of limited use due to the paucity of relevant corpora. Yet, previous encoder-only architectures like RoBERTa that exploit a language modeling objective (that is, relying only on explicit textual knowledge) can clearly make headway in a commonsense reasoning task, and we can further improve upon these approaches with a sequence-to-sequence model. This",
            "and crowdsourced workers interact to complete the task while the second is \"self-dialog\" in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective",
            "Table 4 shows the results of different question generation models. Our approach is abbreviated as QGNet, which stands for the use of a paraphrasing model plus our question generation model. We can see that QGNet performs better than Seq2Seq in terms of BLEU score because many important words of low frequency from the input are replicated to the target sequence. However, the improvement is not significant for the QA task. We also incorporate each of the four KBs into QGNet, and observe slight improvements on NELL and Reverb. Despite the overall accuracy of QGNet being lower than PCNet and",
            "Hierarchical seq2seq Generation Model",
            "annotation technique is much easier for annotators to learn and simpler to apply. We give several baseline models including state-of-the-art neural seq2seq architectures, provide qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.",
            "have better predictive accuracy than LSTM-based models of equal hidden size, even though they use fewer parameters and run substantially faster. Our experiments show that the speed and accuracy advantages remain consistent across tasks and at both word and character levels. Extensions to both CNNs and RNNs are often directly applicable to the QRNN, while the model's hidden states are more interpretable than those of other recurrent architectures as its channels maintain their independence across timesteps. We believe that QRNNs can serve as a building block for long-sequence tasks that were"
        ]
    },
    {
        "title": "Leveraging Discourse Information Effectively for Authorship Attribution",
        "answer": "RoBERTa BIBREF2",
        "evidence": [
            "State-of-the-art",
            "finding is that the choice of target token appears to have an impact on performance, which is also consistent with the above work. Since using true/false as the target token (conditions #3 and #4) did not improve performance much over conditions with entailment/contradiction, we did not run all data size conditions given our limited computational resources. Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot",
            "How Many Expert Annotations?",
            "Previous Work and Evaluation Data",
            "Original Toulmin's model",
            "Invariance under re-training",
            "Acknowledgements\nWe would like to thank Valerio Basile, Julio Villena-Roman, and Preslav Nakov for kindly give us access to the gold-standards of SENTIPOLC'14, TASS'15 and SemEval 2015 & 2016, respectively. The authors also thank Elio Villaseñor for the helpful discussions in early stages of this research.",
            "F1 mean F1_{mean} vs. WiSeBEWiSeBE",
            "NMT setups and performance",
            "Proposed Methodology",
            "Raw corpus statistics",
            "Bi-Layered L-PCFGs"
        ]
    },
    {
        "title": "Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents",
        "answer": "Accuracy, F1, c@1, AUC, AUC@1, F1, or EER.",
        "evidence": [
            "Evaluation Metrics",
            "Display, along with the challenges of each components, and the proposed solution. We report the empirical evaluation of the proposed methods using real-world datasets for a case study in Section SECREF4 . Finally, Section SECREF5 concludes this paper, and states future work directions.",
            "Emotion labels were not considered as mututally exclusive, and each emotion was assigned a score from 0 to 100. Training/developing data amounted to 250 annotated headlines (Affective development), while systems were evaluated on another 1000 (Affective test). Evaluation was done using two different methods: a fine-grained evaluation using Pearson's r to measure the correlation between the system scores and the gold standard; and a coarse-grained method where each emotion score was converted to a binary label, and precision, recall, and f-score were computed to assess performance. As it is",
            "in general, perform best in predicting the labels of reviews when three supervised scores are additionality utilised. We also employed the convolutional neural network model (CNN). However, the SVM classifier, which is a conventional machine learning algorithm, performed better. We did not include the performances of CNN for embedding types here due to the page limit of the paper. As a qualitative assessment of the word representations, given some query words we visualised the most similar words to those words using the cosine similarity metric. By assessing the similarities between a word",
            "Results ::: Performance (WER) analysis on evaluation data ::: Speech type as a third entangled factor?",
            "Discussion of RQE Results",
            "According to our extensive literature research, numerous measures (e. g., Accuracy, F INLINEFORM0 , c@1, AUC, AUC@1, INLINEFORM1 or EER) have been used so far to assess the performance of AV methods. In regard to our experiments, we decided to use c@1 and AUC for several reasons. First, Accuracy, F INLINEFORM2 and INLINEFORM3 are not applicable in cases where AV methods leave verification problems unanswered, which concerns some of our examined AV approaches. Second, using AUC alone is meaningless for non-optimizable AV methods, as explained in Section UID17 . Third, both have been used in",
            "Results and Discussion\nIn this section, we present the corpus' statistics and quality evaluation regarding SMT and NMT systems, as well as the manual evaluation of sentence alignment.",
            "These metrics estimate the ability of our model to integrate elements from the table in its descriptions. Particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ. To do so, we follow the protocol presented in BIBREF10. First, we apply an information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset. Entity-value pairs are extracted from the descriptions. For example, in the sentence Isaiah Thomas led the team in scoring, totaling 23 points [...]., an IE",
            "Methodology ::: Data presentation",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\", “iol\"). In this plan, “R-1\",“C-1\", “C-0\", and “O-3\" are symbols for locations (nodes) in the graph. We compare the performance of translation approaches based on four metrics:",
            "We focused evaluation over a small but diversified dataset composed by 10 YouTube videos in the English language in the news context. The selected videos cover different topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 minutes. To encourage the diversity of content format we included newscasts, interviews, reports and round tables. During the transcription phase we opted for a manual transcription process because we observed that using transcripts from an ASR system will difficult in a large degree the manual segmentation process. The"
        ]
    },
    {
        "title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled Data",
        "answer": "Hurricane Florence, Hurricane Michael, the East Coast Bomb Cyclone, the California Camp Fires, the Mendocino, California wildfires.",
        "evidence": [
            "The second data batch consists of event-related tweets for five natural disasters occurring in the U.S. in 2018. These are: the East Coast Bomb Cyclone (Jan. 2 - 6); the Mendocino, California wildfires (Jul. 27 - Sept. 18); Hurricane Florence (Aug. 31 - Sept. 19); Hurricane Michael (Oct. 7 - 16); and the California Camp Fires (Nov. 8 - 25). For each disaster, we scraped tweets starting from two weeks prior to the beginning of the event, and continuing through two weeks after the end of the event. Summary statistics on the downloaded event-specific tweets are provided in Table TABREF1 . Note",
            "In 2015, the United Nations Development Programme (UNDP) Regional Hub for Europe and CIS launched a Fragments of Impact Initiative (FoI) that helped to collect qualitative (micro-narratives) and quantitative data from multiple countries. Within a six-month period, around 10,000 interviews were conducted in multiple languages. These covered the perception of the local population in countries including Tajikistan, Yemen, Serbia, Kyrgyzstan and Moldova on peace and reconciliation, local and rural development, value chain, female entrepreneurship and empowerment, and youth unemployment issues.",
            "—were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum.",
            "How Many Expert Annotations?",
            "BIBREF9 analyze Islamic fatwas to determine whether Jihadist clerics write about different topics to those by non-Jihadists. Using an STM model, they uncover fifteen topics within the collection of religious writings. The model successfully identifies characteristic words in each topic that are common within the topic but occur infrequently in the fourteen other topics. The fifteen topics are labelled manually where the labels are human interpretations of the common underlying meaning of the characteristic words within each topic. Some of the topics deal with fighting, excommunication,",
            "rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014.",
            "similar unanswerability factors exist. It is important to note that these questions have previously been filtered, according to a criteria for bad questions defined as “(questions that are) ambiguous, incomprehensible, dependent on clear false presuppositions, opinion-seeking, or not clearly a request for factual information.” Annotators made the decision based on the content of the question without viewing the equivalent Wikipedia page. We randomly sample 100 questions from the development set which were identified as unanswerable, and find that 20% of the questions are not questions (e.g.,",
            "Datasets Used for the RQE Study",
            "Using the features described in the previous subsection, we train four independent classifiers using logistic regression, one per each of the four prediction tasks. All the results are obtained using 5-fold cross-validation experiments. In each fold experiment, we use three folds for training, one fold for development, and one fold for testing. All learning parameters are set to their default values except for the regularization parameter, which we tuned on the development set. In Table TABREF19 the leftmost results column reports F1 score based on majority class prediction. The next section",
            "and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone, respectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition,",
            "Emotion labels were not considered as mututally exclusive, and each emotion was assigned a score from 0 to 100. Training/developing data amounted to 250 annotated headlines (Affective development), while systems were evaluated on another 1000 (Affective test). Evaluation was done using two different methods: a fine-grained evaluation using Pearson's r to measure the correlation between the system scores and the gold standard; and a coarse-grained method where each emotion score was converted to a binary label, and precision, recall, and f-score were computed to assess performance. As it is",
            "and sections due to their lack of popularity. Second, for all entities whether popular or not, certain sections might occur for the first time due to real world developments. As an example, the entity Germanwings did not have an `Accidents' section before this year's disaster, which was the first in the history of the airline. Even if sections are missing for certain entities, similar sections usually occur in other entities of the same class (e.g. other airlines had disasters and therefore their pages have an accidents section). We exploit such homogeneity of section structure and construct"
        ]
    },
    {
        "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding",
        "answer": "two typical KG completion tasks.  NewsQA and TriviaQA datasets.  Lung Cancer'+`Treatment'.  $<$ meal, AtLocation, dinner $>$, $<$ meal, RelatedTo, eat $>$.  $<$ meal, AtLocation, dinner $>$, $<$ meal, RelatedTo, eat $>$.  $<$ meal, AtLocation, dinner $>$, $<$ meal, RelatedTo, eat $>$.  $<$ meal, AtLocation, dinner $>$, $<$ meal, RelatedTo, eat $>$.  $<$ meal, AtLocation, dinner $>$, $<$ meal, RelatedTo, eat $>$.  $<$ meal, AtLocation, dinner $>$, $<$ meal, RelatedTo, eat $>$.  $<$ meal, AtLocation, dinner $>$, $<$ meal, RelatedTo, eat $>$.  $<$ meal, AtLocation, dinner $>$, $<$ meal, RelatedTo, eat $>$.  $<$ meal, AtLocation, dinner $>$, $<$ meal, RelatedTo, eat $>$.  $<$ meal, AtLocation, dinner $>$, $<$ meal, RelatedTo, eat $>$.  $<$ meal",
        "evidence": [
            "vocabulary semantic parser has a strong connection with KB completion. In addition to SFE BIBREF6 , our work draws on work on embedding the entities and relations in a KB BIBREF12 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , as well as work on graph-based methods for reasoning with KBs BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 . Our combination of embedding methods with graph-based methods in this paper is suggestive of how one could combine the two in methods for KB completion. Initial work exploring this direction has already been done by Toutanova and Chen",
            "Experiments\nWe test the network on four classification tasks. We begin by specifying aspects of the implementation and the training of the network. We then report the results of the experiments.",
            "Slot Filling and Intent Keyword Extraction Experiments",
            "can be further improved by applying copy mechanism, as our future work. These errors also indicate that story ending generation is challenging, and logic and implicit knowledge plays a central role in this task.",
            "an LM with additional explicit supervision signals to the evaluation task. They hypothesize that LSTMs do have enough capacity to acquire robust syntactic abilities but the learning signals given by the raw text are weak, and show that multi-task learning with a binary classification task to predict the upcoming verb form (singular or plural) helps models aware of the target syntax (subject-verb agreement). Our experiments basically confirm and strengthen this argument, with even stronger learning signals from negative examples, and we argue this allows to evaluate the true capacity of the",
            "understanding, but they do not explicitly address commonsense knowledge. Two notable exceptions are the NewsQA and TriviaQA datasets. NewsQA BIBREF20 is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection with respect to the method of data acquisition. As for our data collection, full texts were not shown to workers as a basis for question formulation, but only the text's title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background",
            "attention network at a fine neighbor level. Experiments show that LAN outperforms baseline models significantly on two typical KG completion tasks.",
            "for specific entity classes. The approach is trained on already-populated entity pages of a given class (e.g. `Diseases') by learning templates about the entity page structure (e.g. diseases have a treatment section). For a new entity page, first, they extract documents via Web search using the entity title and the section title as a query, for example `Lung Cancer'+`Treatment'. As already discussed in the introduction, this has problems with reproducibility and maintainability. However, their main focus is on identifying the best paragraphs extracted from the collected documents. They rank",
            "Knowledge Graph Representation",
            "data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to ensure they match the original author implementations' performances on various benchmarks.",
            "Knowledge graph embedding aims at modeling entities and relations with low-dimensional vectors. Most previous methods require that all entities should be seen during training, which is unpractical for real-world knowledge graphs with new entities emerging on a daily basis. Recent efforts on this issue suggest training a neighborhood aggregator in conjunction with the conventional entity and relation embeddings, which may help embed new entities inductively via their existing neighbors. However, their neighborhood aggregators neglect the unordered and unequal natures of an entity's neighbors.",
            "the knowledge graph ( $<$ meal, AtLocation, dinner $>$ , $<$ meal, RelatedTo, eat $>$ ). Similarly, eat also has the second largest attention weight to cooking through the knowledge graph. For attention weights of state context vector, both words in perfects everything has the largest weight to some of everything to be just right (everything $\\rightarrow $ everything, perfect $\\rightarrow $ right). The example illustrates how the connection between context clues are built through incremental encoding and use of commonsense knowledge. The chain of context clues, such as"
        ]
    },
    {
        "title": "The First Evaluation of Chinese Human-Computer Dialogue Technology",
        "answer": "F1 score, sentence boundary F1 score, non-sentence boundary F1 score, space correct, PER, WER, WER 100, task completion ratio, user satisfaction degree.",
        "evidence": [
            "Evaluation Metrics",
            "We then carried out a human evaluation to evaluate the generated summaries from another perspective. Our evaluators include 8 graduate students and 4 senior undergraduates, while the dataset is 100 randomly-selected articles from the test set. Each sample in this dataset also includes: 1 reference summary, 5 summaries generated by Open-NMT BIBREF21 , INLINEFORM0 BIBREF11 and BiSET under three settings, respectively, and 3 randomly-selected summaries for trapping. We asked the evaluators to independently rate each summary on a scale of 1 to 5, with respect to its quality in informativity,",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\", “iol\"). In this plan, “R-1\",“C-1\", “C-0\", and “O-3\" are symbols for locations (nodes) in the graph. We compare the performance of translation approaches based on four metrics:",
            "We use the following three evaluation metrics: Phoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences. Word Error Rate (WER) is the percentage of words in which the predicted phoneme sequence does not exactly match the gold standard phoneme sequence. Word Error Rate 100 (WER 100) is the percentage of words in the test set for which the correct guess is not in the first 100 guesses of the system. In system evaluations, WER, WER 100, and PER numbers",
            "to start the dialogue, is the same. The tester then begin to converse to each system. A dialogue is finished if the system successfully returns the information which the user inquires or the number of dialogue turns is larger than 30 for a single task. For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There",
            "Experimental Setup\nBelow, we give details on the evaluation dataset and baselines used for comparison. We also describe the model features and provide implementation details.",
            "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised",
            "Cross-Lingual Evaluation ::: Models in Comparison",
            "Benchmark the evaluation ::: Label-fully-unseen.",
            "During the evaluation, each task is assessed using different metrics based on previous works. For Thai sentence segmentation, three metrics are used in the evaluation: sentence boundary F1 score, non-sentence boundary F1 score, and space correct BIBREF8 . In this work, we mainly focus on the performance of sentence boundary prediction and not non-sentence boundary prediction or space prediction. Therefore, we make comparisons with other models regarding only their sentence boundary F1 scores. The equation for the sentence boundary F1 score metric is shown in f1sb. In calculating the F1 score,",
            "language and the possible disagreements that a task like SBD could provoke.  INLINEFORM0 shows to be correlated with standard SBD metrics, however we want to measure its correlation with extrinsic evaluations techniques like automatic summarization and machine translation.",
            "element-level and the structure level. Evaluations on RotoWire show the effectiveness of our model w.r.t. qualitative and quantitative metrics."
        ]
    },
    {
        "title": "BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization",
        "answer": "Through summaries of specific training articles.",
        "evidence": [
            "all the templates manually since this work requires considerable domain knowledge and is also labor-intensive. Fortunately, the summaries of some specific training articles can provide similar guidance to the summarization as hard templates. Accordingly, these summaries are referred to as soft templates, or templates for simplicity, in this paper. Despite their potential in relieving the verbosity and insufficiency problems of natural language data, templates have not been exploited to full advantage. For example, cao2018retrieve simply concatenated template encoding after the source article",
            "Hidden Markov models",
            "we also implemented BiSET with two other types of templates: randomly-selected templates and best templates identified by Fast Rank under different ranges. As shown in Table TABREF47 , the performance of our model improves constantly with the improvement of template quality (larger ranges lead to better chances for good templates). Even with randomly-selected templates, our model still works with stable performance, demonstrating its robustness.",
            "algorithm with the extracted features and class labels. During validation, the system extracts the features of the question and passes it into the model created during training and predicts the answer type.",
            "In total, we created (and annotated) 223 contracts by hand. This corpus was further split into training, validation, and test data with a 6:2:2 split. Our test set contains 44 lease agreements, 11 of which use templates that are not seen in the training set. We report evaluation over both the full test set and on only these unseen templates; the latter condition specifically probes our model's ability to generalize.",
            "What Does Zero-shot Transfer Model Learn? ::: Code-switching Dataset",
            "Training Dataset",
            "Data-driven models for morphological analysis are constructed using training data INLINEFORM0 consisting of INLINEFORM1 training examples. The baseline model BIBREF5 we compare with regards the output space of the model as a subset INLINEFORM2 where INLINEFORM3 is the set of all tag sets seen in this training data. Specifically, they solve the task as a multi-class classification problem where the classes are individual tag sets. In low-resource scenarios, this indicates that INLINEFORM4 and even for those tag sets existing in INLINEFORM5 we may have seen very few training examples. The",
            "all text documents (or interview data from each respondent) prior to analyzing them. Rather, a sufficiently large set of documents can be labelled to train an algorithm and then used to classify the remaining documents. In unsupervised learning, the outcome variable is unknown. The exercise is, therefore, of a more exploratory nature. Here the purpose is to reveal patterns in the data that allow us to distinguish distinct groups whose differences are small, while variations across groups are large. In a set of text documents, we may be interested in the main topics about which respondents are",
            "to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data.",
            "images and coherent text samples [5]. The framework that GANs use for generating new data points employs an end-to-end neural network comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being directly exposed to the real samples. Instead, this model learns to reproduce the general patterns of the input via its interaction with the discriminator. The role of the discriminator, in turn, is to tell apart which data points are ‘real’ and which have been created by the generator. On each run",
            "data and word types, is needed to confirm such patterns in general. Improvements in unseen word embeddings from the classifier embedding space to the Siamese embedding space (such as for democracy, morning, and basketball) are a likely result of optimizing the model for relative distances between words."
        ]
    },
    {
        "title": "AttSum: Joint Learning of Focusing and Summarization with Neural Attention",
        "answer": "BackTrans, Unpaired, CrossAlign, CPTG, BiDAF, KVMemNet.",
        "evidence": [
            "Models for Comparison",
            "models with customizable options including warmup schedules which are relevant when training with Adam.",
            "GLUE and SuperGLUE",
            "We compare our model with the following state-of-art generative models: BackTrans BIBREF7 : In BIBREF7 , authors propose a model using machine translation in order to preserve the meaning of the sentence while reducing stylistic properties. Unpaired BIBREF10 : In BIBREF10 , researchers implement a method to remove emotional words and add desired sentiment controlled by reinforcement learning. CrossAlign BIBREF6 : In BIBREF6 , authors leverage refined alignment of latent representations to perform style transfer and a cross-aligned auto-encoder is implemented. CPTG BIBREF24 : An interpolated",
            "and even degrades the performance. We further explore what does the model learn in zero-shot setting.",
            "What is the Model Learning?",
            "We compare it with a standard MRC model BiDAF, and also provide the difficulty of the dataset and lay out remaining challenges.",
            "QA Model 2: KVMemNet",
            "to parallel RNN. These models are similar to ours in that they incorporate linearized parses into NMT; here, we utilize a multi-source framework.",
            "model but both heads are different. Our conflict head explicitly captures difference between the inputs.",
            "Results and Discussion",
            "paraphrasing model. The first dataset is from Quora dataset, which is built for detecting whether or not a pair of questions are semantically equivalent. 345,989 positive question pairs and 255,027 negative pairs are included in the first dataset. The second dataset includes web queries from query logs, which are obtained by clustering the web queries that click the same web page. In this way we obtain 6,118,023 positive query pairs. We implement a heuristic rule to get 6,118,023 negative instances for the query dataset. For each pair of query text, we clamp the first query and retrieve a"
        ]
    },
    {
        "title": "A Simple Approach to Multilingual Polarity Classification in Twitter",
        "answer": "Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast Rerank, and BiSET.  configuration, model and tokenization classes.  Retrieve, Fast R",
        "evidence": [
            "Framework",
            "through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "there are segmenters in several languages. However, each piece depends on sofisticated linguistic resources, which complicates the reproduction of the experiments in other languages. Consequently, the development of multilingual systems using discursive analysis are yet to be developed. Diverse applications based on the latest technologies require at least one of the three steps mentioned above BIBREF1, BIBREF2, BIBREF3. In this context, the idea of exploring the architecture of a generic system that is able not only of segmenting a text correctly but also of adapting it to any language, was",
            "Identification of argument components",
            "Conclusion and Future Work\nIn this work, we proposed a novel framework for sequence tagging that combines neural networks and graphical models, and showed its effectiveness on the task of morphological tagging. We believe this framework can be extended to other sequence labeling tasks in NLP such as semantic role labeling. Due to the robustness of the model across languages, we believe it can also be scaled to perform morphological tagging for multiple languages together.",
            "Working with Multiple Languages",
            "Figure FIGREF17 shows the performance on four contests, corresponding to three different languages. The performance corresponds to the multilingual set of features, i.e., we do not used language-dependent techniques. Figures UID18 - UID21 illustrates the results on each challenge, all competitors are ordered in score's descending order (higher is better). The achieved performance of our approach is marked with a horizontal line on each figure. Figure UID22 briefly describes each challenge and summarizes our performance on each contest; also, we added three standard measures to simplify the",
            "The Framework\nOur framework includes three key modules: Retrieve, Fast Rerank, and BiSET. For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates. Finally, BiSET mutually selects important information from the source article and the template to generate an enhanced article representation for summarization.",
            "(by number of speakers of the language) for maximum impact of the resource, while simultaneously having a diverse suite of languages based on their typological features (such as morphological type and language family). Table TABREF10 summarizes key information about the languages currently included in Multi-SimLex. We have included a mixture of fusional, agglutinative, isolating, and introflexive languages that come from eight different language families. This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and",
            "Transformers has been designed around a unified frontend for all the models: parameters and configurations, tokenization, and model inference. These steps reflect the recurring questions that arise when building an NLP pipeline: defining the model architecture, processing the text data and finally, training the model and performing inference in production. In the following section, we'll give an overview of the three base components of the library: configuration, model and tokenization classes. All of the components are compatible with PyTorch and TensorFlow (starting 2.0). For complete",
            "Framework for Processing Portuguese Text ::: Lexicon Matching",
            "Recently, sentiment analysis has received a lot of attention due to the interest in mining opinions of social media users. Sentiment analysis consists in determining the polarity of a given text, i.e., its degree of positiveness or negativeness. Traditionally, Sentiment Analysis algorithms have been tailored to a specific language given the complexity of having a number of lexical variations and errors introduced by the people generating content. In this contribution, our aim is to provide a simple to implement and easy to use multilingual framework, that can serve as a baseline for sentiment"
        ]
    },
    {
        "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing",
        "answer": "1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN, and baseline decoder. Also, parse2seq and baseline agent.  Additionally, dictionary-based approaches and baseline agent with training strategy and action generation.  Furthermore, baseline decoder with beam search decoder and parse2seq with linearized parses.  Also, baseline agent with parse2seq and baseline decoder with parse2seq.  Lastly, baseline agent with parse2seq and baseline decoder with beam search decoder.  Also, baseline agent with parse2seq and baseline decoder with beam search decoder and parse2seq with linearized parses.  Furthermore, baseline agent with parse2seq and baseline decoder with beam search decoder and parse2seq with linearized parses and baseline decoder with beam search decoder and parse2seq with linearized parses.  Lastly, baseline agent with parse2seq and baseline decoder with beam search decoder and parse2seq with linearized parses and baseline decoder with beam search decoder and parse2seq with linearized parses.  Also, baseline agent with parse2seq and baseline decoder with beam search decoder and parse2seq with",
        "evidence": [
            "Baseline",
            "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN",
            "Preliminaries and Framework",
            "Experimental Setup ::: Answerability Identification Baselines",
            "latter is much preferable for the overall performance. This might explain that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "the gold reference. The average of the maximum F1 across all reference answers is computed as the human baseline.",
            "Baseline Decoder\nOur baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations:",
            "Dictionary-based approaches",
            "contained in both a formal KB and a large corpus, and (3) is not limited to the schema of the underlying KB. We demonstrate significantly improved performance over state-of-the-art baselines on an open-domain natural language question answering task.",
            "Baseline Agent ::: Training Strategy ::: Action Generation",
            "side, the linearized parses are used. We allow the system to attend equally to words and node labels on the source side, rather than restricting the attention to words. We refer to this baseline as parse2seq.",
            "directly inferred from the tweets. After we retrieve the QA pairs from all HITs, we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script to download the original tweets and detailed"
        ]
    },
    {
        "title": "Story Ending Generation with Incremental Encoding and Commonsense Knowledge",
        "answer": "GraphParser, Moses, SA relevance, TFIDF, uniform weighting, vMF-VAE, BT from Moses, BT from NMT.  GraphParser, Moses, SA relevance, TFIDF, uniform weighting, vMF-VAE, BT from Moses, BT from NMT.  GraphParser, Moses, SA relevance, TFIDF, uniform weighting, vMF-VAE, BT from Moses, BT from NMT.  GraphParser, Moses, SA relevance, TFIDF, uniform weighting, vMF-VAE, BT from Moses, BT from NMT.  GraphParser, Moses, SA relevance, TFIDF, uniform weighting, vMF-VAE, BT from Moses, BT from NMT.  GraphParser, Moses, SA relevance, TFIDF, uniform weighting, vMF-VAE, BT from Moses, BT from NMT.  GraphParser, Moses, SA relevance, TFIDF, uniform weighting, vMF-VAE, BT from Moses, BT from NMT.  GraphParser, Moses, SA relevance, TFIDF, uniform weighting, vMF-VAE, BT from Moses, BT from NMT.  GraphParser, Moses, SA relevance,",
        "evidence": [
            "Standard and baseline methods",
            "Baselines\nWe use GraphParser without paraphrases as our baseline. This gives an idea about the impact of using paraphrases. We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation BIBREF24 , BIBREF36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions.",
            "Dataset Analysis ::: Baseline Experiments: Argument Prediction",
            "Datasets Used for the RQE Study",
            "the gold reference. The average of the maximum F1 across all reference answers is computed as the human baseline.",
            "following baselines: SA relevance, TFIDF and uniform weighting (see section \"Baseline Methods\" ). The two-dimensional PCA projection of the summary vectors obtained via the CNN2 resp. the SVM model, as well as the corresponding TFIDF/uniform weighting baselines are shown in Figure 3 . In these visualizations we group the 20Newsgroups test documents into six top-level categories (the grouping is performed according to the dataset website), and we color each document according to its true category (note however that, as mentioned earlier, the relevance decomposition is always performed in an",
            "Experiments\nTo show the challenge of TweetQA for existing approaches, we consider four representative methods as baselines. For data processing, we first remove the URLs in the tweets and then tokenize the QA pairs and tweets using NLTK. This process is consistent for all baselines.",
            "the baselines. We report negative log likelihood (NLL), KL loss, and perplexity (PPL) on the test set. As expected, all the models have a higher KL loss in the inputless setting than the standard setting, as $\\mathbf {z}$ is required to encode more information about the input data for reconstruction. In terms of overall performance, our model outperforms all the baselines in both datasets (i.e., PTB and E2E). For instance, when comparing with the strongest baseline vMF-VAE in the standard setting, our model reduces NLL from 96 to 79 and PPL from 98 to 43 in PTB, respectively. In the inputless",
            "Baseline Agent ::: Training Strategy ::: Action Generation",
            "latter is much preferable for the overall performance. This might explain that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "better) and characTER BIBREF15 (smaller is better). As they are trained from much smaller amounts of data than current systems, these baselines are not quite competitive to today's best system, but still represent serious baselines for these datasets. Given our setups, fine-tuning with in-domain natural data improves BLEU by almost 4 points for both translation directions on in-domain tests; it also improves, albeit by a smaller margin, the BLEU score of the out-of-domain tests.",
            "Algorithms Used"
        ]
    },
    {
        "title": "Multi-Source Syntactic Neural Machine Translation",
        "answer": "10% to 20% on the BLEU score.",
        "evidence": [
            "syntactic model is able to translate successfully without any parsed input, unlike standard parsed methods. In addition, performance does not deteriorate as much on long sentences as for the baselines.",
            "directly inferred from the tweets. After we retrieve the QA pairs from all HITs, we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script to download the original tweets and detailed",
            "number of missing tokens artificially by randomly removing words from the word embeddings matrix (original vocab size: 400K), we observe that while the overall performance degrades, the modality attention module is able to suppress the peformance degradation. Note also that the performance gap generally gets bigger as we decrease the vocabulary size of the word embeddings matrix. This result is significant in that the modality attention is able to improve the model more robust to missing tokens without having to train an indefinitely large word embeddings matrix for arbitrarily noisy social",
            "data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to ensure they match the original author implementations' performances on various benchmarks.",
            "and even degrades the performance. We further explore what does the model learn in zero-shot setting.",
            "as input to both encoders, while seq + null uses null input for the parsed encoder, where every source sentence is “( )”. The parse2seq system fails when given only sequential source data. On the other hand, both multi-source systems perform reasonably well without parsed data, although the BLEU scores are worse than multi-source with parsed data.",
            "Inference Without Parsed Sentences",
            "Table II shows the accuracy and F1 score for different classifiers with and without eliminating stop words while extracting features. Figure FIGREF21 shows the average results of different classifiers in a bar chart with and without eliminating stop words from the questions. Overall, SGD has shown the best performance on our dataset as it introduces non-linearity and uses back-propagation for updating parameter weights using loss function calculated on training set into classification. K-NN has shown the weakest performance overall, as this algorithm has a bad reputation of not working well",
            "vectors are similarly computed by attending to the last input sentence $X_K$ . There is no need to attend to all the context sentences because the context clues have been propagated within the incremental encoding scheme.",
            "INLINEFORM4 of PPA and APA). While all the proposed models beat the baseline on every course except casebased-2. On medicalneuro-2 and compilers-4 which have the lowest i.ratio among the 12 courses none of the neural models better the reported baseline BIBREF7 (course level not scores not shown in this paper). The effect is pronounced in compilers-4 course where none of the neural models were able to predict any intervened threads. This is due to the inherent weakness of standard neural models, which are unable to learn features well enough when faced with sparse data. The best performance of",
            "are simply initialized to zero vectors. As input normalization, we subtract the mean and divide by the standard deviation obtained over the flattened training data. We train the neural network by minimizing the cross-entropy loss via mini-batch stochastic gradient descent using $l_2$ -norm and dropout as regularization. We tune the ML model hyperparameters by 10-fold cross-validation in case of the SVM, and by employing 1000 random documents as fixed validation set for the CNN model. However, for the CNN hyperparameters we did not perform an extensive grid search and stopped the tuning once",
            "development set, namely all the features plus Google-based embeddings, but excluding the lexicon. This makes our approach completely independent of any manual annotation or handcrafted resource. Our model's performance is compared to the following systems, for which results are reported in the referred literature. Please note that no other existing model was re-implemented, and results are those reported in the respective papers."
        ]
    },
    {
        "title": "Domain Adaptation for Neural Networks by Parameter Augmentation",
        "answer": "1750",
        "evidence": [
            "setting are presented in Table TABREF9 . We considered all possible combinations of source and target domains in this experiment, and we improve the results in each and every case. Without exception, the accuracy rates reached by the transductive string kernels are significantly better than the best baseline string kernel BIBREF10 , according to the McNemar's test performed at a confidence level of INLINEFORM0 . The highest improvements (above INLINEFORM1 ) are obtained when the source domain contains Books reviews and the target domain contains Kitchen reviews. As in the multi-source",
            "Examples and Attention Visualization",
            "How Many Expert Annotations?",
            "Domain Adaptation Approach",
            "We perform two kinds of ablation. For token-level ablation (-Token), we avoid creating negative examples for all verbs that appear as a target verb in the test set. Another is construction-level (-Pattern), by removing all negative examples occurring in a particular syntactic pattern. We ablate a single construction at a time for -Pattern, from four non-local subject-verb dependencies (across a prepositional phrase (PP), subject RC, object RC, and long verb phrase (VP)). We hypothesize that models are less affected by token-level ablation, as knowledge transfer across words appearing in",
            "as the corresponding ground truth target output. In other words, a token representing each class is directly used as the prediction target.",
            "within the privacy policy. Motivated by this need, we contribute PrivacyQA, a corpus consisting of 1750 questions about the contents of privacy policies, paired with over 3500 expert annotations. The goal of this effort is to kickstart the development of question-answering methods for this domain, to address the (unrealistic) expectation that a large population should be reading many policies per day. In doing so, we identify several understudied challenges to our ability to answer these questions, with broad implications for systems seeking to serve users' information-seeking intent. By",
            "data set. Second, we deal with in-domain ten-fold cross validation for each of the six domains. Third, in order to evaluate the domain portability of our approach, we train the system on five domains and test on the remaining one for all six domains (which we report as cross-domain validation).",
            "The natural language template approach enables various options to formulate the WinoGrande commonsense reasoning task as a text-to-text problem with T5. Here we adopt a formulation similar to the MNLI template. Consider a concrete example: He never comes to my home, but I always go to his house because the _ is smaller. Option1: home; Option2: house In this case, the correct replacement for _ is Option1. We decompose the above problem into two source–target training examples, where _ is replaced with each option and annotated with the correct answer as the target token, as shown in Table",
            "and are of interest to us in segmenting the videos. Each segment can be seen as a subtask within a larger video dictating an example. We thus chose these videos because of the amount of procedural information stored in each video for which the user may ask. Though there is only one domain, each video corresponds to a different overall goal.",
            "“Italian architect Andrea Palladio” is considered a positive training example for the predicate instances $\\textit {architect}(\\textsc {Palladio})$ and $\\textit {architect\\_N/N}(\\textsc {Italy}, \\textsc {Palladio})$ . We add the feature vectors for $\\textsc {Palladio}$ and ( $\\textsc {Italy}$ , $\\textsc {Palladio}$ ) to the feature counts for the predicates $\\textit {architect}$ and $\\textit {architect\\_N/N}$ , respectively. This gives a set of counts $\\textsc {count}$ ( $\\pi $ ), $\\textsc {count}$ ( $\\textit {architect\\_N/N}(\\textsc {Italy}, \\textsc {Palladio})$0 ), and $\\textit",
            "A trivial method of domain adaptation is simply ignoring the source dataset, and train the model using only the target dataset. This method is hereafter denoted by TgtOnly. This is a baseline and any meaningful method must beat it. Another trivial method is SrcOnly, where only the source dataset is used for the training. Typically, the source dataset is bigger than that of the target, and this method sometimes works better than TgtOnly. Another method is All, in which the source and target datasets are combined and used for the training. Although this method uses all the data, the training"
        ]
    },
    {
        "title": "Interactive Machine Comprehension with Information Seeking Agents",
        "answer": "Set/Change Destination/Route, Set/Change Driving Behavior/Speed, Finishing the Trip Use-cases, and Others.",
        "evidence": [
            "Model Configuration and Training",
            "single word with the query command. However, a host of other options should be considered in future work. For example, multi-word queries with fuzzy matching are more realistic. It would also be interesting for an agent to generate a vector representation of the query in some latent space. This vector could then be compared with precomputed document representations (e.g., in an open domain QA dataset) to determine what text to observe next, with such behavior tantamount to learning to do IR. As mentioned, our idea for reformulating existing MRC datasets as partially observable and interactive",
            "Experimental Setup ::: Privacy Question Answering ::: Baselines",
            "their pretraining step). Tokenizers can be instantiated from existing configurations available through Transformers originating from the pretrained models or created more generally by the user from user-specifications. Model - All models follow the same hierarchy of abstraction: a base class implements the model's computation graph from encoding (projection on the embedding matrix) through the series of self-attention layers and up to the last layer hidden states. The base class is specific to each model and closely follows the original implementation, allowing users to dissect the inner",
            "support various natural commands for interacting with the vehicle: (1) Set/Change Destination/Route (including turn-by-turn instructions), (2) Set/Change Driving Behavior/Speed, (3) Finishing the Trip Use-cases, and (4) Others (open/close door/window/trunk, turn music/radio on/off, change AC/temperature, show map, etc.). According to those scenarios, 10 types of passenger intents are identified and annotated as follows: SetDestination, SetRoute, GoFaster, GoSlower, Stop, Park, PullOver, DropOff, OpenDoor, and Other. For slot filling task, relevant slots are identified and annotated as:",
            "Do models generalize explicit supervision, or just memorize it?",
            "social computing and information retrieval for their support on the data annotation, establishing the system testing environment and the communication to the participants and help connect their systems to the testing environment.",
            "under the behavioral navigation framework. Inspired by the information retrieval and question answering literature BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , we propose to leverage the behavioral graph as a knowledge base to facilitate the interpretation of navigation commands. More specifically, the proposed model takes as input user directions in text form, the behavioral graph of the environment encoded as INLINEFORM0 node; edge; node INLINEFORM1 triplets, and the initial location of the robot in the graph. The model then predicts a set of behaviors to reach the desired destination according",
            "this unrealistic setup to observe the training over noisy and hardly informative source sentences. We then use the procedures described in § SECREF4 , except that the pseudo-source embeddings in the copy-marked setup are pretrained for three epochs on the in-domain data, while all remaining parameters are frozen. This prevents random parameters from hurting the already trained model.",
            "on the loss on the development set. After the training we generate captions by beam search, where the size of the beam is 5. These settings are the same in the latter experiments. We compare the proposed method with other baseline methods. For all the methods, we use Adam with the same hyper parameters. In FineTune, we did not freeze any parameters during the target training. In Dual, all samples in source and target datasets are weighted equally. We evaluated the performance of the domain adaptation methods by the qualities of the generated captions. We used BLEU, METOR and CIDEr scores for",
            "not exploit negative examples explicitly; essentially a model is only informed of a key position (target word) that determines the grammaticality. This is rather an indirect learning signal, and we expect that it does not outperform the other approaches.",
            "Models"
        ]
    },
    {
        "title": "Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models",
        "answer": "HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model.",
        "evidence": [
            "combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text.",
            "significantly better than HLSTM+MSA in logicality (1.26 vs. 1.06, p-value= $0.014$ for GA; 1.24 vs. 1.02, p-value= $0.022$ for CA). This indicates that incremental encoding is more powerful than traditional (Seq2Seq) and hierarchical (HLSTM) encoding/attention in utilizing context clues. Furthermore, using commonsense knowledge leads to significant improvements in logicality. The comparison in logicality between IE+MSA and IE (1.26/1.24 vs. 1.10, p-value= $0.028/0.042$ for GA/CA, respectively), HLSTM+MSA and HLSTM (1.06/1.02 vs. 0.84, p-value $<0.001/0.001$ for GA/CA, respectively), and",
            "data augmentation approaches have shown their effectiveness on various NMT systems, especially in under-resourced scenarios. While back translation technique is used to generate synthetic data from monolingual corpora, mix-source technique utilizes human-quality corpora in a multilingual setting, leveraging the transfer learning ability across languages. Both are simple but elegantly model the relevant noise needed in training neural architectures in such low-resourced situations. The main contributions of this paper are:",
            "Conclusion\nWe have shown that by exploiting negative examples explicitly, the syntactic abilities of LSTM-LMs greatly improve, demonstrating a new capacity of handling syntax robustly. Given a success of our approach using negative examples, and our final analysis for transferability, which indicates that the negative examples do not have to be complete, one interesting future direction is to extend our approach to automatically inducing negative examples themselves in some way, possibly with orthographic and/or distributional indicators or others.",
            "As pointed out by the reviewers the success of the multichannel approach is likely due to a combination of several quite different effects. First, there is the effect of the embedding learning algorithm. These algorithms differ in many aspects, including in sensitivity to word order (e.g., SENNA: yes, word2vec: no), in objective function and in their treatment of ambiguity (explicitly modeled only by huang2012improving. Second, there is the effect of the corpus. We would expect the size and genre of the corpus to have a big effect even though we did not analyze this effect in this paper.",
            "In an ASR system, a language model (LM) takes an important role by incorporating external knowledge into the system. Conventional ASR systems combine an LM with an acoustic model by FST giving a huge performance gain. This trend is also shown in general including hybrid ASR systems and neural network-based sequence-to-sequence ASR systems. The following experiments show a benefit of using a language model in decoding with the previous stage-2 transferred models. Although the performance gains in %CER are also generally observed over all target languages, the improvement in %WER was more",
            "a sentence span, extending the original uni-directional LSTM-Minus to the bi-directional case. More recently, inspired by the success of LSTM-Minus in both dependency and constituency parsing, lstm-minusdiscourse extend the technique to discourse parsing. They propose a two-stage model consisting of an intra-sentential parser and a multi-sentential parser, learning contextually informed representations of constituents with LSTM-Minus, at the sentence and document level, respectively. Similarly, in this paper, when deciding if a sentence should be included in the summary, the local context of",
            "which learns optimal integration of different modes of correlated information. In essence, the modality attention learns to attenuate irrelevant or uninformative modal information while amplifying the primary modality to extract better overall representations. We showed that the modality attention based model outperforms other state-of-the-art baselines when text was the only modality available, by better combining word and character level information.",
            "of our models with the scores obtained by the CRF-based system MarMoT BIBREF22 , BIBREF18 , retrained on the same corpora and the same external morphosyntactic lexicons. We also compare our results to those obtained by the best bidirectional LSTM models described by BIBREF20 , which both make use of Polyglot word vector representations published by BIBREF23 . We will show that an optimised enrichment of feature-based models with morphosyntactic lexicon results in significant accuracy gains. The macro-averaged accuracy of our enriched MElt models is above that of enriched MarMoT models and",
            "to model relationships between posts. For example, Hasan and Ng use hidden Markov models (HMMs) to model dependent relationships to the preceding post BIBREF9 ; Burfoot et al. use iterative classification to repeatedly generate new estimates based on the current state of knowledge BIBREF11 ; Sridhar et al. use probabilistic soft logic (PSL) to model reply links via collaborative filtering BIBREF12 . In the Facebook dataset we study, we use comments instead of reply links. However, as the ultimate goal in this paper is predicting not comment stance but post stance, we treat comments as extra",
            "the sentences, hence it makes sense that the inference LSTM is more focused on the critical differences between the sentences. This is also observed for the Neutral example as well. It is worth noting that, while revealing similar general trends, the backward LSTM can sometimes focus on different parts of the sentence (e.g., see Fig. 11 of Appendix), suggesting the forward and backward readings provide complementary understanding of the sentence.",
            "To boost performance, the SVM, LSTM, and feed-forward models were combined into an ensemble. For both the LSTM and feed-forward approach, three different models were trained. The first model was trained on the training data (regular), the second model was trained on both the training and translated training data (translated) and the third one was trained on both the training data and the semi-supervised data (silver). Due to the nature of the SVM algorithm, semi-supervised learning does not help, so only the regular and translated model were trained in this case. This results in 8 different"
        ]
    },
    {
        "title": "BERT-Based Arabic Social Media Author Profiling",
        "answer": "1,100 users with gender tags, 162,829 tweets, EXTENDED_Gender, 20,000 tweets for each class from an in-house dataset gold.",
        "evidence": [
            "In-domain and out-of-domain data",
            "data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to ensure they match the original author implementations' performances on various benchmarks.",
            "located in major English speaking countries (i.e. Canada, US, and UK) and have an acceptance rate larger than INLINEFORM0 . Since we use tweets as context, lots of important information are contained in hashtags or even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions. To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require",
            "Training Dataset",
            "Implementation Details",
            "Raw corpus statistics",
            "Data",
            "Methodology ::: Data presentation",
            "To further improve the performance of our models, we introduce in-house labeled data that we use to fine-tune BERT. For the gender classification task, we manually label an in-house dataset of 1,100 users with gender tags, including 550 female users, 550 male users. We obtain 162,829 tweets by crawling the 1,100 users' timelines. We combine this new gender dataset with the gender TRAIN data (from shared task) to obtain an extended dataset, to which we refer as EXTENDED_Gender. For the dialect identification task, we randomly sample 20,000 tweets for each class from an in-house dataset gold",
            "Models Used in the Evaluation",
            "Dictionary-based approaches",
            "Data acquisition"
        ]
    },
    {
        "title": "WiSeBE: Window-based Sentence Boundary Evaluation",
        "answer": "S1 and S2.",
        "evidence": [
            "To exemplify the INLINEFORM0 score we evaluated and compared the performance of two different SBD systems over a set of YouTube videos in a multi-reference enviroment. The first system (S1) employs a Convolutional Neural Network to determine if the middle word of a sliding window corresponds to a SU boundary or not BIBREF30 . The second approach (S2) by contrast, introduces a bidirectional Recurrent Neural Network model with attention mechanism for boundary detection BIBREF31 . In a first glance we performed the evaluation of the systems against each one of the references independently. Then,",
            "—were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum.",
            "Qualitative Comparison",
            "before and were also not familiar with the packaged products including the chosen 20 products; hence, they were \"illiterate\" in terms of comprehending English and in terms of having used any of the products although they might be literate in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this",
            "Implementation of the System ::: Results and Discussion",
            "compared to the BOW. Furthermore, while the ideal points clearly reflect a lessening of Cold War tensions, the RWMD similarities appear to detect greater intra-bloc position variations. Therefore, both appear to provide different and potentially valuable information for the estimation of state preferences.",
            "which involve a fixed decision criterion (for example, NNCD). Additionally, we mentioned that determinism must be fulfilled such that an AV method can be rated as reliable. Moreover, we clarified a number of misunderstandings in previous research works and proposed three clear criteria that allow to classify the model category of an AV method, which in turn influences its design and the way how it should be evaluated. In regard to binary-extrinsic AV approaches, we explained which challenges exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV",
            "In general, distributed word representations are evaluated using a word similarity task. For instance, WordSim353 2002:PSC:503104.503110, MC BIBREF2 , RG BIBREF3 , and SCWS Huang:2012:IWR:2390524.2390645 have been used to evaluate word similarities in English. Moreover, baker-reichart-korhonen:2014:EMNLP2014 built a verb similarity dataset (VSD) based on WordSim353 because there was no dataset of verbs in the word-similarity task. Recently, SimVerb-3500 was introduced to evaluate human understanding of verb meaning Gerz:2016:EMNLP. It provides human ratings for the similarity of 3,500 verb",
            "Drugs, among 12 sub-collections overall. Accordingly, the assessors noticed that the performance of the QA systems was better on questions about diseases than on questions about drugs, which suggests a need for extending our medical QA collection with more information about drugs and associated question types. We also looked closely at the private websites used by the LiveQA-Med annotators to provide some of the reference answers for the test questions. For instance, the ConsumerLab website was useful to answer a question about the ingredients of a Drug (COENZYME Q10). Similarly, the",
            "rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014.",
            "Dictionary-based approaches",
            "when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells"
        ]
    },
    {
        "title": "Recognizing Musical Entities in User-generated Content",
        "answer": "SVM, SGD, Naive Bayes, Kernel Naive Bayes, Decision Tree, Rule Induction, BERT, T5.",
        "evidence": [
            "Algorithms Used",
            "of information theory. In 2015, Bagnall BIBREF3 introduced the first deep learning approach that makes use of language modeling, an important key concept in statistical natural language processing. In 2017, Castañeda and Calvo BIBREF4 proposed an AV method that applies a semantic space model through Latent Dirichlet Allocation, a generative statistical model used in information retrieval and computational linguistics. Despite the increasing number of AV approaches, a closer look at the respective studies reveals that only minor attention is paid to their underlying characteristics such as",
            "What is the Model Learning?",
            "They used a convolutional neural network (CNN) model to classify a question into a restricted set of 10 question types and crawled \"relevant\" online web pages to find the answers. However, the results were lower than those achieved by the systems relying on finding similar answered questions. These results support the relevance of similar question matching for the end-to-end QA task as a new way of approaching QA instead of the classical QA approaches based on Question Analysis and Answer Retrieval.",
            "classifier method in order to build more elaborated systems. Besides the text-transformations, the proposed framework uses a SVM classifier (with linear kernel), and, hyper-parameter optimization using random search and H+M over the space of text-transformations. The experimental results show good overall performance in all international contests considered, and the best results in the other five languages tested. It is important to note that all the methods that outperformed B4MSA in the sentiment analysis contests use extra knowledge (lexicons included) meanwhile B4MSA uses only the",
            "—were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum.",
            "Collectively, the success of large pretrained neural models, both encoder-only BERT-like architectures as well as encoder–decoder architectures like T5, raise interesting questions for the pursuit of commonsense reasoning. Researchers have discovered that previous models perform well on benchmark datasets because they pick up on incidental biases in the dataset that have nothing to do with the task; in contrast, the WinoGrande dataset has devoted considerable effort to reducing such biases, which may allow models to (inadvertently) “cheat” (for example, using simple statistical associations).",
            "Many classifiers have been used in machine learning for QC such as Support Vector Machine (SVM) BIBREF16 BIBREF17, Support Vector Machines and Maximum Entropy Model BIBREF18, Naive Bayes (NB), Kernel Naive Bayes (KNB), Decision Tree (DT) and Rule Induction (RI) BIBREF13. In BIBREF0, they claimed to achieve average precision of 0.95562 for coarse class and 0.87646 for finer class using Stochastic Gradient Descent (SGD).",
            "Recently, the transformative potential of machine learning (ML) has propelled ML into the forefront of mainstream media. In Brazil, the use of such technique has been widely diffused gaining more space. Thus, it is used to search for patterns, regularities or even concepts expressed in data sets BIBREF0 , and can be applied as a form of aid in several areas of everyday life. Among the different definitions, ML can be seen as the ability to improve performance in accomplishing a task through the experience BIBREF1 . Thus, BIBREF2 presents this as a method of inferences of functions or",
            "All the models follow the same philosophy of abstraction enabling a unified API in the library. Configuration - A configuration class instance (usually inheriting from a base class `PretrainedConfig`) stores the model and tokenizer parameters (such as the vocabulary size, the hidden dimensions, dropout rate, etc.). This configuration object can be saved and loaded for reproducibility or simply modified for architecture search. The configuration defines the architecture of the model but also architecture optimizations like the heads to prune. Configurations are agnostic to the deep learning",
            "actors identified. We can classify documents based on human coding of a subset of documents and establish which topics predict/correlate with predefined outcomes such as successful employment or completion of a program. While the application used here to illustrate the discussion focuses on texts in the form of open-ended questions, social networks can be used and their coverage and topicality can be leveraged. Natural language processing has the potential to unlock large quantities of untapped knowledge that could enhance our understanding of micro-level processes and enable us to make",
            "Experimenting with Transformers ::: Language understanding benchmarks"
        ]
    },
    {
        "title": "Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER",
        "answer": "text classification",
        "evidence": [
            "For many of the languages examined, we were able to improve on BERT's zero-resource cross-lingual performance on the MLDoc classification and CoNLL NER tasks. Language-adversarial training was generally effective, though the size of the effect appears to depend on the task. We observed that adversarial training moves the embeddings of English text and their non-English translations closer together, which may explain why it improves cross-lingual performance. Future directions include adding the language-adversarial task during BERT pre-training on the multilingual Wikipedia corpus, which may",
            "Text Classification",
            "Experiments on Additional Losses ::: Which additional loss works better?",
            "correct classification rate (CCR). For sentiment analysis, we have a three-class problem (positive, negative, and neutral), where the classes are mutually exclusive. The CCR, averaged for a set of tweets, is defined to be the number of correctly-predicted sentiments over the number of groundtruth sentiments in these tweets. For NER, we consider that each tweet may reference up to four candidates, i.e., targeted entities. The CCR, averaged for a set of tweets, is the number of correctly predicted entities (candidates) over the number of groundtruth entities (candidates) in this set.",
            "notoriously difficult to train. This difficulty arises from two-parts: 1) First, it is difficult to tune the hyper-parameters correctly for the adversarial model to continue learning throughout all of the training epochs [5]. Since both the discriminator and generator are updated via the same gradient, it is very common for the model to fall into a local minima before completing all of the defined training cycles. 2) GANs are computationally expensive to train, given that both models are updated on each cycle in parallel [5]. This compounds the difficulty of tuning the model’s parameters.",
            "The performances of the NER experiments are reported separately for three different parts of the system proposed. Table 6 presents the comparison of the various methods while performing NER on the bot-generated corpora and the user-generated corpora. Results shown that, in the first case, in the training set the F1 score is always greater than 97%, with a maximum of 99.65%. With both test sets performances decrease, varying between 94-97%. In the case of UGC, comparing the F1 score we can observe how performances significantly decrease. It can be considered a natural consequence of the",
            "In this paper, we examine the use case of general adversarial networks (GANs) in the field of marketing. In particular, we analyze how GAN models can replicate text patterns from successful product listings on Airbnb, a peer-to-peer online market for short-term apartment rentals. To do so, we define the Diehl-Martinez-Kamalu (DMK) loss function as a new class of functions that forces the model's generated output to include a set of user-defined keywords. This allows the general adversarial network to recommend a way of rewording the phrasing of a listing description to increase the likelihood",
            "Bi-LSTM + CRF for Multimodal NER",
            "build hierarchical learning models BIBREF8 , BIBREF9 , BIBREF10 for document-to-sentence level, and sentence-to-word level classification tasks, which are highly domain-dependent and task-specific. Automatic Speech Recognition (ASR) technology has recently achieved human-level accuracy in many fields BIBREF11 , BIBREF12 . For spoken language understanding (SLU), it is shown that training SLU models on true text input (i.e., human transcriptions) versus noisy speech input (i.e., ASR outputs) can achieve varying results BIBREF13 . Even greater performance degradations are expected in more",
            "Named Entity Recognition (NER), or alternatively Named Entity Recognition and Classification (NERC), is the task of detecting entities in an input text and to assign them to a specific class. It starts to be defined in the early '80, and over the years several approaches have been proposed BIBREF2 . Early systems were based on handcrafted rule-based algorithms, while recently several contributions by Machine Learning scientists have helped in integrating probabilistic models into NER systems. In particular, new developments in neural architectures have become an important resource for this",
            "Nowadays, there are several international competitions related to text mining, which include diverse tasks such as: polarity classification (at different levels), subjectivity classification, entity detection, and iron detection, among others. These competitions are relevant to measure the potential of different proposed techniques. In this case, we focused on polarity classification task, hence, we developed a baseline method with an acceptable performance achieved in three different contests, namely, TASS'15 (Spanish) BIBREF17 , SemEval'15-16 (English) BIBREF18 , BIBREF19 , and SENTIPOLC'14",
            "model that utilizes user, topic, and comment information. We attempt to learn user and topic representations which encode user interactions and topic influences to further enhance text classification, and we also incorporate comment information. We evaluate this model on a post stance classification task on forum-style social media platforms. The contributions of this paper are as follows: 1. We propose UTCNN, a neural network for text in modern social media channels as well as legacy social media, forums, and message boards — anywhere that reveals users, their tastes, as well as their"
        ]
    }
]