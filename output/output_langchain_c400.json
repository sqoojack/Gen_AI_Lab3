[
    {
        "title": "Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations",
        "answer": "pretraining on the unlabeled data of each task.",
        "evidence": [
            "on how the model is trained with unlabeled data through the modified CVT, which is our third contribution.",
            "learned using only word-level information. Consequently, the distributed representation for low-frequency words and unknown words cannot be learned well with conventional models. However, low-frequency words and unknown words are often comprise high-frequency morphemes (e.g., unkingly INLINEFORM0 un + king + ly). Some previous studies take advantage of the morphological information to provide a",
            "there may be an upper bound, especially when the labels are native to the data or when the notion of a “gold standard” is not appropriate. For some in the humanities, validation takes the form of close reading, not designed to confirm whether the model output is correct, but to present what BIBREF48 refers to as a form of “further discovery in two directions”. Model outputs tell us something",
            "if learned representations are used for initialization. Pretraining. Sentence classification systems are usually implemented as supervised training regimes where training loss is between true label distribution and predicted label distribution. In this work, we use pretraining on the unlabeled data of each task and show that it can increase the performance of classification systems. Figure 1",
            "(row (g) vs row (h)). This experiment was conducted only on the UGWC dataset because no unlabeled data are available in the Orchid dataset, as mentioned in Section UID33 . The model improves the F1 score slightly, from 88.8% (row (g)) to 88.9% (row (h)) on the UGWC dataset. This result occurs because both the labeled and unlabeled data in the UGWC dataset are drawn from the same finance domain.",
            "incorporating syntactic information into NMT has the potential to improve performance. This is particularly true for source syntax, which can improve the model's representation of the source language. Recently, there have been a number of proposals for using linearized representations of parses within standard NMT BIBREF7 , BIBREF8 , BIBREF9 . Linearized parses are advantageous because they can",
            "There are several recent studies applying domain adaptation methods to deep neural networks. However, few studies have focused on improving the fine tuning and dual outputs methods in the supervised setting. sun2015return have proposed an unsupervised domain adaptation method and apply it to the features from deep neural networks. Their idea is to minimize the domain shift by aligning the",
            "parsing. Therefore, it is necessary to explore other techniques that are free from hand-crafted features. With the development of neural networks and deep learning, it is possible to learn the representations of concepts from unlabeled text corpus automatically. These representations can be treated as concept features for classification. An important advance in this area is the development of the",
            "Model Improvements",
            "the context. One way to improve interpretation is to exploit the response strategy, but the response strategy in our model is predicted independently of interpretation. So one solution could be similar to the one proposed above for the disclosure task problem: jointly learning classifiers that predict both variables simultaneously. Another possibility is to use the temporal sequence of response",
            "without having to train them from scratch. These models are accessed through a simple and unified API that follows a classic NLP pipeline: setting up configuration, processing data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to",
            "average 1.225 new words found in each new unlabeled data passage, as shown in Table TABREF32 ; consequently the model representation learns new information from these new words effectively."
        ]
    },
    {
        "title": "Artificial Error Generation with Machine Translation and Syntactic Patterns",
        "answer": "English, French, Finnish, Russian, Japanese, Vietnamese, Bengali.",
        "evidence": [
            "Multilingual experiments",
            "that reflect the depth of the questions we seek to address. These are just a small sample of the many opportunities and challenges faced in computational analyses of textual data. New possibilities and frustrating obstacles emerge at every stage of research, from identification of the research question to interpretation of the results. In this article, we take the reader through a typical",
            "Paper Overview",
            "Documents (687 000 words) taken from four sources: the Est Républicain newspaper (39 articles, 10 000 words); Wikipedia (30 articles + 30 summaries, 242 000 words); Proceedings of the conference Traitement Automatique des Langues Naturelles (TALN) 2008 (25 articles, 169 000 words); Reports from Institut Français de Relations Internationales (32 raports, 266 000 words). The corpora were noted",
            "operations can be learned implicitly by meta TreeNN, such as the composition of noun phrases and verb phrases. The contributions of the paper can be summed up as follows.",
            "with other intervals. Frequency lists for the presented languages are derived from Wikipedia and Common Crawl corpora. While many concept pairs are direct or approximate translations of English pairs, we can see that the frequency distribution does vary across different languages, and is also related to inherent language properties. For instance, in Finnish and Russian, while we use infinitive",
            "they were able to adapt them for, for a total of 229 languages. This test set omits 23 of the high resource languages that are written in unique scripts or for which language distance metrics could not be computed.",
            "paper, we attempt to build the first NMT systems for a low-resourced language pairs:Japanese-Vietnamese. We have also shown significant improvements when combining advanced methods to reduce the adverse impacts of data sparsity and improve the quality of NMT systems. In addition, we proposed a variant of Byte-Pair Encoding algorithm to perform effective word segmentation for Vietnamese texts and",
            "Analysis of the convolutional language model",
            "unseen languages during training. The main motivation and contribution behind this work is as follows:",
            "complexity) among some machine learning based approaches used in QC for Bengali language.",
            "lean and modular architecture.[3] purpleWe evaluate our model and compare with previous works in both extractive and abstractive summarization on two large scientific paper datasets, which contain documents that are much longer than in previously used corpora.[4] purpleOur model not only achieves state-of-the-art on these two datasets, but in an additional experiment, in which we consider"
        ]
    },
    {
        "title": "Generating Word and Document Embeddings for Sentiment Analysis",
        "answer": "(DISPLAY_FORM4)    $ w_{t}$ denotes the sentiment score of word $t$, $N_{t}$   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .",
        "evidence": [
            "away from each other in the VSM, even though they have common words in their dictionary definitions. We multiply each value in a row with the corresponding row word's raw supervised score, thereby having more meaningful clusters. Using the training data only, the supervised polarity score per word is calculated as in (DISPLAY_FORM4). Here, $ w_{t}$ denotes the sentiment score of word $t$, $N_{t}$",
            "Word Error Rate results",
            "Supervised models",
            "exclude trivial matches, tokens within a list of stop words are not considered while performing string matching. The final score is a weighted combination of the string matching score and the time proximity of the track, aimed to enhance matches from tracks played closer to the time when the user is posting the tweet. The performance of the algorithm depends, apart from the time proximity",
            "a word basis, with supervised three scores extracted on a review basis, we have better state-of-the-art results.",
            "is obtained. Decomposed terms are called relevance scores. These relevance scores can be viewed as highlighted text or can be used to form a list of top-words in the document. The whole procedure is also described visually in Figure 1 . While we detail in this section the LRP method for a specific network architecture and with predefined choices of layers, the method can in principle be extended",
            "training, both the supervised and semi-supervised models are trained until the validation metrics stop improving; the metrics are (1) sentence boundary F1 score and (2) overall F1 score for Thai sentence segmentation and English punctuation restoration, respectively. CVT has three main parameters that impact model accuracy. The first is the drop rate of the masked language model, which determines",
            "inspired by studies that compared the performance of different word embedding versions or investigated the combination of them. For example, turian2010word compared Brown clusters, C&W embeddings and HLBL embeddings in NER and chunking tasks. They found that Brown clusters and word embeddings both can improve the accuracy of supervised NLP systems; and demonstrated empirically that combining",
            "assigned continuous scores. The high transparency of dictionaries makes them sometimes more suitable than supervised machine learning models. However, dictionaries should only be used if the scores assigned to words match how the words are used in the data (see BIBREF38 for a detailed discussion on limitations). There are many off-the-shelf dictionaries available (e.g., LIWC BIBREF39 ). These are",
            "were collected from January 2017 to December 2017. The labeled dataset includes 48,374 passages. To support semi-supervised learning, the first 3 months of data (96,777 passages) are unlabeled. Because the data stem from social media, some text exists that cannot be considered as part of any sentence, such as product links, symbols unrelated to sentences, and space between sentences. These",
            "in the whole corpus. We then normalise each row by dividing entries in the row by the maximum score in it. Secondly, we perform the principal component analysis (PCA) method to reduce the dimensionality. It captures latent meanings and takes into account high-order cooccurrence removing noise. The attribute (column) number of the matrix is reduced to 200. We then compute cosine similarity between",
            "the model to generate words which can get high scores from classifiers and even back-translation cannot stop it. Our solution is that we can lower the probability of decoding a word in decoders if the word has been generated in the previous time steps during testing. We also try to implement this method during training time but obtain worse performances because it may limit the effects of"
        ]
    },
    {
        "title": "Using General Adversarial Networks for Marketing: A Case Study of Airbnb",
        "answer": "30 hours",
        "evidence": [
            "zip code, the host’s description of the listing, the price of the listing, and the occupancy rate of the listing. Airbnb defines a home's occupancy rate, as the percentage of time that a listing is occupied over the time period that the listing is available. This gives us a reasonable metric for defining popular versus less popular listings.",
            "How Many Expert Annotations?",
            "Experiment Setup",
            "rankings in the first place: absolute vocabulary sizes are expected to fluctuate across different languages. However, it is also important to note that the datasets also contain subsets of lower-frequency and rare words, which can be used for rare word evaluations in multiple languages, in the spirit of Pilehvar:2018emnlp's English rare word dataset.  Cross-Linguistic Differences. Table TABREF23",
            "Working with Short Text, Micro-Blogs, Social Media",
            "Amongst the comprehendable questions, 50% were identified as likely to have an answer within the privacy policy, 33.1% were identified as being privacy-related questions but not within the scope of a privacy policy (e.g., 'has Viber had any privacy breaches in the past?') and 16.9% of questions were identified as completely out-of-scope (e.g., `'will the app consume much space?'). In the",
            "Our AV in-cabin dataset includes around 30 hours of multi-modal data collected from 30 passengers (15 female, 15 male) in a total of 20 rides/sessions. In 10 sessions, single passenger was present (i.e., singletons), whereas the remaining 10 sessions include two passengers (i.e., dyads) interacting with the vehicle. The data is collected \"in the wild\" on the streets of Richmond, British Columbia,",
            "BLEU by Sentence Length",
            "sentence input is first fed into an embedding layer, where the input’s text is converted to a GloVe vector. These GloVe vectors are learned via a global word-word co-occurrence matrix using our corpus of Airbnb listing descriptions [8]. At each time step, the GloVe vectors are then fed into an LSTM layer. For each layer, the model forward propagates the output of the LSTM layer to the next",
            "WiSeBEWiSeBE",
            "The data for the project was acquired from Airdna, a data processing service that collaborates with Airbnb to produce high-accuracy data summaries for listings in geographic regions of the United States. For the sake of simplicity, we focus our analysis on Airbnb listings from Manhattan, NY, during the time period of January 1, 2016, to January 1, 2017. The data provided to us contained",
            "is presented in BIBREF0 that employ the Stochastic Gradient Descent (SGD)."
        ]
    },
    {
        "title": "Question Answering for Privacy Policies: Combining Computational and Legal Perspectives",
        "answer": "Yes.",
        "evidence": [
            "Comparison with baseline models",
            "of change in recall and precision when removing other feature groups suggesting that the contribution of non-lexical features to classification performance was limited. However, notable changes in F1-score were observed for the classes lower in the hierarchy including disturbed sleep and fatigue or loss of energy. For instance, changes in F1-scores driven by both recall and precision were",
            "used a similar approach, in which a character-level neural language model is trained on a massively multilingual corpus. A language embedding vector is concatenated to the input at each time step. The language embeddings their system learned correlate closely to the genetic relationships between languages. However, neither of these models was applied to g2p.",
            "contains 220 queries generated in the same fashion as the training data from a separate section of ClueWeb. However, as they did not release a development set with their data, we used this set as a development set. For a final evaluation, we generated another, similar test set from a different held out section of ClueWeb, in the same fashion as done by Krishnamurthy and Mitchell. This final test",
            "accuracy values of all baseline systems on the most frequent question types (appearing >25 times in the test data), as determined based on the question words (see Section SECREF19 ). The numbers depicted on the left-hand side of the y-axis represent model accuracy. The right-hand side of the y-axis indicates the number of times a question type appears in the test data. The neural models",
            "outperforms the traditional extractive models on informativeness (ROUGE-1,2) by a wide margin, but results are mixed on ROUGE-L. Presumably, this is due to the neural training process, which relies on a goal standard based on ROUGE-1. Exploring other training schemes and/or a combination of traditional and neural approaches is left as future work. Similarly, the neural extractive models also",
            "Table TABREF17 shows that the QRNN outperformed the character-level LSTM, almost matching the performance of a word-level attentional baseline.",
            "copy-marked setup, which is the best way to use a copy of the target (although the performance on the out-of-domain tests is at most the same as the baseline). Such gains may look surprising, since the NMT model does not need to learn to translate but only to copy the source. This is indeed what happens: to confirm this, we built a fake test set having identical source and target side (in",
            "BLEU, METOR and CIDEr scores for the evaluation. The results are summarized in Table 3 . We see that the proposed method improves in most of the metrics. The baseline methods SrcOnly and TgtOnly are worse than other methods, because they use limited data for the training. Note that the CIDEr scores correlate with human evaluations better than BLEU and METOR scores BIBREF15 . Generated captions",
            "overlapping is big enough and is much larger that that for the source models. Moreover, these first components have almost zero alignment shifts. Other principal components have very similar singular values, and thus they cannot be determined uniquely with high confidence. Normalized interpretibility measures for different components (calculated for 50 top/bottom words) for the source and the",
            "there are only a few articles studying 0Shot-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress.  ::: This work benchmarks the 0Shot-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our",
            "a collection of similar question pairs, they searched community question answering sites such as Yahoo! and Answers.com. In contrast, the ECNU-ICA system achieved the best performance of 1.895 in the open-domain task but an average score of only 0.402 in the medical task. As the ECNU-ICA approach also relied on a neural network for question matching, this result shows that training"
        ]
    },
    {
        "title": "Dynamic Compositional Neural Networks over Tree Structure",
        "answer": "age, dialect, and gender.  language, age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender.  age, dialect, and gender",
        "evidence": [
            "Translation experiments",
            "of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment to many users, we maintain a consistent set of video segments and extend the possibility to generate a diverse",
            "tasks, we experimentally validate LAN's superiority in terms of the desired properties.",
            "For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test.",
            "Experimenting with Transformers ::: Ecosystem",
            "indicate that the task is challenging and call for the investigation of new algorithms.",
            "signal on the task at test time it may not be very surprising if the task performance goes up. We acknowledge this, and argue that our motivation for this setup is to deepen understanding, in particular the limitation or the capacity of the current architectures, which we expect can be reached with such strong supervision. Another motivation is engineering: we could exploit negative examples in",
            "type for the different tasks. Now, the “All Features” experiment shows how the interaction between feature sets perform than any of the other features groups in isolation. The accuracy metric for each trolling task is meant to provide an overall performance for all the classes within a particular task, and allow comparison between different experiments. In particular, we observe that GloVe",
            "Experiments ::: How do the generated hypotheses influence",
            "Task and Evaluation",
            "RQE Approaches and Experiments",
            "Data details and experimental setup"
        ]
    },
    {
        "title": "Automatic Discourse Segmentation: an evaluation in French",
        "answer": "ROUGE, Hunalign, manual validation.",
        "evidence": [
            "In this section we will compare the results of the different segmentation systems through automatic evaluations. First of all, the human segmentation, from the subcorpus $D$ composed of common documents. The results are presented in the table tab:humains. The first row shows the performance of the $I$ segments, taking the experts as a reference, while the second presents the process in the",
            "the sentence segmentation models improved slightly, from 92.4% and 88.7% (row (f)) to 92.5% and 88.8% (row (g)) on the Orchid and UGWC datasets, respectively. For the IWSLT dataset, the distant feature can recover the overall F1 score of punctuation restoration, which is degraded by the n-gram embedding; it improves from 63.6% (row (f)) to 64.5% (row (g)). The reason is that the self-attention",
            "Evaluation Method",
            "Another important parameter specifies which segmentation strategy should be applied, according to the POS labelling of the document.",
            "about text segments can be extremely valuable, but it is also prone to errors, inconsistencies, bias, and missing information. Examining metadata is a good way to check a collection's balance and representativeness. Are sources disproportionately of one form? Is the collection missing a specific time window? This type of curation can be extremely time consuming as it may require expert labeling,",
            "Thai sentence segmentation",
            "Then, the task is to pick the best segment for given a query. This task is easier than Baseline1's task in that the segmentation information is provided to the model. Unlike Baseline1, however, it is unable to return an answer span at various granularities. Baseline2 is based on the attentive LSTM BIBREF17, which has been developed for the InsuranceQA task. The right diagram in Fig. FIGREF15",
            "it relies on the presence of verbs and nouns. For this version, four rules are considered: If there is no noun in either the left or right segment, we regroup the segments. We regroup the segments if at least one of them has no noun. If at least one noun is present in both segments, they remain independent. If there is no verb-nominal form, the segments remain independent.",
            "271 and 1,602 with a total number of 8,080. We gave clear instructions to three evaluators ( INLINEFORM0 ) of how segmentation was needed to be perform, including the SU concept and how punctuation marks were going to be taken into account. Periods (.), question marks (?), exclamation marks (!) and semicolons (;) were considered SU delimiters (boundaries) while colons (:) and commas (,) were",
            "in term of speed, robustness and performance. On the other hand, we useVNBPE as an alternative way of doing word segmentation. Those two approaches are compared in an extrinsic evaluation of the NMT systems employing them.",
            "We manually validated the alignment quality for 400 sentences randomly selected from the parsed corpus and assigned quality labels according Section SECREF9 . From all the evaluated sentences, 82.30% were correctly aligned, while 13.33% were partially aligned, and 4.35% presented no alignment. The small percentage of no alignment is probably due to the use of Hunalign tool with the provided EN/PT",
            "For evaluation, we adopt the widely-used automatic evaluation metric ROUGE BIBREF14 . It measures the summary quality by counting the overlapping units such as the n-grams, word sequences and word pairs between the peer summary and reference summaries. We take ROUGE-2 as the main measures due to its high capability of evaluating automatic summarization systems BIBREF19 . During the training data"
        ]
    },
    {
        "title": "Multimodal Named Entity Recognition for Short Social Media Posts",
        "answer": "1,888 concept pairs.",
        "evidence": [
            "SnapCaptions Dataset",
            "We proposed a new multimodal NER (MNER: image + text) task on short social media posts. We demonstrated for the first time an effective MNER system, where visual information is combined with textual information to outperform traditional text-based NER baselines. Our work can be applied to myriads of social media posts or other articles across multiple platforms which often include both text and",
            "by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity dataset spanning 1,888 concept pairs (see §SECREF4). Most importantly, semantic similarity datasets in different languages have been created using heterogeneous construction procedures with different guidelines for translation and annotation, as well as different rating scales. For",
            "require different number of attentions. This is highly dependent on each dataset's nature and the underlying interconnections between modalities.",
            "The models are implemented in Neural Monkey BIBREF14 . They are trained using Adam BIBREF15 and have minibatch size 40, RNN size 512, and dropout probability 0.2 BIBREF16 . We train to convergence on the validation set, using BLEU BIBREF17 as the metric. For sequential inputs and outputs, the maximum sentence length is 50 subwords. For parsed inputs, we increase maximum sentence length to 150",
            "and how they reacted. The final dataset contains reports by approximately 3000 respondents from all over the world, for a total of 7665 sentences labelled with an emotion, making this the largest dataset out of the three we use.",
            "commands by exact string matching, there is no guarantee that all sentences are accessible from question tokens only. One token from the union of the question and the current observation: an intermediate level where the action space is larger. One token from the dataset vocabulary: the action space is huge (see Table TABREF16 for statistics of SQuAD and NewsQA). It is guaranteed that all",
            "filters (i.e., feature detectors), in which the concrete filter size is a hyperparameter. They essentially split a sentence into multiple sub-sentences by a sliding window, then determine the sentence label by using the dominant label across all sub-sentences. The underlying assumption is that the sub-sentence with that granularity is potentially good enough to represent the whole sentence.",
            "In this experiment, we explore the benefit of adaptation from both sides of the domains. Flickr30K is another captioning dataset, consisting of 30K images, and each image has five captions BIBREF16 . Although the formats of the datasets are almost the same, the model trained by the MS COCO dataset does not work well for the Flickr 30K dataset and vice versa. The word distributions of the captions",
            "Our dataset consists of 76 tutorial videos pertaining to an image editing software. All of the videos include spoken instructions which are transcribed and manually segmented into multiple segments. Specifically, we asked the annotators to manually divide each video into multiple segments such that each of the segments can serve as an answer to any question. For example, Fig. FIGREF1 shows",
            "data obtained a score of 1.139 on the LiveQA open-domain task. While this gap in performance can be explained in part by the discrepancies between the medical test questions and the open-domain questions, it also highlights the need for larger medical datasets to support deep learning approaches in dealing with the linguistic complexity of consumer health questions and the challenge of finding",
            "sufficient grounding for the data while making it easy for workers to apply labels consistently.  Size: The total of 13,215 dialogs in this corpus is on par with similar, recently released datasets such as MultiWOZ BIBREF13."
        ]
    },
    {
        "title": "Neural Collective Entity Linking",
        "answer": "0.68",
        "evidence": [
            "Wikipedia hyperlinks, NCEL outperforms the state-of-the-art collective methods across five different datasets. Besides, further analysis of the impacts of main modules as well as qualitative results demonstrates its effectiveness. In the future, we will extend our method into cross-lingual settings to help link entities in low-resourced languages by exploiting rich knowledge from high-resourced",
            "Implementation Detail",
            "forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and",
            "show their flexibility in incorporating it in attention and CTC based seq2seq model without compromising loss in performance.",
            "Experiments ::: Label-fully-unseen evaluation",
            "Stage 0 - Naive approach",
            "Results of RQE Approaches",
            "API and Mike Thelwal for his help with TensiStrength. We are also grateful to the Stanford NLP group for clarifying some of the questions we had with regards to the Stanford NER tool.",
            "in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\"",
            "we make is that we have a very robust performance and the results are stable across the years. If we consider the experimental setup, where for year INLINEFORM0 we optimize the learning objective with only 74k training instances and evaluate on the rest of the instances, it achieves a very good performance. We predict with F1=0.68 the remaining 469k instances for the years INLINEFORM1 . The",
            "rather than only one (the self score). This approach is fully supervised unlike the previous two approaches.",
            "Suppose there are INLINEFORM0 mentions in documents on average, among these global models, NCEL not surprisingly has the lowest time complexity INLINEFORM1 since it only considers adjacent mentions, where INLINEFORM2 is the number of sub-GCN layers indicating the iterations until convergence. AIDA has the highest time complexity INLINEFORM3 in worst case due to exhaustive iteratively finding and"
        ]
    },
    {
        "title": "Distant supervision for emotion detection using Facebook reactions",
        "answer": "None mentioned.  They looked at several Facebook pages, but the specific pages are not mentioned.  They also looked at Weibo, RenRen, and Chinese microblogs.  They also looked at Twitter.  They also looked at Reddit.  They also looked at subreddits.  They also looked at news articles.  They also looked at entity pages.  They also looked at the European Court of Human Rights.  They also looked at the names of subreddits themselves.  They also looked at the names of related subreddits.  They also looked at the names of the subreddits themselves.  They also looked at the names of related subreddits.  They also looked at the names of the subreddits themselves.  They also looked at the names of related subreddits.  They also looked at the names of the subreddits themselves.  They also looked at the names of related subreddits.  They also looked at the names of the subreddits themselves.  They also looked at the names of related subreddits.  They also looked at the names of the subreddits themselves.  They also looked at the names of related subreddits.  They also looked at the names of the",
        "evidence": [
            "Selecting Facebook pages",
            "task. Using only the provided training datasets and the assumption that each question contains only one focus, the CMU-LiveMedQA system obtained an average score of 0.353. They used a convolutional neural network (CNN) model to classify a question into a restricted set of 10 question types and crawled \"relevant\" online web pages to find the answers. However, the results were lower than those",
            "Weibo, RenRen, and Chinese microblogs",
            "Research questions",
            "We exploit the Facebook reaction feature in a distant supervised fashion to train a support vector machine classifier for emotion detection, using several feature combinations and combining different Facebook pages. We test our models on existing benchmarks for emotion detection and show that employing only information that is derived completely automatically, thus without relying on any",
            "Social media has been used to estimate preferences as well. The advantage of social media compared to speeches or any other preference indicator is coverage. BIBREF31 use endorsement of official pages on Facebook to scale ideological positions of politicians from different levels of government and the public into a common space. Their method extends to other social media such as Twitter where",
            "from text in the control group (similar subreddits that were not closed). The researchers then returned to the hate speech definition provided by the European Court of Human Rights, and manually filtered the top SAGE words based on this definition. Not all identified words fitted the definition. The others included: the names of the subreddits themselves, names of related subreddits,",
            "Facebook",
            "entity classes. This allows us to add missing sections to entity pages. We carry out an extensive experimental evaluation on 351,983 news articles and 73,734 entities coming from 27 distinct entity classes. For the first stage, we achieve an overall performance with P=0.93, R=0.514 and F1=0.676, outperforming our baseline competitors significantly. For the second stage, we show that we can learn",
            "forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and",
            "and guha2004propagation present a methodology to identify malicious individuals in a network based solely on the network's properties rather than on the textual content of comments. cambria2010not propose a method that involves NLP components, but fail to provide an evaluation of their system. There is extensive work on detecting offensive and abusive language in social media BIBREF2 and BIBREF3",
            "Zhanggong District. The leaks are emails in which individuals claim credit for propaganda posts in the name of the regime. The emails contain social media posts and account names. BIBREF33 used the leaked posts as training data for a classification algorithm that subsequently helped them to identify more propaganda posts. In conjunction with a follow-up survey experiment they found that most"
        ]
    },
    {
        "title": "MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge",
        "answer": "Amazon Mechanical Turk (MTurk) and Google Cloud Platform.",
        "evidence": [
            "Algorithms Used",
            "Therefore, crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce smaller and more focused semantic resources and evaluation benchmarks. This alternative practice has had a profound impact on distributional semantics and representation learning BIBREF14. While some prominent English word pair datasets such as WordSim-353 BIBREF75, MEN BIBREF76, or Stanford",
            "While it is beyond the scope of this work to describe the entire system in detail, there are several platform features that help illustrate how the process works.  Modality: The agents playing the assistant type their input which is in turn played to the user via text-to-speech (TTS) while the crowdsourced workers playing the user speak aloud to the assistant using their laptop and microphone. We",
            "Weibo, RenRen, and Chinese microblogs",
            "Schaub, Joel Reidenberg, Aditya Potukuchi and Igor Shalyminov for helpful discussions related to this work, and to the three anonymous reviewers of this draft for their constructive feedback. Finally, the authors would like to thank all crowdworkers who consented to participate in this study.",
            ", we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk). Section SECREF17 gives information about some necessary postprocessing steps and the dataset validation. Lastly, Section SECREF19 gives statistics about the final dataset.",
            "common space. Their method extends to other social media such as Twitter where endorsements and likes could be leveraged.",
            "task with 0.49 average score using Wikipedia as the first answer source and Yahoo and Google searches as secondary answer sources. Each medical question was decomposed into several subquestions. To extract the answer from the selected text passage, a bi-directional attention model trained on the SQUAD dataset was used. Deep neural network models have been pushing the limits of performance",
            "Cloud Platform",
            "on a single answer to a visual question. We train a model to automatically predict from a visual question whether a crowd would agree on a single answer. We then propose how to exploit this system in a novel application to efficiently allocate human effort to collect answers to visual questions. Specifically, we propose a crowdsourcing system that automatically solicits fewer human responses when",
            "We measured the CCR, across all tweets, to be 31.7% for Rosette Text Analytics, 43.2% for Google Cloud, 44.2% for TensiStrength, and 74.7% for the crowdworkers. This means the difference between the performance of the tools and the crowdworkers is significant – more than 30 percent points. Crowdworkers correctly identified 62% of the neutral, 85% of the positive, and 92% of the negative",
            "Routing To Experts or Crowd"
        ]
    },
    {
        "title": "Stochastic Answer Networks for SQuAD 2.0",
        "answer": "one-layer feed forward neural network.",
        "evidence": [
            "layer is almost identical to the lower layers of SAN, which has a lexicon encoding layer, a contextual layer and a memory generation layer. On top of it, there are different answer modules for different tasks. We employ the SAN answer module for the span detector and a one-layer feed forward neural network for the binary classification task. It can also be viewed as a multi-task learning BIBREF8",
            "Model architecture",
            "Beam-search decoder",
            "Transductive String Kernels",
            "This configuration object can be saved and loaded for reproducibility or simply modified for architecture search. The configuration defines the architecture of the model but also architecture optimizations like the heads to prune. Configurations are agnostic to the deep learning framework used. Tokenizers - A Tokenizer class (inheriting from a base class `PreTrainedTokenizer`) is available for",
            "corpus. In this paper we compare error detection frameworks trained on the same publicly available FCE dataset, thereby removing the confounding factor of dataset size and only focusing on the model architectures. The error generation methods can generate alternative versions of the same input text – the pattern-based method randomly samples the error locations, and the SMT system can provide an",
            "the classification accuracy reached to 76.8 ( INLINEFORM1 ), however the final results of our model only have a small improvement (+0.2 in terms of F1 score). It shows that it is important to make balance between these two components: the span detector and unanswerable classifier.",
            "filters (i.e., feature detectors), in which the concrete filter size is a hyperparameter. They essentially split a sentence into multiple sub-sentences by a sliding window, then determine the sentence label by using the dominant label across all sub-sentences. The underlying assumption is that the sub-sentence with that granularity is potentially good enough to represent the whole sentence.",
            "Stance detection",
            "from the text directly, which might confound the computation of attention values. Also, the attentive reader was originally constructed for reconstructing literal text spans as answers. Our mode of answer collection, however, results in many correct answers that cannot be found verbatim in the text. This presents difficulties for the attention mechanism. The fact that an attention model",
            "data size conditions given our limited computational resources. Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with",
            "lean and modular architecture.[3] purpleWe evaluate our model and compare with previous works in both extractive and abstractive summarization on two large scientific paper datasets, which contain documents that are much longer than in previously used corpora.[4] purpleOur model not only achieves state-of-the-art on these two datasets, but in an additional experiment, in which we consider"
        ]
    },
    {
        "title": "Winograd Schemas and Machine Translation",
        "answer": "English, Czech, Danish, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish.     Turkish, Farsi, Finnish.     Indonesian, Turkish, Farsi, Finnish.     Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish.     Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish.     Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish.     Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish.     Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish.     Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish.     Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish",
        "evidence": [
            "Multilingual experiments",
            "database that collects and normalizes information compiled by grammarians and field linguists about the world's languages. In particular, we focus on information about geography (the areas where the language speakers are concentrated), family (the phylogenetic tree each language belongs to), and typology (including syntax, phonological inventory, and phonology). Moreover, we consider typological",
            "similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish BIBREF26, 500 in Farsi BIBREF27, or 300 in Finnish BIBREF28) and coverage (e.g., all datasets which originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more",
            "Task, 30 users were tasked to first browse the collection of videos, select interesting segments with start and end times, and then asked to conjecture questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves,",
            "unseen languages during training. The main motivation and contribution behind this work is as follows:",
            "We later analyze whether similarity scores obtained from native speakers also loosely follow the patterns described by semantic typology.",
            "Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. Although this language list contains only one non-Indo-European (Indonesian), four major Indo-European sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically",
            "in low-resourced languages by exploiting rich knowledge from high-resourced languages, and deal with NIL entities to facilitate specific applications.",
            "they were able to adapt them for, for a total of 229 languages. This test set omits 23 of the high resource languages that are written in unique scripts or for which language distance metrics could not be computed.",
            "in Czech Republic speak? using the lexical and phrasal rules from the PPDB. Once $G_{\\mathrm {syn}}^{\\prime }$ is generated, we sample paraphrases of the input question $q$ . These paraphrases are further filtered with a classifier to improve the precision of the generated paraphrases. We train the L-PCFG $G_{\\mathrm {syn}}$ on the Paralex corpus BIBREF9 . Paralex is a large monolingual parallel",
            "examination by argumentation mining researchers too. Cabrio2013b examined a connection between five Walton's schemes and discourse markers in PDTB, however an empirical evaluation is missing.",
            "knowledge is (mostly) tacit. Perhaps it is the case that in a humongous corpus of natural language text, someone really has written about trying to stuff a tuba in a backpack?"
        ]
    },
    {
        "title": "How we do things with words: Analyzing text as social and cultural data",
        "answer": "social and cultural phenomena using textual data.",
        "evidence": [
            "work on a variety of languages and being robust to informal writing. The latter problem is not properly tackled in many cases.",
            "Question answering (QA) has drawn a lot of attention in the past few years. QA tasks on images BIBREF0 have been widely studied, but most focused on understanding text documents BIBREF1 . A representative dataset in text QA is SQuAD BIBREF1 , in which several end-to-end neural models have accomplished promising performance BIBREF2 . Although there is a significant progress in machine",
            "are telling us about the existence of something in the data that nobody noticed, or thought important, until then (e.g., the large number of travel journals in Darwin's reading lists). Computational text analysis is not a replacement for but rather an addition to the approaches one can take to analyze social and cultural phenomena using textual data. By moving back and forth between large-scale",
            "have developed Transformers, a library for state-of-the art Natural Language Processing with Transfer Learning models. Transformers addresses several key challenges:",
            "about controversies. We observed that such contributions do not intend to persuade; these documents typically contain story-sharing, personal worries, user interaction (asking questions, expressing agreement), off-topic comments, and others. Such characteristics are typical to on-line discussions in general, but they have not been examined with respect to argumentation or persuasion. Indeed, we",
            "and situation detection problems are far from satisfactory. We believe this is mainly because the Yahoo-based topic categorization task is much closer to the Wikipedia-based topic categorization task; emotion and situation categorizations, however, are relatively further. Our entailment models, pretrained on MNLI/FEVER/RTE respectively, perform more robust on the three $\\textsc {0shot-tc}$",
            "to provide users with more informative answers, and to what extent their disclosures continue to omit issues that matter to users).",
            "languages in order to learn word representations that distinguish between true similarity and conceptual relatedness (see the discussion in §SECREF6)?",
            "challenging questions at the heart of some of the most pressing contemporary cultural and social issues. While a human reader is better equipped to make logical inferences, resolve ambiguities, and apply cultural knowledge than a computer, human time and attention are limited. Moreover, many patterns are not obvious in any specific context, but only stand out in the aggregate. For example, in a",
            "featurization methods to understand the conditions under which discourse features contribute non-trivial performance gains, and analyze discourse embeddings.",
            "information about the questions that could be used for diverse IR and NLP tasks, we automatically annotated the questions with the focus, its UMLS Concept Unique Identifier (CUI) and Semantic Type. We combined two methods to recognize named entities from the titles of the crawled articles and their associated UMLS CUIs: (i) exact string matching to the UMLS Metathesaurus, and (ii) MetaMap Lite",
            "A number of real-world problems related to text data have been studied under the framework of natural language processing (NLP). Example of such problems include topic categorization, sentiment analysis, machine translation, structured information extraction, or automatic summarization. Due to the overwhelming amount of text data available on the Internet from various sources such as"
        ]
    },
    {
        "title": "Quasi-Recurrent Neural Networks",
        "answer": "mean-pooling",
        "evidence": [
            "Pooling Layer",
            "boosting comes along with possible higher optimization times. Finally, the performance of each configuration is obtained using a cross-validation technique on the training data, and the metrics are usually used in classification such as: accuracy, score INLINEFORM0 , and recall, among others.",
            "decoder takes the hidden state at the last time step of a sequence concatenated with both the max-pooled and mean-pooled representation of the hidden states BIBREF19 and outputs a number in the range $[0,1]$. The difficulty of using GANs in text generation comes from the discrete nature of text, making the model non-differentiable hence, we update parameters for the generator model with policy",
            "Each layer of a quasi-recurrent neural network consists of two kinds of subcomponents, analogous to convolution and pooling layers in CNNs. The convolutional component, like convolutional layers in CNNs, allows fully parallel computation across both minibatches and spatial dimensions, in this case the sequence dimension. The pooling component, like pooling layers in CNNs, lacks trainable",
            "takes as input a collection of vectors INLINEFORM1 ( INLINEFORM2 ) and maps them to a single vector. With this observation, the following two types of functions seem to be natural choices for neighborhood aggregators, and have been adopted previously: Pooling Functions. A typical pooling function is mean-pooling, which is defined by INLINEFORM0 . Besides mean-pooling, other previously adopted",
            "Summarization Performance",
            "Standard and baseline methods",
            "in which the $softmax$ function is calculated over a combined logits from both sides.  $$\\begin{split}",
            "embedding plays two roles, both the pooling item and the pooling weight. On the one hand, if a sentence is highly related to the query, its pooling weight is large. On the other hand, if a sentence is salient in the document cluster, its embedding should be representative. As a result, the weighted-sum pooling generates the document representation which is automatically biased to embeddings of",
            "Loss Function",
            "In the social sciences, text is often used to infer preferences. Various scaling techniques have been developed and refined over recent years. Wordfish is a scaling algorithm that enables us to estimate policy positions based on word frequencies BIBREF18 . Researchers have used this approach to measure policy positions on European integration in the European Parliament BIBREF19 , on austerity",
            "Feature selection"
        ]
    },
    {
        "title": "Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction",
        "answer": "sentence",
        "evidence": [
            "Sentence Selection",
            "language corpora perform in specific, and potentially highly-specialized, domains? There are aspects of this task that make it both easier and more difficult than “traditional” IE. Even though they are expressed in natural language, business documents frequently take constrained forms, sometimes even “template-like” to a certain degree. As such, it may be easy to learn cue phrases and other fixed",
            "articles whereas the E2E dataset contains over 50,000 sentences of restaurant reviews. The statistics of these two datasets are summarised in Table TABREF11.",
            "pronoun is `it'. The two antecedents are `trophy' and `brown suitcase'. A human reader will naturally interpret `it' as referring to the trophy in the first sentence and to the suitcase in the second sentence, using the world knowledge that a small object can fit in a large container, but a large object cannot fit in a small container (Davis 2013). Condition 4 is satisfied because either a trophy",
            "Expert annotations of Random and Difficult Instances",
            "exists in that IE does not attend to knowledge graph vectors in a preceding sentence, and thus it does use any commonsense knowledge. The incremental encoding scheme without MSA obtained the best grammar score and our full mode IE+MSA(GA) has the best logicality score. All the models have fairly good grammar scores (maximum is 2.0), while the logicality scores differ remarkably, much lower than",
            "Visual Question - Predicting If a Crowd Will (Dis)Agree on an Answer\nWe now explore the following two questions: 1) Given a novel visual question, can a machine correctly predict whether multiple independent members of a crowd would supply the same answer? and 2) If so, what insights does our machine-learned system reveal regarding what humans are most likely to agree about?",
            "have more than one argument component. Although in 19 cases (0.5%) the sentence contains a Claim-Premise pair which is an important distinction from the argumentation perspective, given the overall small number of such occurrences, we simplify the task by treating each sentence as if it has either one argument component or none. The approximation with sentence-level units is explained in the",
            "information of the sentence at various levels. For example, we have a unary lexical rule (NN-*-142 day) indicating that we observe day with NN of the paraphrase type 142. We could use this information to extract unary rules of the form (NN-*-142 $w$ ) in the treebank that will generate words $w$ which are paraphrases to day. Similarly, any node WHNP-*-291 in the treebank will generate paraphrases",
            "information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset. Entity-value pairs are extracted from the descriptions. For example, in the sentence Isaiah Thomas led the team in scoring, totaling 23 points [...]., an IE tool will extract the pair (Isaiah Thomas, 23, PTS). Second, we compute three metrics on the extracted information:",
            "Question Types",
            "knowledge is (mostly) tacit. Perhaps it is the case that in a humongous corpus of natural language text, someone really has written about trying to stuff a tuba in a backpack?"
        ]
    },
    {
        "title": "TWEETQA: A Social Media Focused Question Answering Dataset",
        "answer": "crowd-sourcing to elicit questions and answers.",
        "evidence": [
            "can yield irrelevant tweets with no valuable information, we restrict ourselves only to tweets which have been used by journalists in news articles thus implicitly implying that such tweets contain useful and relevant information. To obtain such relevant tweets, we crawled thousands of news articles that include tweet quotations and then employed crowd-sourcing to elicit questions and answers",
            "in detecting two types of musical named entities, Contributor and Musical Work. The method proposed makes use of the information extracted from the radio schedule for creating links between users' tweets and tracks broadcasted. Thanks to this linking, we aim to detect when users refer to entities included into the schedule. Apart from that, we consider a series of linguistic features, partly",
            "both before and after specific natural disasters; this removes bias from over-weighting Twitter users who are only compelled to compose tweets after a disaster.",
            "determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy.",
            "of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator.",
            "corpora, entities annotated are only about 5% of the whole amount of tokens. In the case of the automatically generated tweets, the percentage is significantly greater and entities represent about the 50%.",
            "a tweet based on the difficulty of the task. Tweets are labeled by 2, 3, 5, or 7 workers based on the difficulty of the task and the level of disagreement between the crowdworkers. The model computes the number of workers based on how long a tweet is, the presence of a link in a tweet, and the number of present sarcasm signals. Sarcasm is often used in political tweets and causes disagreement",
            "by using TweetTokenizer. All text was lowercased. In a post-processing step, it was ensured that each emoji is tokenized as a single token.",
            "Experiments ::: Tweet-Level Models ::: BERT.",
            "digitized but not others, and what does this say about the collection? Similar questions arise with the use of born-digital data. For instance, when using the Internet Archive’s Wayback Machine to gather data from archived web pages, we need to consider what pages were captured, which are likely missing, and why. We must often repurpose born-digital data (e.g., Twitter was not designed to measure",
            "frequently used as a news source, with INLINEFORM2 of its users obtaining their news from Twitter. All these statistical facts suggest that understanding user-generated noisy social media text from Twitter is a significant task. In recent years, while several tools for core natural language understanding tasks involving syntactic and semantic analysis have been developed for noisy social media",
            "from text in the control group (similar subreddits that were not closed). The researchers then returned to the hate speech definition provided by the European Court of Human Rights, and manually filtered the top SAGE words based on this definition. Not all identified words fitted the definition. The others included: the names of the subreddits themselves, names of related subreddits,"
        ]
    },
    {
        "title": "Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation",
        "answer": "WW BIBREF19, SNIPS-NLU, MOOC iterations, DBLPRiosK18, DBLPXiaZYCY18, Coursera.org, YouTube videos, annotated citations, medical data, English language news context.  # Corrected Answer",
        "evidence": [
            "Models Used in the Evaluation",
            "such resources other than for English (e.g., Japanese) seldom exist. In addition, most of these datasets comprise high-frequency nouns so that they tend not to include other parts of speech. Hence, previous data fail to evaluate word representations of other parts of speech, including content words such as verbs and adjectives. To address the problem of the lack of a dataset for evaluating",
            "The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics.",
            "We focused evaluation over a small but diversified dataset composed by 10 YouTube videos in the English language in the news context. The selected videos cover different topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 minutes. To encourage the diversity of content format we included newscasts, interviews, reports and round tables. During the",
            "Emotion datasets",
            "and Mix-Source, our NMT systems achieved the best improvement on the dataset. In the future, we will exploit more domain and multilingual information to improve the quality of the systems.",
            "10-cross-validation scheme, evaluation was conducted on a set of annotated citations. The results showed that word embeddings are effective on classifying positive and negative citations. However, hand-crafted features performed better for the overall classification.",
            "the performance on a challenging dataset WW BIBREF19 as well as qualitative results, investigating the effectiveness of each key module.",
            "that is, some “correct” answers were actually incorrect and vice versa. Therefore, we manually validated the complete dataset in a final step. We asked annotators to read all texts, questions, and answers, and to mark for each question whether the correct and incorrect answers were appropriate. If an answer was inappropriate or contained any errors, they selected a different answer from the set",
            "Prior work is mostly evaluated on different datasets and adopted different evaluation setups, which makes it hard to compare them fairly. For example, DBLPRiosK18 work on medical data while reporting R@K as metric; DBLPXiaZYCY18 work on SNIPS-NLU intent detection data while only unseen intents are in the label-searching space in evaluation. In this work, we benchmark the datasets and evaluation",
            "by providing standardized datasets, evaluations, and a state-of-the-art entailment system. All datasets and codes are released.",
            "We evaluate our proposed models over a corpus of 12 MOOC iterations (offerings) on Coursera.org In partnership with Coursera and in line with its Terms of Service, we obtained the data for use in our academic research. Following prior work BIBREF7 we evaluate over a diverse dataset to represent MOOCs of varying sizes, instructor styles, instructor team sizes and number of threads intervened. We"
        ]
    },
    {
        "title": "Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance",
        "answer": "French broadcast corpora.  The corpora being widely used within the speech processing community, they are a primary material for training automatic speech recognition (ASR) systems.  They are four major corpora.  The names of the corpora are not specified in the text.  However, the text mentions that the corpora are widely used within the speech processing community, which suggests that they are well-known corpora in the field.  One of the corpora mentioned is the \"French broadcast corpora\" which is also known as the \"French TV news corpus\".  Another corpus mentioned is the \"French parliamentary debates corpus\".  The other two corpora are not specified in the text.  However, based on the context, it is likely that they are also corpora of French broadcast or other types of French text.  The names of the corpora are not specified in the text, but they are likely to be well-known corpora in the field of speech processing and natural language processing.  The names of the corpora are not specified in the text, but they are likely to be corpora of French text, such as the \"French TV news corpus\" or the \"French parliamentary debates corpus\".  The names of the corpora are not specified in",
        "evidence": [
            "Corpora",
            "operations can be learned implicitly by meta TreeNN, such as the composition of noun phrases and verb phrases. The contributions of the paper can be summed up as follows.",
            "This paper analyzes the gender representation in four major corpora of French broadcast. These corpora being widely used within the speech processing community, they are a primary material for training automatic speech recognition (ASR) systems. As gender bias has been highlighted in numerous natural language processing (NLP) applications, we study the impact of the gender imbalance in TV and",
            "Relations Internationales (32 raports, 266 000 words). The corpora were noted using Glozz. Annodis aims at building an annotated corpus. The proposed annotations are on two levels of analysis, that is, two perspectives: Ascendant: part of EDU are used in the construction of more complex structures, through the relations of discourse; Descending: approaches the text in its entirety and relies on",
            "Analysis of the convolutional language model",
            "CAPES, and URL for the full-text PDF in Portuguese. An excerpt of the corpus is shown in Table TABREF13",
            "in the whole corpus. We then normalise each row by dividing entries in the row by the maximum score in it. Secondly, we perform the principal component analysis (PCA) method to reduce the dimensionality. It captures latent meanings and takes into account high-order cooccurrence removing noise. The attribute (column) number of the matrix is reduced to 200. We then compute cosine similarity between",
            "language corpora perform in specific, and potentially highly-specialized, domains? There are aspects of this task that make it both easier and more difficult than “traditional” IE. Even though they are expressed in natural language, business documents frequently take constrained forms, sometimes even “template-like” to a certain degree. As such, it may be easy to learn cue phrases and other fixed",
            "that reflect the depth of the questions we seek to address. These are just a small sample of the many opportunities and challenges faced in computational analyses of textual data. New possibilities and frustrating obstacles emerge at every stage of research, from identification of the research question to interpretation of the results. In this article, we take the reader through a typical",
            "Annotation studies and corpus creation",
            "A serious challenge in the field of AV is the lack of publicly available (and suitable) corpora, which are required to train and evaluate AV methods. Among the few publicly available corpora are those that were released by the organizers of the well-known PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 . In regard to our experiments, however, we cannot use these corpora, due to the absence of",
            "Given the six controversial topics and four different registers, we compiled a collection of plain-text documents, which we call the raw corpus. It contains 694,110 tokens in 5,444 documents. As a coarse-grained analysis of the data, we examined the lengths and the number of paragraphs (see Figure FIGREF43 ). Comments and forum posts follow a similar distribution, being shorter than 300 tokens on"
        ]
    },
    {
        "title": "Knowledge Based Machine Reading Comprehension",
        "answer": "instructional videos.",
        "evidence": [
            "Question Answering Model",
            "consider as candidates any entity that has either been seen at training time with a query entity or is directly connected to a query entity in Freebase. This candidate entity generation is common practice for recent question answering models over Freebase BIBREF8 , though, for the reasons stated above, it has not been used previously in open vocabulary semantic parsing models.",
            "`how do i get to Warsaw Missouri from here'). 4% of questions are vague, and a further 7% have unknown sources of error. 2% still contain false presuppositions (e.g., `what is the only fruit that does not have seeds?') and the remaining 42% do not have an answer within the document. This reinforces our belief that though they have been understudied in past work, any question answering system",
            "for paraphrase generation but instead learn the grammar directly from a large scale question corpus. The main contributions of this paper are two fold. First, we present an algorithm (§ \"Paraphrase Generation Using Grammars\" ) to generate paraphrases using latent-variable PCFGs. We use the spectral method of narayan-15 to estimate L-PCFGs on a large scale question treebank. Our grammar model",
            "The IR models and the RQE Logistic Regression model bring different perspectives to the search for relevant candidate questions. In particular, question entailment allows understanding the relations between the important terms, whereas the traditional IR methods identify the important terms, but will not notice if the relations are opposite. Moreover, some of the question types that the RQE",
            "of the outputs of two BiLSTM: INLINEFORM0 for questions and INLINEFORM1 for passages. Memory Generation Layer. In this layer, we generate a working memory by fusing information from both passages INLINEFORM0 and questions INLINEFORM1 . The attention function BIBREF11 is used to compute the similarity score between passages and questions as: INLINEFORM2  Note that INLINEFORM0 and INLINEFORM1 is",
            "In this paper, we focus on knowledge based machine reading comprehension. We create a manually labeled dataset for the task, and develop a framework consisting of both question answering model and question generation model. We further incorporate four open KBs as external knowledge into both question answering and generative approaches, and demonstrate that incorporating additional evidence from",
            "Context has been used and modelled in various ways for different problems in discussion forums. In a work on a closely related problem of forum thread retrieval BIBREF2 models context using inter-post discourse e.g., Question-Answer. BIBREF3 models the structural dependencies and relationships between forum posts using a conditional random field in their problem to infer the reply structure.",
            "Researches on question classification, question taxonomies and QA system have been undertaken in recent years. There are two types of approaches for question classification according to Banerjee et al in BIBREF13 - by rules and by machine learning approach. Rule based approaches use some hard coded grammar rules to map the question to an appropriate answer type BIBREF14 BIBREF15. Machine Learning",
            "What is the Model Learning?",
            "of question words is a much smaller action space in contrast to the other two settings. In the 4-action setting, an agent can rely on issuing next and previous actions to reach any sentence in a document. The effect of action space size on model performance is particularly clear when using a datasets' entire vocabulary as query candidates in the 2-action setting. From Figure FIGREF40 (and figures",
            "of an overall goal, such as cooking or educational videos. We have shown that current baseline models for finding the answer spans are not sufficient for achieving high accuracy and hope that by releasing this new dataset and task, more appropriate question answering models can be developed for question answering on instructional videos."
        ]
    },
    {
        "title": "Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health",
        "answer": "Ancient-Modern Chinese Dataset, SnapCaptions Dataset, TutorialVQA Dataset, Code-switching Dataset, and the labeled dataset.  # Corrected Answer",
        "evidence": [
            "Datasets Used for the RQE Study",
            "What Does Zero-shot Transfer Model Learn? ::: Code-switching Dataset",
            "require different number of attentions. This is highly dependent on each dataset's nature and the underlying interconnections between modalities.",
            "were collected from January 2017 to December 2017. The labeled dataset includes 48,374 passages. To support semi-supervised learning, the first 3 months of data (96,777 passages) are unlabeled. Because the data stem from social media, some text exists that cannot be considered as part of any sentence, such as product links, symbols unrelated to sentences, and space between sentences. These",
            "an overview of the recent application of these techniques in a United Nations Development Program (UNDP) study.",
            "Acknowledgments\nThis research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada. We would like to thank Google Colab for providing support in terms of computational resources.",
            "Data Set",
            "Ancient-Modern Chinese Dataset",
            "dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort.",
            "and development datasets are also available on another Github repositorypage BIBREF13 for further research and comparison purposes.",
            "TutorialVQA Dataset ::: Overview",
            "SnapCaptions Dataset"
        ]
    },
    {
        "title": "A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media",
        "answer": "GLAD, COAV, and parsing-based approaches.",
        "evidence": [
            "Proposed Approach",
            "point to build new sentiment analysis systems. We compare our approach in eight different languages, three of them have important international contests, namely, SemEval (English), TASS (Spanish), and SENTIPOLC (Italian). Within the competitions our approach reaches from medium to high positions in the rankings; whereas in the remaining languages our approach outperforms the reported results.",
            "Despite the increasing number of AV approaches, a closer look at the respective studies reveals that only minor attention is paid to their underlying characteristics such as reliability and robustness. These, however, must be taken into account before AV methods can be applied in real forensic settings. The objective of this paper is to fill this gap and to propose important properties and",
            "are completely shared across the domains. Augmentation of parameters in the other part of the network would be an interesting direction of future work.",
            "to be adjusted, human coders retrained, more training data prepared, algorithms adjusted, or in some instances, even a new analytical method adopted. Other types of validation are also possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go",
            "to see the strengths of our model as well as possible limitations of our current approaches.",
            "Comparison to Other Datasets",
            "on AUC, it turns out that all three methods perform very similar to each other, whereas big discrepancies between GLAD and COAV can be observed regarding c@1. When we consider the current and maximum achievable results (depicted by the circles and triangles, respectively) it becomes apparent that GLAD's model behaves stable, while the one of COAV becomes increasingly vulnerable the more the",
            "the data are discussed and applications which use a bag-of-words treatment are presented. Second, the vector space approach and distance measure are introduced and discussed. Finally, the network clustering approach and model evaluations are presented.",
            "§ SECREF4 and § SECREF5 ) suggest promising directions for efficient domain adaptation with cheaper techniques than conventional BT.",
            "answer. Ranking-based approaches achieve better performances than parsing-based approaches on WebQuestions, a benchmark dataset for KBQA. We follow ranking-based approaches, and develop both a matching-based model with features at different levels and a question generation model. More references can be found at https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art). Our work also",
            "rather than only one (the self score). This approach is fully supervised unlike the previous two approaches."
        ]
    },
    {
        "title": "External Lexical Information for Multilingual Part-of-Speech Tagging",
        "answer": "Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.  Also, cmn-yue, rus-pol, est-fin, and 229 languages.  Additionally, French, and 11 languages in addition to English.  Furthermore, conversational telephone speech (CTS) in 80 languages.  Also, 23 high resource languages that are written in unique scripts or for which language distance metrics could not be computed.  Finally, 16 languages considered in our experiments.",
        "evidence": [
            "Multilingual experiments",
            "modules. In fact, we have explored the use of already existing modules and adopted and integrated several of these tools into our pipeline. It is important to point out that, as far as we know, there is no integrated architecture supporting the full processing pipeline for the Portuguese language. We evaluated several systems like Rembrandt BIBREF22 or LinguaKit: the former only has the initial",
            "in low-resourced languages by exploiting rich knowledge from high-resourced languages, and deal with NIL entities to facilitate specific applications.",
            "Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. Although this language list contains only one non-Indo-European (Indonesian), four major Indo-European sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically",
            "unseen languages during training. The main motivation and contribution behind this work is as follows:",
            "FIGREF20, where all of the Spearman's correlation coefficients are greater than 0.6 for all language pairs. Languages that share the same language family are highly correlated (e.g, cmn-yue, rus-pol, est-fin). In addition, we observe high correlations between English and most other languages, as expected. This is due to the effect of using English as the base/anchor language to create the",
            "they were able to adapt them for, for a total of 229 languages. This test set omits 23 of the high resource languages that are written in unique scripts or for which language distance metrics could not be computed.",
            "analyses.  Language Selection. Multi-SimLex comprises eleven languages in addition to English. The main objective for our inclusion criteria has been to balance language prominence (by number of speakers of the language) for maximum impact of the resource, while simultaneously having a diverse suite of languages based on their typological features (such as morphological type and language family).",
            "In this first exploratory work, only documents in French were considered, but the system can be adapted to other languages. The evaluation is based on the correspondence of word pairs representing a border. In this way we compare the Annodis segmentation with the automatically produced segmentation. For each pair of reference segments, a $L_r$ list of word pairs is provided: the last word of the",
            "In this work, the experiments are conducted using the BABEL speech corpus collected from the IARPA babel program. The corpus is mainly composed of conversational telephone speech (CTS) but some scripted recordings and far field recordings are presented as well. Table TABREF14 presents the details of the languages used in this work for training and evaluation. 80 dimensional Mel-filterbank (fbank)",
            "Open-ended survey questions",
            "Language Resource References\nlrec lrec2018"
        ]
    },
    {
        "title": "Disunited Nations? A Multiplex Network Approach to Detecting Preference Affinity Blocs using Texts and Votes",
        "answer": "SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR, ROUGE F1, METEOR, Moses, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR, ROUGE F1, METEOR, Moses, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR, ROUGE F1, METEOR, Moses, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR, ROUGE F1, METEOR, Moses, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR, ROUGE F1, METEOR, Moses, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR,",
        "evidence": [
            "Datasets",
            "is presented in BIBREF0 that employ the Stochastic Gradient Descent (SGD).",
            "The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics.",
            "require different number of attentions. This is highly dependent on each dataset's nature and the underlying interconnections between modalities.",
            "by providing standardized datasets, evaluations, and a state-of-the-art entailment system. All datasets and codes are released.",
            "Data details and experimental setup",
            "TutorialVQA Dataset ::: Basis",
            "Datasets Used for the RQE Study",
            "parallel sentences, respectively. Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with subword units BIBREF20 with a joint vocabulary and 32k merge operations. Our implementation was done on a fork of the",
            "Results: SnapCaptions Dataset",
            "Algorithms Used",
            "Ancient-Modern Chinese Dataset"
        ]
    },
    {
        "title": "When to reply? Context Sensitive Models to Predict Instructor Interventions in MOOC Forums",
        "answer": "latent context necessary to predict instructor intervention.",
        "evidence": [
            "models to infer the amount of latent context necessary to predict instructor intervention. Such models also allow themselves to be tuned to instructor's preference to intervene early or late. Our three proposed attentive model variants to infer the latent context improve over the state-of-the-art by a significant, large margin of 11% in F1 and 10% in recall, on average. Further, introspection of",
            "since the instructor's and subsequent posts will bias the prediction due to the instructor's post.",
            "Instructor Intervention in MOOC forums",
            "Link prediction in the inductive setting aims at reasoning the missing part “?” in a triplet when given INLINEFORM0 or INLINEFORM1 with emerging entities INLINEFORM2 or INLINEFORM3 respectively. To tackle the task, we firstly hide the object (subject) of each testing triplet in Subject-R (Object-R) to produce a missing part. Then we replace the missing part with all entities in the entity set",
            "at a time for -Pattern, from four non-local subject-verb dependencies (across a prepositional phrase (PP), subject RC, object RC, and long verb phrase (VP)). We hypothesize that models are less affected by token-level ablation, as knowledge transfer across words appearing in similar contexts is promoted by language modeling objective. We expect that construction-level supervision would be",
            "simultaneously. Another possibility is to use the temporal sequence of response comments and make use of older response interpretation as input features for later comments. This could be useful since commenters seem to influence each other as they read through the conversation. Errors on Response Strategy (B) prediction: In some cases there is a blurry line between “Frustrate” and “Neutralize”.",
            "the word embeddings matrix indefinitely. Figure FIGREF19 (b) shows the cases where the modality attention led to incorrect predictions. For example, the model predicts missing tokens HUUUGE and Shampooer incorrectly as named entities by amplifying misleading character-based contexts (capitalized first letters) or visual contexts (concert scenes, associated contexts of which often include named",
            "When they intervene, instructors either pay attention to a specific post or a series of posts, which trigger their reply. However, instructors rarely explicitly indicate to which post(s) their intervention is in relation to. This is the case in our corpus, party due to Coursera's user interface which only allows for single level comments (see Figure FIGREF2 ). Based solely on the binary,",
            "Do models generalize explicit supervision, or just memorize it?",
            "of last hidden states from the forward and backward LSTMs in Bi-LSTM case) will be used to classify the intent type of the given utterance. The idea behind is that the last hidden state of the sequence will contain a latent semantic representation of the whole input utterance, which can be utilized for utterance-level intent prediction. See Fig. FIGREF24 (a) for sample network architecture of the",
            "processes the task-specific context INLINEFORM0 and predicts the probability distribution over classes.",
            "The 17.5% improvement when adding user information suggests that user information is especially useful when the dataset is highly imbalanced. All models that consider user information predict the minority class successfully. UCTNN without topic information works well but achieves lower performance than the full UTCNN model. The 4.9% performance gain brought by LDA shows that although it is"
        ]
    },
    {
        "title": "A Hierarchical Model for Data-to-Text Generation",
        "answer": "93% and 84%  (article-entity and article-section placement)  and significant improvements.  (EF-LSTM)  and  (Majority)  (human performance)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)  (0.13cm)",
        "evidence": [
            "Standard and baseline methods",
            "on improving explanatory power than making small improvements in predictive performance.",
            "gold descriptions, adequately picking up on domain lingo. Qualitative metrics are either better or on par with baselines. We show in Figure FIGREF29 a text generated by our best model, which can be directly compared to the gold description in Figure FIGREF1. Generation is fluent and contains domain-specific expressions. As reflected in Table TABREF25, the number of correct mentions (in green)",
            "test datasets. The hybrid method (LR+IR) provided the best results on both datasets. On the 2016 test data, the LR+IR method outperformed the best system in all measures, with 80.57% Accuracy and 77.47% MAP (official system ranking measure in SemEval-cQA). On the cQA-2017 test data, the LR+IR method obtained 44.66% MAP and outperformed the cQA-2017 best system in Accuracy with 67.27%.",
            "Proposed Approach",
            "the MOS scores indicating the performance of 3 approaches. The mean of MOS scores of baseline 1 is the lowest one: 2.57 (the standard deviation (stdev) is 1.17), the baseline 2 mean score is slightly higher than the baseline 1's: 2.86 (the stdev is 1.27), while SimplerVoice evaluation score is the highest one: 4.82 (the stdev is 0.35) which means the most effective approach to provide users with",
            "is further optimized with local variance loss and global variance loss. The mix of loss functions is: where $\\lambda _1$ and $\\lambda _2$ are hyper-parameters. -0.13cm",
            "aim is to have perfect predictions such that the minimum number of answers ( INLINEFORM12 ) are allocated only for visual questions with one true answer so that all diverse answers are safely captured. We measure diversity per visual question as the number of all true answers collected per visual question ( INLINEFORM0 ). Larger values reflect greater captured diversity. The motivation for this",
            "that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "Quantitative Evaluation",
            "a high precision value of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\% for the \\emph{article-section placement}. Finally, we compare our approach against competitive baselines and show significant improvements.",
            "The best performing model is reported as EF-LSTM $_{(\\star )}$ , $\\star \\in \\lbrace  \\textrm {-, s, b, sb}\\rbrace $ denoting vanilla, stacked, bidirectional and stacked bidirectional LSTMs respectively. Majority performs majority voting for classification tasks, and predicts the expected label for regression tasks. This baseline is useful as a lower bound of model performance. Human performance"
        ]
    },
    {
        "title": "Fully Convolutional Speech Recognition",
        "answer": "37 million tokens.",
        "evidence": [
            "State-of-the-art",
            "to train language models, which contain 37 million tokens for WSJ, 800 million tokens for Librispeech. Our language models are trained separately for each dataset on the official text data only. These datasets were chosen to study the impact of the different components of our system at different scales of training data and in different recording conditions. The models are evaluated in Word Error",
            "data. Time and The Guardian perform well on most emotions but Disney helps to boost the performance for the Joy class.",
            "Benchmark the dataset ::: Situation detection",
            "MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classification.",
            "yielded 0.7673 AUC on March 13, 2020, which is the best known result at this time and beats the previous state of the art by over five points.",
            "techniques today are designed to process (Wikipedia articles, news stories, web pages, etc.): They are heterogeneous and frequently contain a mix of both free text as well as semi-structured elements (tables, headings, etc.). They are, by definition, domain specific, often with vocabulary, phrases, and linguistic structures (e.g., legal boilerplate and terms of art) that are rarely seen in",
            "that the template-equipped BiSET model manages to improve the summarization performance significantly with a new state of the art.",
            "https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art). Our work also relates to BIBREF21 khot2017answering, which uses open IE outputs from external text corpora to improve multi-choice question answering. However, our work differs from them in that their task does not contain document information. Furthermore, we develop a question generation approach while they regard the QA task as",
            "News Article Suggestion",
            "state $s_t$, and the attention on $i$-th position should be weakened to avoid confusing the model. The attention distribution is updated as follows (the symbol $\\odot $ means element-wise product):",
            "data size conditions given our limited computational resources. Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with"
        ]
    },
    {
        "title": "Massively Multilingual Neural Grapheme-to-Phoneme Conversion",
        "answer": "SQUAD, FB15K, WordNet, BIBREF5, BIBREF9, Waseem and Hovy, SnapCaptions, MarMoT, MElt, Czech, French, Italian, Indonesian, Wikipedia, Yahoo, Google, Wikipedia, Yahoo, Google, Davidson et al.  Waseem and Hovy, SQUAD, FB15K, WordNet, BIBREF5, BIBREF9, Waseem and Hovy, SnapCaptions, MarMoT, MElt, Czech, French, Italian, Indonesian, Wikipedia, Yahoo, Google, Davidson et al.  Waseem and Hovy, SQUAD, FB15K, WordNet, BIBREF5, BIBREF9, Waseem and Hovy, SnapCaptions, MarMoT, MElt, Czech, French, Italian, Indonesian, Wikipedia, Yahoo, Google, Davidson et al.  Waseem and Hovy, SQUAD, FB15K, WordNet, BIBREF5, BIBREF9, Waseem and Hovy, SnapCaptions, MarMoT, MElt, Czech, French, Italian, Indonesian, Wikipedia, Yahoo, Google, Davidson et",
        "evidence": [
            "Datasets Used for the RQE Study",
            "that is, some “correct” answers were actually incorrect and vice versa. Therefore, we manually validated the complete dataset in a final step. We asked annotators to read all texts, questions, and answers, and to mark for each question whether the correct and incorrect answers were appropriate. If an answer was inappropriate or contained any errors, they selected a different answer from the set",
            "extremely well, surpassed by MarMoT on only 3 out of 16 datasets (Czech, French and Italian), and by MElt only once (Indonesian).",
            "In both tasks, we need datasets whose test sets contain new entities unseen during training. For the task of triplet classification, we directly use the datasets released by BIBREF9 ijcai2017-250 which are based on WordNet11 BIBREF29 . Since they do not conduct experiments on the link prediction task, we construct the required datasets based on FB15K BIBREF11 following a similar protocol used in",
            "and how they reacted. The final dataset contains reports by approximately 3000 respondents from all over the world, for a total of 7665 sentences labelled with an emotion, making this the largest dataset out of the three we use.",
            "Results and Analysis",
            "by providing standardized datasets, evaluations, and a state-of-the-art entailment system. All datasets and codes are released.",
            "Data Set",
            "SnapCaptions Dataset",
            "Experiments ::: Preliminaries ::: Dataset and Metrics.",
            "task with 0.49 average score using Wikipedia as the first answer source and Yahoo and Google searches as secondary answer sources. Each medical question was decomposed into several subquestions. To extract the answer from the selected text passage, a bi-directional attention model trained on the SQUAD dataset was used. Deep neural network models have been pushing the limits of performance",
            "We evaluate our method on two widely-studied datasets provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9. Waseem and Hovy BIBREF5 collected $16k$ of tweets based on an initial ad-hoc approach that searched common slurs and terms related to religious, sexual, gender, and ethnic minorities. They annotated their dataset manually as racism, sexism, or neither. To extend this dataset,"
        ]
    },
    {
        "title": "A Neural Approach to Irony Generation",
        "answer": "assessors: a medical doctor (Assessor A), a medical librarian (B) and a researcher in medical informatics (C). None of the assessors participated in the development of the QA methods. Assessors B and C evaluated 1,000 answers retrieved by each method (IR+RQE) according to the same reference answers used in LiveQA-Med. The answers were anonymized (the method names were blinded) and presented to 3 assessors: a medical doctor (Assessor A), a medical librarian (B) and a researcher in medical informatics (C). None of the assessors participated in the development of the QA methods. Assessors B and C evaluated 1,000 answers retrieved by each method (IR+RQE) according to the same reference answers used in LiveQA-Med. The answers were anonymized (the method names were blinded) and presented to 3 assessors: a medical doctor (Assessor A), a medical librarian (B) and a researcher in medical informatics (C). None of the assessors participated in the development of the QA methods. Assessors B and C evaluated 1,000 answers retrieved by each method (IR+RQE) according to the same reference answers used in LiveQA",
        "evidence": [
            "and implement denoising auto-encoder and back-translation to control content preservation but also design a sentiment reward to control sentiment preservation. Experimental results demonstrate that our model achieves a high irony accuracy with well-preserved sentiment and content. The contributions of our work are as follows:",
            "rebuttals to the author's claim or by formulating arguments for both sides when the overall stance is neutral. While 85% of the documents do not consider any opposing side, only 8% documents present a rebuttal, which is then attacked by refutation in 4% of the documents. Multiple rebuttals and refutations were found in 3% of the documents. Only 4% of the documents were overall neutral and",
            "method (IR+RQE) according to the same reference answers used in LiveQA-Med. The answers were anonymized (the method names were blinded) and presented to 3 assessors: a medical doctor (Assessor A), a medical librarian (B) and a researcher in medical informatics (C). None of the assessors participated in the development of the QA methods. Assessors B and C evaluated 1,000 answers retrieved by each",
            "that the threshold for human inter- and intra-coder reliability should be particularly high. Accuracy, as well as other measures such as precision, recall and F-score, are sometimes presented as a measure of validity, but if we do not have a genuinely objective determination of what something is supposed measure—as is often the case in text analysis—then accuracy is perhaps a better indication of",
            "0.797 and Response 0.776. In the end, the annotators discussed their discrepancies and managed to resolve all of them.",
            "non-experimental, and verifiable experimental) and ignored non-argumentative texts. Using multi-class SVM and a wide range of features (n-grams, POS, sentiment clue words, tense, person) they achieved Macro INLINEFORM0 0.69. Peldszus.2014 experimented with a rather complex labeling schema of argument segments, but their data were artificially created for their task and manually cleaned, such as",
            "Measuring the Quality of Word Relevances through Intrinsic Validation",
            "emphasized the similarity of the act itself rather than the different degrees of feeling, and vice versa. In this case, Annotator A assigned a rating of 9, but Annotator B assigned a rating of 2. Although it was necessary to distinguish similarity and semantic relatedness BIBREF7 and we asked annotators to rate the pairs based on semantic similarity, it was not straightforward to put paraphrase",
            "Although the Hunalign tool usually presents a good alignment between sentences, we also conducted a manual validation to evaluate the quality of the aligned sentences. We randomly selected 400 pairs of sentences. If the pair was fully aligned, we marked it as \"correct\"; if the pair was incompletely aligned, due to segmentation errors, for instance, we marked it as \"partial\"; otherwise, when the",
            "of the source website or the discussion forum thread. This also applies for sarcasm and irony. Second, the decision whether a particular topic is persuasive was always made with respect to the controversial topic under examination. Some authors shift the focus to a particular aspect of the given controversy or a related issue, making the document less relevant. We achieved moderate agreement",
            "identified 62% of the neutral, 85% of the positive, and 92% of the negative sentiments. Google Cloud correctly identified 88% of the neutral sentiments, but only 3% of the positive, and 19% of the negative sentiments. TensiStrength correctly identified 87.2% of the neutral sentiments, but 10.5% of the positive, and 8.1% of the negative sentiments. Rosette Text Analytics correctly identified 22.7%",
            "unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns). Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant. Raw agreement between"
        ]
    },
    {
        "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer",
        "answer": "Hybrid HMM-DNN architecture.",
        "evidence": [
            "Model architecture",
            "The ASR system used in this work is described in BIBREF25. It uses the KALDI toolkit BIBREF26, following a standard Kaldi recipe. The acoustic model is based on a hybrid HMM-DNN architecture and trained on the data summarized in Table . Acoustic training data correspond to 100h of non-spontaneous speech type (mostly broadcast news) coming from both radio and TV shows. A 5-gram language model is",
            "self-attention layers and up to the last layer hidden states. The base class is specific to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of the base class, adding a specific head on top of the base model hidden states. Examples of these heads are language",
            "model can achieve much more stable training process and can generate text with significantly better quality.",
            "In this section, we will describe the general architecture of NMT as a kind of sequence-to-sequence modeling framework. In this kind of sequence-to-sequence modeling framework, often there is an encoder trying to encode context information from the input sequence and a decoder to generate one item of the output sequence at a time based on the context of both input and output sequences. Besides,",
            "We have defined a major design principle for our architecture: it should be modular and not rely on human made rules allowing, as much as possible, its independence from a specific language. In this way, its potential application to another language would be easier, simply by changing the modules or the models of specific modules. In fact, we have explored the use of already existing modules and",
            "architecture learns sentence features layer by layer, then those features are justified by all constituent words. During pretraining, all the model parameters, including mutichannel input, convolution parameters and fully connected layer, will be updated until they are mature to extract the sentence features. Subsequently, the same sets of parameters will be fine-tuned for supervised",
            "Models Used in the Evaluation",
            "the model is indeed using the input instructions and is not just approximating shortest paths in the behavioral graph. Other examples on the prediction of sub-obtimal paths are described in the Appendix.",
            "What is the Model Learning?",
            "Hybrid models",
            "it particularly easy to switch from one framework to the other one along the model life-time (training, serving, etc.). Auto classes - In many cases, the architecture to use can be automatically guessed from the shortcut name of the pretrained weights (e.g. `bert-base-cased`). A set of Auto classes provides a unified API that enable very fast switching between different models/configs/tokenizers."
        ]
    },
    {
        "title": "From Textual Information Sources to Linked Data in the Agatha Project",
        "answer": "Pearson's r, precision, and paired-samples t-test.",
        "evidence": [
            "Results and Evaluation",
            "plays an important role in delivering the values of existing entity expansion techniques to potential users including nontechnical people without supposing a large amount of human cost. Moreover, we believe that this design makes it easy to compare performances between interactive entity population pipelines and develop more sophisticated ones.",
            "As pointed out by the reviewers the success of the multichannel approach is likely due to a combination of several quite different effects. First, there is the effect of the embedding learning algorithm. These algorithms differ in many aspects, including in sensitivity to word order (e.g., SENNA: yes, word2vec: no), in objective function and in their treatment of ambiguity (explicitly modeled",
            "amounted to 250 annotated headlines (Affective development), while systems were evaluated on another 1000 (Affective test). Evaluation was done using two different methods: a fine-grained evaluation using Pearson's r to measure the correlation between the system scores and the gold standard; and a coarse-grained method where each emotion score was converted to a binary label, and precision,",
            "Baselines ::: Baseline3: Pipeline Segment retrieval",
            "the method performed during the evaluation of the gender bias presence. In this method we try to evaluate the accuracy of the analogies generated through the model, that is, to verify the cases of association matching generated between the words. [!htb] Model Evaluation [1] w2v_evaluate INLINEFORM0 open_model( INLINEFORM1 ) count = 0 INLINEFORM2 in INLINEFORM3 read list of tuples x =",
            "to determine how different sentences and EDUs affect downstream tasks. Therefore, the evaluation of downstream tasks from different sources needs to be studied.",
            "investigate the question whether the evaluation results of this paper hold more generally. Furthermore, we will address the important question how the results of AV methods can be interpreted in a more systematic manner, which will further influence the practicability of AV methods besides the proposed properties. This work was supported by the German Federal Ministry of Education and Research",
            "stdev is 0.35) which means the most effective approach to provide users with products' usage. Additionally, a paired-samples t-test was conducted to compare the MOS scores of users' responses among all products using baseline 1 and SimplerVoice system. There was a significant difference in the scores for baseline 1 (Mean = 2.57, Stdev = 1.17) and SimplerVoice (Mean = 4.82, Stdev = 0.35); t=",
            "which emphasizes mostly on how well two sequences repel each other and finally empirically show how this method in conjunction with attention can boost the overall performance.",
            "the data are discussed and applications which use a bag-of-words treatment are presented. Second, the vector space approach and distance measure are introduced and discussed. Finally, the network clustering approach and model evaluations are presented.",
            "algorithm's confidence in its answer offers a valuable indicator over today's status quo of passively budgeting. While we acknowledge this method is not intended for our task specifically, it still serves as an important baseline (as discussed above). We attribute the further performance gains of our prediction system to it directly predicting whether humans will disagree rather than predicting a"
        ]
    },
    {
        "title": "Sparse and Constrained Attention for Neural Machine Translation",
        "answer": "Japanese-Vietnamese, Welsh-English, English-English, cmn-spa, heb-est, rus-fin, cmn-spa, heb-est, rus-fin, German-French, Russian-Italian, Dutch-Chinese, Portuguese-Swedish, Spanish-Arabic, Farsi-English, English-Welsh, English-Japanese, English-Vietnamese, English-Portuguese, English-Spanish, English-French, English-Russian, English-Italian, English-Dutch, English-Chinese, English-Arabic, English-Farsi, English-German, English-Swedish, English-Hebrew, English-Russian, English-Finnish, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English-English, English",
        "evidence": [
            "low across the board for lower-resource languages such as Welsh and Kiswahili. It also seems that the lack of monolingual data is a larger problem than typological dissimilarity between language pairs, as we do observe reasonably high correlation scores with vecmap for language pairs such as cmn-spa, heb-est, and rus-fin. However, typological differences (e.g., morphological richness) still play",
            "Following that, we describe the two proposed contextual language models that utilize the dialog level context.",
            "operations can be learned implicitly by meta TreeNN, such as the composition of noun phrases and verb phrases. The contributions of the paper can be summed up as follows.",
            "Results on Unseen Languages",
            "the translators are asked to choose the best option given the semantic context (relation) expressed by the pair in English, otherwise select one of the translations arbitrarily. This is also used to remove duplicate pairs in the translated set, by differentiating the duplicates using a variant at each instance. Further, many translation instances were resolved using near-synonymous terms in the",
            "FAQ. Two sets of pairs were constructed: (i) positive pairs of CHQs and FAQs sharing at least one common question type and the question focus, and (ii) negative pairs corresponding to a focus mismatch or type mismatch. For each category of negative examples, we randomly selected the same number of pairs for a balanced dataset. Then, we manually validated the constructed pairs and corrected the",
            "be interesting to apply the artificial token approach to other problems besides multilingual g2p. One closely related application is monolingual English g2p. Some of the ambiguity of English spelling is due to the wide variety of loanwords in the language, many of which have unassimilated spellings. Knowing the origins of these loanwords could provide a useful hint for figuring out their",
            "This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. Dong Nguyen is supported with an Alan Turing Institute Fellowship (TU/A/000006). Maria Liakata is a Turing fellow at 40%. We would also like to thank the participants of the “Bridging disciplines in analysing text as social and cultural data” workshop held at the Turing Institute (2017) for insightful",
            "paper, we attempt to build the first NMT systems for a low-resourced language pairs:Japanese-Vietnamese. We have also shown significant improvements when combining advanced methods to reduce the adverse impacts of data sparsity and improve the quality of NMT systems. In addition, we proposed a variant of Byte-Pair Encoding algorithm to perform effective word segmentation for Vietnamese texts and",
            "across languages arose in the Welsh concept pair yn llwye – yn gyfan gwbl, where Welsh speakers agreed that the two concepts are very similar. When asked, bilingual speakers considered the two Welsh concepts more similar than English equivalents purely – completely, potentially explaining why a higher average similarity score was reached in Welsh. The example of woman – wife can illustrate",
            "and WordSim-353 to 11 resource-rich target languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic, Farsi), but they did not provide details concerning the translation process and the resolution of translation disagreements. More importantly, they also did not reannotate the translated pairs in the target languages. As we discussed in § SECREF6 and",
            "in the International Phonetic Alphabet (IPA). Our results show that the parameters learned by the shared encoder–decoder are able to exploit the orthographic and phonemic similarities between the various languages in our data."
        ]
    },
    {
        "title": "Dialog Context Language Modeling with Recurrent Neural Networks",
        "answer": "30 turns.",
        "evidence": [
            "of years BIBREF8, BIBREF9, BIBREF10. However, the dearth of high quality, goal-oriented dialog data is considered a major hindrance to more significant progress in this area BIBREF9, BIBREF11. To help solve the data problem we present Taskmaster-1, a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls",
            "Discourse Segmenter Overall Description",
            "our corpus containing conversations, API call and argument annotations, and also the human judgments.",
            "phrases. Technical issues involved dropped sessions (e.g. WebRTC connections failed) or cases in which the user could not hear the agent or vice-versa. In addition, weekly meetings were held with the agents to answer questions and gather feedback on their experiences. Agents typically work four hours per day with dialog types changing every hour. Crowdsourced workers playing the user are accessed",
            "contains 74,064 speech utterances produced by 1,268 speakers for a total of 70 hours of speech. Training data by show, medium and speech type is summarized in Table and evaluation data in Table . Evaluation data has a higher variety of shows with both prepared (P) and spontaneous (S) speech type (accented speech from African radio broadcast is also included in the evaluation set).",
            "and argumentation as well as from their original analysis of the phenomena. The annotation scheme is claimed to cover three dimensions of an utterance, namely speech act, topic, and response or reference to a previous utterance. They annotated 21 dialogs and reached Krippendorff's INLINEFORM0 between 0.38 and 0.57. Given the broad landscape of various approaches to argument analysis and",
            "language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3% on perplexity. The proposed models also demonstrate advantageous performance over other competitive contextual language models.",
            "that part of the methods are able to cope with very challenging verification cases such as 250 characters long informal chat conversations (72.7% accuracy) or cases in which two scientific documents were written at different times with an average difference of 15.6 years (>75% accuracy). However, we also identified that all involved methods are prone to cross-topic verification cases.",
            "Conversation Excerpts",
            "Interactive Dialog Context LM",
            "interaction, i.e. the use of TTS and slower turn taking cadence, prevents the conversation from becoming fully fledged, overly complex human discourse. This creates an idealized spoken environment, revealing how users would openly and candidly express themselves with an automated assistant that provided superior natural language understanding. Perhaps the most relevant work to consider here is",
            "we have a penalty rule that for a dialogue task, if the system cannot return the result (or accomplish the task) in 30 turns, the dialogue task is end by force. Meanwhile, if a system cannot accomplish a task in less than 30 dialogue turns, the number of dialogue turns is set to 30."
        ]
    },
    {
        "title": "Shallow Syntax in Deep Water",
        "answer": "POS, Case, Gender, chunk features, syntactic phrases.",
        "evidence": [
            "Shallow syntactic information is obtained automatically using a highly accurate model (97% $F_1$ on standard benchmarks). In both settings, we observe only modest gains on three of the four downstream tasks relative to ELMo-only baselines (§SECREF4). Recent work has probed the knowledge encoded in cwrs and found they capture a surprisingly large amount of syntax BIBREF10, BIBREF1, BIBREF11. We",
            "Language Dependent Features",
            "features provided by Transformers like tokenizers, dedicated processing scripts for common downstream tasks and sensible default hyper-parameters for high performance on a range of language understanding and generation tasks. The last direction is related to machine learning research frameworks that are specifically used to test, develop and train architectures like Transformers. Typical examples",
            "is often guided by the task definition and the label types. For example, to identify stance towards rumors based on sequential annotations, an algorithm for learning from sequential BIBREF43 or time series data BIBREF44 could be used. The features (sometimes called variables or predictors) are used by the model to make the predictions. They may vary from content-based features such as single",
            "local standard features (for example the current word itself and its prefixes and suffixes of length 1 to 4) and contextual standard features (for example the tag just assigned to the preceding word). In particular, with respect to Ratnaparkhi's feature set, MElt's basic feature set lifts the restriction that local standard features used to analyse the internal composition of the current word",
            "The Taskmaster Corpus ::: Self-dialogs (one-person written dataset)",
            "sentences, like the phrase “so young” that forms an independent sentence in the example), but also from tweet-specific expressions (such as inferring that it is “Jay Sean” feeling sad about Paul's death because he posted the tweet). Furthermore, we show the distinctive nature of TweetQA by comparing the collected data with traditional QA datasets collected primarily from formal domains. In",
            "Target Task and Setup ::: Syntactic evaluation task",
            "However, most current supervised summarization systems often perform the two tasks in isolation. Usually, they design query-dependent features (e.g., query word overlap) to learn the relevance ranking, and query-independent features (e.g., term frequency) to learn the saliency ranking. Then, the two types of features are combined to train an overall ranking model. Note that the only supervision",
            "reading respectively. For the GR discourse features, in the case of local reading, we process the entity roles one sentence pair at a time (Figure FIGREF18 , left). For example, in processing the pair INLINEFORM0 , we find the first non-empty role INLINEFORM1 for entity INLINEFORM2 in INLINEFORM3 . If INLINEFORM4 also has a non-empty role INLINEFORM5 in the INLINEFORM6 , we collect the entity",
            "is similar. This holds even for phrase-structure parsing, where (gold) chunks align with syntactic phrases, indicating that task-relevant signal learned from exposure to shallow syntax is already learned by ELMo. On sentiment classification, chunk features are slightly harmful on average (but variance is high); mSynC again performs similarly to ELMo-transformer. Overall, the performance",
            "Morphological analysis involves predicting the syntactic traits of a word (e.g. {POS: Noun, Case: Acc, Gender: Fem}). Previous work in morphological tagging improves performance for low-resource languages (LRLs) through cross-lingual training with a high-resource language (HRL) from the same family, but is limited by the strict, often false, assumption that tag sets exactly overlap between the"
        ]
    },
    {
        "title": "TTTTTackling WinoGrande Schemas",
        "answer": "RoBERTa",
        "evidence": [
            "Current state of the art",
            "sharing and large-scale community efforts resulting in the development of standard libraries, an increased availability of published research code and strong incentives to share state-of-the-art pretrained models. The combination of these factors has lead researchers to reproduce previous results more easily, investigate current approaches and test hypotheses without having to redevelop them",
            "Revisiting the feature augmentation method",
            "rather than only one (the self score). This approach is fully supervised unlike the previous two approaches.",
            "Translation as Pretraining",
            "Original Toulmin's model",
            "https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art). Our work also relates to BIBREF21 khot2017answering, which uses open IE outputs from external text corpora to improve multi-choice question answering. However, our work differs from them in that their task does not contain document information. Furthermore, we develop a question generation approach while they regard the QA task as",
            "Previous Work and Evaluation Data",
            "Theoretical background",
            "yielded 0.7673 AUC on March 13, 2020, which is the best known result at this time and beats the previous state of the art by over five points.",
            "data size conditions given our limited computational resources. Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with",
            "Merge Recurrent States"
        ]
    },
    {
        "title": "A Question-Entailment Approach to Question Answering",
        "answer": "RNN, CNN, LSTM, zoneout, QRNN, IR+RQE, RC, MRC, AV, GloVe.",
        "evidence": [
            "RQE Approaches and Experiments",
            "direction, with deep learning models being used for both feature extraction and the training of classifiers. These newer models are applying deep learning approaches such as Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs), etc.BIBREF6, BIBREF0 to enhance the performance of hate speech detection models, however, they still suffer from lack of labelled data or",
            "In this paper, we carried out an empirical study of machine learning and deep learning methods for Recognizing Question Entailment in the medical domain using several datasets. We developed a RQE-based QA system to answer new medical questions using existing question-answer pairs. We built and shared a collection of 47K medical question-answer pairs. Our QA approach outperformed the best results",
            "RNN Language Model",
            "complexity) among some machine learning based approaches used in QC for Bengali language.",
            "overfitting is of considerable importance and a major focus of recent research. It is not obvious in advance which of the many RNN regularization schemes would perform well when applied to the QRNN. Our tests showed encouraging results from zoneout applied to the QRNN's recurrent pooling layer, implemented as described in Section SECREF5 . The experimental settings largely followed the “medium”",
            "the highest dev set AP. All models were implemented in Torch BIBREF37 and used the rnn library of BIBREF38 .",
            "question. Table TABREF113 presents the MAP@10 and MRR@10 of our QA methods. The IR+RQE system outperforms the IR-based QA system with 0.311 MAP@10 and 0.333 MRR@10.",
            "RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11. The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance. Nonetheless, RC on languages other than English has been limited due to the absence of sufficient training data. Although some efforts have been made to create RC datasets for Chinese BIBREF13, BIBREF14 and Korean",
            "the evolution of powerful neural models. However, much of the research up to now has been dominated by answering questions that can be well solved solved using superficial information, yet struggles to do accurate natural language understanding and reasoning. For example, BIBREF3 jia2017Adversarial show that existing machine learning systems for MRC perform poorly under adversarial evaluation.",
            "field, AV approaches were becoming more and more diverse, as can be seen in the respective literature. In 2013, for example, Veenman and Li BIBREF2 presented an AV method based on compression, which has its roots in the field of information theory. In 2015, Bagnall BIBREF3 introduced the first deep learning approach that makes use of language modeling, an important key concept in statistical",
            "and a top layer feeding a softmax classifier. The sentence embedding model sums the Recurrent neural network (RNN) embeddings of its words. The word embeddings are first initialized with pretrained GloVe vectors. This adaptation provided the best performance in previous experiments with RQE data. GloVe is an unsupervised learning algorithm to generate vector representations for words BIBREF42 ."
        ]
    },
    {
        "title": "Argumentation Mining in User-Generated Web Discourse",
        "answer": "lexical and semantic challenges of domain-specific data, insufficient domain knowledge, insufficient validation of general-purpose dictionaries, ultra-fine-grained classes, changing classes over time, and different aspects such as topic, emotion, etc.  compound classes.  different overall goals.  insufficient domain knowledge possessed by crowd workers.  insufficient validation of general-purpose dictionaries.  ultra-fine-grained classes.  changing classes over time.  different aspects such as topic, emotion, etc.  compound classes.  different overall goals.  insufficient domain knowledge possessed by crowd workers.  insufficient validation of general-purpose dictionaries.  ultra-fine-grained classes.  changing classes over time.  different aspects such as topic, emotion, etc.  compound classes.  different overall goals.  insufficient domain knowledge possessed by crowd workers.  insufficient validation of general-purpose dictionaries.  ultra-fine-grained classes.  changing classes over time.  different aspects such as topic, emotion, etc.  compound classes.  different overall goals.  insufficient domain knowledge possessed by crowd workers.  insufficient validation of general-purpose dictionaries.  ultra-fine-grained classes.  changing classes over time.  different aspects such as topic, emotion, etc.  compound classes.  different overall goals.",
        "evidence": [
            "Topics and registers",
            "With the availability of rich data on users' locations, profiles and search history, personalization has become the leading trend in large-scale information retrieval. However, efficiency through personalization is not yet the most suitable model when tackling domain-specific searches. This is due to several factors, such as the lexical and semantic challenges of domain-specific data that often",
            "and INLINEFORM8 . On the other hand, there is the elevated or lifted domain of verification problem classes, which are Y and N. The training phase of binary-intrinsic approaches is used for learning to distinguish these two classes, and the verification task can be understood as putting the verification problem as a whole into class Y or class N, whereby the class domain of authors fades from the",
            "require different number of attentions. This is highly dependent on each dataset's nature and the underlying interconnections between modalities.",
            "and finding better stopping criteria for zero-resource cross-lingual tasks besides using the English dev set.",
            "templates about the entity page structure (e.g. diseases have a treatment section). For a new entity page, first, they extract documents via Web search using the entity title and the section title as a query, for example `Lung Cancer'+`Treatment'. As already discussed in the introduction, this has problems with reproducibility and maintainability. However, their main focus is on identifying the",
            "workers often fail to mark target tokens, but do not tend to produce large numbers of false positives. We suspect such failures to identify relevant spans/tokens are due to insufficient domain knowledge possessed by crowd workers. The F1 score achieved after re-annotating the 600 most-difficult articles reaches 68.1%, which is close to the performance when re-annotating 1000 random articles. This",
            "that incorporating information about answerability can help in this difficult domain. We examine this challenging phenomena of unanswerability further in Section .",
            "are many off-the-shelf dictionaries available (e.g., LIWC BIBREF39 ). These are often well-validated, but applying them on a new domain may not be appropriate without additional validation. Corpus- or domain-specific dictionaries can overcome limitations of general-purpose dictionaries. The dictionaries are often manually compiled, but increasingly they are constructed semi-automatically (e.g.,",
            "we explained which challenges exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus was to focus on a different aspect of the methods applicability. Our findings regarding the examined approaches can be summarized as follows: Despite of the good performance of the five AV",
            "recognition of bank consumers. $\\textsc {0shot-tc}$ is challenging because we often have to deal with classes that are compound, ultra-fine-grained, changing over time, and from different aspects such as topic, emotion, etc. Existing $\\textsc {0shot-tc}$ studies have mainly the following three problems.",
            "chose these videos because of the amount of procedural information stored in each video for which the user may ask. Though there is only one domain, each video corresponds to a different overall goal."
        ]
    },
    {
        "title": "Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model",
        "answer": "general patterns of the input.",
        "evidence": [
            "What Does Zero-shot Transfer Model Learn? ::: Unseen Language Dataset",
            "The model",
            "models used in the experiments are trained with batch size 20, using adam with learning rate INLINEFORM0 and the early stop strategy. The dimension of the hidden state is set to 96 for all layers, and the number of self-attention heads is set to 2. The setup is slightly different but better than the setting suggested by the original QAnet.",
            "Do models generalize explicit supervision, or just memorize it? ::: Setup",
            "model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure FIGREF14. These plots were obtained based on the E2E training set using the inputless setting. We can see that the KL loss of VAE-LSTM-base, which uses Sigmoid annealing BIBREF0, collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss. The KL loss for both VAE-CNN",
            "model can achieve much more stable training process and can generate text with significantly better quality.",
            "the learning rate for different parameters at different steps. Thus it is less sensitive to initial parameters than the stochastic gradient descent.",
            "comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being directly exposed to the real samples. Instead, this model learns to reproduce the general patterns of the input via its interaction with the discriminator. The role of the discriminator, in turn, is to tell apart which data points are",
            "An entailment model for @!START@$\\textsc {0shot-tc}$@!END@ ::: Entailment model learning.",
            "dynamics. RQ4: whether different tasks and datasets require different numbers of attentions. RQ1: MARN (no MAB) model can only learn simple rules among modalities such as decision voting or simple co-occurrence rules such as Tensor Fusion baseline. Across all datasets, MARN (no MAB) is outperformed by MARN. This indicates that continuous modeling of cross-view dynamics is crucial in understanding",
            "given the original two-choice question, if T5 outputs the same tokens for the two processed inputs, we simply assign Option1 as the answer. The table also reports “zero-shot” performance, i.e., performing inference on the development set without any model fine tuning. Condition #2 represents our submission to the official leaderboard, which achieves 0.7673 AUC on the held-out test set. From these",
            "in the full label set. This is the most basic setup in $\\textsc {0shot-tc}$. It checks whether the system can generalize to some labels in the same aspect. To satisfy Definition-Wild, we define a new evaluation: ii) Label-fully-unseen evaluation. In this setup, we assume the system is unaware of the upcoming aspects and can not access any labeled data for task-specific training."
        ]
    },
    {
        "title": "Automated News Suggestions for Populating Wikipedia Entity Pages",
        "answer": "salience feature group, relative authority feature, authority and novelty features.",
        "evidence": [
            "as an exponentially decaying function based on the positional index of the paragraph where the entity appears. The performance of INLINEFORM0 with relative entity frequency from the salience feature group is close to that of all the features combined. The authority and novelty features account to a further improvement in terms of precision, by adding roughly a 7%-10% increase. However, if both",
            "Visualizing attention and conflict",
            "languages in order to learn word representations that distinguish between true similarity and conceptual relatedness (see the discussion in §SECREF6)?",
            "Authority. The news domain authority addresses two main aspects. Firstly, if bundled together with the relative authority feature, we can ensure that dependent on the entity authority, we suggest news from authoritative sources, hence ensuring the quality of suggested articles. The second aspect is in a news streaming scenario where multiple news domains report the same event — ideally only",
            "twice in a document. Extensive literature has shown that subject and object relations are a strong signal for salience and it follows from the Centering Theory that you want to avoid rough shifts in the center BIBREF9 , BIBREF10 . B&L thus focus on whether a salient entity is a subject (s), object (o), other (x), or is not present (-) in a given sentence, as illustrated in Table TABREF1 . Every",
            "Word Relevance and Vector-Based Document Representation",
            "selection as part of the semantic parsing problem. We use the features from reddylargescale2014. These include edge alignments and stem overlaps between ungrounded and grounded graphs, and contextual features such as word and grounded relation pairs. In addition to these features, we add two new real-valued features – the paraphrase classifier's score and the entity disambiguation lattice score.",
            "performs surprisingly well, this example has no features in common, and a BOW representation would assign low similarity scores or high distances. When results are projected onto a two dimensional space, language relationships surface, such as the clustering of synonyms, antonyms, scales (e.g. democracy to authoritarianism), hyponym-hypernyms (e.g. democracy is a type of regime), co-hyponyms",
            "architectures used for textual data, it is often unclear how and why a deep learning model reaches its decisions. There are a few attempts toward explaining/interpreting deep learning-based models, mostly by visualizing the representation of words and/or hidden states, and their importances (via saliency or erasure) on shallow tasks like sentiment analysis and POS tagging BIBREF0 , BIBREF1 ,",
            "Entity-grid model. Typical lexical features for AA are relatively superficial and restricted to within the same sentence. F&H14 hypothesize that discourse features beyond the sentence level also help authorship attribution. In particular, they propose an author has a particular style for representing entities across a discourse. Their work is based on the entity-grid model of BIBREF6 (henceforth",
            "models the contradicting associations like \"animal\" and \"lake\" or \"australia\" and \"world\". These two associations are the unique pairs which are instrumental in determining that the two queries are not similar.",
            "kinds of entity, but it might be needed to redefine the features according to the case considered. Similarly, it has not been found a particular advantage of using the pre-trained embeddings instead of the one trained with our corpora. Furthermore, we verified the statistical significance of our experiment by using Wilcoxon Rank-Sum Test, obtaining that there have been not significant difference"
        ]
    },
    {
        "title": "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
        "answer": "English.",
        "evidence": [
            "Translating Data",
            "training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not.",
            "the source sequence is in the wrong script for the language, as is seen in the entry for Arabic.",
            "Language Resource References\nlrec lrec2018",
            "to our experiments, however, we cannot use these corpora, due to the absence of relevant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our three",
            "Results and Analysis",
            "Raw corpus statistics",
            "translated into Spanish. This new set of “Spanish” data was then added to our original training set. Again, the machine translation platform Apertium BIBREF5 was used for the translation of the datasets.",
            "Methodology ::: Data presentation",
            "Data Set",
            "Multilingual experiments",
            "In-domain and out-of-domain data"
        ]
    },
    {
        "title": "Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation",
        "answer": "Japanese-Vietnamese parallel corpus.",
        "evidence": [
            "For Mix-Source, instead of using a subsampled monolingual corpus, we use the Vietnamese part of the Japanese-Vietnamese parallel corpus in order to learn the multilingual information in the same domain. Our datasets are listed in Table TABREF14 .",
            "Emotion datasets",
            "extremely well, surpassed by MarMoT on only 3 out of 16 datasets (Czech, French and Italian), and by MElt only once (Indonesian).",
            "Vietnamese Tokenization",
            "and adjectives. To address the problem of the lack of a dataset for evaluating Japanese distributed word representations, we propose to build a Japanese dataset for the word similarity task. The main contributions of our work are as follows:",
            "in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\"",
            "fine-tuned on the development set. Also, we adopt oversampling on SVMs, CNN and RCNN because the FBFans dataset is highly imbalanced.",
            "Japanese-Vietnamese MT is firstly mentioned in 2005 BIBREF18 . The authors focused on the difference from embedding structures between Japanese and Vietnamese, and then proposed rules for MT system and experiment on very small dataset (714 Japanese embedding sentences). This approach is suitable for small system applied in a specific domain or language, but it is not easily extendable to other",
            "The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics.",
            "baselines: the CRF-based model with n-gram embedding, which is currently the state-of-the-art for Thai sentence segmentation, and the Bi-LSTM-CRF model, which is currently the deep learning state-of-the-art approach for sequence tagging.",
            "training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not.",
            "using both BPE methods (4), we obtained a bigger gain of 1.15 BLEU points. The similar improvements can be found in the Japanese INLINEFORM1 Vietnamese as well: 0.29 BLEU points between (3) and (2) and 0.57 BLEU points between (4) and (3). This draws two conclusions: (i), despite using an unsupervised Vietnamese word segmentation which is fast, robust and does not require linguistic resources,"
        ]
    },
    {
        "title": "Ancient-Modern Chinese Translation with a Large Training Dataset",
        "answer": "arXiv and PubMed.",
        "evidence": [
            "Ancient-Modern Chinese Dataset",
            "of long and structured scientific papers obtained from arXiv and PubMed. These two new datasets contain much longer documents than all the news datasets (See Table TABREF6 ) and are therefore ideal test-beds for the method we present in this paper.",
            "for different topics, languages, data distributions, and platforms. The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post",
            "From gender representation in data to gender bias in AI ::: On the importance of data",
            "The TDC datasets are available in the CAPES open data website divided by years, from 2013 to 2016 in CSV and XLSX formats. We downloaded all CSV files from the respective website and loaded them into an SQL database for better manipulation. The database was then filtered to remove documents without both Portuguese and English abstracts, and additional metadata selected. After the initial",
            "affinity. A core issue with the current datasets concerns a lack of one unified procedure that ensures the comparability of resources in different languages. Further, concept pairs for different languages are sourced from different corpora (e.g., direct translation of the English data versus sampling from scratch in the target language). Moreover, the previous SimLex-based multilingual datasets",
            "by providing standardized datasets, evaluations, and a state-of-the-art entailment system. All datasets and codes are released.",
            "the words that have been matched will be deleted from the original clauses. However, some ancient characters do not appear in its corresponding modern Chinese words. An ancient Chinese dictionary is employed to address this issue. We preprocess the ancient Chinese dictionary and remove the stop words. In this dictionary matching step, we retrieve the dictionary definition of each unmatched",
            "2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}. For dialects, the",
            "Weibo, RenRen, and Chinese microblogs",
            "Data Set",
            "training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not."
        ]
    },
    {
        "title": "UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish",
        "answer": "CVT.",
        "evidence": [
            "Semi-supervised Learning",
            "can be applied to other classification tasks, such as topic classification and concept mining. The experiments show that even unsupervised approaches, as in the corpus-based approach, can outperform supervised approaches in classification tasks. Combining some approaches, which can compensate for what others lack, can help us build better vectors. Our word vectors are created by conventional",
            "and supervised learning based approach to reinforcement learning based approach BIBREF15 . The NLG technology is through pattern-based approach, sentence planning approach and end-to-end deep learning approach BIBREF16 , BIBREF17 , BIBREF18 . In application, there are massive products that are based on the technology of human-computer dialogue, such as Apple Siri, Amazon Echo, Microsoft Cortana,",
            "training, both the supervised and semi-supervised models are trained until the validation metrics stop improving; the metrics are (1) sentence boundary F1 score and (2) overall F1 score for Thai sentence segmentation and English punctuation restoration, respectively. CVT has three main parameters that impact model accuracy. The first is the drop rate of the masked language model, which determines",
            "Do models generalize explicit supervision, or just memorize it?",
            "We propose a simple domain adaptation method for neural networks in a supervised setting. Supervised domain adaptation is a way of improving the generalization performance on the target domain by using the source domain dataset, assuming that both of the datasets are labeled. Recently, recurrent neural networks have been shown to be successful on a variety of NLP tasks such as caption generation;",
            "n-gram presentations was the main contributing factor for Thai, while the semi-supervised training helped the most for English.",
            "by utilising emoticons as features. Most of our approaches do not rely on a neural network model in learning embeddings. However, they produce state-of-the-art results.",
            "is presented in BIBREF0 that employ the Stochastic Gradient Descent (SGD).",
            "social science, we may want to predict voting behavior of a legislator with the goal of inferring ideological positions from such behavior. In development, interest may center around characteristics that predict successful completion of a training program based on a beneficiary's previous experience or demographic characteristics. Supervised learning requires human coding - data must be read and",
            "However, most current supervised summarization systems often perform the two tasks in isolation. Usually, they design query-dependent features (e.g., query word overlap) to learn the relevance ranking, and query-independent features (e.g., term frequency) to learn the saliency ranking. Then, the two types of features are combined to train an overall ranking model. Note that the only supervision",
            "were collected from January 2017 to December 2017. The labeled dataset includes 48,374 passages. To support semi-supervised learning, the first 3 months of data (96,777 passages) are unlabeled. Because the data stem from social media, some text exists that cannot be considered as part of any sentence, such as product links, symbols unrelated to sentences, and space between sentences. These"
        ]
    },
    {
        "title": "Conflict as an Inverse of Attention in Sequence Relationship",
        "answer": "SQuAD, question answering, and dialog understanding.",
        "evidence": [
            "it operates under the assumption that there is always a match to be found inside the sequences. We provide a theoretical limitation to it and propose another technique called conflict that looks for contrasting relationship between words in two sequences. We empirically verify that our proposed conflict mechanism combined with attention can outperform the performance of attention working solely.",
            "phrases. Technical issues involved dropped sessions (e.g. WebRTC connections failed) or cases in which the user could not hear the agent or vice-versa. In addition, weekly meetings were held with the agents to answer questions and gather feedback on their experiences. Agents typically work four hours per day with dialog types changing every hour. Crowdsourced workers playing the user are accessed",
            "Traditional Methodologies",
            "in the test and development set. The workers are shown with the tweet blocks as well as the questions collected in the previous step. At this step, workers are allowed to label the questions as “NA\" if they think the questions are not answerable. We find that INLINEFORM0 of the questions are labeled as unanswerable by the workers (for SQuAD, the ratio is INLINEFORM1 ). Since the answers collected",
            "Conflict model",
            "framework. Chambliss1995 experimented with analyzing 20 written documents in a classroom setting in order to find the argument patterns and parts. Simosi2003 examined employees' argumentation to resolve conflicts. Voss2006 analyzed experts' protocols dealing with problem-solving. The model has also been used in research on computer-supported collaborative learning. Erduran2004 adapt Toulmin's",
            "on the outcome network. In our case, the outcome network is the conflict onset network, and we perform 50 simulations over each of the 30 time steps for 1,500 total simulations for each of the four models. The in-sample areas under the ROC and PR curves for the three models of primary concern are presented in Figure FIGREF36 and Figure FIGREF37 , respectively. Furthermore, the dyad-wise and",
            "Task, 30 users were tasked to first browse the collection of videos, select interesting segments with start and end times, and then asked to conjecture questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves,",
            "that part of the methods are able to cope with very challenging verification cases such as 250 characters long informal chat conversations (72.7% accuracy) or cases in which two scientific documents were written at different times with an average difference of 15.6 years (>75% accuracy). However, we also identified that all involved methods are prone to cross-topic verification cases.",
            "blocs as substitutes for the original vote clusters and report these as Model 3 and 4, respectively. The results of Model 3 in [table:tergm]Table 1 indicate that the text-based clusters exhibit a noticeably larger effect of preference similarity on conflict avoidance compared to the vote-based clusters. Interestingly, the significance of joint democracy falls off, as well as the effects of the",
            "as agreement, disagreement, or supportive behavior) to infer the social goals of dialog participants and presented a semi-supervised model for their classification. Their social act types were inspired by research in psychology and organizational behavior and were motivated by work in dialog understanding. They annotated a corpus in three languages using in-house annotators and achieved",
            "acts as an effective procedure to reduce mismatch between training and evaluation conditions BIBREF29, BIBREF26."
        ]
    },
    {
        "title": "A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification",
        "answer": "SVM, NB, KNB, MEMM, CRF, MaxEnt, HLBL, C&W embeddings, Brown clusters.  Naive Bayes (NB), Kernel Naive Bayes (KNB), Support Vector Machine (SVM), Support Vector Machines and Maximum Entropy Model (MaxEnt), HLBL embeddings, C&W embeddings, Brown clusters.  Naive Bayes (NB), Kernel Naive Bayes (KNB), Support Vector Machine (SVM), Support Vector Machines and Maximum Entropy Model (MaxEnt), HLBL embeddings, C&W embeddings, Brown clusters.  Naive Bayes (NB), Kernel Naive Bayes (KNB), Support Vector Machine (SVM), Support Vector Machines and Maximum Entropy Model (MaxEnt), HLBL embeddings, C&W embeddings, Brown clusters.  Naive Bayes (NB), Kernel Naive Bayes (KNB), Support Vector Machine (SVM), Support Vector Machines and Maximum Entropy Model (MaxEnt), HLBL embeddings, C&W embeddings, Brown clusters.  Naive Bayes (NB), Kernel Naive Bayes (KNB), Support Vector Machine (SVM), Support Vector Machines and Maximum Entropy Model (MaxEnt), HLBL embeddings, C&W",
        "evidence": [
            "Dictionary-based approaches",
            "Although intrinsic validation can be used to compare relevance decomposition methods for a given ML model, this approach is not suited to compare the explanatory power of different ML models, since the latter requires a common evaluation basis. Furthermore, even if we would track the classification performance changes induced by different ML models using an external classifier, it would not",
            "of the MARN to speakers communicating in different languages, we compare with state-of-the-art approaches for sentiment analysis on MOUD, with opinion utterance video clips in Spanish. The final third of Table 2 shows these results where we also achieve significant improvement over state-of-the-art approaches.",
            "the question to an appropriate answer type BIBREF14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in BIBREF16 and BIBREF0. Many classifiers have been used in machine learning for QC such as Support Vector Machine (SVM) BIBREF16 BIBREF17, Support Vector Machines and Maximum Entropy Model BIBREF18, Naive Bayes (NB), Kernel Naive Bayes (KNB),",
            "inspired by studies that compared the performance of different word embedding versions or investigated the combination of them. For example, turian2010word compared Brown clusters, C&W embeddings and HLBL embeddings in NER and chunking tasks. They found that Brown clusters and word embeddings both can improve the accuracy of supervised NLP systems; and demonstrated empirically that combining",
            "can be rated as reliable. Moreover, we clarified a number of misunderstandings in previous research works and proposed three clear criteria that allow to classify the model category of an AV method, which in turn influences its design and the way how it should be evaluated. In regard to binary-extrinsic AV approaches, we explained which challenges exist and how they affect their applicability. In",
            "web pages to find the answers. However, the results were lower than those achieved by the systems relying on finding similar answered questions. These results support the relevance of similar question matching for the end-to-end QA task as a new way of approaching QA instead of the classical QA approaches based on Question Analysis and Answer Retrieval.",
            "and evaluate the “fairness-aware\" ML interface called themis-ml. In this interface, the main idea is to pick up a data set from a modified dataset. Themis-ml implements two methods for training fairness-aware models. The tool relies on two methods to make agnostic model type predictions: Reject Option Classification and Discrimination-Aware Ensemble Classification, these procedures being used to",
            "Models for Comparison",
            "type restrictions and generating all types of errors, our pattern-based method consistently outperformed the system by Felice2014a. The combination of the pattern-based method with the machine translation approach gave further substantial improvements and the best performance on all datasets.",
            "In the first experiment, we evaluated the DL and ML methods on SNLI, multi-NLI, Quora, and Clinical-QE. For the datasets that did not have a development and test sets, we randomly selected two sets, each amounting to 10% of the data, for test and development, and used the remaining 80% for training. For MultiNLI, we used the dev1-matched set for validation and the dev2-mismatched set for testing.",
            "is to compare the relative improvements linked to the use of external lexical information in the two feature-based models, which use different models (MEMM vs. CRF) and feature sets. More specifically, our starting point is the MElt system BIBREF12 , an MEMM tagging system. We first briefly describe this system and the way we adapted it by integrating our own set of corpus-based and lexical"
        ]
    },
    {
        "title": "Extractive Summarization of Long Documents by Combining Global and Local Context",
        "answer": "global context: whole document; local context: surrounding words or adjacent mentions.",
        "evidence": [
            "topic information (i.e. local context) is added. And the improvement is even greater when we only consider long documents. Rather surprisingly, this is not the case for the global context. Adding a representation of the whole document (i.e. global context) never significantly improves performance. In essence, it seems that all the benefits of our model come exclusively from modeling the local",
            "Local Features",
            "Interactive Dialog Context LM",
            "languages, including but not limited to cultural context, polysemy, metonymy, translation, regional and generational differences, and most commonly, the fact that words and meanings do not exactly map onto each other across languages. For example, it is likely that the other three languages do not have two separate words for describing the concepts in the concept pair: big – large, and the",
            "Preliminaries and Framework",
            "in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\"",
            "Do models generalize explicit supervision, or just memorize it?",
            "Comparison of CNN and n-gram models for local representation",
            "source of contextual information for the translation task. Additionally, it is possible to leverage this kind of information in an end-to-end fashion.",
            "As shown in Figure FIGREF10 , for the current mention England, we utilize its surrounding words as local contexts (e.g., surplus), and adjacent mentions (e.g., Hussian) as global information. Collectively, we utilize the candidates of England INLINEFORM0 as well as those entities of its adjacencies INLINEFORM1 to construct feature vectors for INLINEFORM2 and the subgraph of relatedness as inputs",
            "about the connection between large-scaled deep neural language model and certain kinds of commonsense.",
            "together with web-crawled evidence documents from Wikipedia and Bing. While a majority of questions require world knowledge for finding the correct answer, it is mostly factual knowledge."
        ]
    },
    {
        "title": "Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets",
        "answer": "Accuracy, F1-score, c@1, AUC, AUC@1, EER, ROUGE-2, F1-score.",
        "evidence": [
            "Evaluation Metrics",
            "the data are discussed and applications which use a bag-of-words treatment are presented. Second, the vector space approach and distance measure are introduced and discussed. Finally, the network clustering approach and model evaluations are presented.",
            "According to our extensive literature research, numerous measures (e. g., Accuracy, F INLINEFORM0 , c@1, AUC, AUC@1, INLINEFORM1 or EER) have been used so far to assess the performance of AV methods. In regard to our experiments, we decided to use c@1 and AUC for several reasons. First, Accuracy, F INLINEFORM2 and INLINEFORM3 are not applicable in cases where AV methods leave verification",
            "answers are filtered and evaluated in answer evaluation phase as there can be multiple possible answers for a query. In the final step, an answer of the question is returned.",
            "For evaluation, we adopt the widely-used automatic evaluation metric ROUGE BIBREF14 . It measures the summary quality by counting the overlapping units such as the n-grams, word sequences and word pairs between the peer summary and reference summaries. We take ROUGE-2 as the main measures due to its high capability of evaluating automatic summarization systems BIBREF19 . During the training data",
            "measures for user information Data Retention: How long user information will be stored User Choice/Control: Control options available to users User Access, Edit and Deletion: If/how users can access, edit or delete information Policy Change: Informing users if policy information has been changed International and Specific Audiences: Practices pertaining to a specific group of users Other: General",
            "Experimental Setup\nBelow, we give details on the evaluation dataset and baselines used for comparison. We also describe the model features and provide implementation details.",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\",",
            "in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\"",
            "Benchmark the evaluation ::: Label-fully-unseen.",
            "The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric.",
            "Cross-Lingual Evaluation ::: Results and Discussion"
        ]
    },
    {
        "title": "Creative GANs for generating poems, lyrics, and metaphors",
        "answer": "SnapCaptions Dataset, Ancient-Modern Chinese Dataset, TutorialVQA Dataset, Basis, and ROUGE.",
        "evidence": [
            "Datasets",
            "Models Used in the Evaluation",
            "The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics.",
            "algorithm. The two datasets annotated with extractive labels will be made public.",
            "require different number of attentions. This is highly dependent on each dataset's nature and the underlying interconnections between modalities.",
            "and development datasets are also available on another Github repositorypage BIBREF13 for further research and comparison purposes.",
            "Data details and experimental setup",
            "TutorialVQA Dataset ::: Basis",
            "Dictionary-based approaches",
            "Datasets Used for the RQE Study",
            "Results: SnapCaptions Dataset",
            "Ancient-Modern Chinese Dataset"
        ]
    },
    {
        "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text",
        "answer": "S1, S2, Baseline 1, Baseline 2, Segmenter$_{\\mu }$, elementary system, Moses, NMT, Baseline Agent, Baseline 1, Baseline 2, S1, S2, Random Forests (RF)  and elementary system Segmenter$_{\\mu }$  and Baseline 1 and Baseline 2 and Baseline Agent and Moses and NMT and elementary system Segmenter$_{\\mu }$ and S1 and S2 and Random Forests (RF) and Baseline 1 and Baseline 2 and Baseline Agent and Moses and NMT and elementary system Segmenter$_{\\mu }$ and S1 and S2 and Random Forests (RF) and Baseline 1 and Baseline 2 and Baseline Agent and Moses and NMT and elementary system Segmenter$_{\\mu }$ and S1 and S2 and Random Forests (RF) and Baseline 1 and Baseline 2 and Baseline Agent and Moses and NMT and elementary system Segmenter$_{\\mu }$ and S1 and S2 and Random Forests (RF) and Baseline 1 and Baseline 2 and Baseline Agent and Moses and NMT",
        "evidence": [
            "Baseline",
            "Besides the analysis on different reasoning types, we also look into the performance over questions with different first tokens in the development set, which provide us an automatic categorization of questions. According to the results in Table TABREF46 , the three neural baselines all perform the best on “Who” and “Where” questions, to which the answers are often named entities. Since the tweet",
            "that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "Preliminaries and Framework",
            "The elementary system Segmenter$_{\\mu }$ (baseline) relies solely on a list of discursive markers to perform the segmentation. It replaces the appearance of a marker in the list with a special symbol, for example $\\mu $, which indicates a boundary between the right and left segment. Be the sentence of the preceding example: La ville d'Avignon est la capitale du Vaucluse, qui est un département du",
            "Baseline Agent ::: Memory and Reward Shaping ::: Memory",
            "of the baseline model, we resume training with a 2M sentence in-domain corpus mixed with an equal amount of randomly selected out-of-domain natural sentences, with the same architecture and training parameters, running validation every 2000 updates with a patience of 10. Since BPE units are selected based only on the out-of-domain statistics, fine-tuning is performed on sentences that are",
            "TutorialVQA Dataset ::: Basis",
            "of the 3 approaches is provided in Fig. FIGREF23 . Baseline 1: We captured and displayed the product package photos and the product title text as product description. Baseline 2: The product description was retrieved by search engine using the product titles, and then presented to the users as the top images result from Google and Bing. We also provided the product title along with the images.",
            "Dictionary-based approaches",
            "Experiments ::: Label-partially-unseen evaluation ::: Baselines.",
            "templates. Baselines. To the best of our knowledge, we are not aware of any comparable approach for this task. Therefore, the baselines we consider are the following: S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2  S2: Place the news into the most frequent section in INLINEFORM0  Learning Models. We use Random Forests (RF)"
        ]
    },
    {
        "title": "Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity",
        "answer": "manually cleaned, sampled, and annotated with maximum entropy classifier, BIO, and different rating scales.",
        "evidence": [
            "Data Collection and Annotation",
            "data were artificially created for their task and manually cleaned, such as removing segments that did not meet the criteria or non-argumentative segments. In the first step of their two-phase approach, Goudas.et.al.2014 sampled the dataset to be balanced and identified argumentative sentences with INLINEFORM0 0.77 using the maximum entropy classifier. For identifying premises, they used BIO",
            "for translation and annotation, as well as different rating scales. For instance, some datasets were obtained by directly translating the English SimLex-999 in its entirety BIBREF25, BIBREF16 or in part BIBREF28. Other datasets were created from scratch BIBREF26 and yet others sampled English concept pairs differently from SimLex-999 and then translated and reannotated them in target languages",
            "and annotated as: Location, Position/Direction, Object, Time Guidance, Person, Gesture/Gaze (e.g., `this', `that', `over there', etc.), and None/O. In addition to utterance-level intents and slots, word-level intent related keywords are annotated as Intent. We obtained 1331 utterances having commands to AMIE agent from our in-cabin dataset. We expanded this dataset via the creation of similar",
            "Three annotators with near-native English proficiency annotated a set of 990 documents (a random subset of comments and forum posts) reaching 0.59 Fleiss' INLINEFORM0 . The final label was selected by majority voting. The annotation study took on average of 15 hours per annotator with approximately 55 annotated documents per hour. The resulting labels were derived by majority voting. Out of 990",
            "used several extraordinary annotators. It is crucial to filter insincere annotators and provide straightforward instructions to improve the quality of the similarity annotation like we did. To gain better similarity, each dataset should utilize the reliability score to exclude extraordinary annotators. For example, for SCWS, an annotator rating the similarity of pair of “CD” and “aglow” assigned",
            "Datasets Used for the RQE Study",
            "sentences (830 positive, 280 negative and 7,626 objective) for testing.  The second dataset (dataset-implicit) was used for evaluating implicit citation classification, containing 200,222 excluded (x), 282 positive (p), 419 negative (n) and 2,880 objective (o) annotated sentences. Every sentence which does not contain any direct or indirect mention of the citation is labeled as being excluded (x)",
            "require different number of attentions. This is highly dependent on each dataset's nature and the underlying interconnections between modalities.",
            "algorithm. The two datasets annotated with extractive labels will be made public.",
            "can serve as an answer to any question. For example, Fig. FIGREF1 shows example segments marked in red (each which are a complete unit as an answer span). Each sentence is associated with the starting and ending time-stamps, which can be used to access the relevant visual information. The dataset contains 6,195 non-factoid QA pairs, where the answers are the segments that were manually annotated.",
            "on the reannotated difficult subset and the random subset. The first two rows show the results for models trained with expert annotations. The model trained on random data has a slightly better F1 than that trained on the same amount of difficult data. The model trained on random data has higher precision but lower recall. Rows 3 and 4 list the results for models trained on the same data but with"
        ]
    },
    {
        "title": "Multi-attention Recurrent Network for Human Communication Comprehension",
        "answer": "Hybrid Memory allows each modality's LSTHM to model both view-specific and cross-view dynamics through time. LSTMs lack this capability.",
        "evidence": [
            "Long-short Term Hybrid Memory",
            "The main accuracy comparison across target constructions for different settings is presented in Table main. We first notice that our baseline LSTM-LMs (Section lm) perform much better than BIBREF0's LM. A similar observation is recently made by BIBREF6. This suggests that the original work underestimates the true syntactic ability induced by LSTM-LMs. The table also shows the results by their",
            "vocabularies, the softmax would likely dominate computation time. It is also important to note that the cuDNN library's RNN primitives do not natively support any form of recurrent dropout. That is, running an LSTM that uses a state-of-the-art regularization scheme at cuDNN-like speeds would likely require an entirely custom kernel.",
            "HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text.",
            "Hybrid-1: RNN (Seq2seq Bi-LSTM for intent keywords extraction) + Rule-based (mapping extracted intent keywords to utterance-level intents) Hybrid-2: RNN (Seq2seq Bi-LSTM for intent keywords & slots extraction) + Rule-based (mapping extracted intent keywords & ‘Position/Direction’ slots to utterance-level intents) Hybrid-3: RNN (Seq2seq Bi-LSTM for intent keywords & slots extraction) + Rule-based",
            "LSTM-Minus",
            "modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time.",
            "LSTM architecture is specified by the following equations: DISPLAYFORM0   where INLINEFORM0 and INLINEFORM1 denote the weight matrices and bias terms, respectively. The sigmoid ( INLINEFORM2 ) and INLINEFORM3 are activation functions applied element-wise, and INLINEFORM4 denotes the element-wise vector product. LSTM has a memory vector INLINEFORM5 to read/write or reset using a gating mechanism",
            "Limitations of LSTM-LMs ::: Setup",
            "MARN is distinguishable from previous approaches in that it explicitly accounts for both view-specific and cross-view dynamics in the network architecture and continuously models both dynamics through time. In MARN, view-specific dynamics within each modality are modeled using a Long-short Term Hybrid Memory (LSTHM) assigned to that modality. The hybrid memory allows each modality's LSTHM to",
            "including document-level sentiment classification, language modeling, and character-level machine translation. These models outperform strong LSTM baselines on all three tasks while dramatically reducing computation time.",
            "Hybrid Memory (LSTHM) and Multi-attention Block (MAB) Formulation [1] $\\textrm {MARN}$ $\\mathbf {X}^m$ $c_0,h_0,z_0 \\leftarrow \\mathbf {0}$ $t = 1, ..., T$ : $h_t \\leftarrow \\textrm {LSTHM\\_Step} (\\bigcup _{m \\in M} \\lbrace  x^m_t\\rbrace , z_{t-1})$ $z_t \\leftarrow \\textrm {MAB\\_Step} (h_t)$ $h_T, z_T$   $\\textrm {LSTHM\\_Step}$ $\\bigcup _{m \\in M} \\lbrace {x}^m_t\\rbrace $ , $z_{t-1}$ $m \\in M$ :"
        ]
    },
    {
        "title": "Construction of a Japanese Word Similarity Dataset",
        "answer": "Fairy Tales dataset, TutorialVQA Dataset, datasets used for the RQE study.  In-domain and out-of-domain data.  Data Collection and Annotation.  Data details and experimental setup.  Research questions.  Data Collection.  Datasets Used for the RQE Study.  Data Collection and Annotation.  In-domain and out-of-domain data.  Data details and experimental setup.  Research questions.  Data Collection.  TutorialVQA Dataset.  Fairy Tales dataset.  datasets used for the RQE study.  In-domain and out-of-domain data.  Data Collection and Annotation.  Data details and experimental setup.  Research questions.  Data Collection.  TutorialVQA Dataset.  Fairy Tales dataset.  datasets used for the RQE study.  In-domain and out-of-domain data.  Data Collection and Annotation.  Data details and experimental setup.  Research questions.  Data Collection.  TutorialVQA Dataset.  Fairy Tales dataset.  datasets used for the RQE study.  In-domain and out-of-domain data.  Data Collection and Annotation.  Data details and experimental setup.  Research questions.  Data Collection.  TutorialVQA Dataset.  Fairy Tales dataset.  datasets used for the RQE study",
        "evidence": [
            "Data Statistics",
            "based on full tags. This is why we only report results based on (coarse) PoS extracted from the original lexicons.",
            "From gender representation in data to gender bias in AI ::: On the importance of data",
            "Overview of datasets and emotions",
            "In-domain and out-of-domain data",
            "TutorialVQA Dataset ::: Data Collection",
            "Datasets Used for the RQE Study",
            "Data Collection and Annotation",
            "Research questions",
            "Data details and experimental setup",
            "Fairy Tales dataset",
            "Methodology ::: Data presentation"
        ]
    },
    {
        "title": "Assessing the Applicability of Authorship Verification Methods",
        "answer": "None mentioned.  However, a self-dialog corpus is mentioned.  It is a corpus created by crowdsourced workers writing the entire dialog themselves.  Another type of corpus mentioned is a self-learning corpus, which is a corpus created by \"self-learning\" method to generate the target sentences based on monolingual data on the source-side.  A self-compiled corpus is not explicitly mentioned.  However, a self-compiled dictionary is mentioned, which is a dictionary constructed semi-automatically.  A self-compiled dictionary is not the same as a self-compiled corpus.  A self-compiled corpus is not mentioned in the text.  However, a self-compiled dictionary is mentioned.  A self-compiled dictionary is constructed semi-automatically.  A self-compiled corpus is not mentioned.  However, a self-compiled dictionary is mentioned.  A self-compiled dictionary is constructed semi-automatically.  A self-compiled corpus is not mentioned.  However, a self-compiled dictionary is mentioned.  A self-compiled dictionary is constructed semi-automatically.  A self-compiled corpus is not mentioned.  However, a self-compiled dictionary is mentioned.  A self-compiled dictionary is constructed semi-automatically.",
        "evidence": [
            "Corpus statistics",
            "domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken \"Wizard of Oz\" (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is \"self-dialog\" in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a",
            "parsing. Therefore, it is necessary to explore other techniques that are free from hand-crafted features. With the development of neural networks and deep learning, it is possible to learn the representations of concepts from unlabeled text corpus automatically. These representations can be treated as concept features for classification. An important advance in this area is the development of the",
            "like TIMIT BIBREF10, Switchboard BIBREF11 or Fisher BIBREF12 which date back to the early 1990s. The scarceness of available corpora is justified by the fact that gathering and annotating audio data is costly both in terms of money and time. Telephone conversations and broadcast recordings have been the primary source of spontaneous speech used. Out of all the 130 audio resources proposed by LDC",
            "In addition to using the three approaches independently, we also combined all the matrices generated in the previous approaches. That is, we concatenate the reduced forms (SVD - U) of corpus-based, dictionary-based, and the whole of 4-score vectors of each word, horizontally. Accordingly, each corpus word is represented by a 404-dimensional vector, since corpus-based and dictionary-based vector",
            "corpora, entities annotated are only about 5% of the whole amount of tokens. In the case of the automatically generated tweets, the percentage is significantly greater and entities represent about the 50%.",
            "lean and modular architecture.[3] purpleWe evaluate our model and compare with previous works in both extractive and abstractive summarization on two large scientific paper datasets, which contain documents that are much longer than in previously used corpora.[4] purpleOur model not only achieves state-of-the-art on these two datasets, but in an additional experiment, in which we consider",
            "The availability of cross-language parallel corpora is one of the basis of current Statistical and Neural Machine Translation systems (e.g. SMT and NMT). Acquiring a high-quality parallel corpus that is large enough to train MT systems, specially NMT ones, is not a trivial task, since it usually demands human curating and correct alignment. In light of that, the automated creation of parallel",
            "We create a new corpus which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation mining field to date. We choose several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming. A novel aspect of the corpus is its coverage of different registers of user-generated Web content, such as comments to",
            "Answer Collection System",
            "have shown significant improvements by \"self-learning\" method to generate the target sentences based on monolingual data on the source-side and then combined them with original bilingual data to train. BIBREF12 convert monolingual corpus on the target-side into a bitexts by copying target sentences to the source sentence and then combined original bilingual data together on training. Our systems",
            "are many off-the-shelf dictionaries available (e.g., LIWC BIBREF39 ). These are often well-validated, but applying them on a new domain may not be appropriate without additional validation. Corpus- or domain-specific dictionaries can overcome limitations of general-purpose dictionaries. The dictionaries are often manually compiled, but increasingly they are constructed semi-automatically (e.g.,"
        ]
    },
    {
        "title": "Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach",
        "answer": "GRU network, Moses, B1, SOTA, BiDAF, LSTM, UPA, PPA, NMT, MRC, TABREF15, MOOC, INLINEFORM0, INLINEFORM1, INLINEFORM2, INLINEFORM3, INLINEFORM4, INLINEFORM5, INLINEFORM6, INLINEFORM7, INLINEFORM8, INLINEFORM9, INLINEFORM10, INLINEFORM11, INLINEFORM12, INLINEFORM13, INLINEFORM14, INLINEFORM15, INLINEFORM16, INLINEFORM17, INLINEFORM18, INLINEFORM19, INLINEFORM20, INLINEFORM21, INLINEFORM22, INLINEFORM23, INLINEFORM24, INLINEFORM25, INLINEFORM26, INLINEFORM27, INLINEFORM28, INLINEFORM29, INLINEFORM30, INLINEFORM31, INLINEFORM32, INLINEFORM33, INLINEFORM34, INLINEFORM35, INLINEFORM36, INLINEFORM37, INLINEFORM38, INLINEFORM39, INLINEFORM40, INLINEFORM41, INLINEFORM42, INLINEFORM43, INLINEFORM44, INLINEFORM45, INLINEFORM46, INLINEFORM47, INLINEFORM48, INLINEFORM49, INLINEFORM50, INLINEFORM51, INLINEFORM52, INLINEFORM53, INLINEFORM54",
        "evidence": [
            "Baseline Models",
            "in terms of precision, by adding roughly a 7%-10% increase. However, if both feature groups are considered separately, they significantly outperform the baseline B1.",
            "a fast baseline decoder. Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
            "What is the Model Learning?",
            "description with accurate mentions in a more human-like fashion. We would also like to draw attention to the number of parameters used by those architectures. We note that our scenarios relies on a lower number of parameters (14 millions) compared to all baselines (ranging from 23 to 45 millions). This outlines the effectiveness in the design of our model relying on a structure encoding, in",
            "that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "including document-level sentiment classification, language modeling, and character-level machine translation. These models outperform strong LSTM baselines on all three tasks while dramatically reducing computation time.",
            "baseline question answer system. We compare it with a standard MRC model BiDAF, and also provide the difficulty of the dataset and lay out remaining challenges.",
            "Table TABREF15 shows performance of all our proposed models and the neural baseline over our 12 MOOC dataset. Our models of UPA, PPA individually better the baseline by 5 and 2% on INLINEFORM0 and 3 and 6% on recall respectively. UPA performs the best in terms of INLINEFORM1 on average while PPA performs the best in terms of recall on average. At the individual course level, however, the results",
            "RNN-based NMT model",
            "Our baseline is a GRU network for each of the three tasks. We use the same network architecture across the 3 tasks. For each network, the network contains a layer unidirectional GRU, with 500 units and an output linear layer. The network is trained end-to-end. Our input embedding layer is initialized with a standard normal distribution, with $\\mu =0$, and $\\sigma =1$, i.e., $W \\sim N(0,1)$. We",
            "labels. For instance, for a specific phrase in Portuguese (um estado), the baseline model predicted {POS: Det, Gender: Masc, Number: Sing} INLINEFORM0 , {POS: Noun, Gender: Fem (X), Number: Sing} INLINEFORM1 , whereas our model was able to get the gender correct because of the transition factors in our model."
        ]
    },
    {
        "title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU",
        "answer": "seq + null",
        "evidence": [
            "a fast baseline decoder. Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
            "Baseline",
            "Learning and Decoding",
            "baseline, we use only sequential (seq) data as input. For the lexicalized and unlexicalized multi-source systems, two options are considered: seq + seq uses identical sequential data as input to both encoders, while seq + null uses null input for the parsed encoder, where every source sentence is “( )”. The parse2seq system fails when given only sequential source data. On the other hand, both",
            "in terms of precision, by adding roughly a 7%-10% increase. However, if both feature groups are considered separately, they significantly outperform the baseline B1.",
            "encoder layers used INLINEFORM2 . Optimization was performed for 10 epochs on minibatches of 16 examples using Adam BIBREF28 with INLINEFORM3 , INLINEFORM4 , INLINEFORM5 , and INLINEFORM6 . Decoding was performed using beam search with beam width 8 and length normalization INLINEFORM7 . The modified log-probability ranking criterion is provided in the appendix. Results using this architecture",
            "Experiments ::: Tweet-Level Models ::: Baseline GRU.",
            "be it a copy or an actual back-translation, only marginally helps to significantly improve the quality. Finally, in the fwdtrans-nmt setup, freezing the decoder does not seem to harm learning with a natural source.",
            "We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base: A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue BIBREF0; VAE-CNN: A variational autoencoder model with a LSTM encoder and a dilated CNN decoder BIBREF7; vMF-VAE: A variational autoencoder model using",
            "etc. For that, we will only need a list of markers for each language. The performance of the systems remains modest, of course, but we must not forget that this is a baseline and its primary objective is to provide standard systems that can be used in testing protocols such as the one we proposed. Despite this evolution, these baselines (or their improved versions) can be used in applications",
            "Beam-search decoder",
            "phase, we optimize on the creativity reward from the discriminator. The discriminator's encoder has the same architecture as the generator encoder module with the addition of a pooled decoder layer. The decoder contains 3 $[Dense Batch Normalization,ReLU]$ blocks and an addtional $Sigmoid$ layer. The discriminator decoder takes the hidden state at the last time step of a sequence concatenated"
        ]
    },
    {
        "title": "A Stable Variational Autoencoder for Text Modelling",
        "answer": "Exact Match (EM) and Macro-averaged F1 scores (F1) between the predicted text answer and the ground-truth text answer.",
        "evidence": [
            "Sentence alignment quality",
            "in an adversarial manner. The discriminator objective here is to score the quality of the creative text. The discriminator is trained for 3 iterations for every iteration of the generator, a practice seen in previous work BIBREF25. Creative-GAN relies on using the reward from the discriminator BIBREF12, BIBREF15 for backpropagation. We follow a similar training procedure for GumbelGAN. Outputs",
            "been generated. Many historians, for example, work with text produced from an analogue original using Optical Character Recognition (OCR). Often, there will be limited information available regarding the accuracy of the OCR, and the degree of accuracy may even vary within a single corpus (e.g. where digitized text has been produced over a period of years, and the software has gradually improved).",
            "way to evaluate the text answer is to directly compute the Exact Match (EM) and Macro-averaged F1 scores (F1) between the predicted text answer and the ground-truth text answer. We used the standard evaluation script from SQuAD BIBREF1 to evaluate the performance.",
            "words are also generated based on the article representations. Since natural language text is complicated and verbose in nature, and training data is insufficient in size to help the models distinguish important article information from noise, sequence-to-sequence models tend to deteriorate with the accumulation of word generation, e.g., they generate irrelevant and repeated words frequently",
            "23, PTS). Second, we compute three metrics on the extracted information: $\\bullet $ Relation Generation (RG) estimates how well the system is able to generate text containing factual (i.e., correct) records. We measure the precision and absolute number (denoted respectively RG-P% and RG-#) of unique relations $r$ extracted from $\\hat{y}_{1:T}$ that also appear in $s$. $\\bullet $ Content Selection",
            "et al. propose synthesizing images from text descriptions [3]. The group demonstrates how GANs can produce images that correspond to a user-defined text description. It thus seems feasible that by using a similar model, we can produce text samples that are conditioned upon a set of user-specified keywords. We were similarly influenced by the work of Radford et. al, who argue for the importance of",
            "An evaluation of how good a method identifies relevant words in text documents can be performed qualitatively, e.g. at the document level, by inspecting the heatmap visualization of a document, or by reviewing the list of the most (or of the least) relevant words per document. A similar analysis can also be conducted at the dataset level, e.g. by compiling the list of the most relevant words for",
            "model can achieve much more stable training process and can generate text with significantly better quality.",
            "generated by Open-NMT BIBREF21 , INLINEFORM0 BIBREF11 and BiSET under three settings, respectively, and 3 randomly-selected summaries for trapping. We asked the evaluators to independently rate each summary on a scale of 1 to 5, with respect to its quality in informativity, conciseness, and readability. While collecting the results, we rejected the samples in which more than half evaluators rate",
            "to determine how different sentences and EDUs affect downstream tasks. Therefore, the evaluation of downstream tasks from different sources needs to be studied.",
            "aggregate of these sentences. Usually, summaries generated in this way have a better performance on fluency and grammar, but they may contain much redundancy and lack in coherence across sentences. In contrast, abstractive methods attempt to mimic what humans do by first extracting content from the source document and then produce new sentences that aggregate and organize the extracted"
        ]
    },
    {
        "title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances",
        "answer": "architecture",
        "evidence": [
            "We would like to investigate effectiveness the proposed joint model. To do so, the same shared layer/architecture is employed in the following variants of the proposed model: The results in terms of EM and F1 is summarized in Table TABREF20 . We observe that Joint SAN outperforms the SAN baseline with a large margin, e.g., 67.89 vs 69.27 (+1.38) and 70.68 vs 72.20 (+1.52) in terms of EM and F1",
            "What is the Model Learning?",
            "Original Toulmin's model",
            "models the contradicting associations like \"animal\" and \"lake\" or \"australia\" and \"world\". These two associations are the unique pairs which are instrumental in determining that the two queries are not similar.",
            "Exp II: Transferability of Shared Sentence Representation",
            "sharing and large-scale community efforts resulting in the development of standard libraries, an increased availability of published research code and strong incentives to share state-of-the-art pretrained models. The combination of these factors has lead researchers to reproduce previous results more easily, investigate current approaches and test hypotheses without having to redevelop them",
            "Hybrid models",
            "objective function of the joint model has two parts: DISPLAYFORM0  Following BIBREF0 , the span loss function is defined: DISPLAYFORM0  The objective function of the binary classifier is defined: DISPLAYFORM0  where INLINEFORM0 is a binary variable: INLINEFORM1 indicates the question is unanswerable and INLINEFORM2 denotes the question is answerable.",
            "tokens are appended to the shorter input sequence before level-2 for joint learning. Note that in this case, using Joint-1 model (jointly training annotated slots & utterance-level intents) for the second level of the hierarchy would not make much sense (without intent keywords). Hence, Joint-2 model is used for the second level as described below: Hierarchical & Joint-2: Level-1 (Seq2seq Bi-LSTM",
            "from the source and target domains separately by two different embedding encoders with a shared embedding encoder for modeling domain-general features. The domain-general parameters are adversarially trained by domain discriminator. Row (e) is the model that the weights of all layers are tied between the source domain and the target domain. Row (f) uses the same architecture as row (e) with an",
            "UTCNN Model Description",
            "Effect of model structure"
        ]
    },
    {
        "title": "Is there Gender bias and stereotype in Portuguese Word Embeddings?",
        "answer": "word2vec, GloVe, C&W, skip-gram, CBOW, Skip-Gram, monolingual embedding versions, latent semantic analysis (LSA)  and classifier embedding space.",
        "evidence": [
            "Word embeddings",
            ", the authors propose a method to hardly reduce bias in English word embeddings collected from Google News. Using word2vec, they performed a geometric analysis of gender direction of the bias contained in the data. Using this property with the generation of gender-neutral analogies, a methodology was provided for modifying an embedding to remove gender stereotypes. Some metrics were defined to",
            "and pretrained encoders across different languages. Among other causes, the lowest absolute scores with ft are reported for languages with least resources available to train monolingual word embeddings, such as Kiswahili, Welsh, and Estonian. The low performance on Welsh is especially indicative: Figure FIGREF20 shows that the ratings in the Welsh dataset match up very well with the English",
            "In the literature, the main consensus is that the use of dense word embeddings outperform the sparse embeddings in many tasks. Latent semantic analysis (LSA) used to be the most popular method in generating word embeddings before the invention of the word2vec and other word vector algorithms which are mostly created by shallow neural network models. Although many studies have been employed on",
            "features, syntactic features, topic distribution, sentiment distribution, semantic features, coreference feaures, discourse features, and features based on word embeddings) performs best in both in-domain and all-data cross validation, while (2) features based only on word embeddings yield best results in cross-domain evaluation. Since there is no one-size-fits-all argumentation theory to be",
            "More study of learned embeddings, using more data and word types, is needed to confirm such patterns in general. Improvements in unseen word embeddings from the classifier embedding space to the Siamese embedding space (such as for democracy, morning, and basketball) are a likely result of optimizing the model for relative distances between words.",
            "words INLINEFORM1 by computing their similarities based on joint embeddings: INLINEFORM2 and INLINEFORM3 , where INLINEFORM4 is the context embedding of INLINEFORM5 conditioned on candidate INLINEFORM6 and is defined as the average sum of word global vectors weighted by attentions: INLINEFORM7  where INLINEFORM0 is the INLINEFORM1 -th word's attention from INLINEFORM2 . In this way, we",
            "We study principal components of publicly available word embedding models for the Russian language. We see that the first principal components indeed are good interpretable. Also, we show that these components are almost invariant under re-learning. It will be interesting to explore the regularities in canonical components between different models (such as CBOW versus Skip-Gram, different train",
            "so that we reduced the variance of each word pair. However, we did not restrict the attributes of words, such as the level of feeling, during annotation. Error analysis revealed that the notion of similarity should be carefully defined when constructing a similarity dataset. As a future work, we plan to construct a word analogy dataset in Japanese by translating an English dataset to Japanese. We",
            "monolingual embedding versions: skip-gram BIBREF15 , GloVe BIBREF16 and C&W BIBREF3 in some cases. These prior studies motivate us to explore combining multiple versions of word embeddings, treating each of them as a distinct description of words. Our expectation is that the combination of these embedding versions, trained by different NNs on different corpora, should contain more information",
            "word2vec embeddings. As a dataset-wide analysis, we determine the words identified through LRP as constituting class representatives. For that purpose we set one class as target class for the relevance decomposition, and conduct LRP over all test set documents (i.e. irrespectively of the true or ML model's predicted class). Subsequently, we sort all the words appearing in the test data in",
            "representation learning is well beyond the scope of this article. We therefore focus our cross-lingual analyses on several well-established and indicative state-of-the-art cross-lingual models, again spanning both static and contextualized cross-lingual word embeddings."
        ]
    },
    {
        "title": "An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models",
        "answer": "Recurrent Neural Networks, Transformers, Bi-LSTM-based models, Convolutional Neural Networks, Character-level neural language model.  MEMMs, CRFs.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2, CNN3.  CNN1.  CNN2,",
        "evidence": [
            "RNN Language Model",
            "carry out a comprehensive set of experiments with deep learning models of extractive summarization across different domains, i.e. news, personal stories, meetings, and medical articles, as well as across different neural architectures, in order to better understand the general pros and cons of different design choices. They find that non auto-regressive sentence extraction performs as well or",
            "lexicons to feature-based models (MEMMs, CRFs) and that of word vector representations to bi-LSTM-based models as reported by BIBREF20 . As mentioned above, work on the contribution of word vector representations to feature-based approaches has been carried out by BIBREF18 . However, the exploitation of existing morphosyntactic or morphological lexicons in neural models is a less studied",
            "Recent advances in modern Natural Language Processing (NLP) research have been dominated by the combination of Transfer Learning methods with large-scale language models, in particular based on the Transformer architecture. With them came a paradigm shift in NLP with the starting point for training a model on a downstream task moving from a blank specific model to a general-purpose pretrained",
            "the number of times a question type appears in the test data. The neural models unsurprisingly outperform the other models in most cases, and the difference for who questions is largest. A large number of these questions ask for the narrator of the story, who is usually not mentioned literally in the text, since most stories are written in the first person. It is also apparent that all models",
            "We propose new visualization and interpretation strategies for neural models to understand how and why they work. We demonstrate the effectiveness of the proposed strategies on a complex task (NLI). Our strategies are able to provide interesting insights not achievable by previous explanation techniques. Our future work will extend our study to consider other NLP tasks and models with the goal of",
            "used a similar approach, in which a character-level neural language model is trained on a massively multilingual corpus. A language embedding vector is concatenated to the input at each time step. The language embeddings their system learned correlate closely to the genetic relationships between languages. However, neither of these models was applied to g2p.",
            "enough for the considered classification problem), the learned filters in CNN2 and CNN3 do not only focus on isolated words but additionally consider bigrams or trigrams of words, as their results differ a lot from the CNN1 model in the first deletion experiment. In order to quantitatively evaluate and compare the ML models in combination with a relevance decomposition or explanation technique,",
            "Since this paper uses convolutional language models for speech recognition systems for the first time, we present additional studies of the language model in isolation. These experiments use our best language model on Librispeech, and evaluations in WER are carried out using the baseline system trained on mel-filterbanks. The decoder parameters are tuned using the grid search described in Section",
            "What is the Model Learning?",
            "intro Despite not being exposed to explicit syntactic supervision, neural language models (LMs), such as recurrent neural networks, are able to generate fluent and natural sentences, suggesting that they induce syntactic knowledge about the language to some extent. However, it is still under debate whether such induced knowledge about grammar is robust enough to deal with syntactically",
            "Experimenting with Transformers ::: Language understanding benchmarks"
        ]
    },
    {
        "title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset",
        "answer": "food, non-food, formal, social media, mainstreaming, red-shirting.",
        "evidence": [
            "images for those domains are quite limited due to the annotation cost. We use domain adaptation methods to improve the captions of the target domain. To simulate the scenario, we split the Microsoft COCO dataset into food and non-food domain datasets. The MS COCO dataset contains approximately 80K images for training and 40K images for validation; each image has 5 captions BIBREF12 . The dataset",
            "In-domain and out-of-domain data",
            "Dataset",
            "in INLINEFORM5 . The optimal configurations are INLINEFORM6 for all the datasets.",
            "data with traditional QA datasets collected primarily from formal domains. In particular, we demonstrate empirically that three strong neural models which achieve good performance on formal data do not generalize well to social media data, bringing out challenges to developing QA systems that work well on social media domains. In summary, our contributions are:",
            "What Does Zero-shot Transfer Model Learn? ::: Unseen Language Dataset",
            "are completely shared across the domains. Augmentation of parameters in the other part of the network would be an interesting direction of future work.",
            "of our approach, we train the system on five domains and test on the remaining one for all six domains (which we report as cross-domain validation).",
            "Datasets Used for the RQE Study",
            "The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics.",
            "to the cross-validation scenario, the overall best results were achieved using the largest feature set (01234). For mainstreaming and red-shirting, the best results were achieved using only the feature set 4 (embeddings). These two domains contain also fewer documents, compared to other domains (refer to Table TABREF71 ). We suspect that embeddings-based features convey important information when",
            "and Mix-Source, our NMT systems achieved the best improvement on the dataset. In the future, we will exploit more domain and multilingual information to improve the quality of the systems."
        ]
    },
    {
        "title": "Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge",
        "answer": "Freebase",
        "evidence": [
            "together with web-crawled evidence documents from Wikipedia and Bing. While a majority of questions require world knowledge for finding the correct answer, it is mostly factual knowledge.",
            "How Many Expert Annotations?",
            "only assign meaning to language that falls within the KB's manually-produced schema. Recently proposed methods for open vocabulary semantic parsing overcome this limitation by learning execution models for arbitrary language, essentially using a text corpus as a kind of knowledge base. However, all prior approaches to open vocabulary semantic parsing replace a formal KB with textual information,",
            "RQE-based QA Approach",
            "Converting Freebase queries to features",
            "Knowledge graphs (KGs) such as Freebase BIBREF0 , DBpedia BIBREF1 , and YAGO BIBREF2 play a critical role in various NLP tasks, including question answering BIBREF3 , information retrieval BIBREF4 , and personalized recommendation BIBREF5 . A typical KG consists of numerous facts about a predefined set of entities. Each fact is in the form of a triplet INLINEFORM0 (or INLINEFORM1 for short),",
            "in low-resourced languages by exploiting rich knowledge from high-resourced languages, and deal with NIL entities to facilitate specific applications.",
            "by utilising emoticons as features. Most of our approaches do not rely on a neural network model in learning embeddings. However, they produce state-of-the-art results.",
            "Incorporating External Knowledge",
            "The Behavioral Graph: A Knowledge Base For Navigation",
            "can be collected using a crowdsourcing platform, a smaller number of highly-trained annotators, or a group of experts. Which type of annotator to use should be informed by the complexity and specificity of the concept. For more complex concepts, highly-trained or expert annotators tend to produce more reliable results. However, complex concepts can sometimes be broken down into micro-tasks that",
            "GAN setups"
        ]
    },
    {
        "title": "SimplerVoice: A Key Message&Visual Description Generator System for Illiteracy",
        "answer": "SimplerVoice model.",
        "evidence": [
            "Using Target Language Models",
            "based on planning and templates, to ensure factual and coherent mentions of records in generated descriptions. For example, Puduppully et al. BIBREF12 propose a two-step decoder which first targets specific records and then use them as a plan for the actual text generation. Similarly, Li et al. BIBREF28 proposed a delayed copy mechanism where their decoder also acts in two steps: 1) using a",
            "We introduce SimplerVoice: a key message and visual description generator system to help low-literate adults navigate the information-dense world with confidence, on their own. SimplerVoice can automatically generate sensible sentences describing an unknown object, extract semantic meanings of the object usage in the form of a query string, then, represent the string as multiple types of visual",
            "model, with key-guided hierarchical attention, i.e. where attention over records is computed only on the record key representations, as in equation (DISPLAY_FORM17).",
            "(LRLs). The output space of this model consists of tag sets such as {POS: Adj, Gender: Masc, Number: Sing}, which are predicted for a token at each time step. However, this model relies heavily on the fact that the entire space of tag sets for the LRL must match those of the HRL, which is often not the case, either due to linguistic divergence or small differences in the annotation schemes",
            "This section discusses the process of generating key message from the object's input. Based on the retrieved input, we can easily obtain the object's title through searching in database, or using search engine; hence, we assume that the input of object2text is the object's title. The workflow of object2text is provided in Figure FIGREF4 . S-V-O query is constructed by the 3 steps below. In order",
            "models the contradicting associations like \"animal\" and \"lake\" or \"australia\" and \"world\". These two associations are the unique pairs which are instrumental in determining that the two queries are not similar.",
            "Error Generation Methods",
            "words are also generated based on the article representations. Since natural language text is complicated and verbose in nature, and training data is insufficient in size to help the models distinguish important article information from noise, sequence-to-sequence models tend to deteriorate with the accumulation of word generation, e.g., they generate irrelevant and repeated words frequently",
            "system to translate user instructions to a high-level navigation plan. Our model utilized an attention mechanism to merge relevant information from the navigation instructions with a behavioral graph of the environment. The model then used a decoder to predict a sequence of navigation behaviors that matched the input commands. As part of this effort, we contributed a new dataset of 11,051 pairs",
            ", BIBREF14 . Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity relatedness) that are unavailable to local model BIBREF15 . For example (Figure FIGREF1 ), an EL model seemly cannot find",
            "model can achieve much more stable training process and can generate text with significantly better quality."
        ]
    },
    {
        "title": "A Parallel Corpus of Theses and Dissertations Abstracts",
        "answer": "SMT, NMT, Tree-to-sequence NMT, Recurrent NMT, Transformer, PBSMT, ultra low-resource NMT, parallel RNN.  # Corrected Answer",
        "evidence": [
            "In this experiment, we analyzed and compared the performance of the SMT and various NMT models on our built dataset. To verify the effectiveness of our data augmented method. We trained the NMT and SMT models on both unaugmented dataset (including 0.46M training pairs) and augmented dataset, and test all the models on the same Test set which is augmented. The models to be tested and their",
            "Among the first proposals for using source syntax in NMT was that of luong2015multi, who introduced a multi-task system in which the source data was parsed and translated using a shared encoder and two decoders. More radical changes to the standard NMT paradigm have also been proposed. eriguchi2016tree introduced tree-to-sequence NMT; this model took parse trees as input using a tree-LSTM",
            "NMT Systems",
            "Translation experiments",
            "while NMT systems produce translations on a subword-level, with varying success (blue-flect, bleed; spaniers, Spanians). NMT systems learn some syntactic disambiguation even with very little data, for example the translation of das and die as relative pronouns ('that', 'which', 'who'), while PBSMT produces less grammatical translation. On the flip side, the ultra low-resource NMT system ignores",
            "that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "experiments. The Recurrent NMT model follows the attention-based architecture proposed by BIBREF0 . The bidirectional recurrent encoder reads every words INLINEFORM0 of a source sentence INLINEFORM1 and encodes a representation INLINEFORM2 of the sentence into a fixed-length vector INLINEFORM3 concatenated from those of the forward and backward directions: INLINEFORM4  Here INLINEFORM0 is the",
            "We implemented all models with OpenNMT BIBREF11 . Our hyperparameters, which we determined by experimentation, are listed in Table TABREF8 .",
            "model, we do not attempt to compare our model to parallel RNN. These models are similar to ours in that they incorporate linearized parses into NMT; here, we utilize a multi-source framework.",
            "auxiliary words, conjunctions and function words, which is not consistent with human translation habits. To further confirm this conclusion, the average length of the translation results of the three models are measured (RNN-based NMT:17.12, SMT:15.50, Transformer:16.78, Reference:16.47). We can see that the average length of the SMT outputs is shortest, and the length gaps between the SMT",
            "as possible. The criteria ensures the basic principle of NMT as well as the reproducibility of the systems. On the other hand, the methods are chosen in the direction that they would help alleviate the data sparsity problem of NMT when being applied in this low-resourced setting. Specifically, to deal with rare-word translation problems, we experiment with translation units in different levels:",
            "copy-marked setup, which is the best way to use a copy of the target (although the performance on the out-of-domain tests is at most the same as the baseline). Such gains may look surprising, since the NMT model does not need to learn to translate but only to copy the source. This is indeed what happens: to confirm this, we built a fake test set having identical source and target side (in"
        ]
    },
    {
        "title": "Data Innovation for International Development: An overview of natural language processing for qualitative data analysis",
        "answer": "Vector space approaches, embedding, Transfer Learning methods, large-scale pretrained Transformers, named entity recognition, abstractive summarization, discursive segmentation.",
        "evidence": [
            "and evaluation. These data may be difficult to process and analyze both systematically and at scale. This, in turn, limits the ability of timely data driven decision-making which is essential in fast evolving complex social systems. In this paper, we discuss the potential of using natural language processing to systematize analysis of qualitative data, and to inform quick decision-making in the",
            "techniques today are designed to process (Wikipedia articles, news stories, web pages, etc.): They are heterogeneous and frequently contain a mix of both free text as well as semi-structured elements (tables, headings, etc.). They are, by definition, domain specific, often with vocabulary, phrases, and linguistic structures (e.g., legal boilerplate and terms of art) that are rarely seen in",
            "complexity) among some machine learning based approaches used in QC for Bengali language.",
            "language processing. This interest is motivated by the desire to move away from simple counts and weights of words, towards representations which can preserve word context and linguistic features of human speech. Studies find that these approaches are not only intuitively desirable, but also increase classification accuracy in machine learning tasks. Vector space approaches involve the embedding",
            "current approaches and test hypotheses without having to redevelop them first, and focus their efforts on formulating and testing new hypotheses. To bring Transfer Learning methods and large-scale pretrained Transformers back into the realm of these best practices, the authors (and the community of contributors) have developed Transformers, a library for state-of-the art Natural Language",
            "To detect online hate speech, a large number of scientific studies have been dedicated by using Natural Language Processing (NLP) in combination with Machine Learning (ML) and Deep Learning (DL) methods BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF0. Although supervised machine learning-based approaches have used different text mining-based features such as surface features, sentiment",
            "Framework for Processing Portuguese Text ::: Named Entity Recognition",
            "Discussion and Qualitative Analysis",
            "these baselines (or their improved versions) can be used in applications such as automatic document summarisation (e.g., BIBREF12, BIBREF13), or sentences compression BIBREF14. The main feature of the proposed baseline system is its flexibility with respect to the language considered. In fact, it only uses a list of language markers and the grammatical category of words. The first resource,",
            "unimportant information when encoding. Some other work attempts to incorporate external knowledge for abstractive summarization. For example, Nallapati2016Abstractive proposed to enrich their encoder with handcrafted features such as named entities and part-of-speech (POS) tags. guu2018generating also attempted to encode human-written sentences to improve neural text generation. Similar to our",
            "corresponding to different periods in time. As a first preprocessing step, we remove the headers from the documents (by splitting at the first blank line) and tokenize the text with NLTK. Then, we filter the tokenized data by retaining only tokens composed of the following four types of characters: alphabetic, hyphen, dot and apostrophe, and containing at least one alphabetic character. Hereby we",
            "tree. Regarding the discursive segmentation, there are segmenters in several languages. However, each piece depends on sofisticated linguistic resources, which complicates the reproduction of the experiments in other languages. Consequently, the development of multilingual systems using discursive analysis are yet to be developed. Diverse applications based on the latest technologies require at"
        ]
    },
    {
        "title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages",
        "answer": "SemEval (English), TASS (Spanish), SENTIPOLC (Italian), Time, The Guardian, Disney, Jochim et al., BIBREF1, BIBREF2, Google Cloud, TensiStrength, Rosette Text Analytics.",
        "evidence": [
            "point to build new sentiment analysis systems. We compare our approach in eight different languages, three of them have important international contests, namely, SemEval (English), TASS (Spanish), and SENTIPOLC (Italian). Within the competitions our approach reaches from medium to high positions in the rankings; whereas in the remaining languages our approach outperforms the reported results.",
            "of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator.",
            "are both “similar\" and therefore considering one source of information alone is analytically sufficient). However, in years where a given state's voting behavior and speeches diverge (e.g. a NATO member voting with the bloc but delivering a speech which contains position information which diverges from the rest of the bloc), then favoring one source of information over the other will likely yield",
            "data. Time and The Guardian perform well on most emotions but Disney helps to boost the performance for the Joy class.",
            "Qualitative Comparison",
            "features for citation purpose and polarity classification, such as reference count, contrary expression and dependency relations. Jochim et al. tried to improve the result by using unigram and bigram features BIBREF1 . BIBREF2 used word level features, contextual polarity features, and sentence structure based features to detect sentiment citations. Although they generated good results using the",
            "Overview of datasets and emotions",
            "of mean sentiment differences pre- and post-event (see Section SECREF4 ). Note that we perform these mean comparisons on all event-related data, since the low number of geo-tagged samples would produce an underpowered study.",
            "It makes sense because they are all reviews related to movie. However, another movie review “MR” has very low similarity to these three task. It's probably that the text in “MR” is very short that makes it different from these tasks. The similarity of INLINEFORM5 from “Books” and “Video” is also very high because these two datasets share a lot of similar sentiment expressions. As shown in Figure",
            "identified 62% of the neutral, 85% of the positive, and 92% of the negative sentiments. Google Cloud correctly identified 88% of the neutral sentiments, but only 3% of the positive, and 19% of the negative sentiments. TensiStrength correctly identified 87.2% of the neutral sentiments, but 10.5% of the positive, and 8.1% of the negative sentiments. Rosette Text Analytics correctly identified 22.7%",
            "Sentiment Classification",
            "model, we do not attempt to compare our model to parallel RNN. These models are similar to ours in that they incorporate linearized parses into NMT; here, we utilize a multi-source framework."
        ]
    },
    {
        "title": "TutorialVQA: Question Answering Dataset for Tutorial Videos",
        "answer": "ConceptNet",
        "evidence": [
            "Experiments on Triplet Classification",
            "We use the following cheap ways to generate pseudo-source texts: copy: in this setting, the source side is a mere copy of the target-side data. Since the source vocabulary of the NMT is fixed, copying the target sentences can cause the occurrence of OOVs. To avoid this situation, BIBREF24 decompose the target words into source-side units to make the copy look like source sentences. Each OOV found",
            "Multi-Source",
            "Theoretical background",
            "Documents (687 000 words) taken from four sources: the Est Républicain newspaper (39 articles, 10 000 words); Wikipedia (30 articles + 30 summaries, 242 000 words); Proceedings of the conference Traitement Automatique des Langues Naturelles (TALN) 2008 (25 articles, 169 000 words); Reports from Institut Français de Relations Internationales (32 raports, 266 000 words). The corpora were noted",
            "GLUE and SuperGLUE",
            "Acknowledgments\nWe would like to thank the anonymous reviewers for their valuable comments and thank Kaiyu Qian, Jiachen Xu, Jifan Chen for useful discussions. This work was partially funded by National Natural Science Foundation of China (No. 61532011 and 61672162), Shanghai Municipal Science and Technology Commission (No. 16JC1420401).",
            "a margin of 0.4 in the cos-hinge loss. In training the Siamese networks, each training mini-batch consists of INLINEFORM0 triplets. INLINEFORM1 triplets are of the form INLINEFORM2 where INLINEFORM3 and INLINEFORM4 are examples of the same class (a pair from the 100k same-word pair set) and INLINEFORM5 is a randomly sampled example from a different class. Then, for each of these INLINEFORM6",
            "reasoning over multiple triplets because a single triplet may not cover the entire question. Multi-hop reasoning is also a long-standing goal in question answering. Second, building a question answering system based on triplet-style knowledge facilitates the interpretability of the decision making process. Triplets organize the document together with KBs as a graph, where a well-designed model",
            "Effect of Cross-View Training (CVT)",
            "triples from ConceptNet and stored those whose head entity and tail entity are noun or verb, meanwhile both occurring in SCT. Moreover, we retained at most 10 triples if there are too many. The average number of triples for each query word is 3.4.",
            "Properties of back-translated data"
        ]
    },
    {
        "title": "Transductive Learning with String Kernels for Cross-Domain Text Classification",
        "answer": "MLP, SVM, GBC, RF, Gradient Boosting Classifier, Random Forest, Structured Perceptron Model.  Stochastic Gradient Descent.  Cross-Entropy Loss.  Structured Perceptron.  Bag-of-Words.  Vector Space Approach.  Network Clustering.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.  Structured Perceptron Model.  Structured Perceptron.",
        "evidence": [
            "Algorithms Used",
            "What is the Model Learning?",
            "and SVM have shown similar performance. MLP takes advantage of multiple hidden layers in order to take non-linearly separable samples in a linearly separable condition. SVM accomplishes this same feat by taking the samples to a higher dimensional hyperplane where the samples are linearly separable. Gradient Boosting Classifier (GBC) and Random Forest (RF) both utilize a set of decision trees and",
            "Recently, the transformative potential of machine learning (ML) has propelled ML into the forefront of mainstream media. In Brazil, the use of such technique has been widely diffused gaining more space. Thus, it is used to search for patterns, regularities or even concepts expressed in data sets BIBREF0 , and can be applied as a form of aid in several areas of everyday life. Among the different",
            "For insight-driven analysis, we are often interested in why a prediction has been made and features that can be interpreted by humans may be preferred. Recent neural network approaches often use simple features as input (such as word embeddings or character sequences), which requires less feature engineering but make interpretation more difficult. Supervised models are powerful, but they can",
            "carefully crafted model implementations and high-performance pretrained weights for two main deep learning frameworks, PyTorch and TensorFlow, while supporting all the necessary tools to analyze, evaluate and use these models in downstream tasks such as text/token classification, questions answering and language generation among others. The library has gained significant organic traction and",
            "is often guided by the task definition and the label types. For example, to identify stance towards rumors based on sequential annotations, an algorithm for learning from sequential BIBREF43 or time series data BIBREF44 could be used. The features (sometimes called variables or predictors) are used by the model to make the predictions. They may vary from content-based features such as single",
            "the learning rate for different parameters at different steps. Thus it is less sensitive to initial parameters than the stochastic gradient descent.",
            "the data are discussed and applications which use a bag-of-words treatment are presented. Second, the vector space approach and distance measure are introduced and discussed. Finally, the network clustering approach and model evaluations are presented.",
            "experiments, our method can be applied to any neural networks trained with a cross-entropy loss.",
            "Implementation of the System ::: Classification Algorithms ::: Multi Layer Perceptron (MLP)",
            "from the question's ungrounded graphs. These oracle graphs are then used to train a structured perceptron model. These steps are discussed in detail below."
        ]
    },
    {
        "title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches",
        "answer": "SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR, ROUGE F1, METEOR, Moses, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR, ROUGE F1, METEOR, Moses, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR, ROUGE F1, METEOR, Moses, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR, ROUGE F1, METEOR, Moses, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR, ROUGE F1, METEOR, Moses, SnapCaptions Dataset, TutorialVQA Dataset, Ancient-Modern Chinese Dataset, Basis, parallel sentences, ROUGE F1, METEOR,",
        "evidence": [
            "Datasets",
            "is presented in BIBREF0 that employ the Stochastic Gradient Descent (SGD).",
            "The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics.",
            "require different number of attentions. This is highly dependent on each dataset's nature and the underlying interconnections between modalities.",
            "by providing standardized datasets, evaluations, and a state-of-the-art entailment system. All datasets and codes are released.",
            "Data details and experimental setup",
            "TutorialVQA Dataset ::: Basis",
            "Datasets Used for the RQE Study",
            "parallel sentences, respectively. Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with subword units BIBREF20 with a joint vocabulary and 32k merge operations. Our implementation was done on a fork of the",
            "Results: SnapCaptions Dataset",
            "Algorithms Used",
            "Ancient-Modern Chinese Dataset"
        ]
    },
    {
        "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
        "answer": "4",
        "evidence": [
            "What is the Model Learning?",
            "connected layer are at around 0.3 AP or less. This may raise the question of whether some simple fully connected model may be all that is needed; however, previous work has shown that this approach is not competitive BIBREF13 , and convolutional or recurrent layers are needed to summarize arbitrary-length segments into a fixed-dimensional representation.",
            "How Many Expert Annotations?",
            "subspaces and calculates corresponding attention. The attention function outputs are concatenated and projected again before giving the final output. Multi-head attention allows the model to attend to multiple features at different positions. The encoder is composed of a stack of INLINEFORM0 identical layers. Each layer has two sub-layers: multi-head self-attention mechanism and position-wise",
            "Principal components of different models",
            "(FC) layers on top, and obtains improvements similar to a deep RNN model at a fraction of the training and decoding cost.",
            "depend on their involvement in the game. We followed the data partitions introduced with the dataset and used a train/validation/test sets of respectively $3,398$/727/728 (data-structure, description) pairs.",
            "MLP contains three layers - an input layer, an output layer and some hidden layers. Input layer receives the signal, the output layer gives a decision or prediction about the input and the computation of the MLP is conducted in the hidden layers. In our system, we use 100 layers. For weight optimization, we use Limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS) optimization algorithm.",
            "phase, a proper one is selected based on the input information. Although these models accomplished their mission to a certain extent, they still suffer from the following three challenges. First, the predefined compositional functions cannot cover all the compositional rules; Second, they require more learnable parameters, suffering from the problem of overfitting; Third, it is difficult to",
            "machine translation. Their model's encoder uses a convolutional layer followed by max-pooling to reduce sequence length, a four-layer highway network, and a bidirectional GRU. The parallelism of the convolutional, pooling, and highway layers allows training speed comparable to subword-level models without hard-coded text segmentation. The QRNN encoder–decoder model shares the favorable",
            "influences. The block “tricks” (28–29) shows the system performance when no mutual-learning or no pretraining is used. The block “layers” (30–33) demonstrates how the system performs when it has different numbers of convolution layers. From the “layers” block, we can see that our system performs best with two layers of convolution in Standard Sentiment Treebank and Subjectivity Classification",
            "Inductive Embedding Models"
        ]
    },
    {
        "title": "\"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach",
        "answer": "better modeling of cross-view dynamics.",
        "evidence": [
            "document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.",
            "number of parameters further (by increasing dense layers, LSTHM cellsizes etc.) did not improve performance. This indicates that the better performance of MARN with multiple attentions is not due to the higher number of parameters but rather due to better modeling of cross-view dynamics. RQ4: Different tasks and datasets require different number of attentions. This is highly dependent on each",
            "especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures BIBREF6 , BIBREF5 , BIBREF7 . Current CNN systems usually implement a convolution layer with fixed-size filters (i.e., feature detectors), in which the concrete filter size is",
            "outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.",
            "This leaves us with two possible explanations: despite careful controls, the WinoGrande challenge still contains incidental biases that these more sophisticated models can exploit, or that we are genuinely making at least some progress in commonsense reasoning. The latter, in particular, challenges the notion that commonsense knowledge is (mostly) tacit. Perhaps it is the case that in a humongous",
            "We first investigate an approach that does not require the RNN to be modified in order to make it understandable, as the interpretation happens after the fact. Here, we model the big picture of the state changes in the LSTM, by extracting the hidden states and approximating them with a continuous emission hidden Markov model (HMM). We then take the reverse approach where the HMM state",
            "meaningful weights to the neighbors. Specifically, in order to generate representations for unseen entities, it is crucial to incorporate the logic rules to train the aggregator, instead of depending solely on neural networks to learn from the data. By combining the logic rules and neural networks, LAN takes a step further in outperforming all the other models. To find out whether the superiority",
            "questions, to which the answers are often named entities. Since the tweet contexts are short, there are only a small number of named entities to choose from, which could make the answer pattern easy to learn. On the other hand, the neural models fail to perform well on the “Why” questions, and the results of neural baselines are even worse than that of the matching baseline. We find that these",
            "prediction, but harder to interpret. The value we place on interpretability can therefore influence the approach we choose. There is an increasing interest in developing interpretable or transparent models in the NLP and machine learning communities.",
            "to our analysis. For example, since the introduction of GANs, research groups have studied how variational autoencoders can be incorporated into these models, to increase the clarity and accuracy of the models’ outputs. Wang et al. in particular demonstrate how using a variational autoencoder in the generator model can increase the accuracy of generated text samples [11]. Most promisingly, the",
            "improvement in another metric. Although our method is applicable to any neural network with a cross entropy loss, all the experiments use caption generation models because it is one of the most successful neural network applications in NLP.",
            "(ii) and (iii). Only fine-tuned multichannel representations (case (i)) are kept for subsequent supervised training. The rationale for this pretraining is similar to auto-encoder: for an object composed of smaller-granular elements, the representations of the whole object and its components can learn each other. The CNN architecture learns sentence features layer by layer, then those features are"
        ]
    },
    {
        "title": "Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks",
        "answer": "Slot Filling and Intent Keyword Extraction, APDA@FIRE2019 submission, Mastering Training Games, RQE Approaches, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@FIRE2019 submission, APDA@F",
        "evidence": [
            "Translation experiments",
            "of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment to many users, we maintain a consistent set of video segments and extend the possibility to generate a diverse",
            "Experiments\nWe test the network on four classification tasks. We begin by specifying aspects of the implementation and the training of the network. We then report the results of the experiments.",
            "2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}. For dialects, the",
            "signal on the task at test time it may not be very surprising if the task performance goes up. We acknowledge this, and argue that our motivation for this setup is to deepen understanding, in particular the limitation or the capacity of the current architectures, which we expect can be reached with such strong supervision. Another motivation is engineering: we could exploit negative examples in",
            "Slot Filling and Intent Keyword Extraction Experiments",
            "type for the different tasks. Now, the “All Features” experiment shows how the interaction between feature sets perform than any of the other features groups in isolation. The accuracy metric for each trolling task is meant to provide an overall performance for all the classes within a particular task, and allow comparison between different experiments. In particular, we observe that GloVe",
            "Task and Evaluation",
            "Experimental Results ::: Mastering Training Games",
            "RQE Approaches and Experiments",
            "Experiments ::: APDA@FIRE2019 submission",
            "Experimenting with Transformers\nIn this section, we present some of the major tools and examples provided in the library to experiment on a range of downstream Natural Language Understanding and Natural Language Generation tasks."
        ]
    },
    {
        "title": "Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation",
        "answer": "2% EM score and over 1.5% F1 score.",
        "evidence": [
            "Comparison with baseline models",
            "successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score.",
            "engage in armed conflict. To increase confidence in these results, however, we follow BIBREF8 in the assessment of out-of-sample predictive accuracy by training models on five year windows and then assessing predictions on the next year. The areas under the precision recall curves are then summed over the entire date range. The predictive capability of Model 3 outperforms their paper's baseline",
            "correspond to individual models, and row 5 reports the ensemble performance. Columns correspond to label type. Results from all models outperform the baseline SVR model: Pearson's correlation coefficients range from 0.550 to 0.622. The regression correlations are the lowest. The RNN model realizes the strongest performance among the stand-alone (non-ensemble) models, outperforming variants that",
            "On the 229 languages for which deri2016grapheme presented their final results, the LangID version of our system outperforms the baseline by a wide margin. The best performance came with the version of our model that was trained on data in all available languages, not just the languages it was tested on. Using a language ID token improves results considerably, but even NoLangID beats the baseline",
            "the baseline S1 has an average precision of P=0.12. The performance across the years varies slightly, with the year 2011 having the highest average precision of P=0.13. Always picking the most frequent section as in S2, as shown in Figure FIGREF66 , results in an average precision of P=0.17, with a uniform distribution across the years. Here we show the performance of INLINEFORM0 decomposed for",
            "models were removed from the ensemble in a stepwise manner if the removal increased the average score. This was done based on their original scores, i.e. starting out by trying to remove the worst individual model and working our way up to the best model. We only consider it an increase in score if the difference is larger than 0.002 (i.e. the difference between 0.716 and 0.718). If at some point",
            "baselines yielding F1-score of 81% and 91% for datasets of Waseem and Davidson respectively. Inserting a CNN to pre-trained BERT model for fine-tuning on downstream task provides the best results as F1- score of 88% and 92% for datasets of Waseem and Davidson and it clearly exceeds the baselines. Intuitively, this makes sense that combining all pre-trained BERT layers with a CNN yields better",
            "description with accurate mentions in a more human-like fashion. We would also like to draw attention to the number of parameters used by those architectures. We note that our scenarios relies on a lower number of parameters (14 millions) compared to all baselines (ranging from 23 to 45 millions). This outlines the effectiveness in the design of our model relying on a structure encoding, in",
            "regularisation on all the hidden states, allowing a better regularisation of the model learning process. Empirical results show that our model can effectively mitigate the latent variable collapse issue while giving a better predictive performance than the baselines.",
            "Table TABREF17 shows that the QRNN outperformed the character-level LSTM, almost matching the performance of a word-level attentional baseline.",
            "given a fixed model, while the latter requires a retraining of the model (note that both performed almost equal in terms of AUC). Our hypothesis, which we leave open for future work, is that AV methods relying on a complex model INLINEFORM0 are more robust than methods based on a scalar-threshold INLINEFORM1 . Lastly, we wish to underline that all examined approaches failed in the cross-topic"
        ]
    },
    {
        "title": "Sentiment Analysis of Citations Using Word2vec",
        "answer": "accuracy, score, recall, F1 score, BLEU Score.",
        "evidence": [
            "Comparison Metrics",
            "training, both the supervised and semi-supervised models are trained until the validation metrics stop improving; the metrics are (1) sentence boundary F1 score and (2) overall F1 score for Thai sentence segmentation and English punctuation restoration, respectively. CVT has three main parameters that impact model accuracy. The first is the drop rate of the masked language model, which determines",
            "concept into numbers or labels BIBREF13 . Choices made during this phase are always tied to the question “Are we measuring what we intend to measure?” Does our operationalization match our conceptual definition? To ensure validity we must recognize gaps between what is important and what is easy to measure. We first discuss modeling considerations. Next, we describe several frequently used",
            "the data are discussed and applications which use a bag-of-words treatment are presented. Second, the vector space approach and distance measure are introduced and discussed. Finally, the network clustering approach and model evaluations are presented.",
            "calculates metrics for each label, and find their average, weighted by support (the number of true instances for each label). Weighted-F alters macro-F to account for label imbalance.",
            "an embedding to remove gender stereotypes. Some metrics were defined to quantify both direct and indirect gender biases in embeddings and to develop algorithms to reduce bias in some embedding. Hence, the authors show that embeddings can be used in applications without amplifying gender bias.",
            "boosting comes along with possible higher optimization times. Finally, the performance of each configuration is obtained using a cross-validation technique on the training data, and the metrics are usually used in classification such as: accuracy, score INLINEFORM0 , and recall, among others.",
            "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we",
            "( BIBREF22 , as well as two new metrics that we describe next to account for over and under-translation.",
            "systems that can be reproducible in order to serve as the baseline for further researches in the direction. Furthermore, we conduct experiments using some advanced methods to improve the quality of the systems. An important criteria for those methods is that they must be scalable and language-independent as much as possible. The criteria ensures the basic principle of NMT as well as the",
            "Experimental setup ::: Evaluation metrics ::: BLEU Score.",
            "Dataset and Analysis Methodology"
        ]
    },
    {
        "title": "Neural Factor Graph Models for Cross-lingual Morphological Tagging",
        "answer": "Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.",
        "evidence": [
            "Multilingual experiments",
            "such resources other than for English (e.g., Japanese) seldom exist. In addition, most of these datasets comprise high-frequency nouns so that they tend not to include other parts of speech. Hence, previous data fail to evaluate word representations of other parts of speech, including content words such as verbs and adjectives. To address the problem of the lack of a dataset for evaluating",
            "in low-resourced languages by exploiting rich knowledge from high-resourced languages, and deal with NIL entities to facilitate specific applications.",
            "Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. Although this language list contains only one non-Indo-European (Indonesian), four major Indo-European sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically",
            "tasks such as SQuAD1.1 are also available. Overall, more than 30 pretrained weights are provided through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "unseen languages during training. The main motivation and contribution behind this work is as follows:",
            "in Czech Republic speak? using the lexical and phrasal rules from the PPDB. Once $G_{\\mathrm {syn}}^{\\prime }$ is generated, we sample paraphrases of the input question $q$ . These paraphrases are further filtered with a classifier to improve the precision of the generated paraphrases. We train the L-PCFG $G_{\\mathrm {syn}}$ on the Paralex corpus BIBREF9 . Paralex is a large monolingual parallel",
            "they were able to adapt them for, for a total of 229 languages. This test set omits 23 of the high resource languages that are written in unique scripts or for which language distance metrics could not be computed.",
            "way, a linked data knowledge base. It is important to point out that, having the text information represented in an ontology allows us to perform complex queries and inferences, which can detect patterns of typical criminal actions. Another axe of innovation in this research is the development, for the Portuguese language, of a pipeline of Natural Language Processing (NLP) processes, that allows",
            "In this first exploratory work, only documents in French were considered, but the system can be adapted to other languages. The evaluation is based on the correspondence of word pairs representing a border. In this way we compare the Annodis segmentation with the automatically produced segmentation. For each pair of reference segments, a $L_r$ list of word pairs is provided: the last word of the",
            "Language Resource References\nlrec lrec2018",
            "This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili. We hope to further include additional languages and inspire other researchers to contribute to the effort over the lifetime of this project. The work on data collection can be divided into two crucial phases: 1) a translation phase where the extended"
        ]
    },
    {
        "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
        "answer": "BERT, T5, FlauBERT, xlm, bert-base, bert-large, bert-large with whole word masking (wwm) from the original work on BERT.",
        "evidence": [
            "without having to train them from scratch. These models are accessed through a simple and unified API that follows a classic NLP pipeline: setting up configuration, processing data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to",
            "is presented in BIBREF0 that employ the Stochastic Gradient Descent (SGD).",
            "(bert-base architecture, uncased), French BIBREF142 (the FlauBERT model based on xlm), English (bert-base, uncased), Mandarin Chinese (bert-base) BIBREF29 and Spanish (bert-base, uncased). In addition, we also evaluate a series of pretrained encoders available for English: (i) bert-base, bert-large, and bert-large with whole word masking (wwm) from the original work on BERT BIBREF29, (ii)",
            "PyTorch code, and features production code and integration with the TensorFlow Extended framework.",
            "Subsequently, the same sets of parameters will be fine-tuned for supervised classification tasks. In sum, this pretraining is designed to produce good initial values for both model parameters and word embeddings. It is especially helpful for pretraining the embeddings of unknown words.",
            "tasks such as SQuAD1.1 are also available. Overall, more than 30 pretrained weights are provided through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",
            "by providing standardized datasets, evaluations, and a state-of-the-art entailment system. All datasets and codes are released.",
            "the model to calculate the standard supervised loss for each mini-batch and the model weights are updated regularly. Meanwhile, each mini-batch of unlabeled data is selected randomly from the pool of all unlabeled data; the model computes the loss for CVT from the mini-batch of unlabeled data. This CVT loss is used to train auxiliary prediction modules, which see restricted views of the input, to",
            "on top of the base model hidden states. Examples of these heads are language modeling or sequence classification heads. These classes follow similar naming pattern: XXXForSequenceClassification or XXXForMaskedLM where XXX is the name of the model and can be used for adaptation (fine-tuning) or pre-training. All models are available both in PyTorch and TensorFlow (starting 2.0) and offer deep",
            "and shared information into a unified space. SSP-MTL and PSP-MTL achieve similar performance and are outperformed by ASP-MTL which can better separate the task-specific and task-invariant features by using adversarial training. Our proposed models (SA-MTL and DA-MTL) outperform ASP-MTL because we model a richer representation from these 16 tasks. Compared to SA-MTL, DA-MTL achieves a further",
            "Collectively, the success of large pretrained neural models, both encoder-only BERT-like architectures as well as encoder–decoder architectures like T5, raise interesting questions for the pursuit of commonsense reasoning. Researchers have discovered that previous models perform well on benchmark datasets because they pick up on incidental biases in the dataset that have nothing to do with the",
            "summarization based on neural networks and continuous sentence features, which outperforms traditional methods on the DailyMail dataset. In particular, they develop a general encoder-decoder architecture, where a CNN is used as sentence encoder, a uni-directional LSTM as document encoder, with another uni-directional LSTM as decoder. To decrease the number of parameters while maintaining the"
        ]
    },
    {
        "title": "Multichannel Variable-Size Convolution for Sentence Classification",
        "answer": "It combines multichannel initialization and variable-size filters.  It enhances system performance as indicated by the high reconstruction loss.  The KL loss for both VAE-CNN and vMF-VAE are nonzero.  The former mitigates the KL collapse issue with a KL loss clipping strategy and the latter by replacing the standard normal distribution for the prior with the vMF distribution.  The KL loss only depends on a fixed concentration parameter.  It results in high bias towards similarities already there in the sequences.  It provides two examples with distinct, there is some overlap, i.e. the distance between them varies.  This information can be leveraged to map out rhetorical network.  It obtains improvements similar to a deep RNN model at a fraction of the training and decoding cost.  It has perfect coverage.  A general comparison between CC+Wiki and Wiki ft vectors, however, supports the intuition that larger corpora (such as CC+Wiki) yield higher correlations.  Another finding is that a single massively multilingual model such as m-bert cannot produce semantically rich word-level representations.  It lacks trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size.  Due to their increased parallelism,",
        "evidence": [
            "This work presented MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization – diverse versions of pretrained word embeddings are used – and variable-size filters – features of multigranular phrases are extracted with variable-size convolution filters. We demonstrated that multichannel initialization and variable-size filters enhance system performance",
            "as indicated by the high reconstruction loss. The KL loss for both VAE-CNN and vMF-VAE are nonzero, where the former mitigates the KL collapse issue with a KL loss clipping strategy and the latter by replacing the standard normal distribution for the prior with the vMF distribution (i.e., with the vMF distribution, the KL loss only depends on a fixed concentration parameter, and hence results in",
            "UTCNN Model Description",
            "Comparison of CNN and n-gram models for local representation",
            "high bias towards similarities already there in the sequences. Sequence 1: What are the best ways to learn French ? Sequence 2: How do I learn french genders ? Attention only: 1 Attention+Conflict: 0 Ground Truth: 0 Sequence 1: How do I prevent breast cancer ? Sequence 2: Is breast cancer preventable ? Attention only: 1 Attention+Conflict: 0 Ground Truth: 0 We provide two examples with",
            "distinct, there is some overlap, i.e. the distance between them varies. This information can be leveraged to map out rhetorical network.",
            "(FC) layers on top, and obtains improvements similar to a deep RNN model at a fraction of the training and decoding cost.",
            "(OOV) words whose static ft embeddings are not available. On the other hand, m-bert has perfect coverage. A general comparison between CC+Wiki and Wiki ft vectors, however, supports the intuition that larger corpora (such as CC+Wiki) yield higher correlations. Another finding is that a single massively multilingual model such as m-bert cannot produce semantically rich word-level representations.",
            "modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time.",
            "it performs even worse in this task than the simple concatenation. We conjecture the reason is that the DCN Attention attempts to fuse the template information into an article as in machine reading comprehension, rather than selects key information from the two to form an enhanced article representation.",
            "Vanilla Recursive Neural Network",
            "GCNs are a type of neural network model that deals with structured data. It takes a graph as an input and output labels for each node. As a simplification of spectral graph convolutions, the main idea of BIBREF26 is similar to a propagation model: to enhance the features of a node according to its neighbor nodes. The formulation is as follows: INLINEFORM0  where INLINEFORM0 is a normalized"
        ]
    },
    {
        "title": "A Lightweight Front-end Tool for Interactive Entity Population",
        "answer": "Python",
        "evidence": [
            "NLP Toolkits",
            "Using in production To facilitate the transition from research to production, all the models in the library are compatible with TorchScript, an intermediate representation of a PyTorch model that can then be run either in Python in a more efficient way, or in a high-performance environment such as C++. Fine-tuned models can thus be exported to production-friendly environment. Optimizing large",
            "step is applied in the target language which aims to eliminate the noise inserted during the translation process. This is done by training a relation classification tool: it is trained again on the English linguistic constraints and then used on the translated target language constraints, where the transfer is again enabled via a shared cross-lingual word vector space. Finally, a state-of-the-art",
            "How Many Expert Annotations?",
            "the source sequence is in the wrong script for the language, as is seen in the entry for Arabic.",
            "tool. Thus, we carefully design the user interface for users to give feedbacks to the tool intuitively. Furthermore, we also consider the end-to-end user cost reduction. We adhere to the concept of developing installation-free software to distribute the tool among a wide variety of users, including nontechnical clusters. This lightweight design of LUWAK might speed up the procedure of the whole",
            "(but different segments). The last two rows display questions which belong to the same segment but correspond to different properties of the same entity, 'crop tool'. Here we observe different types of questions, such as \"why\", \"how\", \"what\", and \"where\", and can see why the answers may involve multiple steps. Some questions that the worked paraphrased were in the \"yes/no\" style, however our",
            "Data details and experimental setup",
            "tool. However, TreeTagger could be replaced by any other tool producing similar results.",
            "a tool for argumentation diagramming which supports both convergent and linked arguments, missing premises (enthymemes), and refutations. They also released the AracuariaDB corpus which has later been used for experiments in the argumentation mining field. However, the creation of the dataset in terms of annotation guidelines and reliability is not reported – these limitations as well as its",
            "comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being directly exposed to the real samples. Instead, this model learns to reproduce the general patterns of the input via its interaction with the discriminator. The role of the discriminator, in turn, is to tell apart which data points are",
            "BIBREF10, BIBREF11 have used recurrent neural networks with attention modules for NER. Sentiment detection tools like SentiStrength BIBREF12 and TensiStrength BIBREF13 are rule-based tools, relying on various dictionaries of emoticons, slangs, idioms, and ironic phrases, and set of rules that can detect the sentiment of a sentence overall or a targeted sentiment. Given a list of keywords,"
        ]
    },
    {
        "title": "Modeling Trolling in Social Media Conversations",
        "answer": "287,226",
        "evidence": [
            "Datasets and Experimental Setup",
            "sufficient grounding for the data while making it easy for workers to apply labels consistently.  Size: The total of 13,215 dialogs in this corpus is on par with similar, recently released datasets such as MultiWOZ BIBREF13.",
            "require different number of attentions. This is highly dependent on each dataset's nature and the underlying interconnections between modalities.",
            "Results: SnapCaptions Dataset",
            "Ancient-Modern Chinese Dataset",
            "sizes: 200-2000 words, at 200-word intervals, and evaluate our CNNs in the multi-class condition. Generalization-dataset experiments. To confirm that our models generalize, we pick the best models from the baseline-dataset experiments and evaluate on the novel-50 and IMDB62 datasets. For novel-50, the chunking size applied is 2000-word as per the baseline-dataset experiment results, and for",
            "The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics.",
            "TutorialVQA Dataset ::: Overview",
            "Data Collection",
            "Datasets for long documents",
            "Our Dataset",
            "Datasets Used for the RQE Study"
        ]
    },
    {
        "title": "Rotations and Interpretability of Word Embeddings: the Case of the Russian Language",
        "answer": "0, 6.    # Round 1: We ask the annotators to rate all 1,888 pairs with integer scores between 0 and 6.    # Round 2: We compare the scores of all annotators and identify the pairs for each annotator that have shown the most disagreement. We ask the annotators to reconsider the assigned scores for those pairs only. The annotators may chose to either change or keep the scores. As in the case with Round 1,    # The performance of all models on arXiv and Pubmed is shown in Table TABREF28 and Table TABREF29, respectively.    # Follow the work BIBREF18, we use the approximate    # information is often left unmentioned or underspecified. Such information may include encyclopedic and commonsense knowledge. This work focuses on commonsense knowledge about everyday activities, so-called scripts. This paper introduces a dataset to evaluate natural language understanding approaches with a focus on interpretation processes requiring inference based on commonsense knowledge. In    # although presenting similar classification performance may largely differ in terms of explainability.  The work is organized as follows. In section \"Representing Words and Documents\" we describe the related work for explaining classifier decisions with respect to",
        "evidence": [
            "Interpretability measures",
            "the instructions outlined above, and to rate all 1,888 pairs with integer scores between 0 and 6.  Round 2: We compare the scores of all annotators and identify the pairs for each annotator that have shown the most disagreement. We ask the annotators to reconsider the assigned scores for those pairs only. The annotators may chose to either change or keep the scores. As in the case with Round 1,",
            "originally proposed to evaluate translation systems by measuring the alignment between the system output and reference translations. As such, it can also be used as an automatic evaluation metric for summarization BIBREF18 . The performance of all models on arXiv and Pubmed is shown in Table TABREF28 and Table TABREF29 , respectively. Follow the work BIBREF18 , we use the approximate",
            "information is often left unmentioned or underspecified. Such information may include encyclopedic and commonsense knowledge. This work focuses on commonsense knowledge about everyday activities, so-called scripts. This paper introduces a dataset to evaluate natural language understanding approaches with a focus on interpretation processes requiring inference based on commonsense knowledge. In",
            "although presenting similar classification performance may largely differ in terms of explainability.  The work is organized as follows. In section \"Representing Words and Documents\" we describe the related work for explaining classifier decisions with respect to input space variables. In section \"Predicting Category with a Convolutional Neural Network\" we introduce our neural network ML model",
            "investigate the question whether the evaluation results of this paper hold more generally. Furthermore, we will address the important question how the results of AV methods can be interpreted in a more systematic manner, which will further influence the practicability of AV methods besides the proposed properties. This work was supported by the German Federal Ministry of Education and Research",
            "$  because $\\operatorname{tr}Q^T X Q = \\operatorname{tr}Q^{-1} X Q = \\operatorname{tr}X$ . It turns out that in average the interpretability is constant under any orthogonal transformation. But it is possible to make the first components more interpretable due to the other components. For example, $\n(Q^T W^T W W^T W Q)_{1, 1} = \\left(q^T W^T W q\\right)^2",
            "the annotators must explicitly write down the (inferred) stance of the author. By definition, the Toulmin's model is intended to model single argument, with the claim in its center. However, we observed in our data, that some authors elaborate on both sides of the controversy equally and put forward an argument for each side (by argument here we mean the claim and its premises, backings, etc.).",
            "increase the meaning of some components and to make the components more stable under re-learning. We study the interpretability of components for publicly available models for the Russian language (RusVectores, fastText, RDT).",
            "more emphasis on interpretability could increase the adoption of these models in insight-driven analyses. One way would be to only use models that are already somewhat interpretable, for example models that use a small number of human-interpretable features. Rather than imposing such restrictions, there is also work on generating post-hoc explanations for individual predictions (e.g., BIBREF46 ),",
            "Analysis of the convolutional language model",
            "used. In a second step, we told participants to formulate a plausible correct and a plausible incorrect answer candidate to answerable questions (text-based or script-based). To level out the effort between answerable and non-answerable questions, participants had to write a new question when selecting unknown or unfitting. In order to get reliable judgments about whether or not a question can be"
        ]
    },
    {
        "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study",
        "answer": "capitalization, isomorphic assumption, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases, cross-topic verification cases",
        "evidence": [
            "Paper Overview",
            "that part of the methods are able to cope with very challenging verification cases such as 250 characters long informal chat conversations (72.7% accuracy) or cases in which two scientific documents were written at different times with an average difference of 15.6 years (>75% accuracy). However, we also identified that all involved methods are prone to cross-topic verification cases.",
            "to see the strengths of our model as well as possible limitations of our current approaches.",
            "operations can be learned implicitly by meta TreeNN, such as the composition of noun phrases and verb phrases. The contributions of the paper can be summed up as follows.",
            "the shortcomings of MLE include length-normalizing sentence probability BIBREF5, future cost estimation BIBREF6, diversity-boosting objective function BIBREF7, BIBREF1 or penalizing repeating tokens BIBREF8. When it comes to poetry generation using generative text models, Zhang and Lapata BIBREF9, Yi et al. BIBREF10 and Wang et al. BIBREF11 use language modeling to generate Chinese poems.",
            "on UGC and MIR. Afterwards, in Section 3 it is presented the methodology of this work, describing the dataset and the method proposed. In Section 4, the results obtained are shown. Finally, in Section 5 conclusions are discussed.",
            "results in correct answer is shown in Figure 3 . Note that these two graphs are non-isomorphic making it impossible to derive the correct grounding from the ungrounded graph. In fact, at least 15% of the examples in our development set fail to satisfy isomorphic assumption. In order to address this problem, we use paraphrases of the input question to generate additional ungrounded graphs, with",
            "Introduction ::: First problem.",
            "Some examples that were indicative of the shortcomings of our system are shown in Table TABREF20 . First of all, our system did not take into account capitalization. The implications of this are shown in the first sentence, where capitalization intensifies the emotion used in the sentence. In the second sentence, the name Imperator Furiosa is not understood. Since our texts were lowercased, our",
            "Acknowledgements\nIn addition to the anonymous reviewers, we want to thank Lucia Passaro and Barbara Plank for insightful discussions, and for providing comments on draft versions of this paper.",
            "that reflect the depth of the questions we seek to address. These are just a small sample of the many opportunities and challenges faced in computational analyses of textual data. New possibilities and frustrating obstacles emerge at every stage of research, from identification of the research question to interpretation of the results. In this article, we take the reader through a typical",
            "API or what webpages are left out of the Internet Archive—we should be open about the limitations of our analyses, acknowledging the flaws in our data and drawing cautious and reasonable conclusions from them. In all cases, we should report the choices we have made when creating or re-using any dataset."
        ]
    },
    {
        "title": "Attention Optimization for Abstractive Document Summarization",
        "answer": "ROUGE F1, METEOR, BLEU score, F1 score, R@K, and mean opinion score.",
        "evidence": [
            "Evaluation Metrics",
            "The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics.",
            "as well as generalization ability. We also conduct an extensive analysis of time complexity, the impact of key modules, and qualitative results, which demonstrate the effectiveness and efficiency of our proposed method.",
            "metrics. It is interesting to note that there is a strong correlation between BLEU score and human ranking judgments.",
            "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\",",
            "in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\"",
            "Benchmark the evaluation ::: Label-fully-unseen.",
            "metric for evaluating Sentence Boundary Detection systems based on multi-reference (dis)agreement. We evaluate and compare the performance of different SBD systems over a set of Youtube transcripts using WiSeBE and standard metrics. This double evaluation gives an understanding of how WiSeBE is a more reliable metric for the SBD task.",
            "The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric.",
            "Prior work is mostly evaluated on different datasets and adopted different evaluation setups, which makes it hard to compare them fairly. For example, DBLPRiosK18 work on medical data while reporting R@K as metric; DBLPXiaZYCY18 work on SNIPS-NLU intent detection data while only unseen intents are in the label-searching space in evaluation. In this work, we benchmark the datasets and evaluation",
            "by providing standardized datasets, evaluations, and a state-of-the-art entailment system. All datasets and codes are released."
        ]
    },
    {
        "title": "Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling",
        "answer": "Transformers, QRNNs, Bi-LSTM, fconv.  GPT, GPT2, XLNet.  RNN+FC.  Megatron-LM.  tensor2tensor library.  fairseq.  seq2one Bi-LSTM network.  seq2seq.  seq2one.  seq2seq.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.  seq2one.",
        "evidence": [
            "Hierarchical seq2seq Generation Model",
            "to test, develop and train architectures like Transformers. Typical examples are the tensor2tensor library BIBREF36, fairseq BIBREF37 and Megatron-LM. These libraries are usually not provided with the user-facing features that allow easy download, caching, fine-tuning of the models as well as seamless transition to production.",
            "from scratch with different random seeds. We can see that the 2-model ensemble improves results by 0.9 BLEU, but the 3-model ensemble has little additional improvment. Although not presented here, we have found these improvement from decoder speedups and RNN+FC to be consistent across many language pairs. All together, we were able to achieve a BLEU score of 38.3 while decoding at 100 words/sec",
            "Parse2seq outperforms seq2seq by only a small amount compared to multi-source; thus, while adding syntax to NMT can be helpful, some ways of doing so are more effective than others.",
            "intent prediction. See Fig. FIGREF24 (a) for sample network architecture of the seq2one Bi-LSTM network. Note that in the Bi-LSTM implementation for seq2one learning (i.e., when not returning sequences), the outputs of backward/reverse LSTM is actually ordered in reverse time steps ( INLINEFORM0 ... INLINEFORM1 ). Thus, as illustrated in Fig. FIGREF24 (a), we actually concatenate the hidden state",
            "neural seq2seq architectures, provide qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.",
            "ordinary sequence-to-sequence models in Table TABREF46 . The reason might be that templates would overwhelm the original article representations and become noise after concatenation. Then, we removed the Template-to-Article (T2A) gate, and as a result the model shows a great decline in performance, indicating the importance of templates in article representations. Finally, when we removed the",
            "Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multi-lingual seq2seq model",
            "of its composition, works best only when it is given that there is a match somewhere between two sequences. It does not very well adapt to cases when there is no similarity between two sequences or if the relationship is contrastive. We propose an Conflict model which is very similar to how attention works but which emphasizes mostly on how well two sequences repel each other and finally",
            "interactive interface that leverages the generative capabilities of pretrained architectures like GPT, GPT2 and XLNet to suggest text like an auto-completion plugin. Generating samples is also often used to qualitatively (and subjectively) evaluate the generation quality of language models BIBREF9. Given the impact of the decoding algorithm (top-K sampling, beam-search, etc.) on generation",
            "of QRNNs, we extend the model architecture to sequence-to-sequence tasks, such as machine translation, by using a QRNN as encoder and a modified QRNN, enhanced with attention, as decoder. The motivation for modifying the decoder is that simply feeding the last encoder hidden state (the output of the encoder's pooling layer) into the decoder's recurrent pooling layer, analogously to conventional",
            "autoregressively as $P_{\\theta }$, in this work, is parameterized by a recurrent, convolution or Transformer-based seq2seq model. n-gram: We consider 3-gram and 4-gram conditional language model baseline with interpolation. We use random grid search for the best coefficients for the interpolated model. Convolution: We use the fconv architecture BIBREF24 and default hyperparameters from the"
        ]
    },
    {
        "title": "Leveraging Discourse Information Effectively for Authorship Attribution",
        "answer": "RoBERTa BIBREF2.",
        "evidence": [
            "State-of-the-art",
            "Prior work is mostly evaluated on different datasets and adopted different evaluation setups, which makes it hard to compare them fairly. For example, DBLPRiosK18 work on medical data while reporting R@K as metric; DBLPXiaZYCY18 work on SNIPS-NLU intent detection data while only unseen intents are in the label-searching space in evaluation. In this work, we benchmark the datasets and evaluation",
            "Revisiting the feature augmentation method",
            "Original Toulmin's model",
            "Stage 1 - Retraining decoder only",
            "architecture, bringing into focus the transferability of cwrs, as opposed to task-specific adaptation.",
            "baselines: the CRF-based model with n-gram embedding, which is currently the state-of-the-art for Thai sentence segmentation, and the Bi-LSTM-CRF model, which is currently the deep learning state-of-the-art approach for sequence tagging.",
            "yielded 0.7673 AUC on March 13, 2020, which is the best known result at this time and beats the previous state of the art by over five points.",
            "Conclusions and Future Work",
            "Properties of back-translated data",
            "state $s_t$, and the attention on $i$-th position should be weakened to avoid confusing the model. The attention distribution is updated as follows (the symbol $\\odot $ means element-wise product):",
            "data size conditions given our limited computational resources. Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with"
        ]
    },
    {
        "title": "Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents",
        "answer": "F1 score, Accuracy, c@1, AUC, AUC@1, Precision, Pearson's r.",
        "evidence": [
            "Evaluation Data and Metric",
            "amounted to 250 annotated headlines (Affective development), while systems were evaluated on another 1000 (Affective test). Evaluation was done using two different methods: a fine-grained evaluation using Pearson's r to measure the correlation between the system scores and the gold standard; and a coarse-grained method where each emotion score was converted to a binary label, and precision,",
            "(CNN). However, the SVM classifier, which is a conventional machine learning algorithm, performed better. We did not include the performances of CNN for embedding types here due to the page limit of the paper. As a qualitative assessment of the word representations, given some query words we visualised the most similar words to those words using the cosine similarity metric. By assessing the",
            "Results ::: Performance (WER) analysis on evaluation data ::: Speech type as a third entangled factor?",
            "which performance measures were selected with respect to the conclusion made in Section UID17 . Finally, we describe our experiments, present the results and highlight a number of observations.",
            "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we",
            "Discussion of RQE Results",
            "generated by Open-NMT BIBREF21 , INLINEFORM0 BIBREF11 and BiSET under three settings, respectively, and 3 randomly-selected summaries for trapping. We asked the evaluators to independently rate each summary on a scale of 1 to 5, with respect to its quality in informativity, conciseness, and readability. While collecting the results, we rejected the samples in which more than half evaluators rate",
            "in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\"",
            "usually more important for further applications. For the CreateDebate dataset, accuracy was adopted as the evaluation metric to compare the results with related work BIBREF7 , BIBREF9 , BIBREF12 .",
            "According to our extensive literature research, numerous measures (e. g., Accuracy, F INLINEFORM0 , c@1, AUC, AUC@1, INLINEFORM1 or EER) have been used so far to assess the performance of AV methods. In regard to our experiments, we decided to use c@1 and AUC for several reasons. First, Accuracy, F INLINEFORM2 and INLINEFORM3 are not applicable in cases where AV methods leave verification",
            "Full results of Performance Analysis over Human-Labeled Question Types"
        ]
    },
    {
        "title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled Data",
        "answer": "Hurricane Harvey, Hurricane Maria, Hurricane Irma, Hurricane Florence, and the Odisha Cyclone.",
        "evidence": [
            "users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U.S. in 2018. We begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield",
            "in the same fashion as done by Krishnamurthy and Mitchell. This final test set contains 307 queries.",
            "but occur infrequently in the fourteen other topics. The fifteen topics are labelled manually where the labels are human interpretations of the common underlying meaning of the characteristic words within each topic. Some of the topics deal with fighting, excommunication, prayer, or Ramadan. While the topics are relatively distinct, there is some overlap, i.e. the distance between them varies.",
            "In 2015, the United Nations Development Programme (UNDP) Regional Hub for Europe and CIS launched a Fragments of Impact Initiative (FoI) that helped to collect qualitative (micro-narratives) and quantitative data from multiple countries. Within a six-month period, around 10,000 interviews were conducted in multiple languages. These covered the perception of the local population in countries",
            "Models Used in the Evaluation",
            "not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when",
            "FIGREF66 then shows a diagram of the analysis from that example (the content of the argument components was shortened or rephrased). The annotation experiment was split into three phases. All documents were annotated by three independent annotators, who participated in two training sessions. During the first phase, 50 random comments and forum posts were annotated. Problematic cases were resolved",
            "from school is just the leading indicator of a nation that is ‘Falling Away’ from Jehovah. [...] And the disasters we see today are simply God’s finger writing on the wall: Mene, mene, Tekel, Upharsin; that is, God has weighed America in the balances, and we’ve been found wanting. No wonder 50 million babies have been aborted since 1973. [...]] We kept annotations on the pathos dimension as",
            "Documents (687 000 words) taken from four sources: the Est Républicain newspaper (39 articles, 10 000 words); Wikipedia (30 articles + 30 summaries, 242 000 words); Proceedings of the conference Traitement Automatique des Langues Naturelles (TALN) 2008 (25 articles, 169 000 words); Reports from Institut Français de Relations Internationales (32 raports, 266 000 words). The corpora were noted",
            "of our approach, we train the system on five domains and test on the remaining one for all six domains (which we report as cross-domain validation).",
            "aspects of emotional reactions. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of seven major emotions (joy, fear, anger, sadness, disgust, shame and guilt). In each case, the questions covered the way they had appraised a given situation and how they reacted. The final dataset contains reports by approximately 3000",
            "How Many Expert Annotations?"
        ]
    },
    {
        "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding",
        "answer": "two typical KG completion tasks.  # Corrected",
        "evidence": [
            "Slot Filling and Intent Keyword Extraction Experiments",
            "Knowledge graph embedding aims at modeling entities and relations with low-dimensional vectors. Most previous methods require that all entities should be seen during training, which is unpractical for real-world knowledge graphs with new entities emerging on a daily basis. Recent efforts on this issue suggest training a neighborhood aggregator in conjunction with the conventional entity and",
            "to that, the news articles are classified for their appropriateness for an entity, where as features for the classification task they use entity, document, entity-document and temporal features. The best performing features are those that measure similarity between an entity and the news document. West et al. BIBREF13 consider the problem of knowledge base completion, through question answering",
            "Experiments\nWe test the network on four classification tasks. We begin by specifying aspects of the implementation and the training of the network. We then report the results of the experiments.",
            "carry out a comprehensive set of experiments with deep learning models of extractive summarization across different domains, i.e. news, personal stories, meetings, and medical articles, as well as across different neural architectures, in order to better understand the general pros and cons of different design choices. They find that non auto-regressive sentence extraction performs as well or",
            "only assign meaning to language that falls within the KB's manually-produced schema. Recently proposed methods for open vocabulary semantic parsing overcome this limitation by learning execution models for arbitrary language, essentially using a text corpus as a kind of knowledge base. However, all prior approaches to open vocabulary semantic parsing replace a formal KB with textual information,",
            "Task, 30 users were tasked to first browse the collection of videos, select interesting segments with start and end times, and then asked to conjecture questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves,",
            "manner, considering both the redundancy of neighbors and the query relation. The weights are estimated from data with logic rules at a coarse relation level, and neural attention network at a fine neighbor level. Experiments show that LAN outperforms baseline models significantly on two typical KG completion tasks.",
            "we build ungrounded graphs from its paraphrases. We convert these graphs to Freebase graphs. To learn this mapping, we rely on manually assembled question-answer pairs. For each training question, we first find the set of oracle grounded graphs—Freebase subgraphs which when executed yield the correct answer—derivable from the question's ungrounded graphs. These oracle graphs are then used to",
            "explore and memorize the environment via interaction; the action space in our setting (especially for the Ctrl+F command as defined in later section) is arguably larger than the tree sampling action space in DocQN. Closely related to iMRC is work by BIBREF15, in which the authors introduce a collection of synthetic tasks to train and test information-seeking capabilities in neural models. We",
            "et al. propose synthesizing images from text descriptions [3]. The group demonstrates how GANs can produce images that correspond to a user-defined text description. It thus seems feasible that by using a similar model, we can produce text samples that are conditioned upon a set of user-specified keywords. We were similarly influenced by the work of Radford et. al, who argue for the importance of",
            "Experiments"
        ]
    },
    {
        "title": "The First Evaluation of Chinese Human-Computer Dialogue Technology",
        "answer": "Exact Match (EM), Macro-averaged F1 score (F1), Mean Rank (MR), Mean Reciprocal Rank (MRR), Hits@k, BLEU, METOR, CIDEr.  F1-score, Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDEr.  Hits@k, BLEU, METOR, CIDE",
        "evidence": [
            "Evaluation Metrics",
            "way as a wrongly predicted argument component type. Therefore we also report two another evaluation metrics that help to put our results into a context. Krippendorff's INLINEFORM0 — It was also used for evaluating inter-annotator agreement (see section UID75 ). Boundary similarity BIBREF125 — Using this metric, the problem is treated solely as a segmentation task without recognizing the argument",
            "23K passages and they come from approximately 500 Wikipedia articles. All the questions and answers are obtained by crowd-sourcing. Two evaluation metrics are used: Exact Match (EM) and Macro-averaged F1 score (F1) BIBREF14 .",
            "While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\",",
            "neural seq2seq architectures, provide qualitative human performance evaluations for these models, and find that automatic evaluation metrics correlate well with human judgments.",
            "The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric.",
            "Benchmark the evaluation ::: Label-fully-unseen.",
            "Cross-Lingual Evaluation ::: Results and Discussion",
            "ahead of other entities. We use traditional evaluation metrics as in the KG completion literature, i.e., Mean Rank (MR), Mean Reciprocal Rank (MRR), and the proportion of ground truth entities ranked top-k (Hits@k, INLINEFORM6 ). Since certain candidate triplets might also be true, we follow previous works and filter out these fake negatives before ranking.",
            "if the system successfully returns the information which the user inquires or the number of dialogue turns is larger than 30 for a single task. For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The",
            "These metrics estimate the ability of our model to integrate elements from the table in its descriptions. Particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ. To do so, we follow the protocol presented in BIBREF10. First, we apply an information extraction (IE) system trained on labeled relations from the gold",
            "BLEU, METOR and CIDEr scores for the evaluation. The results are summarized in Table 3 . We see that the proposed method improves in most of the metrics. The baseline methods SrcOnly and TgtOnly are worse than other methods, because they use limited data for the training. Note that the CIDEr scores correlate with human evaluations better than BLEU and METOR scores BIBREF15 . Generated captions"
        ]
    },
    {
        "title": "BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization",
        "answer": "Through Fast Rank under different ranges.",
        "evidence": [
            "the templates to form the final summaries. The advantage of such approach is it can guarantee concise and coherent summaries in no need of any training data. However, it is unrealistic to create all the templates manually since this work requires considerable domain knowledge and is also labor-intensive. Fortunately, the summaries of some specific training articles can provide similar guidance to",
            "What is the Model Learning?",
            "More study of learned embeddings, using more data and word types, is needed to confirm such patterns in general. Improvements in unseen word embeddings from the classifier embedding space to the Siamese embedding space (such as for democracy, morning, and basketball) are a likely result of optimizing the model for relative distances between words.",
            "Training Dataset",
            "through Transformers originating from the pretrained models or created more generally by the user from user-specifications. Model - All models follow the same hierarchy of abstraction: a base class implements the model's computation graph from encoding (projection on the embedding matrix) through the series of self-attention layers and up to the last layer hidden states. The base class is",
            "set respectively. During training, after selecting the possible class labels, the system extracts the features of the questions and creates a model by passing through a classifier algorithm with the extracted features and class labels. During validation, the system extracts the features of the question and passes it into the model created during training and predicts the answer type.",
            "and best templates identified by Fast Rank under different ranges. As shown in Table TABREF47 , the performance of our model improves constantly with the improvement of template quality (larger ranges lead to better chances for good templates). Even with randomly-selected templates, our model still works with stable performance, demonstrating its robustness.",
            "network potentials, making it possible to (1) utilize the expressive power of neural network representations to smooth over superficial differences in the surface forms, (2) model pairwise and transitive relationships between tags, and (3) accurately generate tag sets that are unseen or rare in the training data. Experiments on four languages from the Universal Dependencies Treebank demonstrate",
            "annotated data is provided for training a model (since usually this kind of model fails to generalize in other domains and other tasks), therefore, we are encouraged to learn models with open-data or test-agnostic data. In this way, the learned models behave more like humans.",
            "a certain degree. As such, it may be easy to learn cue phrases and other fixed expressions that indicate the presence of some element (i.e., pattern matching). On the other hand, the structure and vocabulary of the texts may be very different from the types of corpora modern deep models are trained on; for example, researchers have shown that models for processing the scientific literature",
            "What Does Zero-shot Transfer Model Learn? ::: Code-switching Dataset",
            "Inductive Embedding Models"
        ]
    },
    {
        "title": "AttSum: Joint Learning of Focusing and Summarization with Neural Attention",
        "answer": "He BIBREF29, Chisholm BIBREF6, NTEE BIBREF16, AIDA BIBREF22, GLUE, SuperGLUE, KVMemNet.",
        "evidence": [
            "Models for Comparison",
            "lean and modular architecture.[3] purpleWe evaluate our model and compare with previous works in both extractive and abstractive summarization on two large scientific paper datasets, which contain documents that are much longer than in previously used corpora.[4] purpleOur model not only achieves state-of-the-art on these two datasets, but in an additional experiment, in which we consider",
            "in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data.",
            "GLUE and SuperGLUE",
            "We compare NCEL with the following state-of-the-art EL methods including three local models and three types of global models: Local models: He BIBREF29 and Chisholm BIBREF6 beat many global models by using auto-encoders and web links, respectively, and NTEE BIBREF16 achieves the best performance based on joint embeddings of words and entities. Iterative model: AIDA BIBREF22 links entities by",
            "procedure for the model which has both attention and conflict, the updates are much smoother.",
            "of any manual annotation or handcrafted resource. Our model's performance is compared to the following systems, for which results are reported in the referred literature. Please note that no other existing model was re-implemented, and results are those reported in the respective papers.",
            "What is the Model Learning?",
            "benchmarks and is known to be challenging for many current models. The second kind of questions is tweet-specific and is related to specific properties of social media data. Since both models are designed for formal-text passages and there is no special treatment for understanding user IDs and hashtags, the performance is severely limited on the questions requiring such reasoning abilities. We",
            "QA Model 2: KVMemNet",
            "models the contradicting associations like \"animal\" and \"lake\" or \"australia\" and \"world\". These two associations are the unique pairs which are instrumental in determining that the two queries are not similar.",
            "to see the strengths of our model as well as possible limitations of our current approaches."
        ]
    },
    {
        "title": "A Simple Approach to Multilingual Polarity Classification in Twitter",
        "answer": "stop words, foreign words, words with special orthography, names and surnames. 7 languages, XQuAD, TyDiQA, PAWS-X, MLQA, RusVectōrēs, fastText, web, RDT.  English, Vietnamese, Portuguese, Spanish, German, French, Italian.  monolingual data, BT, NMT, paraphrase identification task, translation, domain adaptation, quality.  RusVectōrēs, fastText, web, RDT, spelling features, emoticons, word-based n-grams, character-based q-grams, language dependent features.  SVO Extraction, commodity server, attention loss, CTC loss, linear interpolation weight, INLINEFORM4, INLINEFORM5, INLINEFORM6.  INLINEFORM0, INLINEFORM1, INLINEFORM2, INLINEFORM3, INLINEFORM4, INLINEFORM5, INLINEFORM6.  INLINEFORM7, INLINEFORM8, INLINEFORM9, INLINEFORM10, INLINEFORM11, INLINEFORM12, INLINEFORM13, INLINEFORM14, INLINEFORM15, INLINEFORM16, INLINEFORM17, INLINEFORM18, INLINEFORM19, INLINEFORM20, INLINEFORM21, INLINEFORM22, INLINEFORM23",
        "evidence": [
            "Framework",
            "important follow-up analysis might involve the comparison of the performance of representation learning models on multilingual datasets for both word-level semantic similarity and sentence-level Natural Language Understanding. In particular, Multi-SimLex fills a gap in available resources for multilingual NLP and might help understand how lexical and compositional semantics interact if put",
            "Principal components of different models",
            "Cross-language Features",
            "Framework for Processing Portuguese Text ::: SVO Extraction",
            "The search space contains more than 331 thousand configurations when limited to multilingual and language independent parameters; while the search space reaches close to 4 million configurations when we add our three language-dependent parameters. Depending on the size of the training set, each configuration needs several minutes on a commodity server to be evaluated; thus, an exhaustive",
            "training and decoding. The basic network architecture is shown in Fig. FIGREF7 . Let INLINEFORM0 be a INLINEFORM1 -length speech feature sequence and INLINEFORM2 be a INLINEFORM3 -length grapheme sequence. A multi-objective learning framework INLINEFORM4 proposed in BIBREF17 is used in this work to unify attention loss INLINEFORM5 and CTC loss INLINEFORM6 with a linear interpolation weight",
            "in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\"",
            "In this paper, we have analyzed various ways to integrate monolingual data in an NMT framework, focusing on their impact on quality and domain adaptation. While confirming the effectiveness of BT, our study also proposed significantly cheaper ways to improve the baseline performance, using a slightly modified copy of the target, instead of its full BT. When no high quality BT is available, using",
            "7 languages ii) XQuAD BIBREF87 10 languages; iii) TyDiQA BIBREF88 9 widely spoken typologically diverse languages. While MLQA and XQuAD result from the translation from an English dataset, TyDiQA was built independently in each language. Another multilingual dataset, PAWS-X BIBREF89, focused on the paraphrase identification task and was created translating the original English PAWS BIBREF90 into",
            "the following interesting features in the components: stop words: prepositions, conjunctions, etc. (RDT 1, fastText 1; in RusVectōrēs models they are absent just because they were filtered out before training); foreign words with separation into languages (fastText 2, web 2), words with special orthography or tokens in broken encoding (not presented here); names and surnames (RDT 8, fastText 3,",
            "We presented a simple to implement multilingual framework for polarity classification whose main contributions are in two aspects. On one hand, our approach can serve as a baseline to compare other classification systems. It considers techniques for text representation such as spelling features, emoticons, word-based n-grams, character-based q-grams and language dependent features. On the other"
        ]
    },
    {
        "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing",
        "answer": "S1, S2, Baseline 1, Baseline 2, Segmenter$_{\\mu }$, elementary system, Moses, NMT, Baseline Agent, Baseline 1, Baseline 2, S1, S2, Random Forests (RF)  and elementary system Segmenter$_{\\mu }$  and Baseline 1 and Baseline 2 and Baseline Agent and Moses and NMT and elementary system Segmenter$_{\\mu }$ and S1 and S2 and Random Forests (RF) and Baseline 1 and Baseline 2 and Baseline Agent and Moses and NMT and elementary system Segmenter$_{\\mu }$ and S1 and S2 and Random Forests (RF) and Baseline 1 and Baseline 2 and Baseline Agent and Moses and NMT and elementary system Segmenter$_{\\mu }$ and S1 and S2 and Random Forests (RF) and Baseline 1 and Baseline 2 and Baseline Agent and Moses and NMT and elementary system Segmenter$_{\\mu }$ and S1 and S2 and Random Forests (RF) and Baseline 1 and Baseline 2 and Baseline Agent and Moses and NMT",
        "evidence": [
            "Baseline",
            "Besides the analysis on different reasoning types, we also look into the performance over questions with different first tokens in the development set, which provide us an automatic categorization of questions. According to the results in Table TABREF46 , the three neural baselines all perform the best on “Who” and “Where” questions, to which the answers are often named entities. Since the tweet",
            "that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "Preliminaries and Framework",
            "The elementary system Segmenter$_{\\mu }$ (baseline) relies solely on a list of discursive markers to perform the segmentation. It replaces the appearance of a marker in the list with a special symbol, for example $\\mu $, which indicates a boundary between the right and left segment. Be the sentence of the preceding example: La ville d'Avignon est la capitale du Vaucluse, qui est un département du",
            "Baseline Agent ::: Memory and Reward Shaping ::: Memory",
            "of the baseline model, we resume training with a 2M sentence in-domain corpus mixed with an equal amount of randomly selected out-of-domain natural sentences, with the same architecture and training parameters, running validation every 2000 updates with a patience of 10. Since BPE units are selected based only on the out-of-domain statistics, fine-tuning is performed on sentences that are",
            "TutorialVQA Dataset ::: Basis",
            "of the 3 approaches is provided in Fig. FIGREF23 . Baseline 1: We captured and displayed the product package photos and the product title text as product description. Baseline 2: The product description was retrieved by search engine using the product titles, and then presented to the users as the top images result from Google and Bing. We also provided the product title along with the images.",
            "Dictionary-based approaches",
            "Experiments ::: Label-partially-unseen evaluation ::: Baselines.",
            "templates. Baselines. To the best of our knowledge, we are not aware of any comparable approach for this task. Therefore, the baselines we consider are the following: S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2  S2: Place the news into the most frequent section in INLINEFORM0  Learning Models. We use Random Forests (RF)"
        ]
    },
    {
        "title": "Story Ending Generation with Incremental Encoding and Commonsense Knowledge",
        "answer": "B1, B2, B3, B4, B5, B6, B7, B8, B9, B10, B11, B12, B13, B14, B15, B16, B17, B18, Baseline1, Baseline2, Baseline3, Baseline4, Baseline5, Baseline6, Baseline7, Baseline8, Baseline9, Baseline10, Baseline11, Baseline12, Baseline13, Baseline14, Baseline15, Baseline16, Baseline17, Baseline18, Oracle, Lead, HLBL, CRF-based model, Bi-LSTM-CRF model, Moses, NMT, Baseline1, Baseline2, Baseline3, Baseline4, Baseline5, Baseline6, Baseline7, Baseline8, Baseline9, Baseline10, Baseline11, Baseline12, Baseline13, Baseline14, Baseline15, Baseline16, Baseline17, Baseline18, Baseline1, Baseline2, Baseline3, Baseline4, Baseline5, Baseline6, Baseline7, Baseline8, Baseline",
        "evidence": [
            "Standard and baseline methods",
            "description with accurate mentions in a more human-like fashion. We would also like to draw attention to the number of parameters used by those architectures. We note that our scenarios relies on a lower number of parameters (14 millions) compared to all baselines (ranging from 23 to 45 millions). This outlines the effectiveness in the design of our model relying on a structure encoding, in",
            "in terms of precision, by adding roughly a 7%-10% increase. However, if both feature groups are considered separately, they significantly outperform the baseline B1.",
            "HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training.",
            "baselines: the CRF-based model with n-gram embedding, which is currently the state-of-the-art for Thai sentence segmentation, and the Bi-LSTM-CRF model, which is currently the deep learning state-of-the-art approach for sequence tagging.",
            "Datasets Used for the RQE Study",
            "that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "Experiments ::: Label-partially-unseen evaluation ::: Baselines.",
            "Besides the analysis on different reasoning types, we also look into the performance over questions with different first tokens in the development set, which provide us an automatic categorization of questions. According to the results in Table TABREF46 , the three neural baselines all perform the best on “Who” and “Where” questions, to which the answers are often named entities. Since the tweet",
            "We performed an error analysis on Baseline1's results. We first observe that, in 92% of the errors, the predicted span and the ground-truth overlap. Furthermore, in 56% of the errors, the predicted spans are a subset or superset of the ground-truth spans. This indicates that the model finds the rough answer regions but fails to locate the precise boundaries. To address this issue, we plan on",
            "the CNN and RNN sentence encoders that were originally used in the two systems, respectively.  Baseline: Similar to our model, but without local context and global context, i.e. the input to MLP is the sentence representation only. Lead: Given a length limit of INLINEFORM0 words for the summary, Lead will return the first INLINEFORM1 words of the source document. Oracle: uses the Gold Standard",
            "We compare the proposed approach for translating natural language instructions into a navigation plan against alternative deep-learning models: [align=left,leftmargin=0em,labelsep=0.4em,font=] The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline"
        ]
    },
    {
        "title": "Multi-Source Syntactic Neural Machine Translation",
        "answer": "0.5",
        "evidence": [
            "Inference Without Parsed Sentences",
            "model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure FIGREF14. These plots were obtained based on the E2E training set using the inputless setting. We can see that the KL loss of VAE-LSTM-base, which uses Sigmoid annealing BIBREF0, collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss. The KL loss for both VAE-CNN",
            "system fails when given only sequential source data. On the other hand, both multi-source systems perform reasonably well without parsed data, although the BLEU scores are worse than multi-source with parsed data.",
            "information and it achieves comparable performance to UTCNN without topic information, which shows that comments also benefit performance. For platforms where user IDs are pixelated or otherwise hidden, adding comments to a text model still improves performance. In its integration of user, content, and comment information, the full UTCNN produces the highest f-scores on all Sup, Neu, and Uns",
            "on how the model is trained with unlabeled data through the modified CVT, which is our third contribution.",
            "without having to train them from scratch. These models are accessed through a simple and unified API that follows a classic NLP pipeline: setting up configuration, processing data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to",
            "answer. Ranking-based approaches achieve better performances than parsing-based approaches on WebQuestions, a benchmark dataset for KBQA. We follow ranking-based approaches, and develop both a matching-based model with features at different levels and a question generation model. More references can be found at https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art). Our work also",
            "incorporating syntactic information into NMT has the potential to improve performance. This is particularly true for source syntax, which can improve the model's representation of the source language. Recently, there have been a number of proposals for using linearized representations of parses within standard NMT BIBREF7 , BIBREF8 , BIBREF9 . Linearized parses are advantageous because they can",
            "here the LangID models have a slight advantage in WER and PER. This is somewhat surprising because the LangID models have not learned embeddings for the language ID tokens of unseen languages. Perhaps negative associations are also being learned, driving the model towards predicting more common pronunciations for unseen languages.",
            "the learning rate for different parameters at different steps. Thus it is less sensitive to initial parameters than the stochastic gradient descent.",
            "the performance (En v.s. En-XX, Zh v.s. Zh-XX). Even though we translate the training data into the same language as testing data, using the untranslated data still yield better results. For example, when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8, while the F1 score is only 44.1 for the model training on Zh-En. This shows that translation degrades the quality",
            "not shown in this paper). The effect is pronounced in compilers-4 course where none of the neural models were able to predict any intervened threads. This is due to the inherent weakness of standard neural models, which are unable to learn features well enough when faced with sparse data. The best performance of UPA indicates that the reply context of the instructor's post INLINEFORM0 correlates"
        ]
    },
    {
        "title": "Domain Adaptation for Neural Networks by Parameter Augmentation",
        "answer": "270K tokens.",
        "evidence": [
            "set on which the system is trained (450K tokens), and 2) example sentences extracted from the English Vocabulary Profile (270K tokens).. While there are other text corpora that could be used (e.g., Wikipedia and news articles), our development experiments showed that keeping the writing style and vocabulary close to the target domain gives better results compared to simply including more data. We",
            "How Many Expert Annotations?",
            "The model used here is similar to the original implementation of BIBREF4 . The exact target GRU equation is:  $",
            "Target Task and Setup",
            "by the lack of labeled data in a target domain, researchers have studied the behavior of machine learning methods in cross-domain settings BIBREF2 , BIBREF11 , BIBREF10 and came up with various domain adaptation techniques BIBREF12 , BIBREF5 , BIBREF6 , BIBREF9 . In cross-domain classification, a classifier is trained on data from a source domain and tested on data from a (different) target",
            "Examples and Attention Visualization",
            "Raffel et al. provide the following example for MNLI BIBREF6, where the goal is to predict whether a premise implies (“entailment”) or contradicts (“contradiction”) a hypothesis, or neither (“neutral”). Thus, a training example becomes: “mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.” with “entailment” as the corresponding ground truth target",
            "set and test set are only in the target language. From Table TABREF32 , we can see there is also considerable variance in the number of unique tags and tag sets found in each of these language pairs.",
            "of our approach, we train the system on five domains and test on the remaining one for all six domains (which we report as cross-domain validation).",
            "of INLINEFORM5 . An overview framework of SQA is shown in Figure FIGREF1 . In this paper, we frame the source domain as reference transcriptions and the target domain as ASR hypotheses. Hence, we can collect source domain data more easily, and adapt the model to the target domain. In this task, when the machine is given a spoken document, it needs to find the answer of a question from the spoken",
            "chose these videos because of the amount of procedural information stored in each video for which the user may ask. Though there is only one domain, each video corresponds to a different overall goal.",
            "our transductive algorithm. Although the test samples are required beforehand, their labels are not necessary. Hence, our approach is suitable in situations where unlabeled data from the target domain can be collected cheaply, and such situations appear very often in practice, considering the great amount of data available on the Web."
        ]
    },
    {
        "title": "Interactive Machine Comprehension with Information Seeking Agents",
        "answer": "navigation commands, query commands, and document commands.",
        "evidence": [
            "Model Configuration and Training",
            "contained therein. The supporting document is, more often than not, static and fully observable. This raises concerns, since models may find answers simply through shallow pattern matching; e.g., syntactic similarity between the words in questions and documents. As pointed out by BIBREF5, for questions starting with when, models tend to predict the only date/time answer in the supporting",
            "a knowledge base to facilitate the interpretation of navigation commands. More specifically, the proposed model takes as input user directions in text form, the behavioral graph of the environment encoded as INLINEFORM0 node; edge; node INLINEFORM1 triplets, and the initial location of the robot in the graph. The model then predicts a set of behaviors to reach the desired destination according to",
            "Experimental Setup ::: Privacy Question Answering ::: Baselines",
            "We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making. We believe that this setting can contribute in scaling models to web-level QA scenarios.",
            "Do models generalize explicit supervision, or just memorize it?",
            "models used in the experiments are trained with batch size 20, using adam with learning rate INLINEFORM0 and the early stop strategy. The dimension of the hidden state is set to 96 for all layers, and the number of self-attention heads is set to 2. The setup is slightly different but better than the setting suggested by the original QAnet.",
            "environments is straightforward and general. Almost all MRC datasets can be used to study interactive, information-seeking behavior through similar modifications. We hypothesize that such behavior can, in turn, help in solving real-world MRC problems involving search.",
            "interactive interface that leverages the generative capabilities of pretrained architectures like GPT, GPT2 and XLNet to suggest text like an auto-completion plugin. Generating samples is also often used to qualitatively (and subjectively) evaluate the generation quality of language models BIBREF9. Given the impact of the decoding algorithm (top-K sampling, beam-search, etc.) on generation",
            "models the contradicting associations like \"animal\" and \"lake\" or \"australia\" and \"world\". These two associations are the unique pairs which are instrumental in determining that the two queries are not similar.",
            "setup and baseline agent presently use only a single word with the query command. However, a host of other options should be considered in future work. For example, multi-word queries with fuzzy matching are more realistic. It would also be interesting for an agent to generate a vector representation of the query in some latent space. This vector could then be compared with precomputed document",
            "phase, a proper one is selected based on the input information. Although these models accomplished their mission to a certain extent, they still suffer from the following three challenges. First, the predefined compositional functions cannot cover all the compositional rules; Second, they require more learnable parameters, suffering from the problem of overfitting; Third, it is difficult to"
        ]
    },
    {
        "title": "Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models",
        "answer": "jointly trained hybrid model. and hierarchical (HLSTM) encoding/attention.  and HLSTM+MSA and HLSTM.  and IE+MSA and IE.  and HLSTM+MSA and HLSTM.  and IE+MSA and IE.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HLSTM+MSA and HLSTM.  and HL",
        "evidence": [
            "HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text.",
            "and hierarchical (HLSTM) encoding/attention in utilizing context clues. Furthermore, using commonsense knowledge leads to significant improvements in logicality. The comparison in logicality between IE+MSA and IE (1.26/1.24 vs. 1.10, p-value= $0.028/0.042$ for GA/CA, respectively), HLSTM+MSA and HLSTM (1.06/1.02 vs. 0.84, p-value $<0.001/0.001$ for GA/CA, respectively), and HLSTM+MSA and",
            "labeling methods have been used to model relationships between posts. For example, Hasan and Ng use hidden Markov models (HMMs) to model dependent relationships to the preceding post BIBREF9 ; Burfoot et al. use iterative classification to repeatedly generate new estimates based on the current state of knowledge BIBREF11 ; Sridhar et al. use probabilistic soft logic (PSL) to model reply links via",
            "into their models, on top of the training data BIBREF12 . They showed that performances are better when this external lexical information is integrated in the form of additional lexical features than when the external lexicon is used as constraints at tagging time. These lexical features can also be divided into local lexical features (for example the list of possible tags known to the external",
            "Limitations of LSTM-LMs ::: Results",
            "copy-marked setup, which is the best way to use a copy of the target (although the performance on the out-of-domain tests is at most the same as the baseline). Such gains may look surprising, since the NMT model does not need to learn to translate but only to copy the source. This is indeed what happens: to confirm this, we built a fake test set having identical source and target side (in",
            "operations can be learned implicitly by meta TreeNN, such as the composition of noun phrases and verb phrases. The contributions of the paper can be summed up as follows.",
            "author, simply adding comments and post contents together merely adds noise to the model. Among all UTCNN variations, we find that user information is most important, followed by topic and comment information. UTCNN without user information shows results similar to SVMs — it does well for Sup and Neu but detects no Uns. Its best f-scores on both Sup and Neu among all methods show that with enough",
            "pretraining. In remaining parts, Section \"Related Work\" presents related work. Section \"Model Description\" gives details of our classification model. Section \"Model Enhancements\" introduces two tricks that enhance system performance: mutual-learning and pretraining. Section \"Experiments\" reports experimental results. Section \"Conclusion\" concludes this work.",
            "voting on the output of each modality network. EF-LSTM (Early Fusion LSTM) concatenates the inputs from different modalities at each time-step and uses that as the input to a single LSTM. We also implement the Stacked, (EF-SLSTM) Bidirectional (EF-BLSTM) and Stacked Bidirectional (EF-SBLSTM) LSTMs for stronger baselines. The best performing model is reported as EF-LSTM $_{(\\star )}$ , $\\star \\in",
            "given by the raw text are weak, and show that multi-task learning with a binary classification task to predict the upcoming verb form (singular or plural) helps models aware of the target syntax (subject-verb agreement). Our experiments basically confirm and strengthen this argument, with even stronger learning signals from negative examples, and we argue this allows to evaluate the true capacity",
            "data involving related languages, are not available. Even though we focused on only using parallel data, our results are also relevant for work on using auxiliary data to improve low-resource MT. Supervised systems serve as an important baseline to judge the effectiveness of semisupervised or unsupervised approaches, and the quality of supervised systems trained on little data can directly impact"
        ]
    },
    {
        "title": "BERT-Based Arabic Social Media Author Profiling",
        "answer": "1,100 users' tweets, 162,829 tweets, gender dataset.",
        "evidence": [
            "In-domain and out-of-domain data",
            "To further improve the performance of our models, we introduce in-house labeled data that we use to fine-tune BERT. For the gender classification task, we manually label an in-house dataset of 1,100 users with gender tags, including 550 female users, 550 male users. We obtain 162,829 tweets by crawling the 1,100 users' timelines. We combine this new gender dataset with the gender TRAIN data (from",
            "Datasets Used for the RQE Study",
            "by providing standardized datasets, evaluations, and a state-of-the-art entailment system. All datasets and codes are released.",
            "Implementation Details",
            "Data",
            "Methodology ::: Data presentation",
            "and Mix-Source, our NMT systems achieved the best improvement on the dataset. In the future, we will exploit more domain and multilingual information to improve the quality of the systems.",
            "Previous Work and Evaluation Data",
            "TutorialVQA Dataset ::: Data Collection",
            "depend on their involvement in the game. We followed the data partitions introduced with the dataset and used a train/validation/test sets of respectively $3,398$/727/728 (data-structure, description) pairs.",
            "Data acquisition"
        ]
    },
    {
        "title": "WiSeBE: Window-based Sentence Boundary Evaluation",
        "answer": "S1, S2.",
        "evidence": [
            "To exemplify the INLINEFORM0 score we evaluated and compared the performance of two different SBD systems over a set of YouTube videos in a multi-reference enviroment. The first system (S1) employs a Convolutional Neural Network to determine if the middle word of a sliding window corresponds to a SU boundary or not BIBREF30 . The second approach (S2) by contrast, introduces a bidirectional",
            "Results and discussions",
            "that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
            "In general, distributed word representations are evaluated using a word similarity task. For instance, WordSim353 2002:PSC:503104.503110, MC BIBREF2 , RG BIBREF3 , and SCWS Huang:2012:IWR:2390524.2390645 have been used to evaluate word similarities in English. Moreover, baker-reichart-korhonen:2014:EMNLP2014 built a verb similarity dataset (VSD) based on WordSim353 because there was no dataset of",
            "and SVM have shown similar performance. MLP takes advantage of multiple hidden layers in order to take non-linearly separable samples in a linearly separable condition. SVM accomplishes this same feat by taking the samples to a higher dimensional hyperplane where the samples are linearly separable. Gradient Boosting Classifier (GBC) and Random Forest (RF) both utilize a set of decision trees and",
            "a better comparison between AV methods. Among others, we explained that the performance measure AUC is meaningless in regard to unary or specific non-optimizable AV methods, which involve a fixed decision criterion (for example, NNCD). Additionally, we mentioned that determinism must be fulfilled such that an AV method can be rated as reliable. Moreover, we clarified a number of misunderstandings",
            "is suggestive of how one could combine the two in methods for KB completion. Initial work exploring this direction has already been done by Toutanova and Chen toutanova-2015-observed-vs-latent-kbc.",
            "SBD research has been focused on two different aspects; features and methods. Regarding the features, some work focused on acoustic elements like pauses duration, fundamental frequencies, energy, rate of speech, volume change and speaker turn BIBREF17 , BIBREF18 , BIBREF19 . The other kind of features used in SBD are textual or lexical features. They rely on the transcript content to extract",
            "of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size influences. The block “tricks” (28–29) shows the system performance when no",
            "in Vietnamese. Each participated user was shown the product description generated from each approach, and was asked to identify what the products were and how to use them. The users' responses were then recorded in Vietnamese and were assigned to a score if they \"matched\" the correct answer by 3 experts who were bilingual in English and Vietnamese. In this study, we used the \"mean opinion score\"",
            "a collection of similar question pairs, they searched community question answering sites such as Yahoo! and Answers.com. In contrast, the ECNU-ICA system achieved the best performance of 1.895 in the open-domain task but an average score of only 0.402 in the medical task. As the ECNU-ICA approach also relied on a neural network for question matching, this result shows that training",
            "are sentimentally similar to each other could be clustered more accurately. Besides clustering, we also employed the SVD method to perform dimensionality reduction on the unsupervised dictionary algorithm and used the newly generated matrix by combining it with other subapproaches. The number of dimensions is chosen as 200 again according to the $U$ matrix. The details are given in Section 3.4."
        ]
    },
    {
        "title": "Recognizing Musical Entities in User-generated Content",
        "answer": "Naive LSTM-LMs, Transformers, CNN1, CNN2, CNN3, LSTM-LMs, ROUGE.  BIBREF13, BIBREF14, BIBREF24, BIBREF25, BIBREF4.  Koppel, Stein, BIBREF.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.  ROUGE.",
        "evidence": [
            "complexity) among some machine learning based approaches used in QC for Bengali language.",
            "sharing and large-scale community efforts resulting in the development of standard libraries, an increased availability of published research code and strong incentives to share state-of-the-art pretrained models. The combination of these factors has lead researchers to reproduce previous results more easily, investigate current approaches and test hypotheses without having to redevelop them",
            "What is the Model Learning?",
            "Task, 30 users were tasked to first browse the collection of videos, select interesting segments with start and end times, and then asked to conjecture questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves,",
            "methods. In regard to this, only a few attempts have been made so far. In 2004, for example, Koppel and Schler BIBREF13 described for the first time the connection between AV and unary classification, also known as one-class classification. In 2008, Stein et al. BIBREF14 compiled an overview of important algorithmic building blocks for AV where, among other things, they also formulated three AV",
            "enough for the considered classification problem), the learned filters in CNN2 and CNN3 do not only focus on isolated words but additionally consider bigrams or trigrams of words, as their results differ a lot from the CNN1 model in the first deletion experiment. In order to quantitatively evaluate and compare the ML models in combination with a relevance decomposition or explanation technique,",
            "In the summarization area, the application of deep learning techniques has attracted more and more interest. BIBREF24 used unsupervised auto-encoders to represent both manual and system summaries for the task of summary evaluation. Their method, however, did not surpass ROUGE. Recently, some works BIBREF25 , BIBREF4 have tried to use neural networks to complement sentence ranking features.",
            "Experiments on Additional Losses ::: Naive LSTM-LMs perform well",
            "recognition. We instantiate several machine learning algorithms to perform entity recognition combining task-specific and corpus-based features. We also show how to improve recognition results by jointly considering formal and user-generated content",
            "features provided by Transformers like tokenizers, dedicated processing scripts for common downstream tasks and sensible default hyper-parameters for high performance on a range of language understanding and generation tasks. The last direction is related to machine learning research frameworks that are specifically used to test, develop and train architectures like Transformers. Typical examples",
            "large datasets to improve their VQA algorithms. These datasets include visual questions and human-supplied answers. Such data is critical for teaching machine learning algorithms how to answer questions by example. Such data is also critical for evaluating how well VQA algorithms perform. In general, “bigger\" data is better. Current methods to create these datasets assume a fixed number of human",
            "the data are discussed and applications which use a bag-of-words treatment are presented. Second, the vector space approach and distance measure are introduced and discussed. Finally, the network clustering approach and model evaluations are presented."
        ]
    },
    {
        "title": "Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER",
        "answer": "Text classification.",
        "evidence": [
            "For many of the languages examined, we were able to improve on BERT's zero-resource cross-lingual performance on the MLDoc classification and CoNLL NER tasks. Language-adversarial training was generally effective, though the size of the effect appears to depend on the task. We observed that adversarial training moves the embeddings of English text and their non-English translations closer",
            "Text Classification",
            "Named Entity Recognition (NER), or alternatively Named Entity Recognition and Classification (NERC), is the task of detecting entities in an input text and to assign them to a specific class. It starts to be defined in the early '80, and over the years several approaches have been proposed BIBREF2 . Early systems were based on handcrafted rule-based algorithms, while recently several",
            "2–10% improvements in accuracy as compared to the above-mentioned RNTN and paragraph vector methods. We also seek to inject user information into the neural network model. In comparison to the research of Tang et al. on sentiment classification for product reviews, the difference is two-fold. First, we take into account multiple users (one author and potentially many likers) for one post, whereas",
            "Bi-LSTM + CRF for Multimodal NER",
            "to tune the hyper-parameters correctly for the adversarial model to continue learning throughout all of the training epochs [5]. Since both the discriminator and generator are updated via the same gradient, it is very common for the model to fall into a local minima before completing all of the defined training cycles. 2) GANs are computationally expensive to train, given that both models are",
            "that any new machine learning system for political tweet analysis would compete against. We here explore whether existing NLP systems can answer the questions \"What sentiment?\" and \"Towards whom?\" accurately for the dataset of political tweets provided by BIBREF2. In our analysis, we include NLP tools with publicly-available APIs, even if the tools were not specifically designed for short texts",
            "In this paper, we examine the use case of general adversarial networks (GANs) in the field of marketing. In particular, we analyze how GAN models can replicate text patterns from successful product listings on Airbnb, a peer-to-peer online market for short-term apartment rentals. To do so, we define the Diehl-Martinez-Kamalu (DMK) loss function as a new class of functions that forces the model's",
            "a considerable advantage toward classification accuracy. Similar results have also been reported in previous studies BIBREF34 , where it was observed that for document classification a convolutional neural network model starts to outperform a TFIDF-based linear classifier only on datasets in the order of millions of documents. This can be explained by the fact that for most topic categorization",
            "INLINEFORM9 , INLINEFORM10 , INLINEFORM11 , scalar) are the output projections for the task-specific loss and discriminator respectively, INLINEFORM12 (dim: INLINEFORM13 ) is the one-hot vector representation for the task label and INLINEFORM14 (dim: scalar) is the binary label for the adversarial task (i.e. 1 or 0 for English or non-English). In the case of NER, the task-specific loss has an",
            "The performances of the NER experiments are reported separately for three different parts of the system proposed. Table 6 presents the comparison of the various methods while performing NER on the bot-generated corpora and the user-generated corpora. Results shown that, in the first case, in the training set the F1 score is always greater than 97%, with a maximum of 99.65%. With both test sets",
            "$\\%$ ) of the proposed models comparing to Single-Turn-RNNLM. A negative number indicates performance gain. Table 2 shows the model perplexity per POS tag. All the three context dependent models produce consistent performance gain over the Single-Turn-RNNLM for pronouns, prepositions, and adverbs, with pronouns having the largest perplexity improvement. However, the proposed contextual models are"
        ]
    }
]