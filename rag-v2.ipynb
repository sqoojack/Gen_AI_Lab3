{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8371c6",
   "metadata": {},
   "source": [
    "# HW3: Document QA based on RAG\n",
    "> #### üëÄ Attention‚ùó:\n",
    "> All parameters, prompts, and code appearing in this tutorial are for demonstration purposes only and may not be the best practices.  \n",
    "> We encourage you to think creatively and try new things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f13efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas==2.2.3 jupyter==1.1.1 langchain==0.3.23 langchain-community==0.3.21 rich==14.0.0 openai==1.71.0 langchain-groq==0.3.2 langchain-ollama==0.3.1 faiss-gpu==1.7.2 numpy<2 rouge-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabeaa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.logging import RichHandler\n",
    "\n",
    "console = Console(stderr=True, record=True)\n",
    "log_handler = RichHandler(rich_tracebacks=True, console=console, markup=True)\n",
    "logging.basicConfig(format=\"%(message)s\",datefmt=\"[%X]\",handlers=[log_handler])\n",
    "log = logging.getLogger(\"rich\")\n",
    "# log.setLevel(logging.INFO)\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "DEBUG: bool = False\n",
    "DATASET_PATH: str = \"../datasets/public_dataset.json\"\n",
    "\n",
    "USING_MODEL: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "USING_PORT: int = 8092\n",
    "API_ENDPOINT: str = f\"http://192.168.0.7:{USING_PORT}/v1\"\n",
    "API_KEY: str = \"abc\"\n",
    "\n",
    "MODEL_TEMPERATURE: float = 0.3\n",
    "MODEL_MAX_TOKENS: int = 128\n",
    "RETRIEVE_TOP_K: int = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951fb75e",
   "metadata": {},
   "source": [
    "## Query a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106d611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I hope you're doing well. I'm not sure if you remember me, but we met at the conference last year. I wanted to reach out and follow\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.vllm import VLLMOpenAI\n",
    "\n",
    "def get_llm(temperature: float=0.7, max_tokens: int=128):\n",
    "  return VLLMOpenAI(\n",
    "    base_url=API_ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    "    model=USING_MODEL,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0,\n",
    "    # streaming=True,\n",
    "    # model_kwargs={\"stop\": [\".\"]},\n",
    "  )\n",
    "\n",
    "llm = get_llm(temperature=0.3, max_tokens=32)\n",
    "response = llm.invoke(\"How are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8c9e0",
   "metadata": {},
   "source": [
    "### Using Groq API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7409b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"I'm just a language model, so I don't have feelings or physical sensations like humans do. I'm here to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">provide information and help with tasks, though. How can I assist you today?\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.149090909</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009527503</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'queue_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.241976264</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.158618412</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama-3.3-70b-versatile'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'fp_3f3b593e33'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-0452b314-6899-43df-b4f2-55b8edaef9a1-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m\"I\u001b[0m\u001b[32m'm just a language model, so I don't have feelings or physical sensations like humans do. I'm here to\u001b[0m\n",
       "\u001b[32mprovide information and help with tasks, though. How can I assist you today?\"\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m41\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m39\u001b[0m,\n",
       "            \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m80\u001b[0m,\n",
       "            \u001b[32m'completion_time'\u001b[0m: \u001b[1;36m0.149090909\u001b[0m,\n",
       "            \u001b[32m'prompt_time'\u001b[0m: \u001b[1;36m0.009527503\u001b[0m,\n",
       "            \u001b[32m'queue_time'\u001b[0m: \u001b[1;36m0.241976264\u001b[0m,\n",
       "            \u001b[32m'total_time'\u001b[0m: \u001b[1;36m0.158618412\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'llama-3.3-70b-versatile'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[32m'fp_3f3b593e33'\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run-0452b314-6899-43df-b4f2-55b8edaef9a1-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m39\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m41\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m80\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "  model=\"llama-3.3-70b-versatile\",\n",
    "  # model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "  # model=\"llama-3.3-70b-specdec\",\n",
    "  # model=\"llama-3.1-8b-instant\",\n",
    "  api_key=\"gsk_xxx\",\n",
    "  temperature=0.6,\n",
    "  max_tokens=128,\n",
    "  # model_kwargs={\"frequency_penalty\": 0.8},\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"How are you?\")\n",
    "console.print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befcd852",
   "metadata": {},
   "source": [
    "## A simple chat chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d17edca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\".\n",
      "human: context: {document}\n",
      "question: {input}\n",
      "assistant: \n"
     ]
    }
   ],
   "source": [
    "# SYSTEM_PROMPT: str = \"You are a helpful assistant.\"\n",
    "# SYSTEM_PROMPT: str = \"You are a helpful assistant. You will try your best to help the user.\"\n",
    "SYSTEM_PROMPT: str = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\".\"\"\"\n",
    "\n",
    "CHAT_TEMPLATE_CAG = (\n",
    "f\"\"\"system: {SYSTEM_PROMPT}\n",
    "human: context: {{document}}\\nquestion: {{input}}\n",
    "assistant: \"\"\"\n",
    ")\n",
    "# {{document}} => {document}\n",
    "print(CHAT_TEMPLATE_CAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "957c875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.llms import BaseLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LLM Chain\n",
    "def llm_chat_chain(llm: BaseLLM):\n",
    "  chat_prompt = PromptTemplate.from_template(template=CHAT_TEMPLATE_CAG)\n",
    "  llm_chain = chat_prompt | llm | StrOutputParser()\n",
    "  return llm_chain\n",
    "\n",
    "\n",
    "llm = get_llm(MODEL_TEMPERATURE, MODEL_MAX_TOKENS)\n",
    "llm_chain = llm_chat_chain(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b4ea4",
   "metadata": {},
   "source": [
    "## Load and show the Pubilc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b71349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Extractive Summarization of Long Documents by Combining Global and Local Context',\n",
       " 'full_text': 'Abstract\\nIn this paper, we propose a novel neural single document extractive summarization model for long documents, incorporating both the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers, Pubmed and arXiv, where it outperforms previous work, both extractive and abstractive models, on ROUGE-1, ROUGE-2 and METEOR scores. We also show that, consistently with our goal, the benefits of our method become stronger as we apply it to longer documents. Rather surprisingly, an ablation study indicates that the benefits of our model seem to come exclusively from modeling the local context, even for the longest documents.\\n\\n\\nIntroduction\\nSingle-document summarization is the task of generating a short summary for a given document. Ideally, the generated summaries should be fluent and coherent, and should faithfully maintain the most important information in the source document. purpleThis is a very challenging task, because it arguably requires an in-depth understanding of the source document, and current automatic solutions are still far from human performance BIBREF0 . Single-document summarization can be either extractive or abstractive. Extractive methods typically pick sentences directly from the original document based on their importance, and form the summary as an aggregate of these sentences. Usually, summaries generated in this way have a better performance on fluency and grammar, but they may contain much redundancy and lack in coherence across sentences. In contrast, abstractive methods attempt to mimic what humans do by first extracting content from the source document and then produce new sentences that aggregate and organize the extracted information. Since the sentences are generated from scratch they tend to have a relatively worse performance on fluency and grammar. Furthermore, while abstractive summaries are typically less redundant, they may end up including misleading or even utterly false statements, because the methods to extract and aggregate information form the source document are still rather noisy. In this work, we focus on extracting informative sentences from a given document (without dealing with redundancy), especially when the document is relatively long (e.g., scientific articles). Most recent works on neural extractive summarization have been rather successful in generating summaries of short news documents (around 650 words/document) BIBREF1 by applying neural Seq2Seq models BIBREF2 . However when it comes to long documents, these models tend to struggle with longer sequences because at each decoding step, the decoder needs to learn to construct a context vector capturing relevant information from all the tokens in the source sequence BIBREF3 . Long documents typically cover multiple topics. In general, the longer a document is, the more topics are discussed. As a matter of fact, when humans write long documents they organize them in chapters, sections etc.. Scientific papers are an example of longer documents and they follow a standard discourse structure describing the problem, methodology, experiments/results, and finally conclusions BIBREF4 . To the best of our knowledge only one previous work in extractive summarization has explicitly leveraged section information to guide the generation of summaries BIBREF5 . However, the only information about sections fed into their sentence classifier is a categorical feature with values like Highlight, Abstract, Introduction, etc., depending on which section the sentence appears in. In contrast, in order to exploit section information, in this paper we propose to capture a distributed representation of both the global (the whole document) and the local context (e.g., the section/topic) when deciding if a sentence should be included in the summary Our main contributions are as follows: (i) In order to capture the local context, we are the first to apply LSTM-minus to text summarization. LSTM-minus is a method for learning embeddings of text spans, which has achieved good performance in dependency parsing BIBREF6 , in constituency parsing BIBREF7 , as well as in discourse parsing BIBREF8 . With respect to more traditional methods for capturing local context, which rely on hierarchical structures, LSTM-minus produces simpler models i.e. with less parameters, and therefore faster to train and less prone to overfitting. (ii) We test our method on the Pubmed and arXiv datasets and results appear to support our goal of effectively summarizing long documents. In particular, while overall we outperform the baseline and previous approaches only by a narrow margin on both datasets, the benefit of our method become much stronger as we apply it to longer documents. purpleFurthermore, in an ablation study to assess the relative contributions of the global and the local model we found that, rather surprisingly, the benefits of our model seem to come exclusively from modeling the local context, even for the longest documents.[6] (iii) In order to evaluate our approach, we have created oracle labels for both Pubmed and arXiv BIBREF9 , by applying a greedy oracle labeling algorithm. The two datasets annotated with extractive labels will be made public.\\n\\n\\nExtractive summarization\\nTraditional extractive summarization methods are mostly based on explicit surface features BIBREF10 , relying on graph-based methods BIBREF11 , or on submodular maximization BIBREF12 . Benefiting from the success of neural sequence models in other NLP tasks, chenglapata propose a novel approach to extractive summarization based on neural networks and continuous sentence features, which outperforms traditional methods on the DailyMail dataset. In particular, they develop a general encoder-decoder architecture, where a CNN is used as sentence encoder, a uni-directional LSTM as document encoder, with another uni-directional LSTM as decoder. To decrease the number of parameters while maintaining the accuracy, summarunner present SummaRuNNer, a simple RNN-based sequence classifier without decoder, outperforming or matching the model of BIBREF2 . They take content, salience, novelty, and position of each sentence into consideration when deciding if a sentence should be included in the extractive summary. Yet, they do not capture any aspect of the topical structure, as we do in this paper. So their approach would arguably suffer when applied to long documents, likely containing multiple and diverse topics. While SummaRuNNer was tested only on news, EMNLP2018 carry out a comprehensive set of experiments with deep learning models of extractive summarization across different domains, i.e. news, personal stories, meetings, and medical articles, as well as across different neural architectures, in order to better understand the general pros and cons of different design choices. They find that non auto-regressive sentence extraction performs as well or better than auto-regressive extraction in all domains, where by auto-regressive sentence extraction they mean using previous predictions to inform future predictions. Furthermore, they find that the Average Word Embedding sentence encoder works at least as well as encoders based on CNN and RNN. In light of these findings, our model is not auto-regressive and uses the Average Word Embedding encoder.\\n\\n\\nExtractive summarization on Scientific papers\\nResearch on summarizing scientific articles has a long history BIBREF13 . Earlier on, it was realized that summarizing scientific papers requires different approaches than what was used for summarizing news articles, due to differences in document length, writing style and rhetorical structure. For instance, BIBREF14 presented a supervised Naive Bayes classifier to select content from a scientific paper based on the rhetorical status of each sentence (e.g., whether it specified a research goal, or some generally accepted scientific background knowledge, etc.). More recently, researchers have extended this work by applying more sophisticated classifiers to identify more fine-grain rhetorical categories, as well as by exploiting citation contexts. 2013-discourse propose the CoreSC discourse-driven content, which relies on CRFs and SVMs, to classify the discourse categories (e.g. Background, Hypothesis, Motivation, etc.) at the sentence level. The recent work most similar to ours is BIBREF5 where, in order to determine whether a sentence should be included in the summary, they directly use the section each sentence appears in as a categorical feature with values like Highlight, Abstract, Introduction, etc.. In this paper, instead of using sections as categorical features, we rely on a distributed representation of the semantic information within each section, as the local context of each sentence. In a very different line of work, cohan-2015-scientific form the summary by also exploiting information on how the target paper is cited in other papers. Currently, we do not use any information from citation contexts.\\n\\n\\nDatasets for long documents\\nsummarydataset provide a comprehensive overview of the current datasets for summarization. Noticeably, most of the larger-scale summarization datasets consists of relatively short documents, like CNN/DailyMail BIBREF1 and New York Times BIBREF15 . One exception is BIBREF9 that recently introduce two large-scale datasets of long and structured scientific papers obtained from arXiv and PubMed. These two new datasets contain much longer documents than all the news datasets (See Table TABREF6 ) and are therefore ideal test-beds for the method we present in this paper.\\n\\n\\nNeural Abstractive summarization on long documents\\nWhile most current neural abstractive summarization models have focused on summarizing relatively short news articles (e.g., BIBREF16 ), few researchers have started to investigate the summarization of longer documents by exploiting their natural structure. agents present an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. The encoding task is divided across several collaborating agents, each is responsible for a subsection of text through a multi-layer LSTM with word attention. Their model seems however overly complicated when it comes to the extractive summarization task, where word attention is arguably much less critical. So, we do not consider this model further in this paper. discourselongdocument also propose a model for abstractive summarization taking the structure of documents into consideration with a hierarchical approach, and test it on longer documents with section information, i.e. scientific papers. In particular, they apply a hierarchical encoder at the word and section levels. Then, in the decoding step, they combine the word attention and section attention to obtain a context vector. This approach to capture discourse structure is however quite limited both in general and especially when you consider its application to extractive summarization. First, their hierarchical method has a large number of parameters and it is therefore slow to train and likely prone to overfitting. Secondly, it does not take the global context of the whole document into account, which may arguably be critical in extractive methods, when deciding on the salience of a sentence (or even a word). The extractive summarizer we present in this paper tries to address these limitations by adopting the parameter lean LSTM-minus method, and by explicitly modeling the global context.\\n\\n\\nLSTM-Minus\\nThe LSTM-Minus method is first proposed in BIBREF6 as a novel way to learn sentence segment embeddings for graph-based dependency parsing, i.e. estimating the most likely dependency tree given an input sentence. For each dependency pair, they divide a sentence into three segments (prefix, infix and suffix), and LSTM-Minus is used to represent each segment. They apply a single LSTM to the whole sentence and use the difference between two hidden states INLINEFORM0 to represent the segment from word INLINEFORM1 to word INLINEFORM2 . This enables their model to learn segment embeddings from information both outside and inside the segments and thus enhancing their model ability to access to sentence-level information. The intuition behind the method is that each hidden vector INLINEFORM3 can capture useful information before and including the word INLINEFORM4 . Shortly after, lstm-minusconstituency use the same method on the task of constituency parsing, as the representation of a sentence span, extending the original uni-directional LSTM-Minus to the bi-directional case. More recently, inspired by the success of LSTM-Minus in both dependency and constituency parsing, lstm-minusdiscourse extend the technique to discourse parsing. They propose a two-stage model consisting of an intra-sentential parser and a multi-sentential parser, learning contextually informed representations of constituents with LSTM-Minus, at the sentence and document level, respectively. Similarly, in this paper, when deciding if a sentence should be included in the summary, the local context of that sentence is captured by applying LSTM-Minus at the document level, to represent the sub-sequence of sentences of the document (i.e., the topic/section) the target sentence belongs to.\\n\\n\\nOur Model\\nIn this work, we propose an extractive model for long documents, incorporating local and global context information, motivated by natural topic-oriented structure of human-written long documents. The architecture of our model is shown in Figure FIGREF10 , each sentence is visited sequentially in the original document order, and a corresponding confidence score is computed expressing whether the sentence should be included in the extractive summary. Our model comprises three components: the sentence encoder, the document encoder and the sentence classifier.\\n\\n\\nSentence Encoder\\nThe goal of the sentence encoder is mapping sequences of word embeddings to a fixed length vector (See bottom center of Figure FIGREF10 ). There are several common methods to embed sentences. For extractive summarization, RNN were used in BIBREF17 , CNN in BIBREF2 , and Average Word Embedding in BIBREF18 . EMNLP2018 experiment with all the three methods and conclude that Word Embedding Averaging is as good or better than either RNNs or CNNs for sentence embedding across different domains and summarizer architectures. Thus, we use the Average Word Embedding as our sentence encoder, by which a sentence embedding is simply the average of its word embeddings, i.e. INLINEFORM0  Besides, we also tried the popular pre-trained BERT sentence embedding BIBREF19 , but initial results were rather poor. So we do not pursue this possibility any further.\\n\\n\\nDocument Encoder\\nAt the document level, a bi-directional recurrent neural network BIBREF20 is often used to encode all the sentences sequentially forward and backward, with such model achieving remarkable success in machine translation BIBREF21 . As units, we selected gated recurrent units (GRU) BIBREF22 , in light of favorable results shown in BIBREF23 . The GRU is represented with the standard reset, update, and new gates. The output of the bi-directional GRU for each sentence INLINEFORM0 comprises two hidden states, INLINEFORM1 as forward and backward hidden state, respectively. A. Sentence representation As shown in Figure FIGREF10 (A), for each sentence INLINEFORM0 , the sentence representation is the concatenation of both backward and forward hidden state of that sentence. INLINEFORM1  In this way, the sentence representation not only represents the current sentence, but also partially covers contextual information both before and after this sentence. B. Document representation The document representation provides global information on the whole document. It is computed as the concatenation of the final state of the forward and backward GRU, labeled as B in Figure FIGREF10 . BIBREF24 INLINEFORM0  C. Topic segment representation To capture the local context of each sentence, namely the information of the topic segment that sentence falls into, we apply the LSTM-Minus method, a method for learning embeddings of text spans. LSTM-Minus is shown in detail in Figure 1 (left panel C), each topic segment is represented as the subtraction between the hidden states of the start and the end of that topic. As illustrated in Figure FIGREF10 , the representation for section 2 of the sample document (containing three sections and eight sentences overall) can be computed as INLINEFORM0 , where INLINEFORM1 are the forward hidden states of sentence 5 and 2, respectively, while INLINEFORM2 are the backward hidden states of sentence 3 and 6, respectively. In general, the topic segment representation INLINEFORM3 for segment INLINEFORM4 is computed as: INLINEFORM5  where INLINEFORM0 is the index of the beginning and the end of topic INLINEFORM1 , INLINEFORM2 and INLINEFORM3 denote the topic segment representation of forward and backward, respectively. The final representation of topic INLINEFORM4 is the concatenation of forward and backward representation INLINEFORM5 . To obtain INLINEFORM6 and INLINEFORM7 , we utilize subtraction between GRU hidden vectors of INLINEFORM8 and INLINEFORM9 , and we pad the hidden states with zero vectors both in the beginning and the end, to ensure the index can not be out of bound. The intuition behind this process is that the GRUs can keep previous useful information in their memory cell by exploiting reset, update, and new gates to decide how to utilize and update the memory of previous information. In this way, we can represent the contextual information within each topic segment for all the sentences in that segment.\\n\\n\\nDecoder\\nOnce we have obtained a representation for the sentence, for its topic segment (i.e., local context) and for the document (i.e., global context), these three factors are combined to make a final prediction INLINEFORM0 on whether the sentence should be included in the summary. We consider two ways in which these three factors can be combined. Concatenation We can simply concatenate the vectors of these three factors as, INLINEFORM0  where sentence INLINEFORM0 is part of the topic INLINEFORM1 , and INLINEFORM2 is the representation of sentence INLINEFORM3 with topic segment information and global context information. Attentive context As local context and global context are all contextual information of the given sentence, we use an attention mechanism to decide the weight of each context vector, represented as INLINEFORM0  where the INLINEFORM0 is the weighted context vector of each sentence INLINEFORM1 , and assume sentence INLINEFORM2 is in topic INLINEFORM3 . Then there is a final multi-layer perceptron(MLP) followed with a sigmoid activation function indicating the confidence score for selecting each sentence: INLINEFORM0 \\n\\n\\nExperiments\\nTo validate our method, we set up experiments on the two scientific paper datasets (arXiv and PubMed). With ROUGE and METEOR scores as automatic evaluation metrics, we compare with previous works, both abstractive and extractive.\\n\\n\\nTraining\\nThe weighted negative log-likelihood is minimized, where the weight is computed as INLINEFORM0 , to solve the problem of highly imbalanced data (typical in extractive summarization). INLINEFORM1  where INLINEFORM0 represent the ground-truth label of sentence INLINEFORM1 , with INLINEFORM2 meaning sentence INLINEFORM3 is in the gold-standard extract summary.\\n\\n\\nExtractive Label Generation\\nIn the Pubmed and arXiv datasets, the extractive summaries are missing. So we follow the work of BIBREF18 on extractive summary labeling, constructing gold label sequences by greedily optimizing ROUGE-1 on the gold-standard abstracts, which are available for each article. The algorithm is shown in Appendix A.\\n\\n\\nImplementation Details\\nWe train our model using the Adam optimizer BIBREF25 with learning rate INLINEFORM0 and a drop out rate of 0.3. We use a mini-batch with a batch size of 32 documents, and the size of the GRU hidden states is 300. For word embeddings, we use GloVe BIBREF26 with dimension 300, pre-trained on the Wikipedia and Gigaword. The vocabulary size of our model is 50000. All the above parameters were set based on BIBREF18 without any fine-tuning. Again following BIBREF18 , we train each model for 50 epochs, and the best model is selected with early stopping on the validation set according to Rouge-2 F-score.\\n\\n\\nModels for Comparison\\nWe perform a systematic comparison with previous work in extractive summarization. For completeness, we also compare with recent neural abstractive approaches. In all the experiments, we use the same train/val/test splitting. Traditional extractive summarization models: SumBasic BIBREF27 , LSA BIBREF28 , and LexRank BIBREF29  Neural abstractive summarization models: Attn-Seq2Seq BIBREF1 , Pntr-Gen-Seq2Seq BIBREF16 and Discourse-aware BIBREF9  Neural extractive summarization models: Cheng&Lapata BIBREF2 and SummaRuNNer BIBREF17 . Based on BIBREF18 , we use the Average Word Encoder as sentence encoder for both models, instead of the CNN and RNN sentence encoders that were originally used in the two systems, respectively.  Baseline: Similar to our model, but without local context and global context, i.e. the input to MLP is the sentence representation only. Lead: Given a length limit of INLINEFORM0 words for the summary, Lead will return the first INLINEFORM1 words of the source document. Oracle: uses the Gold Standard extractive labels, generated based on ROUGE (Sec. 4.2).\\n\\n\\nResults and Analysis\\nFor evaluation, we follow the same procedure as in BIBREF18 . Summaries are generated by selecting the top ranked sentences by model probability INLINEFORM0 , until the length limit is met or exceeded. Based on the average length of abstracts in these two datasets, we set the length limit to 200 words. We use ROUGE scores BIBREF30 and METEOR scores BIBREF31 between the model results and ground-truth abstractive summaries as evaluation metric. The unigram and bigram overlap (ROUGE-1,2) are intended to measure the informativeness, while longest common subsequence (ROUGE-L) captures fluency to some extent BIBREF2 . METEOR was originally proposed to evaluate translation systems by measuring the alignment between the system output and reference translations. As such, it can also be used as an automatic evaluation metric for summarization BIBREF18 . The performance of all models on arXiv and Pubmed is shown in Table TABREF28 and Table TABREF29 , respectively. Follow the work BIBREF18 , we use the approximate randomization as the statistical significance test method BIBREF32 with a Bonferroni correction for multiple comparisons, at the confidence level 0.01 ( INLINEFORM0 ). As we can see in these tables, on both datasets, the neural extractive models outperforms the traditional extractive models on informativeness (ROUGE-1,2) by a wide margin, but results are mixed on ROUGE-L. Presumably, this is due to the neural training process, which relies on a goal standard based on ROUGE-1. Exploring other training schemes and/or a combination of traditional and neural approaches is left as future work. Similarly, the neural extractive models also dominate the neural abstractive models on ROUGE-1,2, but these abstractive models tend to have the highest ROUGE-L scores, possibly because they are trained directly on gold standard abstract summaries. Compared with other neural extractive models, our models (both with attentive context and concatenation decoder) have better performances on all three ROUGE scores, as well as METEOR. In particular, the improvements over the Baseline model show that a combination of local and global contextual information does help to identify the most important sentences (more on this in the next section). Interestingly, just the Baseline model already achieves a slightly better performance than previous works; possibly because the auto-regressive approach used in those models is even more detrimental for long documents. Figure FIGREF32 shows the most important result of our analysis: the benefits of our method, explicitly designed to deal with longer documents, do actually become stronger as we apply it to longer documents. As it can be seen in Figure FIGREF32 , the performance gain of our model with respect to current state-of-the-art extractive summarizer is more pronounced for documents with INLINEFORM0 words in both datasets. Finally, the result of Lead (Table TABREF28 , TABREF29 ) shows that scientific papers have less position bias than news; i.e., the first sentences of these papers are not a good choice to form an extractive summary. As a teaser for the potential and challenges that still face our approach, its output (i.e., the extracted sentences) when applied to this paper is colored in red and the order in which the sentences are extracted is marked with the Roman numbering. If we set the summary length limit to the length of our abstract, the first five sentences in the conclusions section are extracted. If we increase the length to 200 words, two more sentences are extracted, which do seem to provide useful complementary information. Not surprisingly, some redundancy is present, as dealing explicitly with redundancy is not a goal of our current proposal and left as future work.\\n\\n\\nAblation Study\\nIn order to assess the relative contributions of the global and local models to the performance of our approach, we ran an ablation study. This was done for each dataset both with the whole test set, as well as with a subset of long documents. The results for Pubmed and arXiv are shown in Table TABREF34 and Table TABREF35 , respectively. For statistical significance, as it was done for the general results in Section 4.5, we use the approximate randomization method BIBREF32 with the Bonferroni correction at ( INLINEFORM0 ). From these tables, we can see that on both datasets the performance significantly improves when local topic information (i.e. local context) is added. And the improvement is even greater when we only consider long documents. Rather surprisingly, this is not the case for the global context. Adding a representation of the whole document (i.e. global context) never significantly improves performance. In essence, it seems that all the benefits of our model come exclusively from modeling the local context, even for the longest documents. Further investigation of this finding is left as future work.\\n\\n\\nConclusions and Future Work\\npurpleIn this paper, we propose a novel extractive summarization model especially designed for long documents, by incorporating the local context within each topic, along with the global context of the whole document.[2] purpleOur approach integrates recent findings on neural extractive summarization in a parameter lean and modular architecture.[3] purpleWe evaluate our model and compare with previous works in both extractive and abstractive summarization on two large scientific paper datasets, which contain documents that are much longer than in previously used corpora.[4] purpleOur model not only achieves state-of-the-art on these two datasets, but in an additional experiment, in which we consider documents with increasing length, it becomes more competitive for longer documents.[5] purpleWe also ran an ablation study to assess the relative contribution of the global and local components of our approach. [1] Rather surprisingly, it appears that the benefits of our model come only from modeling the local context. For future work, we initially intend to investigate neural methods to deal with redundancy. Then, it could be beneficial to integrate explicit features, like sentence position and salience, into our neural approach. More generally, we plan to combine of traditional and neural models, as suggested by our results. Furthermore, we would like to explore more sophistical structure of documents, like discourse tree, instead of rough topic segments. As for evaluation, we would like to elicit human judgments, for instance by inviting authors to rate the outputs from different systems, when applied to their own papers. More long term, we will study how extractive/abstractive techniques can be integrated; for instance, the output of an extractive system could be fed into an abstractive one, training the two jointly.\\n\\n\\nAcknowledgments\\nThis research was supported by the Language & Speech Innovation Lab of Cloud BU, Huawei Technologies Co., Ltd. Extractive Label Generation The algorithm SECREF6 is used to generate the extractive labels based on the human-made abstractive summaries, i.e. abstracts of scientific papers. Extractive label generation LabelGenerationReference,sentences,lengthLimit INLINEFORM0 = ‚Äù INLINEFORM1 = 0 INLINEFORM2 = [] INLINEFORM3 INLINEFORM4 INLINEFORM5 INLINEFORM6 in range(len( INLINEFORM7 )) INLINEFORM8 INLINEFORM9 INLINEFORM10 INLINEFORM11 INLINEFORM12 != INLINEFORM13 INLINEFORM14 .append( INLINEFORM15 ) INLINEFORM16 = INLINEFORM17 + INLINEFORM18 [ INLINEFORM19 ] INLINEFORM20 += NumberOfWords( INLINEFORM21 [ INLINEFORM22 ]) break INLINEFORM23 \\n\\n\\n',\n",
       " 'question': 'What do they mean by global and local context?',\n",
       " 'answer': ['global (the whole document) and the local context (e.g., the section/topic) '],\n",
       " 'evidence': ['In contrast, in order to exploit section information, in this paper we propose to capture a distributed representation of both the global (the whole document) and the local context (e.g., the section/topic) when deciding if a sentence should be included in the summary']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(DATASET_PATH, \"r\") as f:\n",
    "  dataset = json.load(f)\n",
    "\n",
    "demo_id = 45\n",
    "demo_title = dataset[demo_id][\"title\"]\n",
    "demo_full_text = dataset[demo_id][\"full_text\"]\n",
    "demo_question = dataset[demo_id][\"question\"]\n",
    "demo_answer = dataset[demo_id][\"answer\"]\n",
    "demo_evidence = dataset[demo_id][\"evidence\"]\n",
    "\n",
    "display(dataset[demo_id])\n",
    "# print(f'title: {demo_title}')\n",
    "# print(f'question: {demo_question}')\n",
    "# print(f'answer: {demo_answer}')\n",
    "# print(f'evidence: {demo_evidence}')\n",
    "# print(f'\\nFull text: \\n{demo_full_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1583dbb",
   "metadata": {},
   "source": [
    "### Here's a quick implementation of Cache-Augmented Generation (CAG)\n",
    "Let's just `put the whole paper context into the prompt`!\n",
    "\n",
    "---\n",
    "To build a multi-stage solution, we can simply change:\n",
    "\n",
    "`document` in `query = {\"document\": doc_content, \"input\": demo_question}` to `history`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d35936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_question = 'What do they mean by global and local context?'\n",
      "demo_answer = ['global (the whole document) and the local context (e.g., the section/topic) ']\n",
      " In this context, global context refers to the overall information or representation of the entire document, while local context refers to the specific information or representation of a particular topic or section within the document. The authors use these terms to describe how their model incorporates both global and local information when deciding which sentences to include in an extractive summary.  In other words, the global context provides a broad understanding of the document as a whole, while the local context provides more focused information about specific topics or sections within that document.  The authors use LSTM-Minus to capture the local context, which is a method for learning embeddings of text spans that can represent\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> In this context, global context refers to the overall information or representation of the entire document, while \n",
       "local context refers to the specific information or representation of a particular topic or section within the \n",
       "document. The authors use these terms to describe how their model incorporates both global and local information \n",
       "when deciding which sentences to include in an extractive summary.  In other words, the global context provides a \n",
       "broad understanding of the document as a whole, while the local context provides more focused information about \n",
       "specific topics or sections within that document.  The authors use LSTM-Minus to capture the local context, which \n",
       "is a method for learning embeddings of text spans that can represent\n",
       "</pre>\n"
      ],
      "text/plain": [
       " In this context, global context refers to the overall information or representation of the entire document, while \n",
       "local context refers to the specific information or representation of a particular topic or section within the \n",
       "document. The authors use these terms to describe how their model incorporates both global and local information \n",
       "when deciding which sentences to include in an extractive summary.  In other words, the global context provides a \n",
       "broad understanding of the document as a whole, while the local context provides more focused information about \n",
       "specific topics or sections within that document.  The authors use LSTM-Minus to capture the local context, which \n",
       "is a method for learning embeddings of text spans that can represent\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.callbacks.tracers import ConsoleCallbackHandler  # It provides additional details during LLM invocation with chaining.\n",
    "\n",
    "doc_content = f\"{demo_full_text}\"\n",
    "query = {\"document\": doc_content, \"input\": demo_question}\n",
    "\n",
    "print(f\"{demo_question = }\")\n",
    "print(f\"{demo_answer = }\")\n",
    "response = []\n",
    "for chunk in llm_chain.stream(query, config={\"callbacks\": [ConsoleCallbackHandler()]} if DEBUG else None):\n",
    "  response.append(chunk)\n",
    "  print(chunk, end=\"\")\n",
    "print(\"\")\n",
    "\n",
    "chat_response = \"\".join(response)\n",
    "console.print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c2b30",
   "metadata": {},
   "source": [
    "# How to RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c6d10",
   "metadata": {},
   "source": [
    "## Construct the Document and perform preprocessing\n",
    "A `Document` in LangChain is a fundamental data structure used to represent and manage text-based information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c86ae9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Converting texts to documents\n",
    "documents = demo_full_text.split(\"\\n\\n\\n\")[:-1]\n",
    "docs = [Document(page_content=doc) for doc in documents]\n",
    "\n",
    "# Split the Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=256, # number of characters\n",
    "  chunk_overlap=128,\n",
    "  length_function=len,\n",
    "  add_start_index=True,\n",
    ")\n",
    "\n",
    "docs_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b5b44",
   "metadata": {},
   "source": [
    "### Construct the `embedding model` and `vectorstore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1b3fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Using Ollama embedding model\n",
    "embeddings = OllamaEmbeddings(\n",
    "  model=\"snowflake-arctic-embed2:568m-l-fp16\",\n",
    "  keep_alive=3000,\n",
    ")\n",
    "\n",
    "# FAISS vector_store\n",
    "# vector_store = FAISS.from_documents(docs_splits, embeddings)\n",
    "vector_store = InMemoryVectorStore.from_documents(docs_splits, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0f479",
   "metadata": {},
   "source": [
    "The function `RetrievalQA.from_chain_type` is outdated.\n",
    "([see more](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html))\n",
    "\n",
    "Feel free to apply new functions to implement the information retrieval. \n",
    "Check out [Migrating from RetrievalQA](https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/)\n",
    "\n",
    "*For the latest implementation, please refer to the cell two cells after this one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d5204f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What do they mean by global and local context?\n",
      "GT: ['global (the whole document) and the local context (e.g., the section/topic) ']\n",
      "\n",
      "A:  In the context of the given text, global context refers to the information about the whole document, while local context refers to the information about a specific sentence or topic segment within that document. \n",
      "\n",
      "Note: The question is based on the provided text and requires an understanding of the context in which \"global\" and \"local\" are used. The answer should be concise and accurate based on the information given. \n",
      "\n",
      "Final Answer: Global context refers to information about the whole document, while local context refers to information about a specific sentence or topic segment within that document. I don't know if you need more of an explanation than that. If you do\n"
     ]
    }
   ],
   "source": [
    "# from langchain.chains import RetrievalQA\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "rag_qa_chain = RetrievalQA.from_chain_type(\n",
    "  llm=llm,\n",
    "  chain_type=\"stuff\",  # Merging all retrieved documents into a single context.\n",
    "  retriever=vector_store.as_retriever(search_kwargs={\"k\": RETRIEVE_TOP_K}, search_type=\"similarity\"),\n",
    "  return_source_documents=True\n",
    ")\n",
    "\n",
    "response = rag_qa_chain.invoke(f\"{SYSTEM_PROMPT} {demo_question}\")\n",
    "# response = rag_qa_chain.invoke(f\"{demo_question}\")\n",
    "# response\n",
    "print(f\"Q: {demo_question}\")\n",
    "print(f\"GT: {demo_answer}\\n\")\n",
    "print(f\"A: {response['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c12ff9",
   "metadata": {},
   "source": [
    "latest implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bdeb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What do they mean by global and local context?\n",
      "GT: ['global (the whole document) and the local context (e.g., the section/topic) ']\n",
      "\n",
      "A:  In the context of natural language processing (NLP), \"global context\" and \"local context\" refer to different types of information that can be used to understand the meaning of a sentence or a piece of text.\n",
      "\n",
      "**Local Context:**\n",
      "Local context refers to the information that is immediately surrounding a word or phrase, such as the words or phrases that appear before and after it. This can include things like:\n",
      "\n",
      "* The previous sentence or paragraph\n",
      "* The current sentence or phrase\n",
      "* The words or phrases that are closely related to the current word or phrase\n",
      "\n",
      "In other words, local context is about understanding the immediate surroundings of a piece of\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# PROMPT_RAG: str = (\n",
    "#   \"\"\"Use the given context to answer the question. \n",
    "# If you don't know the answer, say you don't know. \n",
    "# Keep the answer concise. Don't provide irrelevant information.\"\"\"\n",
    "# )\n",
    "\n",
    "# retrieval_qa_prompt = ChatPromptTemplate(\n",
    "#   [\n",
    "#     (\"system\", PROMPT_RAG),\n",
    "#     (\"human\", \"Context: {context}\\nQuestion: {input}\\nAnswer:\"),\n",
    "#     # (\"human\", \"Context: {context}\\nQuestion: {input}\"),\n",
    "#   ]\n",
    "# )\n",
    "\n",
    "CHAT_TEMPLATE_RAG = (\n",
    "\"\"\"human: context: {context}\\nquestion: {input}\n",
    "assistant: \"\"\"\n",
    ")\n",
    "retrieval_qa_prompt = PromptTemplate.from_template(template=CHAT_TEMPLATE_RAG)\n",
    "\n",
    "# console.print(retrieval_qa_prompt)\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_prompt)\n",
    "rag_qa_chain = create_retrieval_chain(\n",
    "  retriever=vector_store.as_retriever(search_kwargs={\"k\": RETRIEVE_TOP_K}, search_type=\"similarity\"), \n",
    "  combine_docs_chain=combine_docs_chain\n",
    "  )\n",
    "response_new = rag_qa_chain.invoke({\"input\": f\"{demo_question}\"})\n",
    "\n",
    "# response_new\n",
    "print(f\"Q: {demo_question}\")\n",
    "print(f\"GT: {demo_answer}\\n\")\n",
    "print(f\"A: {response_new['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f935f",
   "metadata": {},
   "source": [
    "### Collect all the retrieved chunk in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c5e15d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['information (i.e. local context) is added. And the improvement is even greater when we only consider long documents. Rather surprisingly, this is not the case for the global context. Adding a representation of the whole document (i.e. global context)',\n",
       " 'surprisingly, this is not the case for the global context. Adding a representation of the whole document (i.e. global context) never significantly improves performance. In essence, it seems that all the benefits of our model come exclusively from modeling',\n",
       " 'global context are all contextual information of the given sentence, we use an attention mechanism to decide the weight of each context vector, represented as INLINEFORM0  where the INLINEFORM0 is the weighted context vector of each sentence INLINEFORM1 ,',\n",
       " 'of sentence INLINEFORM3 with topic segment information and global context information. Attentive context As local context and global context are all contextual information of the given sentence, we use an attention mechanism to decide the weight of each',\n",
       " 'the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers, Pubmed and arXiv, where it outperforms previous work, both extractive and abstractive models, on ROUGE-1,']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# retrieved_list = []\n",
    "# for retrieved_chunk in response[\"source_documents\"]:\n",
    "#   retrieved_list.append(retrieved_chunk.page_content)\n",
    "# display(retrieved_list)\n",
    "\n",
    "# This is the latest implementation, but it may not be significantly different.\n",
    "retrieved_list = []\n",
    "for retrieved_chunk in response_new[\"context\"]:\n",
    "  retrieved_list.append(retrieved_chunk.page_content)\n",
    "display(retrieved_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b30bbc",
   "metadata": {},
   "source": [
    "## Evidence score (ROUGE-L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3619d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_evidence_score = 0.2170\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "fmeasure_scores = []\n",
    "\n",
    "for chunk in retrieved_list:\n",
    "  scores = scorer.score_multi(  # using maximum f-measure\n",
    "    targets=demo_evidence,\n",
    "    prediction=chunk\n",
    "    )\n",
    "  # print(f\"{chunk = }\")\n",
    "  # print(f\"* f-measure = {scores['rougeL'].fmeasure:.4f}\\n\")\n",
    "  fmeasure_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "final_evidence_score = sum(fmeasure_scores) / len(fmeasure_scores)\n",
    "print(f\"{final_evidence_score = :.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377bfc84",
   "metadata": {},
   "source": [
    "## Using LLM to Judge the answer generated by the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eeade21",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_JUDGEMENT: str = (\n",
    "  \"\"\"Assume you are a human expert in grading predictions given by a model. You are given a document, a question and a model prediction. Judge if the prediction matches the ground truth answer by following these steps:\n",
    "1: Take it as granted that the Ground Truth is always correct.\n",
    "2: If the Prediction indicates it is not sure about the answer, \"score\" should be \"0\"; otherwise, go the next step.\n",
    "3: If the Prediction exactly matches the Ground Truth, \"score\" is 1.\n",
    "4: If the Prediction does not exactly match the Ground Truth, go through the following steps.\n",
    "5: If the Ground Truth is a number, \"score\" is 1 if and only if the Prediction gives a number that almost exactly matches the ground truth.\n",
    "6: If the Prediction is self-contradictory, \"score\" must be 0.\n",
    "7: If the prediction is not answering the question, \"score\" must be 0.\n",
    "8: If the prediction is a concise and correct summary of the ground truth, \"score\" is 1.\n",
    "9: If ground truth contains a set of items, prediction must contain exactly same items for the score to be 1.\n",
    "10: Otherwise, \"score\" is 0.\n",
    "Keep the answer concise. Don't provide irrelevant information.\n",
    "\"\"\")\n",
    "\n",
    "PROMPT_JUDGE_CONTENT = (\n",
    "\"\"\"document: {document}\n",
    "question: {question}\n",
    "Ground Truth: {answer}\n",
    "Prediction: {prediction}\n",
    "\"\"\")\n",
    "\n",
    "CHAT_JUDGE_TEMPLATE = (\n",
    "  f\"system: {PROMPT_JUDGEMENT}\\n\"\n",
    "  f\"human: {PROMPT_JUDGE_CONTENT}\\n\"\n",
    "  \"assistant: The score is \"\n",
    ")\n",
    "\n",
    "# print(CHAT_JUDGE_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e692c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score is 0. The prediction is not concise and does not exactly match the Ground Truth, but it provides additional information that is not present in the Ground Truth. Additionally, while related to the topic, it's more of an explanation than a direct answer to what global and local context mean in this specific paper.  (Note: This response assumes that \"score\" should be 1 if the prediction matches or closely matches ground truth without extra unrelated information) \n",
      "However If I try to squeeze this into one of your steps:\n",
      "8: Prediction is a concice correct summary -> No\n",
      "9: Prediction contains same items as GT -> Yes for \"global\n"
     ]
    }
   ],
   "source": [
    "llm_judge = VLLMOpenAI(\n",
    "  base_url=API_ENDPOINT,\n",
    "  api_key=API_KEY,\n",
    "  model=USING_MODEL,\n",
    "  temperature=0.6,\n",
    "  max_tokens=128,\n",
    "  frequency_penalty=1.6,\n",
    "  presence_penalty=0.8,\n",
    "  model_kwargs={\"stop\": [\"```\", \"}}\"]},\n",
    ")\n",
    "\n",
    "chat_prompt = PromptTemplate.from_template(template=CHAT_JUDGE_TEMPLATE)\n",
    "llm_judge_chain = chat_prompt | llm_judge | StrOutputParser()\n",
    "\n",
    "query = {\n",
    "  # \"document\": f\"Paper title: {demo_title}\\n\" + demo_full_text.split(\"\\n\\n\\n\")[0] + \"\\n\" + str(demo_evidence),\n",
    "  \"document\": f\"Paper title: {demo_title}\\n\" + str(demo_evidence),\n",
    "  \"question\": demo_question,\n",
    "  \"answer\": \" \".join(demo_answer),\n",
    "  # \"prediction\": response[\"result\"]\n",
    "  \"prediction\": response_new[\"answer\"]\n",
    "  }\n",
    "\n",
    "# print(f\"{demo_question = }\")\n",
    "# print(f\"{demo_answer = }\")\n",
    "_response = [\"The score is \"]\n",
    "print(\"The score is \", end=\"\")\n",
    "for chunk in llm_judge_chain.stream(query, config={\"callbacks\": [ConsoleCallbackHandler()]} if DEBUG else None):\n",
    "  _response.append(chunk)\n",
    "  print(chunk, end=\"\")\n",
    "print(\"\")\n",
    "\n",
    "judge_response = \"\".join(_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8385c84",
   "metadata": {},
   "source": [
    "### Using another LLM to extract the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43496085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\": 0}\n",
      "Answer correctness: 0\n"
     ]
    }
   ],
   "source": [
    "llm_judge_refine = VLLMOpenAI(\n",
    "  base_url=API_ENDPOINT,\n",
    "  api_key=API_KEY,\n",
    "  model=USING_MODEL,\n",
    "  temperature=0.3,\n",
    "  max_tokens=4,\n",
    "  frequency_penalty=1.6,\n",
    "  presence_penalty=0.8,\n",
    "  model_kwargs={\"stop\": [\" \"]},\n",
    ")\n",
    "\n",
    "CHAT_JUDGE_REFINE_TEMPLATE = (\n",
    "\"\"\"assistant: {history}\n",
    "user: {input}\n",
    "assistant: {{\"score\": \"\"\"\n",
    ")\n",
    "\n",
    "chat_prompt = PromptTemplate.from_template(template=CHAT_JUDGE_REFINE_TEMPLATE)\n",
    "llm_judge_refine_chain = chat_prompt | llm_judge_refine | StrOutputParser()\n",
    "\n",
    "query = {\n",
    "  \"history\": judge_response,\n",
    "  \"input\": \"\"\"Please extract the score result from the last conversation and output in the JSON format. e.g., {\"score\": 0} or {\"score\": 1}.\n",
    "  Please don't respond with extra content. Just output a JSON.\n",
    "  \"\"\"\n",
    "  }\n",
    "\n",
    "# print(f\"{demo_question = }\")\n",
    "# print(f\"{demo_answer = }\")\n",
    "_response = ['{\"score\": ']\n",
    "print('{\"score\": ', end=\"\")\n",
    "for chunk in llm_judge_refine_chain.stream(query, config={\"callbacks\": [ConsoleCallbackHandler()]} if DEBUG else None):\n",
    "  _response.append(chunk)\n",
    "  print(chunk, end=\"\")\n",
    "print(\"\")\n",
    "\n",
    "judge_refine_response = \"\".join(_response)\n",
    "correctness = round(json.loads(judge_refine_response)[\"score\"], 0)\n",
    "print(f\"Answer correctness: {correctness}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96fdeda",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "# sample_submission.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5190ad27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'aaa', 'answer': 'bbb', 'evidence': ['xxx', 'yyy']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_json = {\n",
    "  \"title\": \"aaa\",\n",
    "  \"answer\": \"bbb\",\n",
    "  \"evidence\": [\"xxx\", \"yyy\"],\n",
    "}\n",
    "\n",
    "display(dummy_json)\n",
    "json_data = json.dumps(dummy_json, indent=2)\n",
    "sample_submission = [dummy_json for _ in range(100)]  # emulate 100 data in private dataset\n",
    "\n",
    "with open(\"sample_submission.json\", \"w\") as f:\n",
    "  json.dump(sample_submission, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw3-rag-Inh6RjkC-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
