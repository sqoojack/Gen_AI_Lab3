[
  {
    "title": "Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations",
    "evidence": [
      "r training with unlabeled data to improve the representation. Thus, we construct both types of prediction modules for our model. The flow of unlabeled data, which is processed to obtain a prediction by each module, is shown in Fig. .",
      "pervised learning technique whose goal is to improve the model representation using a combination of labeled and unlabeled data. During training, the model is trained alternately with one mini-batch of labeled data and INLINEFORM0 mini-batches of unlabeled data.",
      "ose to use self-attention modules to extract distant representations along with local representations to improve model accuracy. Third, we also apply semi-supervised learning BIBREF15 , allowing us to employ unlimited amounts of unlabeled data, which is particularly important for low-resource languages such as Thai, for which annotation is costly and time-consuming.",
      "results\nin Table TABREF45 show that using n-gram to obtain the local representation improves the F1 score of the model from 90.9% (row (e)) to 92.4% (row (f)) on the Orchid dataset and from 87.6% (row (e)) to 88.7% (row (f)) on the UGWC dataset. These",
      "ining (CVT) as a semi-supervised learning technique, allowing us to utilize unlabeled data to improve the model representations. In the Thai sentence segmentation experiments, our model reduced the relative error by 7.4% and 10.5% compared with the baseline models on the Orchid and UGWC datasets, respectively.",
      " input into the model to calculate the standard supervised loss for each mini-batch and the model weights are updated regularly. Meanwhile, each mini-batch of unlabeled data is selected randomly from the pool of all unlabeled data; the model computes the loss for CVT from the mini-batch of unlabeled data.",
      " modules for our model. The flow of unlabeled data, which is processed to obtain a prediction by each module, is shown in Fig. . The output of each prediction module is transformed into the probability distribution of each class by the softmax function and then used to calculate INLINEFORM0 , as shown in cvtloss.",
      "tioned in Section UID33 . The model improves the F1 score slightly, from 88.8% (row (g)) to 88.9% (row (h)) on the UGWC dataset. This result occurs because both the labeled and unlabeled data in the UGWC dataset are drawn from the same finance domain. The average number of new words found in a new unlabeled data passage is only 0.650, as shown in Table TABREF32 .",
      " talks, the number of vocabulary words grows substantially more than in the UGWC dataset because the talks cover various topics. In this dataset, average 1.225 new words found in each new unlabeled data passage, as shown in Table TABREF32 ; consequently the model representation learns new information from these new words effectively.",
      "g training, the model is trained alternately with one mini-batch of labeled data and INLINEFORM0 mini-batches of unlabeled data. Labeled data are input into the model to calculate the standard supervised loss for each mini-batch and the model weights are updated regularly.",
      "odule. Our first and second contributions, which are local and distant representations, are mainly described in this subsection. Meanwhile, the second subsection expounds on how the model is trained with unlabeled data through the modified CVT, which is our third contribution.",
      "s on the keywords of dependent clauses, we combine the model with a distant representation obtained from self-attention modules. Finally, due to the scarcity of labeled data, for which annotation is difficult and time-consuming, we also investigate and adapt Cross-View Training (CVT) as a semi-supervised learning technique, allowing us to utilize unlabeled data to improve the model representations.",
      "uracy in terms of the F1 score on both Thai and English datasets, as shown in tab:thairesult,tab:engresult (row (g) vs row (h)). This experiment was conducted only on the UGWC dataset because no unlabeled data are available in the Orchid dataset, as mentioned in Section UID33 . The model improves the F1 score slightly, from 88.8% (row (g)) to 88.9% (row (h)) on the UGWC dataset."
    ],
    "answer": [
      "The model utilizes unlabeled data to improve model representations through Cross-View Training (CVT), a semi-supervised learning technique, and self-attention modules to extract distant representations along with local representations."
    ]
  },
  {
    "title": "Artificial Error Generation with Machine Translation and Syntactic Patterns",
    "evidence": [
      " paper investigates two alternative methods for artificially generating writing errors, in order to create additional resources. We propose treating error generation as a machine translation task, where grammatically correct text is translated to contain errors.",
      "Conclusion\nThis paper investigated two AEG methods, in order to create additional training data for error detection.",
      "esearch into AEG. Brockett2006 constructed regular expressions for transforming correct sentences to contain noun number errors. Rozovskaya2010a learned confusion sets from an annotated corpus in order to generate preposition errors.",
      "r the majority of errors. In this paper, we investigate two supervised approaches for generating all types of artificial errors. We propose a framework for generating errors based on statistical machine translation (SMT), training a model to translate from correct into incorrect sentences.",
      "hich the system is trained (450K tokens), and 2) example sentences extracted from the English Vocabulary Profile (270K tokens).. While there are other text corpora that could be used (e.g., Wikipedia and news articles), our development experiments showed that keeping the writing style and vocabulary close to the target domain gives better",
      "of applicable patterns. This process is repeated until all the required errors have been generated or the sentence is exhausted. During generation, we try to balance the distribution of error types as well as keeping the same proportion of incorrect and correct sentences as in the background corpus BIBREF10 .",
      "ing probabilities from the background corpus) and attempt to create them by sampling from the collection of applicable patterns. This process is repeated until all the required errors have been generated or the sentence is exhausted.",
      "sentence as input, the system would learn to translate it to contain likely errors, based on a training corpus of parallel data. Existing SMT approaches are already optimised for identifying context patterns that correspond to specific output sequences, which is also required for generating human-like errors.",
      "ror types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection. Combining the pattern-based approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets.",
      "ethods for AEG. The models receive grammatically correct text as input and modify certain tokens to produce incorrect sequences. The alternative versions of each sentence are aligned using Levenshtein distance, allowing us to identify specific words that need to be marked as errors.",
      "ult features, we add character-level Levenshtein distance to each mapping in the phrase table, as proposed by Felice:2014-CoNLL. Decoding is performed using Moses BIBREF7 and the language model used during decoding is built from the original erroneous sentences in the learner corpus.",
      "ntifying context patterns that correspond to specific output sequences, which is also required for generating human-like errors. The reverse of this idea, translating from incorrect to correct sentences, has been shown to work well for error correction tasks BIBREF2 , BIBREF3 , and round-trip translation has also been shown to be promising for correcting grammatical errors BIBREF4 .",
      "s BIBREF2 , BIBREF3 , and round-trip translation has also been shown to be promising for correcting grammatical errors BIBREF4 . Following previous work BIBREF2 , BIBREF5 , we build a phrase-based SMT error generation system."
    ],
    "answer": [
      "{\n\"English\"\n}"
    ]
  },
  {
    "title": "Generating Word and Document Embeddings for Sentiment Analysis",
    "evidence": [
      "rformed better when we used the supervised scores, which are extracted on a review basis, and concatenated them to word vectors. Mostly, the supervised 4-scores feature leads to the highest accuracies, since it employs the annotational information concerned with polarities on a word basis. As can be seen in Table TABREF17, the clustering method, in general, yields the lowest scores.",
      "we combine contextual and supervised information with the general semantic representations of words occurring in the dictionary. Contexts of words help us capture the domain-specific information and supervised scores of words are indicative of the polarities of those words.",
      "a metric to move such words away from each other in the VSM, even though they have common words in their dictionary definitions. We multiply each value in a row with the corresponding row word's raw supervised score, thereby having more meaningful clusters. Using the training data only, the supervised polarity score per word is calculated as in (DISPLAY_FORM4).",
      " help us capture the domain-specific information and supervised scores of words are indicative of the polarities of those words. When we combine supervised features of words with the features extracted from their dictionary definitions, we observe an increase in the success rates.",
      "e meaningful clusters. Using the training data only, the supervised polarity score per word is calculated as in (DISPLAY_FORM4). Here, $ w_{t}$ denotes the sentiment score of word $t$, $N_{t}$ is the number of documents (reviews or tweets) in which $t$ occurs in the dataset of positive polarity, $N$ is the number of all the words in the corpus of positive polarity.",
      "t the data and supervised scores of the words. This work captures the supervised information by utilising emoticons as features. Most of our approaches do not rely on a neural network model in learning embeddings. However, they produce state-of-the-art",
      "results\nfor the dictionary-based algorithm when we employed the SVD reduction method compared to the use of clustering. Methodology ::: Supervised Contextual 4-scores Our last component is a simple metric that uses four supervised scores for each word in the corpus. We extract these scores as follows. For a target word in the corpus, we scan through all of its contexts.",
      "least one of our methods outperforms the baseline word2vec approach for all the Turkish and English corpora, and all categories. All of our approaches performed better when we used the supervised scores, which are extracted on a review basis, and concatenated them to word vectors.",
      " supervised scores. We then feed these inputs into the SVM approach. The flowchart of our framework is given in Figure FIGREF11. When combining the unsupervised features, which are word vectors created on a word basis, with supervised three scores extracted on a review basis, we have better state-of-the-art",
      "re, discarding the polarity scores. However, when we utilise the supervised score (+1 or -1), words of opposite polarities (e.g. “happy\" and “unhappy\") get far away from each other as they are translated across coordinate regions. Positive words now appear in quadrant 1, whereas negative words appear in quadrant 3.",
      " three of these sentiment scores as mentioned. Lastly, we concatenate these three scores to the averaged word vector per review. That is, each review is represented by the average word vector of its constituent word embeddings and three supervised scores. We then feed these inputs into the SVM approach. The flowchart of our framework is given in Figure FIGREF11.",
      "e perform normalisation to prevent the imbalance problem and add a small number to both numerator and denominator for smoothing. As an alternative to multiplying with the supervised polarity scores, we also separately multiplied all the row scores with only +1 if the row word is a positive word, and with -1 if it is a negative word.",
      "s of words occurring in the same contexts as the target word, minimum, maximum, and average scores are taken into consideration. The word polarity scores are computed using (DISPLAY_FORM4). Here, we obtain those scores from the training data. The intuition behind this method is that those four scores are more indicative of a word's polarity rather than only one (the self score)."
    ],
    "answer": [
      "The supervised scores of the words are calculated as the sentiment score of the word, the number of documents in which the word occurs in the dataset of positive polarity, the number of all the words in the corpus of positive polarity, and the minimum, maximum, and average scores of the word occurring in the same contexts as the target word."
    ]
  },
  {
    "title": "Using General Adversarial Networks for Marketing: A Case Study of Airbnb",
    "evidence": [
      " the listing’s zip code, the host’s description of the listing, the price of the listing, and the occupancy rate of the listing. Airbnb defines a home's occupancy rate, as the percentage of time that a listing is occupied over the time period that the listing is available. This gives us a reasonable metric for defining popular versus less popular listings.",
      "ed to us contained information for roughly 40,000 Manhattan listings that were posted on Airbnb during this defined time period. For each listing, we were given information of the amenities of the listing (number of bathrooms, number of bedrooms …), the listing’s zip code, the host’s description of the listing, the price of the listing, and the occupancy rate of the listing.",
      " corresponds to a high/medium/low popularity listing. The architecture of the RNN/LSTM employs Tensorflow’s Dynamic RNN package. Each sentence input is first fed into an embedding layer, where the input’s text is converted to a GloVe vector. These GloVe vectors are learned via a global word-word co-occurrence matrix using our corpus of Airbnb listing descriptions .",
      "ity, we focus our analysis on Airbnb listings from Manhattan, NY, during the time period of January 1, 2016, to January 1, 2017. The data provided to us contained information for roughly 40,000 Manhattan listings that were posted on Airbnb during this defined time period.",
      "e that collaborates with Airbnb to produce high-accuracy data summaries for listings in geographic regions of the United States. For the sake of simplicity, we focus our analysis on Airbnb listings from Manhattan, NY, during the time period of January 1, 2016, to January 1, 2017.",
      "rial network to recommend a way of rewording the phrasing of a listing description to increase the likelihood that it is booked. Although we tailor our analysis to Airbnb data, we believe this framework establishes a more general model for how generative algorithms can be used to produce text samples for the purposes of marketing.",
      "Abstract\nIn this paper, we examine the use case of general adversarial networks (GANs) in the field of marketing. In particular, we analyze how GAN models can replicate text patterns from successful product listings on Airbnb, a peer-to-peer online market for short-term apartment rentals.",
      "er of other websites like Airbnb and eBay rely on sellers and buyers to organically find one-another in a decentralized fashion. In the case of these decentralized systems, sellers are asked to price and market their products in order to attract potential buyers.",
      " use a window size of 5, a minimum occurrence count of 2, and a dimensionality of 50 when training our GloVe vectors was ad hoc. Seeking to create a model which could generate and discriminate a “high-occupancy listing description”, we wanted to evaluate the capabilities of a generative adversarial network trained on either the standard binary cross-entropy loss or the DMK loss proposed above.",
      "s like Uber and Instacart use a centralized system that matches workers with assigned tasks via a series of complex algorithms . Still, a number of other websites like Airbnb and eBay rely on sellers and buyers to organically find one-another in a decentralized fashion.",
      "mory gates, as defined by Mikolov et al., to categorize product descriptions into categories based on the product's popularity . Data The data for the project was acquired from Airdna, a data processing service that collaborates with Airbnb to produce high-accuracy data summaries for listings in geographic regions of the United States.",
      " regardless of price, which is not necessarily a strong assumption. This is coupled with an unexpected sparseness of clean data. With over 40,000 listings, we did not expect to see such poor attention to orthography in what are essentially public advertisements of the properties.",
      "ng relationship actually exist and there be instead a problem with our method, there are a few possibilities of what went wrong. We assumed that listings with similar occupancy rates would have similar listing descriptions regardless of price, which is not necessarily a strong assumption. This is coupled with an unexpected sparseness of clean data."
    ],
    "answer": [
      "The size of the Airbnb is 40,000 listings."
    ]
  },
  {
    "title": "Question Answering for Privacy Policies: Combining Computational and Legal Perspectives",
    "evidence": [
      "sting of 1750 questions about the privacy policies of mobile applications, and over 3500 expert annotations of relevant answers. We observe that a strong neural baseline underperforms human performance by almost 0.3 F1 on PrivacyQA, suggesting considerable room for improvement for future systems.",
      "his work is to promote question-answering research in the specialized privacy domain, where it can have large real-world impact. Strong neural baselines on PrivacyQA achieve a performance of only 39.8 F1 on this corpus, indicating considerable room for future research. Further, we shed light on several important considerations that affect the answerability of questions.",
      "wer bound on performance at this task. We observe that our best-performing baseline, Bert + Unanswerable achieves an F1 of 39.8. This suggest that bert is capable of making some progress towards answering questions in this difficult domain, while still leaving considerable headroom for improvement to reach human performance.",
      "question is to be answered from the privacy policy from our experts. We find that most of these mistakes are relevant questions. However many of them were identified as subjective by the annotators, and at least one annotator marked 19 of these questions as having no answer within the privacy policy.",
      "ories of each question (Table.TABREF34). We compare our best performing BERT variant against the NA model and human performance. We observe significant room for improvement across all categories of questions but especially for first party, third party and data retention categories.",
      "wer sentence selection task. The No-answer (NA) baseline performs at 28 F1, providing a lower bound on performance at this task. We observe that our best-performing baseline, Bert + Unanswerable achieves an F1 of 39.8.",
      "de the top 2, 3 and 5 candidates as baselines. BERT: We implement two BERT-based baselines BIBREF51 for evidence identification. First, we train BERT on each query-policy sentence pair as a binary classification task to identify if the sentence is evidence for the question or not (Bert).",
      "results\nin vectors of 200, 201 and 228 dimensions respectively, which are provided to an SVM with a linear kernel. CNN: We utilize a CNN neural encoder for answerability prediction.",
      "e selection in Table TABREF32. We observe that bert exhibits the best performance on a binary answerability identification task. However, most baselines considerably exceed the performance of a majority-class baseline. This suggests considerable information in the question, indicating it's possible answerability within this domain.",
      "nce answer provided by an annotator, and compute the F1 with respect to the remaining references, as described in section 4.2.1. Each reference answer is treated as the prediction, and the remaining n-1 answers are treated as the gold reference. The average of the maximum F1 across all reference answers is computed as the human baseline.",
      "Results\nand",
      "Results\nand",
      "Results\nand"
    ],
    "answer": [
      "We observe that a strong neural baseline underperforms human performance by almost 0.3 F1 on PrivacyQA, suggesting considerable room for improvement for future systems."
    ]
  },
  {
    "title": "Dynamic Compositional Neural Networks over Tree Structure",
    "evidence": [
      " for more sophisticated operations on the input. To evaluate our models, we choose two typical NLP tasks involving six datasets. The qualitative and quantitative experiment",
      "tions, we know the compositional function is changed cross child nodes over a tree, which is controlled by a latent vector $z$ . To get an intuitive understanding of how the controlling vector $z$ works, we design an experiment to examine the neuron's behaviours of $\\mathbf {z}$ on each node.",
      " our models to capture various syntactic patterns (As we will discuss later) therefore can more accurately understand sentences. Experiment To make a comprehensive evaluation, we assess our model on five text classification tasks and a semantic matching task.",
      "results\non two typical tasks show the effectiveness of the proposed models.",
      " how the controlling vector $z$ works, we design an experiment to examine the neuron's behaviours of $\\mathbf {z}$ on each node. More concretely, we refer to $z_{jk}$ as the activation of the $k$ -neuron at node $j$ , where $j \\in \\lbrace 1,\\ldots ,N\\rbrace $ and $k \\in \\lbrace 1,\\ldots , z\\rbrace $ .",
      "results\nshow that our models are more expressive due to their learning to learn nature, yet without increasing the number of model's parameters.",
      "a network could capture some syntactic information. For example, the 27- $th$ neuron monitors phrases constructed by light-verb. As shown in Figure 5 -(b), the verb phrase “taking off” has been attended for forthcoming compositional operation, which is more useful for judging the semantic relation between the sentence pair “An airplane is taking off/A plane is landing”.",
      "oduced multiple compositional functions and during compositional phase, a proper one is selected based on the input information. Although these models accomplished their mission to a certain extent, they still suffer from the following three challenges.",
      "information into the the generation process of compositional function. These semantic bias before composition are task-specific. For example, the 21- $st$ neuron is more sensitive to emotional terms, which can be understood as a sentinel, telling the basic neural network that an informative phrase is coming, more attention should be paid in the process of composition.",
      "twork (Tree-LSTM) BIBREF9 . Our work is inspired by recent work on dynamic parameter prediction BIBREF16 , BIBREF17 , BIBREF18 . The meta network is used to extract the shared meta-knowledge across different compositional rules and to dynamically generate the context-specific compositional function.",
      "neural networks over tree structure (DC-TreeNN), in which the compositional function is dynamically generated by a meta network. The role of meta-network is to capture the metaknowledge across the different compositional rules and formulate them. Experimental",
      "NN BIBREF13 , RNTN BIBREF23 , while these models suffer from the problem of hard-coded compositional operations and overfitting. Another thread of work is the idea of using one network to direct the learning of another network BIBREF16 .",
      "ng of the syntactic structure is crucial. In this context, we find that a meta network could capture some syntactic information. For example, the 27- $th$ neuron monitors phrases constructed by light-verb."
    ],
    "answer": [
      "The tasks they experiment with are text classification tasks and a semantic matching task."
    ]
  },
  {
    "title": "Automatic Discourse Segmentation: an evaluation in French",
    "evidence": [
      "Abstract\nIn this article, we describe some discursive segmentation methods as well as a preliminary evaluation of the segmentation quality.",
      "results\nof a generic segmenter composed of several systems (different segmentation strategies). In addition, we describe an automatic evaluation protocol of discursive segmentation.",
      "results\nof the different segmentation systems through automatic evaluations. First of all, the human segmentation, from the subcorpus $D$ composed of common documents. The",
      "presents a very encouraging performance. Of course, the quality of the list is a preponderant factor for a correct segmentation. We have studied the impact of the marker which, even though it may seem fringe-worthy, contributes to improving the performance of our segmenters. Thus, it is an interesting marker that we can consider as a discursive marker.",
      "the system can be adapted to other languages. The evaluation is based on the correspondence of word pairs representing a border. In this way we compare the Annodis segmentation with the automatically produced segmentation. For each pair of reference segments, a $L_r$ list of word pairs is provided: the last word of the first segment and the first word of the second.",
      "discussion\nand perspectives The aim of this work was twofold: to design a discursive segmenter using a minimum of resources and to establish an evaluation protocol to measure the performance of segmenters. The",
      "results\nshow that we can build a simple version of the baseline, which employs only a list of markers and presents a very encouraging performance. Of course, the quality of the list is a preponderant factor for a correct segmentation.",
      "stems (different segmentation strategies). In addition, we describe an automatic evaluation protocol of discursive segmentation. The article is composed by the following sections: state of the art (SECREF2), which presents a brief bibliographic review; Description of the Annodis (SECREF3) corpus used in our tests and of the general architecture of the proposed systems (SECREF4); Segmentation strategies (SECREF5), which characterizes the different methods implemented to segment the text;",
      "results\nin terms of F-score and recall, followed by the Segmentator$_{\\mu +V}$ version, which passes it in precision. Regarding evaluation, we developed a simple protocol to compare the performance of the systems.",
      " this article, we describe some discursive segmentation methods as well as a preliminary evaluation of the segmentation quality. Although our experiment were carried for documents in French, we have developed three discursive segmentation models solely based on resources simultaneously available in several languages: marker lists and a statistic POS labeling.",
      "ich complicates the evaluation process. Although there are some errors, word boundaries allow us to detect segments more easily. We have built a second $L_c$ list for the automatically identified segments, following the same criteria of $L_r$. The $L_r$ and $L_c$ lists regroup, pair by pair, the segment border. We then count the common pair intersection of the two lists.",
      "ause – PIC] } We decided to count the word pairs instead of the segments, as this is a first version of the evaluation protocol. In fact, the segments may be nested, which complicates the evaluation process. Although there are some errors, word boundaries allow us to detect segments more easily.",
      "n, which passes it in precision. Regarding evaluation, we developed a simple protocol to compare the performance of the systems. This is, to our knowledge, the first automatic evaluation in French. It is necessary to intensify our research in order to propose improvements to our segmenters, as well as to study further the impact of grammar tag rules on segmentation."
    ],
    "answer": [
      "The evaluation of segmentation quality is based on the correspondence of word pairs representing a border, comparing the Annodis segmentation with the automatically produced segmentation."
    ]
  },
  {
    "title": "Multimodal Named Entity Recognition for Short Social Media Posts",
    "evidence": [
      "incomplete syntax and lexical notations with very limited surrounding textual contexts, bringing significant challenges for NER. To this end, we create a new dataset for MNER called SnapCaptions (Snapchat image-caption pairs submitted to public and crowd-sourced stories with fully annotated named entities).",
      " new MNER dataset SnapCaptions, a large collection of informal and extremely short social media posts paired with unique images. Related Work Neural models for NER have been recently proposed, producing state-of-the-art performance on standard NER tasks.",
      "rs (5.81 words) with vocabulary size 15,733, where 6,612 are considered unknown tokens from Stanford GloVE embeddings BIBREF22 . Named entities annotated in the SnapCaptions dataset include many of new and emerging entities, and they are found in various surface forms (various nicknames, typos, etc.) To the best of our knowledge, SnapCaptions is the only dataset that contains natural image-caption pairs with expert-annotated named entities.",
      " superiority, the module itself is flexible and thus can work with other NER architectures or for other multimodal applications. SnapCaptions Dataset The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC).",
      "ion gain and suppressing irrelevant contexts from each modality (we treat words, characters, and images as separate modalities). (3) We show that the proposed approaches outperform the state-of-the-art NER models (both with and without using additional visual contexts) on our new MNER dataset SnapCaptions, a large collection of informal and extremely short social media posts paired with unique images.",
      "d SnapCaptions (Snapchat image-caption pairs submitted to public and crowd-sourced stories with fully annotated named entities). We then build upon the state-of-the-art Bi-LSTM word/character based NER models with 1) a deep image network which incorporates relevant visual context to augment textual information, and 2) a generic modality-attention module which learns to attenuate irrelevant modalities while amplifying the most informative ones to extract contexts from, adaptive to each sample and token.",
      "Results\n: SnapCaptions Dataset Table TABREF6 shows the NER performance on the Snap Captions dataset. We report both entity types recognition (PER, LOC, ORG, MISC) and named entity segmentation (named entity or not)",
      " from the pre-trained InceptionNet are available). We split the dataset into train (70%), validation (15%), and test sets (15%). The captions data have average length of 30.7 characters (5.81 words) with vocabulary size 15,733, where 6,612 are considered unknown tokens from Stanford GloVE embeddings BIBREF22 .",
      "on (MNER) for noisy user-generated data such as tweets or Snapchat captions, which comprise short text with accompanying images. These social media posts often come in inconsistent or incomplete syntax and lexical notations with very limited surrounding textual contexts, bringing significant challenges for NER.",
      "Abstract\nWe introduce a new task called Multimodal Named Entity Recognition (MNER) for noisy user-generated data such as tweets or Snapchat captions, which comprise short text with accompanying images.",
      " our knowledge, SnapCaptions is the only dataset that contains natural image-caption pairs with expert-annotated named entities. Baselines Task: given a caption and a paired image (if used), the goal is to label every token in a caption in BIO scheme (B: beginning, I: inside, O: outside) BIBREF27 .",
      "te irrelevant modalities while amplifying the most informative ones to extract contexts from, adaptive to each sample and token. The proposed MNER model with modality attention significantly outperforms the state-of-the-art text-only NER models by successfully leveraging provided visual contexts, opening up potential applications of MNER on myriads of social media platforms.",
      "s are “New York Story” or “Thanksgiving Story”, which comprise snaps that are aggregated for various public events, venues, etc. All snaps were posted between year 2016 and 2017, and do not contain raw images or other associated information (only textual captions and obfuscated visual descriptor features extracted from the pre-trained InceptionNet are available)."
    ],
    "answer": [
      "The SnapCaptions dataset contains 10K user-generated image and textual caption pairs."
    ]
  },
  {
    "title": "Neural Collective Entity Linking",
    "evidence": [
      "results\nshow that NCEL consistently outperforms various baselines with a favorable generalization ability. Finally, we further present the performance on a challenging dataset WW BIBREF19 as well as qualitative",
      "Results\non TAC2010 and WW In this section, we investigate the effectiveness of NCEL in the “easy\" and “hard\" datasets, respectively.",
      "own in Table TABREF26 , NCEL achieves the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The baseline methods also achieve competitive",
      "ework that aims to provide a unified comparison among different EL methods across datasets including ACE2004, AQUAINT and CoNLL. We compare NCEL with the global models that report the performance on GERBIL. As shown in Table TABREF26 , NCEL achieves the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1.",
      "ents, we first verify the efficiency of NCEL via theoretically comparing its time complexity with other collective alternatives. Afterwards, we train our neural model using collected Wikipedia hyperlinks instead of dataset-specific annotations, and perform evaluations on five public available benchmarks. The",
      "ined on collected Wikipedia hyperlinks, NCEL outperforms the state-of-the-art collective methods across five different datasets. Besides, further analysis of the impacts of main modules as well as qualitative",
      "ocument. To the best of our knowledge, this is the first effort to develop a unified model for neural collective entity linking. In experiments, we first verify the efficiency of NCEL via theoretically comparing its time complexity with other collective alternatives.",
      "s To avoid overfitting with some dataset, we train NCEL using collected Wikipedia hyperlinks instead of specific annotated data. We then evaluate the trained model on five different benchmarks to verify the linking precision as well as the generalization ability. Furthermore, we investigate the effectiveness of key modules in NCEL and give qualitative",
      "proposed method\n, and then the overall structured information shall be reached in a chain-like way. Fixing the size of the subset, NCEL is further speeded up by batch techniques and GPUs, and is efficient to large-scale data.",
      "o improve the robustness of NCEL to data noise and train the model on Wikipedia hyperlinks to avoid overfitting and domain bias. In experiments, we evaluate NCEL on five publicly available datasets to verify the linking performance as well as generalization ability. We also conduct an extensive analysis of time complexity, the impact of key modules, and qualitative",
      "lexity Analysis Compared with local methods, the main disadvantage of collective methods is high complexity and expensive costs. Suppose there are INLINEFORM0 mentions in documents on average, among these global models, NCEL not surprisingly has the lowest time complexity INLINEFORM1 since it only considers adjacent mentions, where INLINEFORM2 is the number of sub-GCN layers indicating the iterations until convergence.",
      "ng precision (Micro) of WW is lower than that of TAC2010, and NCEL outperforms all baseline methods in both easy and hard cases. In the “easy\" case, local models have similar performance with global models since only little global information is available (2 mentions per document).",
      "results\nare shown in Table FIGREF28 and Table FIGREF28 . We can see the average linking precision (Micro) of WW is lower than that of TAC2010, and NCEL outperforms all baseline methods in both easy and hard cases."
    ],
    "answer": [
      "NCEL consistently outperforms various baselines with a favorable generalization ability, achieving the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1."
    ]
  },
  {
    "title": "Distant supervision for emotion detection using Facebook reactions",
    "evidence": [
      "discussion\non this). The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.",
      "d their corresponding reactions from public pages using the Facebook API, which we accessed via the Facebook-sdk python library. We chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but we did so mainly based on intuition (see Section SECREF4 ) and with an eye to the nature of the datasets available for evaluation (see Section SECREF5 ).",
      "conclusion\ns and future work We have explored the potential of using Facebook reactions in a distant supervised setting to perform emotion classification.",
      " (i) which Facebook pages to select as training data, and (ii) which features to use to train the model, which we discuss below. Specifically, we first set on a subset of pages and then experiment with features. Further exploration of the interaction between choice of pages and choice of features is left to future work, and partly discussed in Section SECREF6 .",
      "results\nwithout relying on any handcrafted resource. An interesting aspect of our approach is the view to domain adaptation via the selection of Facebook pages to be used as training data.",
      "results\nbut also interesting insights on extensions of this approach lies in the choice of training instances, both in terms of Facebook pages to get posts from, as well as in which posts to select from the given pages.",
      "results\nalso show that there is large room for improvement, especially by gearing the collection of Facebook pages, with a view to the target domain.",
      "sed measure to select only posts that have a strong emotion rather than just considering the majority emotion as training label. For the former, namely the choice of Facebook pages, which we believe deserves the most investigation, one could explore several avenues, especially in relation to stance-based issues BIBREF24 .",
      " 14 as well as the other two datasets described in Section SECREF3 will be used to evaluate the final models (Section SECREF4 ). Selecting Facebook pages Although page selection is a crucial ingredient of this approach, which we believe calls for further and deeper, dedicated investigation, for the experiments described here we took a rather simple approach.",
      " intuition (see Section SECREF4 ) and with an eye to the nature of the datasets available for evaluation (see Section SECREF5 ). The choice of which pages to select posts from is far from trivial, and we believe this is actually an interesting aspect of our approach, as by using different Facebook pages one can intrinsically tackle the domain-adaptation problem (See Section SECREF6 for further",
      "results\n, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7 .",
      "on the three datasets standardly used for the evaluation of emotion classification, which we have described in Section SECREF3 . Our B-M model relies on subsets of Facebook pages for training, which were chosen according to their performance on the development set as well as on the observation of emotions distribution on different pages and in the different datasets, as described in Section SECREF4 .",
      "r representations to find which words are interchangeable with emoticons but also which emoticons are used in a similar context. We take advantage of distant supervision by using Facebook reactions as proxies for emotion labels, which to the best of our knowledge hasn't been done yet, and we train a set of Support Vector Machine models for emotion recognition."
    ],
    "answer": [
      "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."
    ]
  },
  {
    "title": "MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge",
    "evidence": [
      "ing in the garden, and that the questions require commonsense knowledge, or more specifically, script knowledge, to be answered. We show that our mode of data collection via crowdsourcing",
      "ting a tree, which goes beyond what is mentioned in the text. Texts, questions, and answers were obtained through crowdsourcing. In order to ensure high quality, we manually validated and filtered the dataset. Due to our design of the data acquisition process, we ended up with a substantial subset of questions that require commonsense inference (27.4%).",
      "itten by crowdsourcing workers. NewsQA closely resembles our own data collection with respect to the method of data acquisition. As for our data collection, full texts were not shown to workers as a basis for question formulation, but only the text's title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background knowledge.",
      "rst describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section SECREF4 ). In Section SECREF5 , we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk).",
      "ss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk). Section SECREF17 gives information about some necessary postprocessing steps and the dataset validation. Lastly, Section SECREF19 gives statistics about the final dataset.",
      " text, i.e. it requires inference using commonsense knowledge about everyday activities. An example is given in Figure FIGREF2 . For both questions, the correct choice for an answer requires commonsense knowledge about the activity of planting a tree, which goes beyond what is mentioned in the text. Texts, questions, and answers were obtained through crowdsourcing.",
      " texts to workers and let them formulate questions and answers on the texts, which is what we tried internally in a first pilot. Since our focus is to provide an evaluation framework for inference over commonsense knowledge, we manually assessed the number of questions that indeed require common sense knowledge.",
      "nce decided to rely on majority vote and selected the candidate from the set of incorrect answers that was most often mentioned. For this step, we normalized each candidate by lowercasing, deleting punctuation and stop words (articles, and, to and or), and transforming all number words into digits, using text2num.",
      "A datasets. NewsQA BIBREF20 is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection with respect to the method of data acquisition.",
      "ld be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions.",
      " collected data from 5 participants for each question and decided on the final category via majority vote (at least 3 out of 5). Consequently, for each question with a majority vote on either text-based or script-based, there are 3 to 5 correct and incorrect answer candidates, one from each participant who agreed on the category.",
      "or script-based, there are 3 to 5 correct and incorrect answer candidates, one from each participant who agreed on the category. Questions without a clear majority vote or with ties were not included in the dataset. We performed four post-processing steps on the collected data. We manually filtered out texts that were instructional rather than narrative.",
      "of 3 or less to another answer. The “most frequent answer” was then selected based on how many other answers it was merged with. Only if there was no majority, we selected the candidate with the highest overlap with the text as a fallback."
    ],
    "answer": [
      "Amazon Mechanical Turk (MTurk)"
    ]
  },
  {
    "title": "Stochastic Answer Networks for SQuAD 2.0",
    "evidence": [
      "one of the state-of-the-art machine reading comprehension models, to be able to judge whether a question is unanswerable or not. The extended SAN contains two components: a span detector and a binary classifier for judging whether the question is unanswerable, and both components are jointly optimized. Experiments show that SAN achieves the",
      "p layers for different tasks (the span detector and binary classifier). Our model is pretty simple and intuitive, yet efficient. Without relying on the large pre-trained language models (ELMo) BIBREF7 , the proposed model achieves competitive",
      "g layer, a contextual layer and a memory generation layer. On top of it, there are different answer modules for different tasks. We employ the SAN answer module for the span detector and a one-layer feed forward neural network for the binary classification task. It can also be viewed as a multi-task learning BIBREF8 , BIBREF5 , BIBREF9 .",
      "results\nof our model only have a small improvement (+0.2 in terms of F1 score). It shows that it is important to make balance between these two components: the span detector and unanswerable classifier.",
      "iLSTM to form the final memory: INLINEFORM3 . Span detector. We adopt a multi-turn answer module for the span detector BIBREF1 . Formally, at time step INLINEFORM0 in the range of INLINEFORM1 , the state is defined by INLINEFORM2 . The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM5 , where INLINEFORM6 .",
      "tic Answer Network (SAN) BIBREF1 for MRC answer span detector to include a classifier that whether the question is unanswerable. The unanswerable classifier is a pair-wise classification model BIBREF6 which predicts a label indicating whether the given pair of a passage and a question is unanswerable.",
      " INLINEFORM6 where INLINEFORM0 means that we only drop diagonal elements on the similarity matrix (i.e., attention with itself). At last, INLINEFORM1 and INLINEFORM2 are concatenated and are passed through a BiLSTM to form the final memory: INLINEFORM3 . Span detector. We adopt a multi-turn answer module for the span detector BIBREF1 .",
      "assification model BIBREF6 which predicts a label indicating whether the given pair of a passage and a question is unanswerable. The two models share the same lower layer to save the number of parameters, and separate the top layers for different tasks (the span detector and binary classifier). Our model is pretty simple and intuitive, yet efficient.",
      "he passages/paragraphs, we propose a model that not only extracts answers but also predicts whether such an answer should exist. Using a multi-task learning approach (c.f. BIBREF5 ), we extend the Stochastic Answer Network (SAN) BIBREF1 for MRC answer span detector to include a classifier that whether the question is unanswerable.",
      "hether the question is unanswerable. Roughly, the model includes two different layers: the shared layer and task specific layer. The shared layer is almost identical to the lower layers of SAN, which has a lexicon encoding layer, a contextual layer and a memory generation layer. On top of it, there are different answer modules for different tasks.",
      "mary of the memory: INLINEFORM1 , where INLINEFORM2 . INLINEFORM3 denotes the probability of the question which is unanswerable. Objective The objective function of the joint model has two parts: DISPLAYFORM0 Following BIBREF0 , the span loss function is defined: DISPLAYFORM0 The objective function of the binary classifier is defined: DISPLAYFORM0 where INLINEFORM0 is a binary variable: INLINEFORM1 indicates the question is unanswerable and INLINEFORM2 denotes the question is answerable.",
      "results\non SQuAD v2.0. Model The Machine Reading Comprehension is a task which takes a question INLINEFORM0 and a passage/paragraph INLINEFORM1 as inputs, and aims to find an answer span INLINEFORM2 in INLINEFORM3 .",
      " is a combination of Stanford Question Answering Dataset (SQuAD) 1.0 BIBREF15 and additional unanswerable question-answer pairs. The answerable pairs are around 100K; while the unanswerable questions are around 53K. This dataset contains about 23K passages and they come from approximately 500 Wikipedia articles. All the questions and answers are obtained by crowd-sourcing."
    ],
    "answer": [
      "The architecture of the span detector in the SAN model consists of a lexicon encoding layer, a contextual layer, and a memory generation layer, followed by a multi-turn answer module."
    ]
  },
  {
    "title": "Winograd Schemas and Machine Translation",
    "evidence": [
      "he sex of the friend; and the clue for that can involve an inference that, as far as AI programs are concerned, is quite remote. Human readers are awfully good at picking up on those, of course. In general, with this kind of thing, if you want to break GT or another translation program, the trick is to place a large separation between the evidence and the word being disambiguated.",
      "t misses a very close clue, as in “Marie is my friend”, but other times it can carry a clue further than one would have guessed. Acknowledgements Thanks to Arianna Bisazza, Gerhard Brewka, Antoine Cerfon, Joseph Davis, Gigi Dopico-Black, Nizar Habash, Leora Morgenstern, Oded Regev, Francesca Rossi, Vesna Sabljakovic-Fritz, and Manuela Veloso for help with the various languages and",
      " test will no longer be useful. In some cases, a translation program can side-step the issue by omitting the pronoun altogether. For example, GT translates the above sentence “The girls sang a song and they danced” into Spanish as “Las chicas cantaron una canción y bailaban” and into Italian as “Le ragazze hanno cantato una canzone e ballavano.” However, with the more complex sentences of the Winograd Schemas, this strategy will rarely give a plausible translation for both elements of the schema.",
      "noun `they' has no gender in English and most other languages (unlike the third person singular pronouns `he', `she', and `it'). However Romance languages such as French, Italian, and Spanish, and Semitic languages such as Hebrew and Arabic distinguish between the masculine and the feminine third-person plural pronouns, at least in some grammatical cases.",
      "on between the evidence and the word being disambiguated. In this case, at the present time, the separation can be pretty small. GT correctly translates “my friend Pierre” and “my friend Marie” as “mon ami Pierre” and “mon amie Marie” and it translates “She is my friend” as “Elle est mon amie”, but rather surpringly it breaks down at “Marie is my friend,” which it translates “Marie est mon ami.” As for something like “Jacques said to Marie, `You have always been a true friend,' ” that is quite hopeless.",
      "es using the background knowledge to resolve the ambiguity, and will therefore be challenging for automatic machine translation. A couple of examples: The word `sie' in German serves as both the formal second person prounoun (always capitalized), the third person feminine singular, and the third person plural.",
      " or another translation program, the trick is to place a large separation between the evidence and the word being disambiguated. In this case, at the present time, the separation can be pretty small.",
      "ing the word `they' into one of these languages, it is necessary to determine whether or not the referent is a group of females. If it is, then the translation must be the feminine pronoun; otherwise, it must be the masculine pronoun.",
      "is a group of females. If it is, then the translation must be the feminine pronoun; otherwise, it must be the masculine pronoun. Therefore, if one can create a Winograd schema in English where the ambiguous pronoun is `they' and the correct referent for one element is a collection of men and for the other is a collection of women, then to translate both elements correctly requires solving the Winograd schema.",
      "er than AI systems will be able to do pronoun resolution in Winograd schemas; at that point, this test will no longer be useful. In some cases, a translation program can side-step the issue by omitting the pronoun altogether.",
      "is much less true; in speech, and even, increasingly, in writing, the masculine pronoun is often used for a feminine antecedent. Looking further ahead, it is certainly possible that gender distinctions will be abandoned in the Romance languages, or even that English will have driven all other languages out of existence, sooner than AI systems will be able to do pronoun resolution in Winograd schemas; at that point, this test will no longer be useful.",
      "rd-person singular verb, it must mean `she'. However, in many case, the disambiguation requires a deeper level of understanding. Thus, it should be possible to construct German Winograd schemas based on the words `sie' or `ihr' that have to be solved in order to translate them into English. For example, Marie fand fünf verlassene Kätzchen im Keller. Ihre Mutter war gestorben.",
      "s into genders in the same way.) Likewise, the possessive pronoun `ihr' in all its declensions can mean either `her' or `their'. In some cases, the disambiguation can be carried out on purely syntactic ground; e.g. if `sie' is the subject of a third-person singular verb, it must mean `she'. However, in many case, the disambiguation requires a deeper level of understanding."
    ],
    "answer": [
      "French, Italian, and Spanish, and Semitic languages such as Hebrew and Arabic distinguish between the masculine and the feminine third-person plural pronouns, at least in some grammatical cases."
    ]
  },
  {
    "title": "How we do things with words: Analyzing text as social and cultural data",
    "evidence": [
      "s that have focused on, and worked to tackle, the challenges of textual analysis—albeit at smaller scales—since their inception. In describing our experiences with computational text analysis, we hope to achieve three primary goals. First, we aim to shed light on thorny issues not always at the forefront of",
      "recisely because they help us identify patterns that would not otherwise be discernible. Yet these approaches are not a panacea. Examining thick social and cultural questions using computational text analysis carries significant challenges. For one, texts are culturally and socially situated.",
      "conclusion\ns, rather than findings that reflect the depth of the questions we seek to address. These are just a small sample of the many opportunities and challenges faced in computational analyses of textual data.",
      " and the computational analysis of rich textual data on larger-than-human scales on the other, are becoming increasingly common. Indeed, computational analysis is opening new possibilities for exploring challenging questions at the heart of some of the most pressing contemporary cultural and social issues.",
      "Abstract\nIn this article we describe our experiences with computational text analysis. We hope to achieve three primary goals. First, we aim to shed light on thorny issues not always at the forefront of",
      "we wish to explore. Can text analysis provide a new perspective on a “big question” that has been attracting interest for years? Or can we raise new questions that have only recently emerged, for example about social media? For social scientists working in computational analysis, the questions are often grounded in theory, asking: How can we explain what we observe?",
      "ith much of the work in computational text analysis, which tends to focus on automating tasks that humans perform inefficiently. These tasks range from core linguistically motivated tasks that constitute the backbone of natural language processing, such as part-of-speech tagging and parsing, to filtering spam and detecting sentiment.",
      "questions using computational text analysis carries significant challenges. For one, texts are culturally and socially situated. They reflect the ideas, values and beliefs of both their authors and their target audiences, and such subtleties of meaning and interpretation are difficult to incorporate in computational approaches.",
      "sion? Is there a clear boundary between hate speech and other types of speech, and what features make it more or less ambiguous? In these cases, it is critical to communicate high-level patterns in terms that are recognizable. This contrasts with much of the work in computational text analysis, which tends to focus on automating tasks that humans perform inefficiently.",
      "discussion\nis critically informed by insights from the humanities and social sciences, fields that have focused on, and worked to tackle, the challenges of textual analysis—albeit at smaller scales—since their inception.",
      "ist in interview methodologies might reach out to commenters to better understand the role of online hate speech in their lives. Computational text analysis represents a step towards better understanding social and cultural phenomena, and it is in many cases better suited towards opening questions rather than closing them.",
      "ir target audiences, and such subtleties of meaning and interpretation are difficult to incorporate in computational approaches. For another, many of the social and cultural concepts we seek to examine are highly contested — hate speech is just one such example.",
      "ey are especially suited for insight-driven analysis, because they are constrained in ways that make their output interpretable. Although there is no guarantee that a “topic” will correspond to a recognizable theme or event or discourse, they often do so in ways that other methods do not."
    ],
    "answer": [
      "The issues that are not on the forefront of computational text analysis include culturally and socially situated texts, subtleties of meaning and interpretation, and highly contested social and cultural concepts, such as hate speech."
    ]
  },
  {
    "title": "Quasi-Recurrent Neural Networks",
    "evidence": [
      "ns with separate filter banks to obtain sequences of vectors for the elementwise gates that are needed for the pooling function. While the candidate vectors are passed through a INLINEFORM0 nonlinearity, the gates use an elementwise sigmoid.",
      "able functions for the pooling subcomponent can be constructed from the familiar elementwise gates of the traditional LSTM cell. We seek a function controlled by gates that can mix states across timesteps, but which acts independently on each channel of the state vector.",
      " the QRNN architecture by modifying the pooling function to keep the previous pooling state for a stochastic subset of channels. Conveniently, this is equivalent to stochastically setting a subset of the QRNN's INLINEFORM0 gate channels to 1, or applying dropout on INLINEFORM1 : DISPLAYFORM0 Thus the pooling function itself need not be modified at all.",
      "e channels to 1, or applying dropout on INLINEFORM1 : DISPLAYFORM0 Thus the pooling function itself need not be modified at all. We note that when using an off-the-shelf dropout layer in this context, it is important to remove automatic rescaling functionality from the implementation if it is present.",
      "hree options f-pooling, fo-pooling, and ifo-pooling respectively; in each case we initialize INLINEFORM0 or INLINEFORM1 to zero. Although the recurrent parts of these functions must be calculated for each timestep in sequence, their simplicity and parallelism along feature dimensions means that, in practice, evaluating them over even long sequences requires a negligible amount of computation time.",
      "results\nfrom zoneout applied to the QRNN's recurrent pooling layer, implemented as described in Section SECREF5 . The experimental settings largely followed the “medium” setup of BIBREF2 .",
      "discussion\nof convolution-like and pooling-like subcomponents.",
      " in CNNs, allows fully parallel computation across both minibatches and spatial dimensions, in this case the sequence dimension. The pooling component, like pooling layers in CNNs, lacks trainable parameters and allows fully parallel computation across minibatch and feature dimensions.",
      "oling function. While the candidate vectors are passed through a INLINEFORM0 nonlinearity, the gates use an elementwise sigmoid. If the pooling function requires a forget gate INLINEFORM1 and an output gate INLINEFORM2 at each timestep, the full set of computations in the convolutional component is then: DISPLAYFORM0 where INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 , each in INLINEFORM3 , are the convolutional filter banks and INLINEFORM4 denotes a masked convolution along the timestep dimension.",
      "to “zone out” at each timestep; for these channels the network copies states from one timestep to the next without modification. As QRNNs lack recurrent weights, the variational inference approach does not apply. Thus we extended zoneout to the QRNN architecture by modifying the pooling function to keep the previous pooling state for a stochastic subset of channels.",
      "Db dataset. Even without any post-processing, changes in the hidden state are visible and interpretable in regards to the input. This is a consequence of the elementwise nature of the recurrent pooling function, which delays direct interaction between different channels of the hidden state until the computation of the next QRNN layer.",
      "n as a masked convolution BIBREF11 , is implemented by padding the input to the left by the convolution's filter size minus one. We apply additional convolutions with separate filter banks to obtain sequences of vectors for the elementwise gates that are needed for the pooling function.",
      "mpute higher INLINEFORM0 -gram features at each timestep; thus larger widths are especially important for character-level tasks. Suitable functions for the pooling subcomponent can be constructed from the familiar elementwise gates of the traditional LSTM cell."
    ],
    "answer": [
      "The pooling function used in the QRNN architecture is a masked convolution, which is implemented by padding the input to the left by the convolution's filter size minus one, and applying additional convolutions with separate filter banks to obtain sequences of vectors for the elementwise gates that are needed for the pooling function."
    ]
  },
  {
    "title": "Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction",
    "evidence": [
      "oving a small number of the most difficult sentences does not harm, and in fact modestly improves, medical IE model performance. However, using the available data we are unable to test if this will be useful in practice, as we would need additional data to determine how many difficult sentences should be dropped.",
      " of easy sentences and considerably fewer difficult sentences, which makes the difficulty ranking task particularly challenging. Better IE with Difficulty Prediction We next present experiments in which we attempt to use the predicted difficulty during training to improve models for information extraction of descriptions of Population, Interventions and Outcomes from medical article",
      "ions in Figure 1 . In these, the sentences are more uniformly distributed, with a fair number of difficult and easier sentences. By contrast, in Population there are a greater number of easy sentences and considerably fewer difficult sentences, which makes the difficulty ranking task particularly challenging.",
      "at sentences predicted to be difficult are indeed noisy, to the extent that they do not seem to provide the model useful signal. Re-weighting by Difficulty We showed above that removing a small number of the most difficult sentences does not harm, and in fact modestly improves, medical IE model performance.",
      "s ground truth and then define the difficulty of sentences in terms of the observed agreement between expert and lay annotators. Formally, for annotation task $t$ and instance $i$ : $$\\text{Difficulty}_{ti} = \\frac{\\sum _{j=1}^n{f(\\text{label}_{ij}, y_i})}{n}$$ (Eq.",
      "abstract\nmay contain some `easy' and some `difficult' sentences. We thus perform our analysis at the sentence level. We split",
      "nificantly lower than that for other types of sentences (i and o), again indicating that they are easier on average to annotate. This aligns with a previous finding that annotating Interventions and Outcomes is more difficult than annotating Participants BIBREF5 .",
      "spectively. We used batch sizes of 16. We used the large version of the universal sentence encoder with a transformer BIBREF23 . We did not update the pretrained sentence encoder parameters during training.",
      "that contain neither manual nor predicted annotations. We treat these as maximally easy sentences (with difficulty scores of 0). Such sentences comprise 51%, 42% and 36% for Population, Interventions and Outcomes data respectively, indicating that it is easier to identify sentences that have no Population spans, but harder to identify sentences that have no Interventions or Outcomes spans.",
      "ses: (1) simply removing the most difficult sentences from the training set, and, (2) re-weighting the most difficult sentences. We again use LSTM-CRF-Pattern as the base model and experimenting on the EBM-NLP corpus BIBREF5 .",
      "abstract\ns into sentences using spaCy. We excluded sentences that comprise fewer than two tokens, as these are likely an artifact of errors in sentence splitting. In total, this resulted in 57,505 and 2,428 sentences in the train and test set",
      " for annotation task $t$ and instance $i$ : $$\\text{Difficulty}_{ti} = \\frac{\\sum _{j=1}^n{f(\\text{label}_{ij}, y_i})}{n}$$ (Eq. 3) where $f$ is a scoring function that measures the quality of the label from worker $j$ for sentence $i$ , as compared to a ground truth annotation, $y_i$ .",
      "identify sentences that have no Population spans, but harder to identify sentences that have no Interventions or Outcomes spans. This is intuitive as descriptions of the latter two tend to be more technical and dense with medical jargon. We show the distribution of the automatically labeled scores for sentences that do contain spans in Figure 1 ."
    ],
    "answer": [
      "An instance is an IE tuple."
    ]
  },
  {
    "title": "TWEETQA: A Social Media Focused Question Answering Dataset",
    "evidence": [
      "ave been used by journalists in news articles thus implicitly implying that such tweets contain useful and relevant information. To obtain such relevant tweets, we crawled thousands of news articles that include tweet quotations and then employed crowd-sourcing to elicit questions and answers based on these event-aligned tweets.",
      "even emojis. Instead of only showing the text to the workers, we use javascript to directly embed the whole tweet into each HIT. This gives workers the same experience as reading tweets via web browsers and help them to better compose questions.",
      "hived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles. In order to get enough data, we first extract the URLs of all section pages (e.g. World, Politics, Money, Tech) from the snapshot of each home page and then crawl all articles with tweets from these section pages.",
      "to write their answers in their own words. We just require the answers to be brief and can be directly inferred from the tweets. After we retrieve the QA pairs from all HITs, we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers.",
      "r tweetsused by journalists to write news articles. Wethen ask human annotators to write questionsand answers upon these tweets. Unlike otherQA datasets like SQuAD in which the answersare extractive, we allow the answers to be ab-stractive. We show that two recently proposedneural models that perform well on formaltexts are limited in their performance when ap-plied to our dataset.",
      "written by different workers, the answers can be written in different text forms even they are semantically equal to each other. For example, one answer can be “Hillary Clinton” while the other is “@HillaryClinton”.",
      " media sites, Twitter is most frequently used as a news source, with INLINEFORM2 of its users obtaining their news from Twitter. All these statistical facts suggest that understanding user-generated noisy social media text from Twitter is a significant task.",
      "about. Given the linguistic variance of tweets, it is generally hard to directly distinguish those tweets from informative ones. In terms of this, rather than starting from Twitter API Search, we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles.",
      "nd WikiMovies BIBREF7 that use Wikipedia. In this paper, we propose the first large-scale dataset for QA over social media data. Rather than naively obtaining tweets from Twitter using the Twitter API which can yield irrelevant tweets with no valuable information, we restrict ourselves only to tweets which have been used by journalists in news articles thus implicitly implying that such tweets contain useful and relevant information.",
      "ate-argument structure of the tweets collected from news articles and keep only the tweets with more than two labeled arguments. This filtering process also automatically filters out most of the short tweets. For the tweets collected from CNN, INLINEFORM0 of them were filtered via semantic role labeling. For tweets from NBC, INLINEFORM1 of the tweets were filtered.",
      "ls developed by the social media community for processing the userIDs and hashtags will significantly help these question types. Besides the easy questions requiring mainly paraphrasing skill, we also find that the questions requiring the understanding of authorship and oral/tweet English habits are not very difficult.",
      "ion answering (QA) forformal text like news and Wikipedia, wepresent the first large-scale dataset for QA oversocial media data. To ensure that the tweetswe collected are useful, we only gather tweetsused by journalists to write news articles. Wethen ask human annotators to write questionsand answers upon these tweets.",
      "r accounts of news media. However, these tweets are often just the summaries of news articles, which are written in formal text. As our focus is to develop a dataset for QA on informal social media text, we do not consider this approach."
    ],
    "answer": [
      "The dataset was created by crawling thousands of news articles that include tweet quotations and then employing crowd-sourcing to elicit questions and answers based on these event-aligned tweets."
    ]
  },
  {
    "title": "Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation",
    "evidence": [
      " past few years. QA tasks on images BIBREF0 have been widely studied, but most focused on understanding text documents BIBREF1 . A representative dataset in text QA is SQuAD BIBREF1 , in which several end-to-end neural models have accomplished promising performance BIBREF2 .",
      "mpute the Exact Match (EM) and Macro-averaged F1 scores (F1) between the predicted text answer and the ground-truth text answer. We used the standard evaluation script from SQuAD BIBREF1 to evaluate the performance. Question Answering Model The used architecture of the QA model is briefly summarized below.",
      "is when QANet is trained on Spoken-SQuAD, and row (c) is when QANet is trained on Text-SQuAD and then finetuned on Spoken-SQuAD. The columns show the evaluation on the testing sets of Text-SQuAD and Spoken-SQuAD.",
      "xt-SQuAD and then finetuned on Spoken-SQuAD. The columns show the evaluation on the testing sets of Text-SQuAD and Spoken-SQuAD. It is clear that the performance drops a lot when the training and testing data mismatch, indicating that model training on ASR hypotheses can not generalize well on reference transcriptions. The performance gap is nearly 20% F1 score (72% to 55%).",
      "results\n. The experiments successfully demonstrate the effectiveness of our proposed model, and the",
      "ataset in text QA is SQuAD BIBREF1 , in which several end-to-end neural models have accomplished promising performance BIBREF2 . Although there is a significant progress in machine comprehension (MC) on text documents, MC on spoken content is a much less investigated field.",
      "ined on T-SQuAD and then fine-tuned on S-SQuAD, and (c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 . We also compare to the approach proposed by Lan et al. BIBREF16 in the row (d). This approach is originally proposed for spoken language understanding, and we adopt the same approach on the setting here.",
      "r generates the question-document similarity matrix and computes the question-aware vector representations of the context words. After that, a model encoder layer containing seven encoder blocks captures the interactions among the context words conditioned on the question.",
      "results\nimply that sharing the context-query attention layer and the model encoder layer are important for domain adaptation on SQA.",
      "lts\nof applying the domain discriminator to the top of context query attention layer in row (g), which obtains poor performance. To sum it up, incorporating adversarial learning by applying the domain discriminator on top of the embedding encoder layer is effective.",
      "ough the discriminator is applied to the output of embedding encoder in Figure FIGREF2 , it can be also applied to other layers. Considering that almost all QA model contains such embedding encoders, the proposed approach is expected to generalize to other QA models in addition to QANet. Corpus Spoken-SQuAD is chosen as the target domain data for training and testing.",
      "adversarial learning allows the end-to-end QA model to learn domain-invariant features and improve the robustness to ASR errors. The experiments demonstrate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score.",
      " up, incorporating adversarial learning by applying the domain discriminator on top of the embedding encoder layer is effective. Layer weight tying or untying within the model indicates different levels of symmetric mapping between the source and target domains. Different combinations are investigated and shown in Table TABREF14 ."
    ],
    "answer": [
      "{\"SQuAD\", \"Text-SQuAD\", \"Spoken-SQuAD\"}"
    ]
  },
  {
    "title": "Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance",
    "evidence": [
      "Abstract\nThis paper analyzes the gender representation in four major corpora of French broadcast. These corpora being widely used within the speech processing community, they are a primary material for training automatic speech recognition (ASR) systems.",
      "iscard the importance of an appropriate architecture, it reaffirms the fact that rich and large corpora are a valuable resource. Corpora are research contributions which do not only allow to save and observe certain phenomena or validate a hypothesis or model, but are also a mandatory part of the technology development.",
      "esentation in a data set composed of four state-of-the-art corpora of French broadcast, widely used within the speech community. Finally we question the impact of such a representation on the systems developed on this data, through the perspective of an ASR system.",
      "ems. The best known are corpora like TIMIT BIBREF10, Switchboard BIBREF11 or Fisher BIBREF12 which date back to the early 1990s. The scarceness of available corpora is justified by the fact that gathering and annotating audio data is costly both in terms of money and time. Telephone conversations and broadcast recordings have been the primary source of spontaneous speech used.",
      " if real-world gender representation might be more balanced today, these corpora are still used as training data for AI systems. The performance difference across gender we observed corroborates (on a larger quantity and variety of language data produced by more than 2400 speakers) the",
      " no clear answer has been reached so far. It seems that many parameters are to take into account to achieve a general agreement. As we established the importance of TV and radio broadcast as a source of data for ASR, and the potential impact it can have, the following content of this paper is structured as this: we first describe statistically the gender representation of a data set composed of four state-of-the-art corpora of French broadcast, widely used within the speech community, introducing the notion of speaker's role to refine our analysis in terms of voice professionalism.",
      " was conducted on a large amount of data it does not reach the exhaustiveness of large-scale studies such as the one of BIBREF2. Nonetheless it does not affect the relevance of our findings, because if real-world gender representation might be more balanced today, these corpora are still used as training data for AI systems.",
      "ontaining radio and TV broadcast are the most widely used: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16. These four corpora have been built alongside evaluation campaigns and are still, to our knowledge, the largest French ones of their type available to date.",
      " aired between 1998 and 2013 which are used by most academic researchers in ASR. Show duration varies between 10min and an hour. As years went by and speech processing research was progressing, the difficulty of the tasks augmented and the content of these evaluation corpora changed.",
      ", we first highlight the importance of TV and radio broadcast as a source of data for ASR, and the potential impact it can have. We then perform a statistical analysis of gender representation in a data set composed of four state-of-the-art corpora of French broadcast, widely used within the speech community.",
      " processing research was progressing, the difficulty of the tasks augmented and the content of these evaluation corpora changed. ESTER1 and ESTER2 mainly contain prepared speech such as broadcast news, whereas ETAPE and REPERE consists also of debates and entertainment shows, spontaneous speech introducing more difficulty in its recognition.",
      "o train automatic speech recognition systems in English, approximately 14% of them are based on broadcast news and conversation. For French speech technologies, four corpora containing radio and TV broadcast are the most widely used: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16.",
      "hodology ::: Data presentation Our data consists of two sets used to train and evaluate our automatic speech recognition system. Four major evaluation campaigns have enabled the creation of wide corpora of French broadcast speech: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16."
    ],
    "answer": [
      "ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16."
    ]
  },
  {
    "title": "Knowledge Based Machine Reading Comprehension",
    "evidence": [
      "eration Model Our question generation model takes the path between an “anchor” and the candidate answer, and outputs a question. We adopt the sequence to sequence architecture as our basic question generation model due to its effectiveness in natural language generation tasks.",
      "s described in the matching model. As illustrated in Figure 3 , the question generation model contains an encoder and a decoder. We use a hierarchical encoder consisting of two layers to model the meaning of each element (subject, predicate or object) and the relationship between them.",
      "swer a score by measuring the semantic relevance between representation and the candidate answer representation in vector space. The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer.",
      " Model In this section, we present the generation model which generates a question based on the semantics of a candidate answer. Afterward, we introduce how our paraphrasing model, which measures the semantic relevance between the generated question and the original question, is pretrained.",
      "results\nof different question generation models. Our approach is abbreviated as QGNet, which stands for the use of a paraphrasing model plus our question generation model.",
      " leveraged in this work. Approach Overview Our framework consists of a question answering model and a question generation model. We implement two question answering models, which directly measure the semantic similarity between questions and candidate answers in the semantic space. First, to make the model's prediction more explainable, we implement a path based QA model PCNet.",
      "raphrasing model, which measures the semantic relevance between the generated question and the original question, is pretrained. Hierarchical seq2seq Generation Model Our question generation model takes the path between an “anchor” and the candidate answer, and outputs a question.",
      "e to sequence architecture as our basic question generation model due to its effectiveness in natural language generation tasks. The hierarchical encoder and the hierarchical attention mechanism are introduced to take into account the structure of the input facts.",
      "rarchical encoder and the hierarchical attention mechanism are introduced to take into account the structure of the input facts. Facts from external KBs are conventionally integrated into the model using the same way as described in the matching model. As illustrated in Figure 3 , the question generation model contains an encoder and a decoder.",
      "sing model is used to measure the semantic relevance between the original question and the question generated from the QG model. We use bidirectional RNN with gated recurrent unit to represent two questions, and compose them with element-wise multiplication. The",
      "wledge. We enhance the representation of an argument or a predicate by concatenating open KB vectors into encoder hidden states. The Paraphrasing Model The paraphrasing model is used to measure the semantic relevance between the original question and the question generated from the QG model.",
      "d a question generation model. More references can be found at https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art). Our work also relates to BIBREF21 khot2017answering, which uses open IE outputs from external text corpora to improve multi-choice question answering. However, our work differs from them in that their task does not contain document information.",
      "labeled dataset for the task, and develop a framework consisting of both question answering model and question generation model. We further incorporate four open KBs as external knowledge into both question answering and generative approaches, and demonstrate that incorporating additional evidence from open KBs improves total accuracy."
    ],
    "answer": [
      "Our question generation model takes the path between an “anchor” and the candidate answer, and outputs a question."
    ]
  },
  {
    "title": "Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health",
    "evidence": [
      "Acknowledgment\ns Research reported in this publication was supported by the National Library of Medicine of the [United States] National Institutes of Health under award numbers K99LM011393 and R00LM011393.",
      "tional Library of Medicine of the [United States] National Institutes of Health under award numbers K99LM011393 and R00LM011393. This study was granted an exemption from review by the University of Utah Institutional Review Board (IRB 00076188). Note that in order to protect tweeter anonymity, we have not reproduced tweets verbatim.",
      "results\nto the much larger feature dataset.",
      "titutional Review Board (IRB 00076188). Note that in order to protect tweeter anonymity, we have not reproduced tweets verbatim. Example tweets shown were generated by the researchers as exemplars only. Finally, we would like to thank the anonymous reviewers of this paper for their valuable comments.",
      "t 17,000 features were reduced by eliminating features that only occurred once in the full dataset, resulting in 5,761 features. We applied Chi-Square feature selection and plotted the top-ranked subset of features for each percentile (at 5 percent intervals cumulatively added) and evaluated their predictive contribution using the support vector machine with linear kernel and stratified, 5-fold cross validation.",
      " Reduction We reduced the dataset encoded for each class by eliminating features that occur less than twice in the full dataset. Selection We iteratively applied Chi-Square feature selection on the reduced dataset, selecting the top percentile of highest ranked features in increments of 5 percent to train and test the support vector model using a linear kernel and 5-fold, stratified cross-validation.",
      "hierarchical model of depression-related symptoms BIBREF12 , BIBREF13 . The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\").",
      "ness of each feature group and a feature elimination study to determine the optimal feature sets for classifying Twitter tweets. We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13 . The dataset contains 9,473 annotations for 9,300 tweets.",
      "RESULTS\nFrom our annotated dataset of Twitter tweets (n=9,300 tweets), we conducted two feature studies to better understand the predictive power of several feature groups for classifying whether or not a tweet contains no evidence of depression (n=6,829 tweets) or evidence of depression (n=2,644 tweets).",
      "Discussion\nWe conducted two feature study experiments: 1) a feature ablation study to assess the contribution of feature groups and 2) a feature elimination study to determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy.",
      "mood (n=1,010 tweets), disturbed sleep (n=98 tweets), or fatigue or loss of energy (n=427 tweets) using support vector machines. From our prior work BIBREF10 and in Figure 1, we report the performance for prediction models built by training a support vector machine using 5-fold, stratified cross-validation with all feature groups as a baseline for each class.",
      "feature sets for efficiently classifying depression-related tweets on a large-scale, we conducted two feature study experiments. In the first experiment, we assessed the contribution of feature groups such as lexical information (e.g., unigrams) and emotions (e.g., strongly negative) using a feature ablation study.",
      "uated their predictive contribution using the support vector machine with linear kernel and stratified, 5-fold cross validation. In Figure 2, we observed optimal F1-score performance using the following top feature counts: no evidence of depression: F1: 87 (15th percentile, 864 features), evidence of depression: F1: 59 (30th percentile, 1,728 features), depressive symptoms: F1: 55 (15th percentile, 864 features), depressed mood: F1: 39 (55th percentile, 3,168 features), disturbed sleep: F1: 46 (10th percentile, 576 features), and fatigue or loss of energy: F1: 72 (5th percentile, 288 features) (Figure 1)."
    ],
    "answer": [
      "The dataset used for this study is an annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms."
    ]
  },
  {
    "title": "A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media",
    "evidence": [
      "results\nshow that our solution obtains considerable performance on these datasets in terms of precision and recall in comparison to existing approaches.",
      "results\nfor fine-tuning strategies along with the official baselines. We use Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10 as baselines and compare the",
      "results\nwith our different fine-tuning strategies using pre-trained BERTbase model. The evaluation",
      "lution obtains considerable performance on these datasets in terms of precision and recall in comparison to existing approaches. Consequently, our model can capture some biases in data annotation and collection process and can potentially lead us to a more accurate model.",
      "Results\n::: Implementation and",
      "speech on social media platforms such as Twitter BIBREF9, Reddit BIBREF12, BIBREF13, and YouTube BIBREF14 in the past few years. The features used in traditional machine learning approaches are the main aspects distinguishing different methods, and surface-level features such as bag of words, word-level and character-level $n$-grams, etc.",
      "standing using a combination of the unsupervised pre-trained model BERT BIBREF11 and some new supervised fine-tuning strategies. As far as we know, it is the first time that such exhaustive fine-tuning strategies are proposed along with a generative pre-trained language model to transfer learning to low-resource hate speech languages and improve performance of the task.",
      "istinguishing different methods, and surface-level features such as bag of words, word-level and character-level $n$-grams, etc. have proven to be the most predictive features BIBREF3, BIBREF4, BIBREF5. Apart from features, different algorithms such as Support Vector Machines BIBREF15, Naive Baye BIBREF1, and Logistic Regression BIBREF5, BIBREF9, etc.",
      "anced machine learning and natural language processing, but also a sufficiently large amount of annotated data to train a model. The lack of a sufficient amount of labelled hate speech data, along with the existing biases, has been the main issue in this domain of research.",
      " approach based on an existing pre-trained language model called BERT (Bidirectional Encoder Representations from Transformers). More specifically, we investigate the ability of BERT at capturing hateful context within social media content by using new fine-tuning methods based on transfer learning.",
      "ifferent algorithms such as Support Vector Machines BIBREF15, Naive Baye BIBREF1, and Logistic Regression BIBREF5, BIBREF9, etc. have been applied for classification purposes. Waseem et al.",
      "eems to be changing direction, with deep learning models being used for both feature extraction and the training of classifiers. These newer models are applying deep learning approaches such as Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs), etc.BIBREF6, BIBREF0 to enhance the performance of hate speech detection models, however, they still suffer from lack of labelled data or inability to improve generalization property.",
      "results\nare reported on the test dataset and on three different metrics: precision, recall, and weighted-average F1-score. We consider weighted-average F1-score as the most robust metric versus class imbalance, which gives insight into the performance of our proposed models."
    ],
    "answer": [
      "Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10, Support Vector Machines BIBREF15, Naive Baye BIBREF1, Logistic Regression BIBREF5, BIBREF9, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs)"
    ]
  },
  {
    "title": "External Lexical Information for Multilingual Part-of-Speech Tagging",
    "evidence": [
      " was designed and tested mostly on this language, by building and evaluating tagging models on a variant of the French TreeBank. Since our goal was to carry out experiments in a multilingual setting, we have decided to design our own set of features, using the standard MElt features as a starting point.",
      "non-Indo-European (Indonesian), four major Indo-European sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically and syntactically fairly diverse. Lexicons We generate our external lexicons using the set of source lexicons listed in Table TABREF3 .",
      " covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs). We show that, on average, all four approaches perform similarly and reach state-of-the-art",
      "s question has already been investigated for 6 languages by BIBREF18 using the state-of-the-art CRF-based tagging system MarMoT. The authors found that their best-performing word-vector-based PoS tagging models outperform their models that rely on morphosyntactic resources (lexicons or morphological analysers).",
      "results\n. We considered UD1.2 corpora for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.",
      "h, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. Although this language list contains only one non-Indo-European (Indonesian), four major Indo-European sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian).",
      "n is exploited via features, there is no need for the external lexicons and the annotated corpora to use the same PoS inventory. Therefore, for each language, we simply extracted from the corresponding lexicon the PoS of each word based on its morphological tags, by removing all information provided except for its coarsest-level category.",
      "results\nare higher on datasets with less lexical variability (e.g. for English). These",
      "xtremely well, surpassed by MarMoT on only 3 out of 16 datasets (Czech, French and Italian), and by MElt only once (Indonesian). Models enriched with external lexical information Table TABREF13 provides the",
      ", we carried out a series of experiments using the multilingual dataset provided during the SPMRL parsing shared task BIBREF42 . This included discarding useless or harmful features and selecting the maximal length of the prefixes and suffixes to be used as features, both for the current word and for the following word.",
      "OV) words is improved, as a result of the fact that words unknown to the training corpus might be known to the external lexicon. Despite a few experiments published with MElt on languages other than French BIBREF12 , BIBREF40 , BIBREF41 , the original feature set used by MElt (standard and lexical features) was designed and tested mostly on this language, by building and evaluating tagging models on a variant of the French TreeBank.",
      "results\nof four systems enriched with lexical information. The feature-based systems MElt and MarMoT, respectively based on MEMMs and CRFs, are extended with the lexical information provided by our morphosyntactic lexicons.",
      "se best-performing model for PoS tagging dialogues was obtained with a version of MElt extended with dialogue-specific features. Yet the motivation of MElt's developers was first and foremost to investigate the best way to integrate lexical information extracted from large-scale morphosyntactic lexical resources into their models, on top of the training data BIBREF12 ."
    ],
    "answer": [
      "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish."
    ]
  },
  {
    "title": "Disunited Nations? A Multiplex Network Approach to Detecting Preference Affinity Blocs using Texts and Votes",
    "evidence": [
      "ized for scalability issues but given that we are working with a fixed size corpus this does not pose an issue for our analysis. Readers are encouraged to consult the GloVe paper for full model details, but we describe our approach and decisions here.",
      "settings using votes and textual data, based on developments in the natural language processing and network science literatures. The approach helps to better exploit information found in textual data, and to locate dense clusters in complex and multilayered networks in ways that were previously not computationally possible.",
      "tate preferences which improve upon current out-of-sample predictions relative to current models which employ UN roll call data. We choose to compare our approach to the recently published models of BIBREF8 , because their goal and applications which use UN roll call data closely parallel our research motivations.",
      "position similarities from textual data and detect dense clusters of affinity or antagonism across multiple relational datasets. These might include social media actors who operate across multiple platforms, as well as contexts like legislatures where complex relations exist across votes, speeches, committee memberships, and others.",
      "rs in other communities. This common approach, however, can be misleading in dense networks, such as vote and speech similarity. Indeed, BIBREF22 use UN voting data to illustrate the challenges related to network clustering on data with high levels of agreement between observations.",
      "nguage drift over time and to ensure that our estimated embeddings correspond to the exact policy language used in a given year. We employ a skip gram window of 4 and search using a word vector size of 50. At present, we follow the computer science literature suggestion of tuning these parameters until reasonable and reliable linear combinations of language are located.",
      "ctors use (dis)similar words, but we want high resolution insights into how and when they speak differently on different topics. For example, we would like to capture the dissimilarity of statements like \"we oppose the proliferation of nuclear weapons\" versus \"the proliferation of atom bombs is necessary.\" The words “necessary\" and “oppose\" would be counted in a BOW matrix, but their usage in the context of nuclear weapons would be lost.",
      "the area under the precision recall curve), that is, we select the model which yields the most consistent predictions over time. Because out-of-sample predictions on sparse networks (such as the conflict onset network) is a challenging task, this selection approach helps to eliminate models with predictions that vary widely from one year to the next.",
      "based clusters alone, we follow BIBREF8 in training on five-year windows and attempt to predict the next year of conflict onset. The multiplex model exhibits a 20.5% increase in area under the precision recall curve compared to the original date-adjusted model (1.156 vs. 0.959).",
      "nt of out-of-sample predictive accuracy by training models on five year windows and then assessing predictions on the next year. The areas under the precision recall curves are then summed over the entire date range.",
      "d models of BIBREF8 , because their goal and applications which use UN roll call data closely parallel our research motivations. This section provides further information on the graph partitioning approach, the new Multilayer Extraction algorithm, and model performance assessments.",
      "efficient use of the statistics. BIBREF15 show that their approach yields state-of-the-art performance on the word analogy task. GloVe is sometimes criticized for scalability issues but given that we are working with a fixed size corpus this does not pose an issue for our analysis.",
      "results\nare reported in [table:tergm]Table 1 of the main paper. For the"
    ],
    "answer": [
      "The dataset used is not explicitly stated in the text, but based on the context, it appears to be UN roll call data."
    ]
  },
  {
    "title": "When to reply? Context Sensitive Models to Predict Instructor Interventions in MOOC Forums",
    "evidence": [
      "psided student--instructor ratio on online forums. We propose the first deep learning models for this binary prediction problem. We propose novel attention based models to infer the amount of latent context necessary to predict instructor intervention. Such models also allow themselves to be tuned to instructor's preference to intervene early or late.",
      "ct instructor intervention. Such models also allow themselves to be tuned to instructor's preference to intervene early or late. Our three proposed attentive model variants to infer the latent context improve over the state-of-the-art by a significant, large margin of 11% in F1 and 10% in recall, on average.",
      "Conclusion\nWe predict instructor intervention on student",
      "f latent states. It is likely that their empirically reported setting will not generalise due to their weak evaluation BIBREF7 . In this paper, we propose models to infer the context that triggers instructor intervention that does not require context lengths to be set apriori. All our proposed models generalise over modelling assumptions made by BIBREF0 .",
      "structor had posted at least once. The problem of predicting instructor intervention is cast as a binary classification problem. Intervened threads are predicted as 1 given while non-intervened threads are predicted as 0 given posts INLINEFORM6 through INLINEFORM7 . The primary problem leads to a secondary problem of inferring the appropriate amount of context to intervene.",
      "ents. We therefore propose attention models to infer the latent context, i.e., the series of posts that trigger an intervention. Earlier studies on MOOC forum intervention either model the entire context or require the context size to be specified explicitly.",
      "discussion\nor resolve conflicts among students. We therefore propose attention models to infer the latent context, i.e., the series of posts that trigger an intervention.",
      "cussion\nin predicting instructor interventions. BIBREF0 proposed probabilistic graphical models to model structure and sequence. They inferred vocabulary dependent latent post categories to model the thread sequence and infer states that triggered intervention. Their model, however, requires a hyperparameter for the number of latent states.",
      "ore the first instructor post since the instructor's and subsequent posts will bias the prediction due to the instructor's post. Model The key innovation of our work is to decompose the intervention prediction problem into a two-stage model that first explicitly tries to discover the proper context to which a potential intervention could be replying to, and then, predict the intervention status.",
      "the state-of-the-art for instructor intervention in MOOC forums. We propose the first neural models for this prediction problem. We show that modelling the thread structure and the sequence of posts explicitly improves performance. Instructors in different MOOCs from different subject areas intervene differently.",
      "ad (Posts #1–6) as the context for her reply. This subproblem of inferring the context scope is where our innovation centers on. To be clear, in order to make the prediction that a instruction intervention is now necessary on a thread, the instructor's reply is not yet available — the model predicts whether a reply is necessary — so in the example, only Posts #1–6 are available in the problem setting.",
      " hypothesize that the prospective instructor intervention is based on the context that any previous post INLINEFORM0 replied to. In this model, each post INLINEFORM1 is set as a query to attend to its previous context INLINEFORM2 . For example, INLINEFORM3 will attend to INLINEFORM4 .",
      "en a post and a thread but also infers reply structure using a attention mechanism since the layout does not reliably encode it. Instructor Intervention in MOOC forums The problem of predicting instructor intervention in MOOCs was proposed by BIBREF0 . Later BIBREF7 evaluated baseline models by BIBREF0 over a larger corpus and found the"
    ],
    "answer": [
      "The latent context used to predict instructor intervention is the series of posts that trigger an intervention."
    ]
  },
  {
    "title": "A Hierarchical Model for Data-to-Text Generation",
    "evidence": [
      "results\nin an effective and more light-weight model. Experimental evaluation on the RotoWire benchmark shows that our model outperforms competitive baselines in terms of BLEU score and is generally better on qualitative metrics.",
      "results\nin terms of BLEU over all models; our best model Hierarchical-k reaching $17.5$ vs. $16.5$ against the best baseline. This means that our models learns to generate fluent sequences of words, close to the gold descriptions, adequately picking up on domain lingo. Qualitative metrics are either better or on par with baselines.",
      "results\nin terms of BLEU over all models; our best model Hierarchical-k reaching $17.5$ vs. $16.5$ against the best baseline.",
      "to the gold descriptions, adequately picking up on domain lingo. Qualitative metrics are either better or on par with baselines. We show in Figure FIGREF29 a text generated by our best model, which can be directly compared to the gold description in Figure FIGREF1. Generation is fluent and contains domain-specific expressions.",
      "ts\nshow that the proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics. In the following, we first present a state-of-the art of data-to-text literature (Section 2), and then describe our proposed hierarchical data encoder (Section 3). The evaluation protocol is presented in Section 4, followed by the",
      "Results\n::: Comparison w.r.t. baselines. From a general point of view, we can see from Table TABREF25 that our scenarios obtain significantly higher",
      "Results\nshow that the proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics.",
      "ions in a more human-like fashion. We would also like to draw attention to the number of parameters used by those architectures. We note that our scenarios relies on a lower number of parameters (14 millions) compared to all baselines (ranging from 23 to 45 millions).",
      "rk shows that our model outperforms competitive baselines in terms of BLEU score and is generally better on qualitative metrics. This way of representing structured databases may lead to automatic inference and enrichment, e.g., by comparing entities. This direction could be driven by very recent operation-guided networks BIBREF39, BIBREF40.",
      "t our scenarios relies on a lower number of parameters (14 millions) compared to all baselines (ranging from 23 to 45 millions). This outlines the effectiveness in the design of our model relying on a structure encoding, in contrast to other approach that try to learn the structure of data/descriptions from a linearized encoding.",
      "Results\nare compared to baselines BIBREF12, BIBREF13, BIBREF10 and variants of our models. We also report the result of the oracle (metrics on the gold descriptions).",
      "g structure in the encoding process is more effective that predicting a structure in the decoder (i.e., planning or templating). While our models sensibly outperform in precision at factual mentions, the baseline Puduppully-plan reaches $34.28$ mentions on average, showing that incorporating modules dedicated to entity extraction leads to over-focusing on entities; contrasting with our models that learn to generate more balanced descriptions.",
      "results\nof the best performing models presented in each paper. $\\bullet $ Wiseman BIBREF10 is a standard encoder-decoder system with copy mechanism."
    ],
    "answer": [
      "$17.5$ vs. $16.5$"
    ]
  },
  {
    "title": "Fully Convolutional Speech Recognition",
    "evidence": [
      "cess the raw waveform and shows state-of-the art performance on Wall Street Journal and on Librispeech among end-to-end systems. This first attempt at exploiting convolutional language models in speech recognition shows significant improvement over a 4-gram language model on both datasets.",
      " shows a WER of INLINEFORM1 but uses 150 times more training data for the acoustic model and huge text datasets for LM training. Finally, the state-of-the-art among end-to-end systems trained only on WSJ, and hence the most comparable to our system, uses lattice-free MMI on augmented data (with speed perturbation) and gets INLINEFORM2 WER.",
      "results\nTable TABREF11 shows Word Error Rates (WER) on WSJ for the current state-of-the-art and our models.",
      "raw waveform, removing the feature extraction step altogether. An external convolutional language model is used to decode words. On Wall Street Journal, our model matches the current state-of-the-art. On Librispeech, we report state-of-the-art performance among end-to-end models, including Deep Speech 2 trained with 12 times more acoustic data and significantly more linguistic data.",
      "dropout at each convolutional and linear layer on both WSJ and Librispeech. We keep all the words (162K) in WSJ training corpus. For Librispeech, we only use the most frequent 200K tokens (out of 900K).",
      "Conclusion\nWe introduced the first fully convolutional pipeline for speech recognition, that can directly process the raw waveform and shows state-of-the art performance on Wall Street Journal and on Librispeech among end-to-end systems.",
      "results\nTable TABREF11 shows Word Error Rates (WER) on WSJ for the current state-of-the-art and our models. The current best model trained on this dataset is an HMM-based system which uses a combination of convolutional, recurrent and fully connected layers, as well as speaker adaptation, and reaches INLINEFORM0 WER on nov92.",
      "ularization. These acoustic models are trained to predict letters directly with the Auto Segmentation Criterion (ASG) BIBREF20 . The only differences between the WSJ and Librispeech models are their depth, the number of feature maps per layer, the receptive field and the amount of dropout.",
      "ead speech, and Librispeech BIBREF26 , which contains 1000 hours with separate train/dev/test splits for clean and noisy speech. Each dataset comes with official textual data to train language models, which contain 37 million tokens for WSJ, 800 million tokens for Librispeech. Our language models are trained separately for each dataset on the official text data only.",
      " the beam is not full. These are chosen according to a trade-off between (near-)optimality of the search and computational cost. Experiments We evaluate our approach on the large vocabulary task of the Wall Street Journal (WSJ) dataset BIBREF25 , which contains 80 hours of clean read speech, and Librispeech BIBREF26 , which contains 1000 hours with separate train/dev/test splits for clean and noisy speech.",
      "am language model. Training/test splits On WSJ, models are trained on si284. nov93dev is used for validation and nov92 for test. On Librispeech, we train on the concatenation of train-clean and train-other. The validation set is dev-clean when testing on test-clean, and dev-other when testing on test-other.",
      "hich has 19 layers in addition to the front-end (mel-filterbanks for the baseline, or the learnable front-end for our approach). On WSJ, we use the lighter version used in BIBREF17 , which has 17 layers. Dropout is applied at each layer after the front-end, following BIBREF20 . The learnable front-end uses 40 or 80 filters.",
      "anguage model. More details on the experimental setup are given below. Baseline Our baseline for each dataset follows BIBREF11 . It uses the same convolutional acoustic model as our approach but a mel-filterbanks front-end and a 4-gram language model. Training/test splits On WSJ, models are trained on si284. nov93dev is used for validation and nov92 for test."
    ],
    "answer": [
      "Our model matches the current state-of-the-art on Wall Street Journal and reports state-of-the-art performance among end-to-end models on Librispeech."
    ]
  },
  {
    "title": "Massively Multilingual Neural Grapheme-to-Phoneme Conversion",
    "evidence": [
      "results\nfor 85 languages. Adapted",
      "results\n, as well as the various languages they were able to adapt them for, for a total of 229 languages. This test set omits 23 of the high resource languages that are written in unique scripts or for which language distance metrics could not be computed.",
      "results\nwhen using a bidirectional encoder, which consists of two separate encoders which process the input in forward and reverse directions. We used long short-term memory units BIBREF10 for both the encoder and decoder.",
      "results\n, as well as the various languages they were able to adapt them for, for a total of 229 languages.",
      "eeds a large quantity of pronunciation data. A standard dataset for g2p is the Carnegie Mellon Pronouncing Dictionary BIBREF12 . However, that is a monolingual English resource, so it is unsuitable for our multilingual task. Instead, we use the multilingual pronunciation corpus collected by deri2016grapheme for all experiments.",
      "came with the version of our model that was trained on data in all available languages, not just the languages it was tested on. Using a language ID token improves",
      "model is trained on a massively multilingual corpus. A language embedding vector is concatenated to the input at each time step. The language embeddings their system learned correlate closely to the genetic relationships between languages. However, neither of these models was applied to g2p.",
      "results\nas wFST. Their",
      "results\n, computed with wFSTs trained on a combination of Wiktionary pronunciation data and g2p rules extracted from Wikipedia IPA Help pages. They report high resource",
      "al networks, which have the advantage of being able to process sequences of arbitrary length and use long histories efficiently. They are trained jointly to minimize cross-entropy on the training data. We had our best",
      "y utilized. We attempt to leverage high resource data by treating g2p as a multisource neural machine translation (NMT) problem. The source sequences for our system are words in the standard orthography in any language. The target sequences are the corresponding representation in the International Phonetic Alphabet (IPA). Our",
      "results\nshow that the parameters learned by the shared encoder–decoder are able to exploit the orthographic and phonemic similarities between the various languages in our data.",
      "ed the neural network output with the output of the wFST-based Phonetisaurus system BIBREF1 did better than either system alone. A different approach came from kim2012universal, who used supervised learning with an undirected graphical model to induce the grapheme–phoneme mappings for languages written in the Latin alphabet."
    ],
    "answer": [
      "The datasets used were the Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme, Wiktionary pronunciation data, and g2p rules extracted from Wikipedia IPA Help pages."
    ]
  },
  {
    "title": "A Neural Approach to Irony Generation",
    "evidence": [
      "results\nclose to those of DualRL and obtains a balance between irony accuracy, sentiment preservation, and content preservation if we also consider the irony accuracy discussed below. And from human evaluation",
      "results\ndemonstrate the effectiveness of our model in terms of irony accuracy, sentiment preservation, and content preservation.",
      "results\ndemonstrate that our model achieves a high irony accuracy with well-preserved sentiment and content.",
      ", we ask four annotators who are proficient in English to evaluate the qualities of the generated sentences of different models. They are required to rank the output sentences of our model and baselines from the best to the worst in terms of irony accuracy (Irony), Sentiment preservation (Senti) and content preservation (Content).",
      "results\nin sentiment and content preservation. Nevertheless, DualRL system and ours get poor performances in irony accuracy.",
      "re BIBREF25 between the input sentences and the output sentences is calculated to evaluate the content preservation performance. In order to evaluate the overall performance of different models, we also report the geometric mean (G2) and harmonic mean (H2) of the sentiment accuracy and the BLEU score. As for the irony accuracy, we only report it in human evaluation",
      "in style transfer, the transformation from non-ironic to ironic sentences has to preserve sentiment polarity as mentioned above. Therefore, we not only design an irony reward to control the irony accuracy and implement denoising auto-encoder and back-translation to control content preservation but also design a sentiment reward to control sentiment preservation. Experimental",
      "rom the best to the worst in terms of irony accuracy (Irony), Sentiment preservation (Senti) and content preservation (Content). The best output is ranked with 1 and the worst output is ranked with 6. That means that the smaller our human evaluation value is, the better the corresponding model is.",
      "ce. (3) Our model considers both aspects and achieves a better balance among irony accuracy, sentiment and content preservation. Error Analysis Although our model outperforms other style transfer baselines according to automatic and human evaluation",
      "results\nin sentiment and content preservation. Nevertheless, DualRL system and ours get poor performances in irony accuracy. The reason may be that the other four baselines tend to generate common and even not fluent sentences which are irrelevant to the input sentences and are hard to be identified as ironies.",
      "y generation based on style transfer. Because of the lack of irony data, we make use of twitter and build a large-scale dataset. In order to control irony accuracy, sentiment preservation and content preservation at the same time, we also design a combination of rewards for reinforcement learning and incorporate reinforcement learning with a pre-training process. Experimental",
      "stem, the CrossAlign system and the CPTG system tends to generate sentences which are towards irony but do not preserve content. (2) The DualRL system preserves content and sentiment very well but even does not change the input sentence. (3) Our model considers both aspects and achieves a better balance among irony accuracy, sentiment and content preservation.",
      "inforcement learning and elaborately design two rewards to describe the irony accuracy and sentiment preservation, respectively. A pre-trained binary irony classifier based on CNN BIBREF20 is used to evaluate how ironic a sentence is. We denote the parameter of the classifier as INLINEFORM0 and it is fixed during the training process."
    ],
    "answer": [
      "Our model considers both aspects and achieves a better balance among irony accuracy, sentiment and content preservation."
    ]
  },
  {
    "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer",
    "evidence": [
      "es, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer. We train the system to predict (dis)agreement labels with training examples, where each example includes an image and question.",
      "confidence in that prediction. We employ the Matlab implementation of random forests, using 25 trees and the default parameters. We next adapt a VQA deep learning architecture BIBREF24 to learn the predictive combination of visual and textual features. The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question.",
      "with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer.",
      "res. The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 .",
      "F24 learned from a LSTM-CNN deep learning architecture to rank the order of priority for visual questions to receive redundancy. The system randomly prioritizes which images receive redundancy. This predictor illustrates the best a user can achieve today with crowd-powered systems BIBREF0 , BIBREF5 or with current dataset collection methods BIBREF2 , BIBREF4 .",
      "ather represents the many causes (described above). For our first system, we use domain knowledge to guide the learning process. We compile a set of features that we hypothesize inform whether a crowd will arrive at an undisputed, single answer. Then we apply a machine learning tool to reveal the significance of each feature.",
      "arning algorithms how to answer questions by example. Such data is also critical for evaluating how well VQA algorithms perform. In general, “bigger\" data is better.",
      "stion more precise. The remaining features come from two one-hot vectors describing each of the first two words in the question. Each one-hot vector is created using the learned vocabularies that define all possible words at the first and second word location of a question respectively (using training data, as described in the next section).",
      "es for a given visual question an answer with a confidence score. This system parallels the deep learning architecture we adapt. However, it predicts the system's uncertainty in its own answer, whereas we are interested in the humans' collective disagreement on the answer. Still, it is a useful baseline to see if an existing algorithm could serve our purpose.",
      "\ndemonstrate it is possible to predict whether a crowd will agree on a single answer from a given image and associated question. Despite the significant variety of questions and image content, and despite the variety of reasons for which the crowd can disagree, our learned model is able to produce quite accurate",
      "a 2,492-dimensional feature vector to represent the question-based features. One feature is the number of words in the question. Intuitively, a longer question offers more information and we hypothesize additional information makes a question more precise. The remaining features come from two one-hot vectors describing each of the first two words in the question.",
      "ould learn how many birds are in their path to decide whether to change course and so avoid costly, life-threatening collisions. These examples illustrate several of the interests from a visual question answering (VQA) system, including tackling problems that involve classification, detection, and counting.",
      " previous study that counting problems typically leads to disagreement for images showing many objects, and agreement otherwise. We employ a 2,492-dimensional feature vector to represent the question-based features. One feature is the number of words in the question."
    ],
    "answer": [
      "The model architecture used is a combination of a Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM) model, specifically VGG16 and a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question."
    ]
  },
  {
    "title": "From Textual Information Sources to Linked Data in the Agatha Project",
    "evidence": [
      " fact, we have explored the use of already existing modules and adopted and integrated several of these tools into our pipeline. It is important to point out that, as far as we know, there is no integrated architecture supporting the full processing pipeline for the Portuguese language.",
      "its potential application to another language would be easier, simply by changing the modules or the models of specific modules. In fact, we have explored the use of already existing modules and adopted and integrated several of these tools into our pipeline.",
      " then, it performs ontology matching procedures. As a final result, the obtained output is inserted into a specialized ontology. We are aware that each of the architecture modules can, and should, be improved but our main goal was the creation of a full working text processing pipeline for the Portuguese language.",
      "hat, as far as we know, there is no integrated architecture supporting the full processing pipeline for the Portuguese language. We evaluated several systems like Rembrandt BIBREF22 or LinguaKit: the former only has the initial steps of our proposal (until NER) and the later performed worse than our system.",
      "2 or LinguaKit: the former only has the initial steps of our proposal (until NER) and the later performed worse than our system. This framework, developed within the context of the Agatha project (described in Section SECREF1) has the full processing pipeline for Portuguese texts: it receives sentences as input and outputs ontological information: a) first performs all NLP typical tasks until semantic role labelling; b) then, it extracts subject-verb-object triples; c) and, then, it performs ontology matching procedures.",
      "ented in an ontology allows us to perform complex queries and inferences, which can detect patterns of typical criminal actions. Another axe of innovation in this research is the development, for the Portuguese language, of a pipeline of Natural Language Processing (NLP) processes, that allows us to fully process sentences and represent their content in an ontology.",
      "results\nsupport our claim that the proposed system can be used as a base tool for information extraction for the Portuguese language.",
      "proposed method\nis able to produce a detailed and well-connected knowledge base. Figure FIGREF23 shows the proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents.",
      "ts\nsupport our claim that the proposed system can be used as a base tool for information extraction for the Portuguese language. Being composed by several modules, each of them with a high level of complexity, it is certain that our approach can be improved and an overall better performance can be achieved.",
      "ls for the processing of the Portuguese language, the combination of all these steps in a integrated tool is a new contribution. Moreover, we have already explored other related research path, namely author profiling BIBREF2, aggression identification BIBREF3 and hate-speech detection BIBREF4 over social media, plus statute law retrieval and entailment for Japanese BIBREF5.",
      "Acknowledgment\ns The authors would like to thank COMPETE 2020, PORTUGAL 2020 Program, the European Union, and ALENTEJO 2020 for supporting this research as part of Agatha Project SI & IDT number 18022 (Intelligent analysis system of open of sources information for surveillance/crime control).",
      "results\nof the R&D project Agatha, where we developed a pipeline of processes that analyses texts (in Portuguese, Spanish, or English) and is able to populate a specialized ontology BIBREF1 (related to criminal law) for the representation of events, depicted in such texts.",
      "h a high level of complexity, it is certain that our approach can be improved and an overall better performance can be achieved. As future work we intend, not only to continue improving the individual modules, but also plan to extend this work to the: automatic creation of event timelines; incorporation in the knowledge base of information obtained from videos or pictures describing scenes relevant to criminal investigations."
    ],
    "answer": [
      "The effectiveness of the proposed pipeline approach is evaluated by comparing its performance with existing systems, such as Rembrandt and LinguaKit, and demonstrating its ability to produce a detailed and well-connected knowledge base, as shown in the proposed ontology populated with 3121 events entries from 51 documents."
    ]
  },
  {
    "title": "Sparse and Constrained Attention for Neural Machine Translation",
    "evidence": [
      "ntion transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19 , and the WMT 2016 dataset for Ro-En. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively.",
      "discussion\ns, and the three anonymous reviewers for their insightful comments. This work was supported by the European Research Council (ERC StG DeepSPIN 758969) and by the Fundação para a Ciência e Tecnologia through contracts UID/EEA/50008/2013, PTDC/EEI-SII/7092/2014 (LearnBig), and CMUPERI/TIC/0046/2014 (GoLocal).",
      "l to add a constant to this number (1 in our experiments). At test time, we use the expected fertilities according to our model. Experiments We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes.",
      "sformation by sparse and constrained alternatives: sparsemax, constrained softmax, and the newly proposed constrained sparsemax. For the latter, we derived efficient forward and backward propagation algorithms. By incorporating a model for fertility prediction, our attention transformations led to sparse alignments, avoiding repeated words in the translation.",
      "em of evaluating INLINEFORM0 , it suffices to show that our problem in Eq. EQREF14 can be rewritten in the form of Eq. EQREF45 . This is indeed the case, if we set: DISPLAYFORM0 Examples of Translations We show some examples of translations obtained for the German-English language pair with different systems.",
      "Acknowledgment\ns We thank the Unbabel AI Research team for numerous",
      "Abstract\nIn NMT, words are sometimes dropped from the source or generated repeatedly in the translation. We explore novel strategies to address the coverage problem that change only the attention transformation.",
      "y in the translation. We explore novel strategies to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive.",
      "as two recently proposed coverage models: We also experimented combining the strategies above with the sparsemax transformation. As evaluation metrics, we report tokenized BLEU, METEOR ( BIBREF22 , as well as two new metrics that we describe next to account for over and under-translation.",
      "results\nin various NLP problems, they have never been applied to NMT, to the best of our knowledge. Furthermore, we combine these two ideas and propose a novel attention transformation, constrained sparsemax, which produces both sparse and bounded attention weights, yielding a compact and interpretable set of alignments.",
      "ples of Translations We show some examples of translations obtained for the German-English language pair with different systems. Blue highlights the parts of the reference that are correct and red highlights the corresponding problematic parts of translations, including repetitions, dropped words or mistranslations.",
      "ility predictor model using a bi-LSTM tagger. At training time, we provide as supervision the fertility estimated by fast_align. Since our model works with fertility upper bounds and the word aligner may miss some word pairs, we found it beneficial to add a constant to this number (1 in our experiments). At test time, we use the expected fertilities according to our model.",
      "this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with subword units BIBREF20 with a joint vocabulary and 32k merge operations. Our implementation was done on a fork of the OpenNMT-py toolkit BIBREF21 with the default parameters ."
    ],
    "answer": [
      "De-En",
      "Ja-En",
      "Ro-En"
    ]
  },
  {
    "title": "Dialog Context Language Modeling with Recurrent Neural Networks",
    "evidence": [
      " language models, such as DRNNLM and CCDCLM, treat dialog history as a sequence of inputs, without modeling dialog interactions. A dialog turn from one speaker may not only be a direct response to the other speaker's query, but also likely to be a continuation of his own previous statement.",
      "the number of turns in the dialog. Perplexity is calculated on the last turn, with preceding turns used as context to the model. As can be seen from the",
      "ne models when using dialog turn size of 5, and produces slightly worse perplexity than DRNNLM when using dialog turn size of 3. To analyze the best potential gain that may be achieved by introducing linguistic context, we compare the proposed contextual models to DACLM, the model that uses true dialog act history for dialog context modeling.",
      "n a dialog as a sequence of inputs might not well capture the pauses, turn-taking, and grounding phenomena BIBREF12 in a dialog. In this work, we propose contextual RNN language models that specially track the interactions between speakers. We expect such models to generate better representations of the dialog context. The remainder of the paper is organized as follows.",
      "Abstract\nIn this work, we propose contextual language models that incorporate dialog level discourse information into language modeling.",
      "\nIn this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions.",
      "xt, we compare the proposed contextual models to DACLM, the model that uses true dialog act history for dialog context modeling. As shown in Table 1 , the gap between our proposed models and DACLM is not wide. This gives a positive hint that the proposed contextual models may implicitly capture the dialog context state changes.",
      "roposed dialog context language models is higher than that of the model using true dialog act tags as context by a small margin. This indicates that the proposed model may implicitly capture the dialog context state for language modeling.",
      " works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network (RNN) based contextual language models that specially track the interactions between speakers in a dialog. Experiment",
      "for dialog language modeling, as they are not designed to capture dialog interactions, such as clarifications and confirmations. By making special design in learning dialog interactions, we expect the models to generate better representations of the dialog context, and thus lower perplexity of the target dialog turn or utterance.",
      " not only be a direct response to the other speaker's query, but also likely to be a continuation of his own previous statement. Thus, when modeling turn $k$ in a dialog, we propose to connect the last RNN state of turn $k-2$ directly to the starting RNN state of turn $k$ , instead of letting it to propagate through the RNN for turn $k-1$ .",
      "Conclusion\ns In this work, we propose two dialog context language models that with special design to model dialog interactions. Our evaluation",
      "e for document level context modeling. In dialog modeling, however, dialog interactions between speakers play an important role. Modeling utterances in a dialog as a sequence of inputs might not well capture the pauses, turn-taking, and grounding phenomena BIBREF12 in a dialog."
    ],
    "answer": [
      "The proposed dialog context language models can capture up to 3 turns of dialog history."
    ]
  },
  {
    "title": "Shallow Syntax in Deep Water",
    "evidence": [
      "ethod involves classical addition of chunk features to cwr-infused architectures for four different downstream tasks (§SECREF3). Shallow syntactic information is obtained automatically using a highly accurate model (97% $F_1$ on standard benchmarks). In both settings, we observe only modest gains on three of the four downstream tasks relative to ELMo-only baselines (§SECREF4).",
      "edicted shallow syntactic parses, instead of just raw text, so that contextual embeddings make use of shallow syntactic context. Our second method involves shallow syntactic features obtained automatically on downstream task data. Neither approach leads to a significant gain on any of the four downstream tasks we considered relative to ELMo-only baselines.",
      "tures Our second approach incorporates shallow syntactic information in downstream tasks via token-level chunk label embeddings. Task training (and test) data is automatically chunked, and chunk boundary information is passed into the task model via BIOUL encoding of the labels.",
      "ks. Though helpful to span-level task models without cwrs, shallow syntactic features offer little to no benefit to ELMo models. mSynC's performance is similar. This holds even for phrase-structure parsing, where (gold) chunks align with syntactic phrases, indicating that task-relevant signal learned from exposure to shallow syntax is already learned by ELMo.",
      "with syntactic phrases, indicating that task-relevant signal learned from exposure to shallow syntax is already learned by ELMo. On sentiment classification, chunk features are slightly harmful on average (but variance is high); mSynC again performs similarly to ELMo-transformer.",
      " For training, $\\mbox{\\textbf {mSynC}}_i$ is used to compute the probability of the next word, using a sampled softmax BIBREF18. For downstream tasks, we use a learned linear weighting of all layers in the encoders to obtain a task-specific mSynC, following BIBREF2.",
      "s minimal downstream task architecture, bringing into focus the transferability of cwrs, as opposed to task-specific adaptation. Experiments ::: Linguistic Probes ::: Probing Tasks The ten different probing tasks we used include CCG supertagging BIBREF26, part-of-speech tagging from PTB BIBREF13 and EWT (Universal Depedencies BIBREF27), named entity recognition BIBREF20, base-phrase chunking BIBREF12, grammar error detection BIBREF28, semantic tagging BIBREF29, preposition supersense identification BIBREF30, and event factuality detection BIBREF31.",
      "odels for each task, we can attribute any difference in performance to the incorporation of shallow syntax or contextualization. Details of downstream architectures are provided below, and overall dataset statistics for all tasks is shown in the Appendix, Table TABREF26.",
      "h reduces training duration by a factor of 10 in our experiments. We discuss the empirical effect of this approach in §SECREF20. Shallow Syntactic Features Our second approach incorporates shallow syntactic information in downstream tasks via token-level chunk label embeddings.",
      "extual word representations (cwrs) derived from pretraining language models on large corpora BIBREF2, BIBREF3, BIBREF4, BIBREF5. Recent work has shown that downstream task performance may benefit from explicitly injecting a syntactic inductive bias into model architectures BIBREF6, even when cwrs are also used BIBREF7.",
      "ation of phrase-syntactic structure of sentences; it can be produced with high accuracy, and is computationally cheap to obtain. We investigate the role of shallow syntax-aware representations for NLP tasks using two techniques.",
      "ear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases. Unlike §SECREF11, there is minimal downstream task architecture, bringing into focus the transferability of cwrs, as opposed to task-specific adaptation.",
      "omly initialized chunk label embeddings to task-specific input encoders, which are then fine-tuned for task-specific objectives. This approach does not require a shallow syntactic encoder or chunk annotations for pretraining cwrs, only a chunker. Hence, this can more directly measure the impact of shallow syntax for a given task."
    ],
    "answer": [
      "Shallow syntactic information is obtained automatically using a highly accurate model (97% $F_1$ on standard benchmarks)."
    ]
  },
  {
    "title": "TTTTTackling WinoGrande Schemas",
    "evidence": [
      "e previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with the observations of Nogueira et al.",
      "ver conditions with entailment/contradiction, we did not run all data size conditions given our limited computational resources. Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture.",
      "n part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada. We would like to thank Google Colab for providing support in terms of computational resources.",
      "Acknowledgment\ns This research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada.",
      " tuning. Condition #2 represents our submission to the official leaderboard, which achieves 0.7673 AUC on the held-out test set. From these",
      "trings, each containing a hypothesis, and using the probabilities assigned to the\"entailment\"token as a score of the hypothesis. Our first (and only) submission to the official leaderboard yielded 0.7673 AUC on March 13, 2020, which is the best known result at this time and beats the previous state of the art by over five points.",
      "Results\nExperimental",
      "lt—the reasoning goes—data-driven techniques (even neural models) will be of limited use due to the paucity of relevant corpora. Yet, previous encoder-only architectures like RoBERTa that exploit a language modeling objective (that is, relying only on explicit textual knowledge) can clearly make headway in a commonsense reasoning task, and we can further improve upon these approaches with a sequence-to-sequence model.",
      "at these more sophisticated models can exploit, or that we are genuinely making at least some progress in commonsense reasoning. The latter, in particular, challenges the notion that commonsense knowledge is (mostly) tacit. Perhaps it is the case that in a humongous corpus of natural language text, someone really has written about trying to stuff a tuba in a backpack?",
      "ce of target tokens affects prediction accuracy is consistent with this observation. How and why is the subject of ongoing work. Implications Collectively, the success of large pretrained neural models, both encoder-only BERT-like architectures as well as encoder–decoder architectures like T5, raise interesting questions for the pursuit of commonsense reasoning.",
      "Introduction\nOther than encoder-only pretrained transformer architectures BIBREF2, BIBREF3, BIBREF4, encoder–decoder style pretrained transformers BIBREF0, BIBREF5 have been proven to be effective in text generation tasks as well as comprehension tasks.",
      "tectures as well as encoder–decoder architectures like T5, raise interesting questions for the pursuit of commonsense reasoning. Researchers have discovered that previous models perform well on benchmark datasets because they pick up on incidental biases in the dataset that have nothing to do with the task; in contrast, the WinoGrande dataset has devoted considerable effort to reducing such biases, which may allow models to (inadvertently) “cheat” (for example, using simple statistical associations).",
      "ake headway in a commonsense reasoning task, and we can further improve upon these approaches with a sequence-to-sequence model. This leaves us with two possible explanations: despite careful controls, the WinoGrande challenge still contains incidental biases that these more sophisticated models can exploit, or that we are genuinely making at least some progress in commonsense reasoning."
    ],
    "answer": [
      "The previous state of the art is based on RoBERTa BIBREF2, which is an encoder-only transformer architecture."
    ]
  },
  {
    "title": "A Question-Entailment Approach to Question Answering",
    "evidence": [
      "Discussion\nof RQE",
      "ing Question Entailment (RQE) and we describe the QA system and resources that we built and evaluated on real medical questions. First, we compare machine learning and deep learning methods for RQE using different kinds of datasets, including textual inference, question similarity and entailment in both the open and clinical domains.",
      "FORM0 type(s) and INLINEFORM1 type(s)), 1 (Overlap between INLINEFORM2 type(s) and INLINEFORM3 type(s)) and 0 (No common types). Datasets Used for the RQE Study We evaluate the RQE methods (i.e. deep learning model and logistic regression classifier) using two datasets of sentence pairs (SNLI and multiNLI), and three datasets of question pairs (Quora, Clinical-QE, and SemEval-cQA).",
      " Our contributions are: The next section is dedicated to related work on question answering, question similarity and entailment. In Section SECREF3 , we present two machine learning (ML) and deep learning (DL) methods for RQE and compare their performance using open-domain and clinical datasets. Section SECREF4 describes the new collection of medical question-answer pairs.",
      " of machine learning and deep learning methods for Recognizing Question Entailment in the medical domain using several datasets. We developed a RQE-based QA system to answer new medical questions using existing question-answer pairs. We built and shared a collection of 47K medical question-answer pairs. Our QA approach outperformed the best",
      "and the high performance achieved by neural networks on larger datasets such as SNLI BIBREF13 , BIBREF39 , BIBREF40 , BIBREF41 . We first define the RQE task, then present the two approaches, and evaluate their performance on five different datasets.",
      "c Regression trained on the clinical-RQE dataset to recognize entailed questions and rank them with their classification scores. RQE-based QA Approach Classifying the full QA collection for each test question is not feasible for real-time applications.",
      "l and semantic features as well as embeddings generated by different neural networks (siamese, Bi-LSTM, GRU and CNNs) BIBREF38 . In the scope of this challenge, a dataset was collected from Qatar Living forum for training. We refer to this dataset as SemEval-cQA. In another effort, an answer-based definition of RQE was proposed and tested BIBREF1 .",
      "results\nshow that RQE improves the overall performance and exceeds the best",
      "results\nwere similar. We have also tested the SemEval-cQA-2016 model and had a similar drop in performance on RQE data. This could be explained by the different types of data leading to wrong internal conceptualizations of medical terms and questions in the deep neural layers.",
      "RQE on the full collection for each user question is not feasible for a real-time system because of the extended execution time. Evaluation of the top ten answers In this evaluation, we used Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) which are commonly used in QA to evaluate the top-10 answers for each question.",
      "onvolution) to map questions to their semantic representations. The models were pre-trained within an encoder-decoder framework. RQE Approaches and Experiments The choice of two methods for our empirical study is motivated by the best performance achieved by Logistic Regression in question-question similarity at SemEval 2017 (best system BIBREF37 and second best system BIBREF38 ), and the high performance achieved by neural networks on larger datasets such as SNLI BIBREF13 , BIBREF39 , BIBREF40 , BIBREF41 .",
      "s retrieved by a search engine, assuming that the answers to the similar questions will be correct answers for the new question. Different machine learning and deep learning approaches were tested in the scope of SemEval 2016 BIBREF3 and 2017 BIBREF4 task 3B."
    ],
    "answer": [
      "The machine learning and deep learning methods used for Recognizing Question Entailment (RQE) include logistic regression, siamese networks, Bi-LSTM, GRU, CNNs, and encoder-decoder frameworks."
    ]
  },
  {
    "title": "Argumentation Mining in User-Generated Web Discourse",
    "evidence": [
      "results\n, but we can state that some registers and topics, given their properties, are more challenging to annotate than others.",
      "results\n, but we can state that some registers and topics, given their properties, are more challenging to annotate than others. Another qualitative analysis of disagreements between annotators was performed by constructing a probabilistic confusion matrix BIBREF77 on the token level.",
      "veral novel aspects. We tackle the above-mentioned research questions as well as the previously discussed challenges and issues. First, we target user-generated Web discourse from several domains across various registers, to examine how argumentation is communicated in different contexts.",
      "l and empirical. We pose several pragmatical constraints, such as register independence (generalization over several registers). Second, our emphasis is put on reliable annotations and sufficient data size (about 90k tokens). Third, we deal with fairly unrestricted Web-based sources, so additional steps of distinguishing whether the texts are argumentative are required.",
      "eparate) versus mixed-sex classes (“co-ed”), and (6) mainstreaming — including children with special needs into regular classes. Since we were also interested in whether argumentation differs across registers, we included four different registers — namely (1) user comments to newswire articles or to blog posts, (2) posts in",
      "take up the challenges given by the variety of registers, multiple domains, and unrestricted noisy user-generated Web discourse. (ii) We bridge the gap between normative argumentation theories and argumentation phenomena encountered in actual data by adapting an argumentation model tested in an extensive annotation study.",
      "pose biggest challenges. Properly detecting boundaries of argument components caused problems, as shown in Figure FIGREF146 (a). This goes in line with the granularity annotation difficulties discussed in section UID86 . The next example in Figure FIGREF146 (b) shows that even if boundaries of components were detected precisely, the distinction between premise and backing fails.",
      "eature set 4 (embeddings). These two domains contain also fewer documents, compared to other domains (refer to Table TABREF71 ). We suspect that embeddings-based features convey important information when not enough in-domain data are available. This observation will become apparent in the next experiment. The cross-domain experiments yield rather poor",
      "results\nindicate that embedding features generalize well across domains in our task of argument component identification. We leave investigating better performing vector representations, such as paragraph vectors BIBREF123 , for future work.",
      " design methods capable of analyzing people's argumentation. In this article, we go beyond the state of the art in several ways. (i) We deal with actual Web data and take up the challenges given by the variety of registers, multiple domains, and unrestricted noisy user-generated Web discourse.",
      " a random ordering of the entire data set. Second, we deal with in-domain ten-fold cross validation for each of the six domains. Third, in order to evaluate the domain portability of our approach, we train the system on five domains and test on the remaining one for all six domains (which we report as cross-domain validation).",
      "strict, as it penalizes a mismatch in argument component boundaries the same way as a wrongly predicted argument component type. Therefore we also report two another evaluation metrics that help to put our",
      "nt analysis and persuasion studies presented in this section, we would like to stress some novel aspects of the current article. First, we aim at adapting a model of argument based on research by argumentation scholars, both theoretical and empirical. We pose several pragmatical constraints, such as register independence (generalization over several registers)."
    ],
    "answer": [
      "The challenges posed by different registers and domains include detecting boundaries of argument components, distinguishing between premise and backing, and dealing with noisy and unrestricted user-generated Web discourse."
    ]
  },
  {
    "title": "Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model",
    "evidence": [
      "arning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the model learn in zero-shot setting.",
      "ike SQuAD may be tackled by some language independent strategies, for example, matching words in questions and context BIBREF20. Is zero-shot learning feasible because the model simply learns this kind of language independent strategies on one language and apply to the other?",
      "results\ncan be found in the Appendix. What Does Zero-shot Transfer Model Learn? ::: Code-switching Dataset We observe linguistic-agnostic representations in the last subsection.",
      "ng datasets, the EM/F1 score drops, indicating that the semantics of representations are not totally disentangled from language. However, the examples of the answers of the model (Table TABREF21) show that multi-BERT could find the correct answer spans although some keywords in the spans have been translated into another language. What Does Zero-shot Transfer Model Learn?",
      "ould also do well on unseen languages. Table TABREF14 shows that the performance of multi-BERT drops drastically on the dataset. It implies that multi-BERT might not totally rely on pattern matching when finding answers. What Does Zero-shot Transfer Model Learn?",
      ". The analysis supports that the characteristics of training data is more important than translated into target language or not. Therefore, although translation degrades the performance, whether translating the corpus into the target language is not critical. What Does Zero-shot Transfer Model Learn?",
      " encode semantic and syntactic information in language-agnostic ways and explains how zero-shot transfer learning could be done. To take a step further, while transfering model from source dataset to target dataset, we align representations in two proposed way, to improve performance on target dataset.",
      "results\nshow that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance.",
      "zero-shot learning on the natural language understanding tasks including XNLI BIBREF19, NER, POS, Dependency Parsing, and so on. We now seek to know if a pre-trained multi-BERT has ability to solve RC tasks in the zero-shot setting.",
      "Conclusion\nIn this paper, we systematically explore zero-shot cross-lingual transfer learning on RC with multi-BERT. The experimental",
      " F1 scores. This shows that the model learned with zero-shot can roughly identify the answer spans in context but less accurate. In row (c), we fine-tuned a BERT model pre-trained on English monolingual corpus (English BERT) on Chinese RC training data directly by appending fastText-initialized Chinese word embeddings to the original word embeddings of English-BERT.",
      "plies that multi-BERT might not totally rely on pattern matching when finding answers. What Does Zero-shot Transfer Model Learn? ::: Embedding in Multi-BERT PCA projection of hidden representations of the last layer of multi-BERT before and after fine-tuning are shown in Fig. FIGREF15. The red points represent Chinese tokens, and the blue points are for English. The",
      "T is fine-tuned on English but tested on Chinese, which achieves competitive performance compared with QANet trained on Chinese. We also find that multi-BERT trained on English has relatively lower EM compared with the model with comparable F1 scores. This shows that the model learned with zero-shot can roughly identify the answer spans in context but less accurate."
    ],
    "answer": [
      "The model learns to encode semantic and syntactic information in language-agnostic ways."
    ]
  },
  {
    "title": "Automated News Suggestions for Populating Wikipedia Entity Pages",
    "evidence": [
      "being relevant enough to be included in an entity page. We therefore use the salience features in BIBREF11 as part of our model. However, these features are document-internal — we will show that they are not sufficient to predict news inclusion into an entity page and add features of entity authority, news authority and novelty that measure the relations between several entities, between entity and news article as well as between several competing news articles.",
      " which INLINEFORM9 is salient, we also look at the reverse property, namely whether INLINEFORM10 is important for INLINEFORM11 . We do this by comparing the authority of INLINEFORM12 (which is a measure of popularity of an entity, such as its frequency of mention in a whole corpus) with the authority of its co-occurring entities in INLINEFORM13 , leading to a feature we call relative authority.",
      "nce of INLINEFORM0 with relative entity frequency from the salience feature group is close to that of all the features combined. The authority and novelty features account to a further improvement in terms of precision, by adding roughly a 7%-10% increase. However, if both feature groups are considered separately, they significantly outperform the baseline B1.",
      " we show the impact of the individual feature groups that contribute to the superior performance in comparison to the baselines. Relative entity frequency from the salience feature, models the entity salience as an exponentially decaying function based on the positional index of the paragraph where the entity appears.",
      "INLINEFORM15 should fulfill the following properties. Table TABREF21 shows a summary of the computed features for INLINEFORM16 . Salience: entity INLINEFORM0 should be a salient entity in news article INLINEFORM1 Relative Authority: the set of entities INLINEFORM0 with which INLINEFORM1 co-occurs should have higher authority than INLINEFORM2 , making INLINEFORM3 important for INLINEFORM4 Novelty: news article INLINEFORM0 should provide novel information for entity INLINEFORM1 taking into account its profile INLINEFORM2 Baseline Features.",
      "y, based on the entity's salience in the news article, its relative authority and the novelty of the article to the entity page. The second stage takes into account the class of the entity for which the news is suggested and constructs section templates from entities of the same class.",
      "ntity's salience in a news article is a prerequisite to the news article being relevant enough to be included in an entity page. We therefore use the salience features in BIBREF11 as part of our model.",
      "e entity salience as an exponentially decaying function based on the positional index of the paragraph where the entity appears. The performance of INLINEFORM0 with relative entity frequency from the salience feature group is close to that of all the features combined.",
      "whole corpus) with the authority of its co-occurring entities in INLINEFORM13 , leading to a feature we call relative authority. The intuition is that for an entity that has overall lower authority than its co-occurring entities, a news article is more easily of importance.",
      " account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates.",
      "n entity in a document, positioning of an entity, grammatical function or internal entity structure (POS tags, head nouns etc.). These approaches are not currently aimed at knowledge base generation or Wikipedia coverage extension but we postulate that an entity's salience in a news article is a prerequisite to the news article being relevant enough to be included in an entity page.",
      "abstract\nof an article BIBREF11 . Frequent features to measure salience include the frequency of an entity in a document, positioning of an entity, grammatical function or internal entity structure (POS tags, head nouns etc.).",
      "ns: salience, relative authority, novelty and placement. First, we need to identify the most salient entities in a news article. This is done to avoid pollution of entity pages with only marginally related news. Second, we need to determine whether the news is important to the entity as only the most relevant news should be added to a precise reference work."
    ],
    "answer": [
      "The features used to represent the salience and relative authority of entities are salience, relative authority, and novelty."
    ]
  },
  {
    "title": "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
    "evidence": [
      "ning data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus.",
      "tive in many subsequent studies. It is however very computationally costly, typically requiring to translate large sets of data. Determining the “right” amount (and quality) of BT data is another open issue, but we observe that experiments reported in the literature only use a subset of the available monolingual resources.",
      " interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French.",
      " Can BT be efficiently simulated? Does BT data play the same role as a target-side language modeling, or are they complementary? BT is often used for domain adaptation: can the effect of having more in-domain data be sorted out from the mere increase of training material BIBREF2 ?",
      "rning with a natural source. Related work The literature devoted to the use of monolingual data is large, and quickly expanding. We already alluded to several possible ways to use such data: using back- or forward-translation or using a target language model.",
      "ns: since there are many ways to generate pseudo parallel corpora, how important is the quality of this data for MT performance? Which properties of back-translated sentences actually matter for MT quality? Does BT act as some kind of regularizer BIBREF3 ? Can BT be efficiently simulated? Does BT data play the same role as a target-side language modeling, or are they complementary?",
      " very poor SMT system trained using only 50k parallel sentences from the out-of-domain data, and no additional monolingual data. For this system as for the next one, we use Moses BIBREF17 out-of-the-box, computing alignments with Fastalign BIBREF18 , with a minimal pre-processing (basic tokenization).",
      "results\n(see Table TABREF6 ) replicate the findings of BIBREF2 : large gains can be obtained from BT (nearly INLINEFORM0 BLEU in French and German); better artificial data yields better translation systems.",
      "rtificial parallel data through \\textsl{back-translation} - a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of back-translation, comparing alternative uses of monolingual data, as well as multiple data generation procedures.",
      "LINEFORM1 French. As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus. The rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario.",
      "German where simple copies yield a significant improvement. Performance drops for both language pairs in the copy-dummies setup. We achieve our best gains with the copy-marked setup, which is the best way to use a copy of the target (although the performance on the out-of-domain tests is at most the same as the baseline).",
      " already alluded to several possible ways to use such data: using back- or forward-translation or using a target language model. The former approach is mostly documented in BIBREF2 , and recently analyzed in BIBREF19 , which focus on fully artificial settings as well as pivot-based artificial data; and BIBREF4 , which studies the effects of increasing the size of BT data.",
      "ng vocabularies of about respectively 32k and 36k for English INLINEFORM0 French and 32k and 44k for English INLINEFORM1 German. Both systems use 512-dimensional word embeddings and a single hidden layer with 1024 cells. They are optimized using Adam BIBREF12 and early stopped according to the validation performance. Training lasted for about three weeks on an Nvidia K80 GPU card."
    ],
    "answer": [
      "The language of the data in the Europarl corpus is English, German, and French."
    ]
  },
  {
    "title": "Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation",
    "evidence": [
      "speculate that it is because the Vietnamese INLINEFORM3 Japanese system is not good enough to produce reasonable synthetic data. In the meanwhile, combining Back Translation and Mix-Source brings a considerable improvements of 0.6 BLEU points compared to not using them.",
      "built the first Japanese INLINEFORM0 Vietnamese NMT systems and released the dataset as well as the associated training scripts. We have also shown that the proposed VNBPE algorithm can be used for Vietnamese word segmentation in order to conduct neural machine translation.",
      " the test set is tst2010. The data augmentation methods has been applied only for the Japanese INLINEFORM0 Vietnamese direction. For Back Translation, we use Vietnamese monolingual data from VNESEcorpus of DongDu which includes 349578 sentences.",
      "bilingual data. In this paper, we attempt to build the first NMT systems for a low-resourced language pairs:Japanese-Vietnamese. We have also shown significant improvements when combining advanced methods to reduce the adverse impacts of data sparsity and improve the quality of NMT systems.",
      "ows the effectiveness in low-resourced scenarios BIBREF13 , BIBREF14 and our Japanese INLINEFORM1 Vietnamese is such a scenario. Data Preparation We collected Japanese-Vietnamese parallel data from TED talks extracted from WIT3's corpus BIBREF15 . After removing blank and duplicate lines we obtained 106758 pairs of sentences.",
      " size with the Japanese-Vietnamese parallel corpus. In the end, the data we have is double in size compared to the original one. Mix-Source Approach Another data augmentation method considered useful in this low-resourced setting is the mix-source method BIBREF12 .",
      "Conclusion\nWe has built the first Japanese INLINEFORM0 Vietnamese NMT systems and released the dataset as well as the associated training scripts.",
      "loying a complicate word segmentation method, (ii) BPE works significantly well for Japanese texts after we tokenized the texts. Data Augmentation. We use the best system for Vietnamese INLINEFORM0 Japanese, which is the NMT systems trained on BPE-processed texts, to generate the synthetic data for Japanese INLINEFORM1 Vietnamese translation.",
      "namese because French and Vietnamese have more similarities in the structures of sentences than between Japanese and Vietnamese. We also built phrase-based systems on the TED data and achieved better BLEU scores when using NMT. Recently, some works use monolingual data to improve the accuracy of NMT systems.",
      "orpus corpus and take out the first 106758 sentences (the same as the number of sentence pairs in the original parallel corpus). For Mix-Source, instead of using a subsampled monolingual corpus, we use the Vietnamese part of the Japanese-Vietnamese parallel corpus in order to learn the multilingual information in the same domain. Our datasets are listed in Table TABREF14 .",
      "e parallel corpus in order to learn the multilingual information in the same domain. Our datasets are listed in Table TABREF14 . Preprocessing After using KyTea to tokenize the Japanese texts, we learn and apply Sennrich's BPE from the tokenized texts. For Vietnamese texts, first we use Moses scripts to normalize the texts from digits, punctuations and special symbols.",
      "Related Works\nJapanese-Vietnamese MT is firstly mentioned in 2005 BIBREF18 . The authors focused on the difference from embedding structures between Japanese and Vietnamese, and then proposed rules for MT system and experiment on very small dataset (714 Japanese embedding sentences).",
      "results\nhave shown in the Table TABREF21 . Baseline. For the baseline systems, the training data includes KyTea-segmented Japanese texts and pyvi-segmented Vietnamese texts."
    ],
    "answer": [
      "{\"dataset\": \"WIT3\", \"corpus\": \"VNESEcorpus of DongDu\"}"
    ]
  },
  {
    "title": "Ancient-Modern Chinese Translation with a Large Training Dataset",
    "evidence": [
      "o build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.",
      "oth lexical-based information and statistical-based information, which achieves 94.2 F1-score on our manual annotation Test set. We use this method to create a new large-scale Ancient-Modern Chinese parallel corpus which contains 1.24M bilingual pairs. To our best knowledge, this is the first large high-quality Ancient-Modern Chinese dataset.",
      "ed above, here we only consider 1-0, 0-1, 1-1, 1-2, 2-1 and 2-2 alignment modes. Ancient-Modern Chinese Dataset Data Collection. To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet.",
      "hich contains 1.24M bilingual pairs. To our best knowledge, this is the first large high-quality Ancient-Modern Chinese dataset. Furthermore, we analyzed and compared the performance of the SMT and various NMT models on this dataset and provided a strong baseline for this task.",
      "iew There are four steps to build the ancient-modern Chinese translation dataset: (i) The parallel corpus crawling and cleaning. (ii) The paragraph alignment. (iii) The clause alignment based on aligned paragraphs. (iv) Augmenting data by merging aligned adjacent clauses. The most critical step is the third step.",
      "ORM0 1.24M bilingual sentence pairs. To our best knowledge, this is the first large high-quality ancient-modern Chinese dataset. Furthermore, we test SMT models and various NMT models on the created dataset and provide a strong baseline for this task. Overview There are four steps to build the ancient-modern Chinese translation dataset: (i) The parallel corpus crawling and cleaning.",
      "core on Test set. Based on it, we build a large scale parallel corpus which contains INLINEFORM0 1.24M bilingual sentence pairs. To our best knowledge, this is the first large high-quality ancient-modern Chinese dataset.",
      " study. In addition, we also compared our method with the longest common subsequence (LCS) based approach proposed by BIBREF12 . To the best of our knowledge, BIBREF12 is the latest related work which are designed for Ancient-Modern Chinese alignment. Hyper-parameters. For the",
      "Abstract\nAncient Chinese brings the wisdom and spirit culture of the Chinese nation. Automatic translation from ancient Chinese to modern Chinese helps to inherit and carry forward the quintessence of the ancients.",
      "proposed method\nto create a large translation parallel corpus which contains INLINEFORM0 1.24M bilingual sentence pairs. To our best knowledge, this is the first large high-quality ancient-modern Chinese dataset.",
      "NMT models and summarize some specific problems that machine translation models will encounter when translating ancient Chinese. For the future work, firstly, we are going to expand the dataset using the",
      " this paper, we propose an Ancient-Modern Chinese clause alignment approach based on the characteristics of these two languages. This method combines both lexical-based information and statistical-based information, which achieves 94.2 F1-score on our manual annotation Test set.",
      "nd lexical-based information to measure the score for each possible clause alignment between ancient and modern Chinese strings. The dynamic programming is employed to further find overall optimal alignment paragraph by paragraph."
    ],
    "answer": [
      "The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era."
    ]
  },
  {
    "title": "UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish",
    "evidence": [
      "emi-supervised learning, as well as the creation of stepwise ensembles, increase the performance of our Spanish-language models. Strong support was found for the translation and semi-supervised learning approaches; our best models for all subtasks use either one of these approaches. These",
      "d Learning One of the aims of this study was to see if using semi-supervised learning is beneficial for emotion intensity tasks. For this purpose, the DISC BIBREF0 corpus was used. This corpus was created by querying certain emotion-related words, which makes it very suitable as a semi-supervised corpus. However, the specific emotion the tweet belonged to was not made public.",
      "e algorithms, it shows that in 22 out of 30 cases, using translated instances as extra training data resulted in an improvement. For the semi-supervised learning approach, an improvement is found in 15 out of 16 cases. Moreover, our best individual model for each subtask (bolded scores in Table TABREF18 ) is always either a translated or semi-supervised model.",
      "itional training data by (i) translating training data from other languages and (ii) applying a semi-supervised learning method. We find strong support for both approaches, with those models outperforming our regular models in all subtasks. However, creating a stepwise ensemble of different models as opposed to simply averaging did not result in an increase in performance.",
      "sed and a parameter search was conducted for epsilon. Detailed parameter settings for each subtask are shown in Table TABREF12 . Each parameter search was performed using 10-fold cross validation, as to not overfit on the development set. Semi-supervised Learning One of the aims of this study was to see if using semi-supervised learning is beneficial for emotion intensity tasks.",
      "ich makes it very suitable as a semi-supervised corpus. However, the specific emotion the tweet belonged to was not made public. Therefore, a method was applied to automatically assign the tweets to an emotion by comparing our scraped tweets to this new data set.",
      "slated training data (translated) and the third one was trained on both the training data and the semi-supervised data (silver). Due to the nature of the SVM algorithm, semi-supervised learning does not help, so only the regular and translated model were trained in this case. This",
      "in better performance compared to simply averaging the models. In addition, some signs of overfitting on the dev set were found. In future work, we would like to apply the methods (translation and semi-supervised learning) used on Spanish on other low-resource languages and potentially also on other tasks.",
      "results\nin 8 different models per subtask. Note that for the valence tasks no silver training data was obtained, meaning that for those tasks the semi-supervised models could not be used.",
      "wed us to create silver datasets per emotion, assigning tweets to an emotion if an annotated emotion-word occurred in the tweet. Our semi-supervised approach is quite straightforward: first a model is trained on the training set and then this model is used to predict the labels of the silver data.",
      "est individual model for each subtask (bolded scores in Table TABREF18 ) is always either a translated or semi-supervised model. Table TABREF18 also shows that, in general, our feed-forward network obtained the best",
      "Abstract\nThe present study describes our submission to SemEval 2018 Task 1: Affect in Tweets. Our Spanish-only approach aimed to demonstrate that it is beneficial to automatically generate additional training data by (i) translating training data from other languages and (ii) applying a semi-supervised learning method.",
      "ining: regular (r), translated (t) and semi-supervised (s). In Tables TABREF17 and TABREF18 , the letter behind each model (e.g. SVM-r, LSTM-r) corresponds to the type of training used. Comparing the regular and translated columns for the three algorithms, it shows that in 22 out of 30 cases, using translated instances as extra training data resulted in an improvement."
    ],
    "answer": [
      "Semi-supervised learning."
    ]
  },
  {
    "title": "Conflict as an Inverse of Attention in Sequence Relationship",
    "evidence": [
      "ts like the inverse of attention and, empirically, we show that how conflict and attention together can improve the performance. Future research work should be based on alternative design of conflict mechanism using other difference operators other than element wise difference which we use.",
      "the limits of attention especially in cases where two sequences have a contradicting relationship based on the task it performs. To alleviate this problem and further improve the performance, we propose a conflict mechanism that tries to capture how two sequences repel each other.",
      "tion to it and propose another technique called conflict that looks for contrasting relationship between words in two sequences. We empirically verify that our proposed conflict mechanism combined with attention can outperform the performance of attention working solely. Related Work Bahdanau et al. BIBREF2 introduced attention first in neural machine translation.",
      "m and further improve the performance, we propose a conflict mechanism that tries to capture how two sequences repel each other. This acts like the inverse of attention and, empirically, we show that how conflict and attention together can improve the performance.",
      "r matching word representations, it operates under the assumption that there is always a match to be found inside the sequences. We provide a theoretical limitation to it and propose another technique called conflict that looks for contrasting relationship between words in two sequences.",
      "s. It does not very well adapt to cases when there is no similarity between two sequences or if the relationship is contrastive. We propose an Conflict model which is very similar to how attention works but which emphasizes mostly on how well two sequences repel each other and finally empirically show how this method in conjunction with attention can boost the overall performance.",
      "our model with attention and conflict combined does better on cases where pairs are non-duplicate and has very small difference. We have observed that the conflict model is very sensitive to even minor differences and compensates in such cases where attention poses high bias towards similarities already there in the sequences. Sequence 1: What are the best ways to learn French ?",
      "tention and attention combined with conflict respectively. Figure 4 and 5 shows these curves for Task 1 and Task 2 respectively. The curves are smoothed using moving average having an window size of 8. We notice that the conflict model has much steeper slope and converges to a much better minima in both the tasks.",
      "dow size of 8. We notice that the conflict model has much steeper slope and converges to a much better minima in both the tasks. It can also be noticed that in the training procedure for the model which has both attention and conflict, the updates are much smoother. Qualitative Comparison We also show qualitative",
      "results\nin Table 1 and Table 2. We observe that model with both attention and conflict combined gives the best",
      "results\nwhere we can observe that our model with attention and conflict combined does better on cases where pairs are non-duplicate and has very small difference.",
      "-conflict model in order to ensure that they are not due to randomness in weight initialization or simply additional parameters. We particularly focused on the examples which were incorrectly marked in attention model but correctly in attention-conflict model.",
      "ntations. We achieve that by visualizing the heatmap of the weight matrix INLINEFORM0 for both attention and conflict from eqns. (3) and (8). While attention successfully learns the alignments, conflict matrix also shows that our approach models the contradicting associations like \"animal\" and \"lake\" or \"australia\" and \"world\"."
    ],
    "answer": [
      "The tasks they test their conflict method on are Task 1 and Task 2."
    ]
  },
  {
    "title": "A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification",
    "evidence": [
      "s of approaches for question classification according to Banerjee et al in BIBREF13 - by rules and by machine learning approach. Rule based approaches use some hard coded grammar rules to map the question to an appropriate answer type BIBREF14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in BIBREF16 and BIBREF0.",
      "n Classifications Researches on question classification, question taxonomies and QA system have been undertaken in recent years. There are two types of approaches for question classification according to Banerjee et al in BIBREF13 - by rules and by machine learning approach.",
      "Conclusion\nBy implementing different machine learning based classifiers on our Bengali question corpus, we perform a comparative analysis among them. The question classification impacts the QA system.",
      "14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in BIBREF16 and BIBREF0. Many classifiers have been used in machine learning for QC such as Support Vector Machine (SVM) BIBREF16 BIBREF17, Support Vector Machines and Maximum Entropy Model BIBREF18, Naive Bayes (NB), Kernel Naive Bayes (KNB), Decision Tree (DT) and Rule Induction (RI) BIBREF13.",
      "MLP takes advantage of multiple hidden layers in order to take non-linearly separable samples in a linearly separable condition. SVM accomplishes this same feat by taking the samples to a higher dimensional hyperplane where the samples are linearly separable. Gradient Boosting Classifier (GBC) and Random Forest (RF) both utilize a set of decision trees and achieve similar",
      "lgorithm has a bad reputation of not working well in high dimensional data BIBREF28. MLP and SVM have shown similar performance. MLP takes advantage of multiple hidden layers in order to take non-linearly separable samples in a linearly separable condition.",
      "r prediction about the input and the computation of the MLP is conducted in the hidden layers. In our system, we use 100 layers. For weight optimization, we use Limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS) optimization algorithm.",
      "sh language such as IBM Watson, Wolfram Alpha. Bengali speakers often fall in difficulty while communicating in English BIBREF1. In this research, we briefly discuss the steps of QA system and compare the performance of seven machine learning based classifiers (Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)) in classifying Bengali questions to classes based on their anticipated answers.",
      "Results\nand",
      " use 100 layers. For weight optimization, we use Limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS) optimization algorithm. Implementation of the System ::: Classification Algorithms ::: Support Vector Machine (SVM) SVM gives an optimal hyper-plane and it maximizes the margin between classes.",
      "ions is an additional challenge. Different difficulties in building a QA System are mentioned in the literature BIBREF2 BIBREF3. The first work on a machine learning based approach towards Bengali question classification is presented in BIBREF0 that employ the Stochastic Gradient Descent (SGD).",
      "results\nof different classifiers in a bar chart with and without eliminating stop words from the questions.",
      "Proposed Method\nology We use different types of classifiers for QA type classification. We separate our methodology into two sections similar to BIBREF0 - one is training section and another is validation section shown in Figure FIGREF5."
    ],
    "answer": [
      "Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)"
    ]
  },
  {
    "title": "Extractive Summarization of Long Documents by Combining Global and Local Context",
    "evidence": [
      "documents. Rather surprisingly, this is not the case for the global context. Adding a representation of the whole document (i.e. global context) never significantly improves performance. In essence, it seems that all the benefits of our model come exclusively from modeling the local context, even for the longest documents.",
      "). From these tables, we can see that on both datasets the performance significantly improves when local topic information (i.e. local context) is added. And the improvement is even greater when we only consider long documents. Rather surprisingly, this is not the case for the global context. Adding a representation of the whole document (i.e.",
      "hes only by a narrow margin on both datasets, the benefit of our method become much stronger as we apply it to longer documents. purpleFurthermore, in an ablation study to assess the relative contributions of the global and the local model we found that, rather surprisingly, the benefits of our model seem to come exclusively from modeling the local context, even for the longest documents.",
      " for long documents, incorporating both the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers, Pubmed and arXiv, where it outperforms previous work, both extractive and",
      "Introduction\n, etc., depending on which section the sentence appears in. In contrast, in order to exploit section information, in this paper we propose to capture a distributed representation of both the global (the whole document) and the local context (e.g., the section/topic) when deciding if a sentence should be included in the summary Our main contributions are as follows: (i) In order to capture the local context, we are the first to apply LSTM-minus to text summarization.",
      "f local and global contextual information does help to identify the most important sentences (more on this in the next section). Interestingly, just the Baseline model already achieves a slightly better performance than previous works; possibly because the auto-regressive approach used in those models is even more detrimental for long documents.",
      "t, their hierarchical method has a large number of parameters and it is therefore slow to train and likely prone to overfitting. Secondly, it does not take the global context of the whole document into account, which may arguably be critical in extractive methods, when deciding on the salience of a sentence (or even a word).",
      "1 , and INLINEFORM2 is the representation of sentence INLINEFORM3 with topic segment information and global context information. Attentive context As local context and global context are all contextual information of the given sentence, we use an attention mechanism to decide the weight of each context vector, represented as INLINEFORM0 where the INLINEFORM0 is the weighted context vector of each sentence INLINEFORM1 , and assume sentence INLINEFORM2 is in topic INLINEFORM3 .",
      " additional experiment, in which we consider documents with increasing length, it becomes more competitive for longer documents. purpleWe also ran an ablation study to assess the relative contribution of the global and local components of our approach. Rather surprisingly, it appears that the benefits of our model come only from modeling the local context.",
      "Conclusion\ns and Future Work purpleIn this paper, we propose a novel extractive summarization model especially designed for long documents, by incorporating the local context within each topic, along with the global context of the whole document.",
      "ation. In this way, we can represent the contextual information within each topic segment for all the sentences in that segment. Decoder Once we have obtained a representation for the sentence, for its topic segment (i.e., local context) and for the document (i.e., global context), these three factors are combined to make a final prediction INLINEFORM0 on whether the sentence should be included in the summary.",
      "s (both with attentive context and concatenation decoder) have better performances on all three ROUGE scores, as well as METEOR. In particular, the improvements over the Baseline model show that a combination of local and global contextual information does help to identify the most important sentences (more on this in the next section).",
      "e, it seems that all the benefits of our model come exclusively from modeling the local context, even for the longest documents. Further investigation of this finding is left as future work."
    ],
    "answer": [
      "The global context refers to the representation of the whole document, while the local context refers to the representation of a specific topic or section within the document."
    ]
  },
  {
    "title": "Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets",
    "evidence": [
      "e task of entity-level sentiment analysis, a 3-scale rating of \"negative,\" \"neutral,\" and \"positive\" was used by the annotators. BIBREF2 proposed a decision tree approach for computing the number of crowdworkers who should analyze a tweet based on the difficulty of the task.",
      "Results\nand",
      "ed to be more difficult to annotate, and hence are allocated more crowdworkers to work on. We conducted two sets of experiments. In the first set, we used BIBREF23, BIBREF17, and BIBREF18, for entity-level sentiment analysis; in the second set, BIBREF17, BIBREF19, BIBREF24, BIBREF25, and BIBREF26, BIBREF18 for named-entity recognition.",
      "th) reports the sentiment towards selected entities in a sentence, based on five levels of relaxation and five levels of stress. Among commercial NLP toolkits (e.g., BIBREF14, BIBREF15, BIBREF16), we selected BIBREF17 and BIBREF18 for our experiments, which, to the best of our knowledge, are the only publicly accessible commercial APIs for the task of entity-level sentiment analysis that is agnostic to the text domain.",
      "ton, and Ted Cruz, provided by crowdworkers, and by experts in political communication, whose labels are considered groundtruth. The crowdworkers were located in the US and hired on the BIBREF22 platform. For the task of entity-level sentiment analysis, a 3-scale rating of \"negative,\" \"neutral,\" and \"positive\" was used by the annotators.",
      "results\nof a comparison of the accuracy of crowdworkers and seven NaturalLanguage Processing (NLP) toolkits in solving two important NLP tasks, named-entity recognition (NER) and entity-level sentiment(ELS) analysis.",
      "timent analysis; in the second set, BIBREF17, BIBREF19, BIBREF24, BIBREF25, and BIBREF26, BIBREF18 for named-entity recognition. In the experiments that we conducted with TwitterNLP for named-entity recognition, we worked with the default values of the model.",
      "results\nof TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, and Stanford NLP NER BIBREF21.",
      "Abstract\nWe report",
      "ional Random Fields BIBREF9. Recent works BIBREF10, BIBREF11 have used recurrent neural networks with attention modules for NER. Sentiment detection tools like SentiStrength BIBREF12 and TensiStrength BIBREF13 are rule-based tools, relying on various dictionaries of emoticons, slangs, idioms, and ironic phrases, and set of rules that can detect the sentiment of a sentence overall or a targeted sentiment.",
      " & Engineering at Boston University (to L.G.) and a Google Faculty Research Award (to M.B. and L.G.) is gratefully acknowledged. Additionally, we would like to thank Daniel Khashabi for his help in running the CogComp-NLP Python API and Mike Thelwal for his help with TensiStrength.",
      "results\nfor our two tasks in terms of the correct classification rate (CCR). For sentiment analysis, we have a three-class problem (positive, negative, and neutral), where the classes are mutually exclusive.",
      "he only publicly accessible commercial APIs for the task of entity-level sentiment analysis that is agnostic to the text domain. We also report"
    ],
    "answer": [
      "The measures used for evaluation are a 3-scale rating of \"negative,\" \"neutral,\" and \"positive\" for entity-level sentiment analysis, and correct classification rate (CCR) for sentiment analysis."
    ]
  },
  {
    "title": "Creative GANs for generating poems, lyrics, and metaphors",
    "evidence": [
      "baseline (a language model equivalent to our pre-trained generator) and a GumbelGAN model BIBREF14 across all proposed datasets. We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres.",
      "results\non computational metrics, hyperparameters and training configurations for our models. Table TABREF4 shows the",
      "taset and Table TABREF5 for hyperparameters of each encoder. We pick these values after experimentation with our validation set. Training and output generation code can be found online. Evaluation and",
      "nberg dataset BIBREF24 for 20 epochs and then fine-tune BIBREF19 them to our target datasets with a language modeling objective. The discriminator's encoder is initialized to the same weights as our fine-tuned language model. Once we have our fine-tuned encoders for each target dataset, we train in an adversarial manner.",
      " evaluated models, Table TABREF5 shows hyperparameters for each encoding method and Table TABREF6 shows our training parameters. In Table TABREF6, the values for Gutenberg dataset in columns, GumbelGAN and Creative-GAN are empty as we only pretrain our LMs with the Gutenberg dataset",
      "ethods, instead of $argmax$ on the log-likelihood probabilities, as sampling has shown to produce better output quality BIBREF4. Please refer to Supplementary Section Table TABREF6 for training parameters of each dataset and Table TABREF5 for hyperparameters of each encoder. We pick these values after experimentation with our validation set.",
      "ial for interesting variation during the generation phase. We use the same pre-processing as in earlier work BIBREF19, BIBREF23. We reserve 10% of our data for test set and another 10% for our validation set. We first pre-train our generator on the Gutenberg dataset BIBREF24 for 20 epochs and then fine-tune BIBREF19 them to our target datasets with a language modeling objective.",
      " model non-differentiable hence, we update parameters for the generator model with policy gradients as described in Yu BIBREF15. We utilize AWD-LSTM BIBREF20 and TransformerXL BIBREF21 based language models. For model hyperparameters please to refer to Supplementary Section Table TABREF5.",
      "eness are encouraged, MLE falls short. Methods optimized for MLE lead to outputs that can be generic, repetitive and incoherent. In this work, we use a Generative Adversarial Network framework to alleviate this problem.",
      "ve text. We show this by evaluating our model on three very different creative datasets containing poetry, metaphors and lyrics. Previous work on handling the shortcomings of MLE include length-normalizing sentence probability BIBREF5, future cost estimation BIBREF6, diversity-boosting objective function BIBREF7, BIBREF1 or penalizing repeating tokens BIBREF8.",
      "et on the evaluated models in the Supplementary Section, Table TABREF4 Our model shows improvements over baseline and GumbelGAN. Common computational methods like BLEU BIBREF27 and perplexity are at best a heuristic and not strong indicators of good performance in text generation models BIBREF28.",
      " TransformerXL BIBREF21 based language models. For model hyperparameters please to refer to Supplementary Section Table TABREF5. We use Adam optimizer BIBREF22 with $\\beta 1= 0.7$ and $\\beta 2= 0.8$ similar to BIBREF19 and use a batch size of 50. Other practices for LM training were the same as BIBREF21 and BIBREF20 for Transformer-XL and AWD-LSTM respectively.",
      "on-differentiable sample from a categorical distribution with a differentiable sample to propagate stronger gradients. Li et al. BIBREF1 use a discriminator for a diversity promoting objective. Yu et al. BIBREF15 use SeqGAN to generate poetry and comment on the performance of SeqGAN over MLE in human evaluations, encouraging our study of GANs for creative text generation."
    ],
    "answer": [
      "The datasets used are:\n1. A corpus of 740 classical and contemporary English poems\n2. A corpus of 14950 metaphor sentences retrieved from a metaphor database website\n3. A corpus of 1500 song lyrics ranging across genres\n4. The Gutenberg dataset"
    ]
  },
  {
    "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text",
    "evidence": [
      "the baselines. We also implement several variations to reveal the effects of features: authors, likers, comment, and commenters. In the",
      "results\nof UTCNN, baselines as we implemented on the FBFans datset and related work on the CreateDebate dataset. We do not adopt oversampling on these models because the CreateDebate dataset is almost balanced.",
      "results\nof UTCNN and the baselines on the FBFans dataset. Here Majority yields good performance on Neu since FBFans is highly biased to the neutral class.",
      "results\nwith related work BIBREF7 , BIBREF9 , BIBREF12 . Baselines We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0 , where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings INLINEFORM1 and INLINEFORM2 for each user; 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset; 8) UTCNN without comments, in which the model predicts the stance label given only user and topic information.",
      "Results\non CreateDebate Dataset Table TABREF24 shows the",
      "Results\non FBFans Dataset In Table TABREF22 we show the",
      "ngs or user embeddings; as replies in this dataset are viewed as posts, the setting without comment embeddings is not available. Table TABREF24 shows the same findings as Table TABREF22 : the 21% improvement in accuracy demonstrates that user information is the most vital. This finding also supports the",
      " dataset, which itself constitutes four topics, the topic labels for posts were used directly without additionally applying LDA. For the FBFans data we report class-based f-scores as well as the macro-average f-score ( INLINEFORM0 ) shown in equation EQREF19 . DISPLAYFORM0 where INLINEFORM0 and INLINEFORM1 are the average precision and recall of the three class.",
      "stic soft logic (PSL) BIBREF23 by considering text features and reply links between authors and posts as in Hasan and Ng's work. Table TABREF24 reports the result of their best AD setting, which represents the full joint stance/disagreement collective model on posts and is hence more relevant to UTCNN.",
      "esentation are fed to a fully connected network, and softmax is applied to yield the final stance label prediction for the post. Experiment We start with the experimental dataset and then describe the training process as well as the implementation of the baselines. We also implement several variations to reveal the effects of features: authors, likers, comment, and commenters.",
      " . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation",
      "etermine the latent topics with which to build topic embeddings, as there is only one general known topic: nuclear power plants. We learned 100 latent topics and assigned the top three topics for each post. For the CreateDebate dataset, which itself constitutes four topics, the topic labels for posts were used directly without additionally applying LDA.",
      "we compare our model with related work. Dataset We tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset."
    ],
    "answer": [
      "1) SVM with unigram, bigram, and trigram features\n2) SVM with average word embedding\n3) SVM with average transformed word embeddings\n4) CNN\n5) RCNN\n6) UTCNN without user information\n7) UTCNN without the LDA model\n8) UTCNN without comments"
    ]
  },
  {
    "title": "Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity",
    "evidence": [
      "urther include additional languages and inspire other researchers to contribute to the effort over the lifetime of this project. The work on data collection can be divided into two crucial phases: 1) a translation phase where the extended English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set.",
      "AA). Both of these use Spearman's correlation ($\\rho $) between annotators scores, the only difference is how they are averaged. They are computed as follows: where $\\rho (s_i,s_j)$ is the Spearman's correlation between annotators $i$ and $j$'s scores ($s_i$,$s_j$) for all pairs in the dataset, and $N$ is the number of annotators.",
      "me datasets were obtained by directly translating the English SimLex-999 in its entirety BIBREF25, BIBREF16 or in part BIBREF28. Other datasets were created from scratch BIBREF26 and yet others sampled English concept pairs differently from SimLex-999 and then translated and reannotated them in target languages BIBREF27.",
      "erogeneous construction procedures with different guidelines for translation and annotation, as well as different rating scales. For instance, some datasets were obtained by directly translating the English SimLex-999 in its entirety BIBREF25, BIBREF16 or in part BIBREF28.",
      " languages (e.g., Mandarin, Russian, and French) as well as several low-resource ones (e.g., Kiswahili, Welsh, and Yue Chinese). We demonstrate that our proposed dataset creation procedure yields data with high inter-annotator agreement rates (e.g., the average mean inter-annotator agreement for Welsh is 0.742).",
      "rget languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set. Detailed guidelines for both phases are available online at: https://multisimlex.com.",
      "annotator must score the entire set of 1,888 pairs in the dataset. The pairs must not be shared between different annotators. 3. Annotators are able to break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries, thesauri, WordNet) if required. 4.",
      "sampled English concept pairs differently from SimLex-999 and then translated and reannotated them in target languages BIBREF27. This heterogeneity makes these datasets incomparable and precludes systematic cross-linguistic analyses.",
      "ebsite available to facilitate easy creation, gathering, dissemination, and use of annotated datasets: https://multisimlex.com/. This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). Thierry Poibeau is partly supported by a PRAIRIE 3IA Institute fellowship (\"Investissements d'avenir\" program, reference ANR-19-P3IA-0001).",
      "nd precise annotation guidelines. In choosing the word pairs and constructing eng Multi-SimLex, we adhere to these requirements. Moreover, we follow good practices established by the research on related resources. In particular, since the",
      " process. The selection criteria for the annotators required that all annotators must be native speakers of the target language. Preference to annotators with university education was given, but not required. Annotators were asked to complete a spreadsheet containing the translated pairs of words, as well as the part-of-speech, and a column to enter the score.",
      "s, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language dataset is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels.",
      "elation between annotators $i$ and $j$'s scores ($s_i$,$s_j$) for all pairs in the dataset, and $N$ is the number of annotators. APIAA has been used widely as the standard measure for inter-annotator agreement, including in the original SimLex paper BIBREF14. It simply averages the pairwise Spearman's correlation between all annotators."
    ],
    "answer": [
      "The datasets were annotated by human raters who scored each pair in the translated set as well as the English set, following detailed guidelines available online at https://multisimlex.com."
    ]
  },
  {
    "title": "Multi-attention Recurrent Network for Human Communication Comprehension",
    "evidence": [
      " (LSTHM) is an extension of the Long-short Term Memory (LSTM) by reformulating the memory component to carry hybrid information. LSTHM is intrinsically designed for multimodal setups and each modality is assigned a unique LSTHM. LSTHM has a hybrid memory that stores view-specific dynamics of its assigned modality and cross-view dynamics related to its assigned modality.",
      "ion Block (MAB) and storing them in the hybrid memory of a recurrent component called the Long-short Term Hybrid Memory (LSTHM). We perform extensive comparisons on six publicly available datasets for multimodal sentiment analysis, speaker trait recognition and emotion recognition. MARN shows state-of-the-art performance on all the datasets.",
      " addition to storing view-specific dynamics, is also able to store the cross-view dynamics that are important for that modality. This allows the memory to function in a hybrid manner. The Long-short Term Hybrid Memory is formulated in Algorithm 1. Given a set of $M$ modalities in the domain of the data, subsequently $M$ LSTHMs are built in the MARN pipeline.",
      " Multi-attention Recurrent Network (MARN). MARN has two key components: Long-short Term Hybrid Memory and Multi-attention Block. Long-short Term Hybrid Memory (LSTHM) is an extension of the Long-short Term Memory (LSTM) by reformulating the memory component to carry hybrid information.",
      "rid Memory. We then proceed to outline the Multi-attention Block and describe how the two components are integrated in the MARN. Long-short Term Hybrid Memory Long-short Term Memory (LSTM) networks have been among the most successful models in learning from sequential data BIBREF21 .",
      "s for both view-specific and cross-view dynamics in the network architecture and continuously models both dynamics through time. In MARN, view-specific dynamics within each modality are modeled using a Long-short Term Hybrid Memory (LSTHM) assigned to that modality. The hybrid memory allows each modality's LSTHM to store important cross-view dynamics related to that modality.",
      "nd learns a neural cross-view dynamics code for LSTHMs to update their hybrid memories. Figure 1 shows the overview of the MARN. MARN is differentiable end-to-end which allows the model to be learned efficiently using gradient decent approaches. In the next subsection, we first outline the Long-short Term Hybrid Memory.",
      "s continuously through time. View-specific dynamics are modeled using a Long-short Term Hybrid Memory (LSTHM) for each modality. Various cross-view dynamics are identified at each time-step using the Multi-attention Block (MAB) which outputs a multimodal neural code for the hybrid memory of LSTHM. MARN achieves state-of-the-art",
      "ry Long-short Term Memory (LSTM) networks have been among the most successful models in learning from sequential data BIBREF21 . The most important component of the LSTM is a memory which stores a representation of its input through time.",
      "o that modality. The hybrid memory allows each modality's LSTHM to store important cross-view dynamics related to that modality. Cross-view dynamics are discovered at each recurrence time-step using a specific neural component called the Multi-attention Block (MAB). The MAB is capable of simultaneously finding multiple cross-view dynamics in each recurrence timestep.",
      "per we modeled multimodal human communication using a novel neural approach called the Multi-attention Recurrent Network (MARN). Our approach is designed to model both view-specific dynamics as well as cross-view dynamics continuously through time. View-specific dynamics are modeled using a Long-short Term Hybrid Memory (LSTHM) for each modality.",
      "e present a novel neural architecture for understanding human communication called the Multi-attention Recurrent Network (MARN). The main strength of our model comes from discovering interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory of a recurrent component called the Long-short Term Hybrid Memory (LSTHM).",
      "5 is the tangent hyperbolic activation function. $M$6 denotes vector concatenation and $M$7 denotes element-wise multiplication. Similar to the LSTM, $M$8 is the input gate, $M$9 is the forget gate, and $m \\in M$0 is the output gate. $m \\in M$1 is the proposed update to the hybrid memory $m \\in M$2 at time $m \\in M$3 . $m \\in M$4 is the time distributed output of each modality."
    ],
    "answer": [
      "The main difference between Long-short Term Hybrid Memory (LSTHM) and Long-short Term Memory (LSTM) is that LSTHM is an extension of LSTM that reformulates the memory component to carry hybrid information, allowing it to store both view-specific and cross-view dynamics, whereas LSTM only stores view-specific dynamics."
    ]
  },
  {
    "title": "Construction of a Japanese Word Similarity Dataset",
    "evidence": [
      "exist. In addition, most of these datasets comprise high-frequency nouns so that they tend not to include other parts of speech. Hence, previous data fail to evaluate word representations of other parts of speech, including content words such as verbs and adjectives.",
      "ned representations achieve the additive property. However, such resources other than for English (e.g., Japanese) seldom exist. In addition, most of these datasets comprise high-frequency nouns so that they tend not to include other parts of speech.",
      ". The difference in the similarity of word pairs originates from each annotator's mind, resulting in different scales of a word. Thus, we propose to use an example-based approach (Table TABREF9 ) to control the variance of the similarity ratings. We remove the context of word when we extracted the word.",
      "o have the largest variance in the datasets shown in Table TABREF17 because it gave the context to annotators during annotation. At the beginning, we thought the context would serve to remove the ambiguity and clarify the meaning of word; however after looking into the dataset, we determined that the construction procedure used several extraordinary annotators.",
      " English dataset to Japanese. We hope that a Japanese database will facilitate research in Japanese distributed representations. Language Resource References lrec lrec2018",
      "Conclusion\nIn this study, we constructed the first Japanese word similarity dataset. It contains various parts of speech and includes rare words in addition to common words.",
      "d using a word similarity task and/or a word analogy task. There are many datasets readily available for these tasks in English. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult.",
      "lish. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult. Therefore, as a first step toward evaluating distributed representations in Japanese, we constructed a Japanese word similarity dataset.",
      "ilarity rather than relatedness and includes adjective, noun and verb pairs. However, this dataset contains only frequent words. In addition, the distributed representation of words is generally learned using only word-level information. Consequently, the distributed representation for low-frequency words and unknown words cannot be learned well with conventional models.",
      "nding regarding the words. To address this problem, such an annotation should be removed before calculating the true similarity. All the datasets except for RW simply calculated the average of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator.",
      "first Japanese word similarity dataset. It contains various parts of speech and includes rare words in addition to common words. Crowdsourced annotators assigned similarity to word pairs during the word similarity task. We gave examples of similarity in the task request sent to annotators, so that we reduced the variance of each word pair.",
      "vious data fail to evaluate word representations of other parts of speech, including content words such as verbs and adjectives. To address the problem of the lack of a dataset for evaluating Japanese distributed word representations, we propose to build a Japanese dataset for the word similarity task.",
      "rd; however after looking into the dataset, we determined that the construction procedure used several extraordinary annotators. It is crucial to filter insincere annotators and provide straightforward instructions to improve the quality of the similarity annotation like we did."
    ],
    "answer": [
      "The data comes from crowdsourced annotators who assigned similarity to word pairs during the word similarity task."
    ]
  },
  {
    "title": "Assessing the Applicability of Authorship Verification Methods",
    "evidence": [
      " assessment in a more systematic manner. By this, we hope to contribute to the further development of this young research field. Based on the proposed properties, we investigate the applicability of 12 existing AV approaches on three self-compiled corpora, where each corpus involves a specific challenge. The rest of this paper is structured as follows.",
      "evant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources.",
      "n three self-compiled corpora, where each corpus involves a specific challenge. The rest of this paper is structured as follows. Section SECREF2 discusses the related work that served as an inspiration for our analysis. Section SECREF3 comprises the proposed criteria and properties to characterize AV methods.",
      "e self-compiled corpora, where the intention behind each corpus was to focus on a different aspect of the methods applicability. Our findings regarding the examined approaches can be summarized as follows: Despite of the good performance of the five AV methods GenIM, ImpGI, Unmasking, Caravel and SPATIUM, none of them can be truly considered as reliable and therefore applicable in real forensic cases.",
      " Methodology In the following, we introduce our three self-compiled corpora, where each corpus represents a different challenge. Next, we describe which authorship verification approaches we considered for the experiments and classify each AV method according to the properties introduced in Section SECREF3 .",
      "he applicability of binary-extrinsic methods in real-world cases, i. e., in real forensic settings, remains highly questionable. Methodology In the following, we introduce our three self-compiled corpora, where each corpus represents a different challenge.",
      "etween the chat lines, as the positions of specific words might play an important role regarding the individual's writing style. As a third corpus, we compiled INLINEFORM0 , which is a collection of 200 aggregated postings crawled from the Reddit platform. Overall, the postings were written by 100 Reddit users and stem from a variety of subreddits.",
      ", we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our three constructed corpora, which are listed together with their statistics in Table TABREF23 .",
      "Based on these properties, we conduct three experiments using 12 existing AV approaches, including the current state of the art. The examined methods were trained, optimized and evaluated on three self-compiled corpora, where each corpus focuses on a different aspect of applicability. Our",
      "uated. In regard to binary-extrinsic AV approaches, we explained which challenges exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus was to focus on a different aspect of the methods applicability.",
      "all corpora are balanced such that verification cases with matching (Y) and non-matching (N) authorships are evenly distributed. As a first corpus, we compiled INLINEFORM0 that represents a collection of 80 excerpts from scientific works including papers, dissertations, book chapters and technical reports, which we have chosen from the well-known Digital Bibliography & Library Project (DBLP) platform.",
      "ur corpora versions, where even for the corpus with the 250 characters long unknown documents, it achieves a c@1 score of 0.727. Surprisingly, Unmasking performs similarly well, despite of the fact that the method has been designed for longer texts i. e., book chunks of at least 500 words BIBREF13 .",
      "results\nacross the four corpora versions, where even for the corpus with the 250 characters long unknown documents, it achieves a c@1 score of 0.727."
    ],
    "answer": [
      "A self-compiled corpus is a collection of documents compiled for a specific purpose, in this case, to investigate the applicability of authorship verification (AV) methods."
    ]
  },
  {
    "title": "Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach",
    "evidence": [
      " with classes that are compound, ultra-fine-grained, changing over time, and from different aspects such as topic, emotion, etc. Existing $\\textsc {0shot-tc}$ studies have mainly the following three problems.",
      "d on MNLI/FEVER/RTE respectively, perform more robust on the three $\\textsc {0shot-tc}$ aspects (except for the RTE on emotion). Recall that they are not trained on any text classification data, and never know the domain and the aspects in the test. This clearly shows the great promise of developing textual entailment models for $\\textsc {0shot-tc}$.",
      " regarding unseen by large margins. At this stage, fine-tuning on an MNLI-based pretrained entailment model seems more powerful. Experiments ::: Label-fully-unseen evaluation Regarding this label-fully-unseen evaluation, apart from our entailment models and three unsupervised baselines “Majority”, “Word2Vec” and “ESA”, we also report the following baseline: Wikipedia-based: We train a binary classifier based on BERT on a dataset collected from Wikipedia.",
      "eport the following baseline: Wikipedia-based: We train a binary classifier based on BERT on a dataset collected from Wikipedia. Wikipedia is a corpus of general purpose, without targeting any specific $\\textsc {0shot-tc}$ task. Collecting categorized articles from Wikipedia is popular way of creating training data for text categorization, such as BIBREF13.",
      "closer to the Wikipedia-based topic categorization task; emotion and situation categorizations, however, are relatively further. Our entailment models, pretrained on MNLI/FEVER/RTE respectively, perform more robust on the three $\\textsc {0shot-tc}$ aspects (except for the RTE on emotion).",
      "$>$ FEVER $>$MNLI; on the contrary, if we fine-tune them on the label-partially-unseen case, the MNLI-based model performs best. This could be due to a possibility that, on one hand, the constructed situation entailment dataset is closer to the RTE dataset than to the MNLI dataset, so an RTE-based model can generalize well to situation data, but, on the other hand, it could also be more likely to over-fit the training set of “situation” during fine-tuning.",
      "cles studying 0Shot-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress. ::: This work benchmarks the 0Shot-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines.",
      "results\nin Table TABREF32, is that the pretrained entailment models work in this order for label-fully-unseen case: RTE $>$ FEVER $>$MNLI; on the contrary, if we fine-tune them on the label-partially-unseen case, the MNLI-based model performs best.",
      "etup, there is annotated data for partial labels as train. So, we report performance for unseen classes as well as seen classes. We compare our entailment approaches, trained separately on MNLI, FEVER and RTE, with the following baselines. Experiments ::: Label-partially-unseen evaluation ::: Baselines. Majority: the text picks the label of the largest size.",
      " by providing standardized datasets, evaluations, and a state-of-the-art entailment system. All datasets and codes are released. Related Work $\\textsc {Zero-stc}$ was first explored by the paradigm “Dataless Classification” BIBREF0.",
      "n both “emotion” and “situation” detection tasks. However, the other two entailment models (RTE and FEVER) mostly prefer “word”. iii) Since it is unrealistic to adopt only one entailment model, such as from {RTE, FEVER, MNLI}, for any open $\\textsc {0shot-tc}$ problem, an ensemble system should be preferred.",
      "om of each article. For each article, apart from its attached positive categories, we randomly sample three negative categories. Then each article and its positive/negative categories act as training pairs for the binary classifier.",
      "e. We can definitely create more natural hypotheses through crowd-sourcing, such as “food” into “the people there are starving”. Here we just set the baseline examples by automatic approaches, more explorations are left as future work, and we welcome the community to contribute."
    ],
    "answer": [
      "Our entailment models, pretrained on MNLI/FEVER/RTE respectively, perform more robust on the three $\\textsc {0shot-tc}$ aspects (except for the RTE on emotion)."
    ]
  },
  {
    "title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU",
    "evidence": [
      "9.9%+ identical. The largest improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount. Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder.",
      " For the source, we use a 3-layer 512-dim bidi GRU in all models (S1)-(S6). Model (S1) and (S2) are one and two layer baselines. Model (S4), which uses 7 intermediate FC layers, has similar decoding cost to (S2) while doubling the improvement over (S1) to 1.2 BLEU. We see minimal benefit from using a GRU on the top layer (S5) or using more FC layers (S6).",
      " vocab and perform standard unk-replacement BIBREF1 on out-of-vocabulary words. Training is performed using an in-house toolkit. Baseline Decoder Our baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations: Decoder Speed Improvements This section describes a number of speedups that can be made to a CPU-based attentional sequence-to-sequence beam decoder.",
      ", but all speedups contribute a significant amount. Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder. Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",
      "NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep.",
      "BIBREF2 , decodes at 28 words/sec on one CPU core, using all of the speedups described in Section \"Decoder Speed Improvements\" . BIBREF12 has a similar computational cost to (S7), but we were not able to replicate those",
      "ot presented here, we have found these improvement from decoder speedups and RNN+FC to be consistent across many language pairs. All together, we were able to achieve a BLEU score of 38.3 while decoding at 100 words/sec on a single CPU core.",
      "dups. We have found that the benefit of using RNN+FC layers on the source is minimal, so we only perform ablation on the target. For the source, we use a 3-layer 512-dim bidi GRU in all models (S1)-(S6). Model (S1) and (S2) are one and two layer baselines.",
      "t and logistical constraints such as batch processing. Efficient CPU decoders can also be used for on-device mobile translation. We focus on single-threaded decoding and single-sentence processing, since multiple threads can be used to reduce latency but not total throughput.",
      "tational improvements. In this work, we consider a production scenario which requires low-latency, high-throughput NMT decoding. We focus on CPU-based decoders, since GPU/FPGA/ASIC-based decoders require specialized hardware deployment and logistical constraints such as batch processing. Efficient CPU decoders can also be used for on-device mobile translation.",
      " which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost.",
      "p. This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost. By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on WMT English-French NewsTest2014, while decoding at 100 words/sec on single-threaded CPU.",
      " source, and a 1-layer 1024-dimensional attentional GRU for the target. Each sentence is decoded independently with a beam of 6. Since these speedups are all mathematical identities excluding quantization noise, all outputs achieve 36.2 BLEU and are 99.9%+ identical. The largest improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount."
    ],
    "answer": [
      "The baseline decoder used is a standard beam search decoder with several performance optimizations."
    ]
  },
  {
    "title": "A Stable Variational Autoencoder for Text Modelling",
    "evidence": [
      "hitectures. We evaluate our model against several strong baselines which apply VAE for text modelling BIBREF0, BIBREF7, BIBREF5. We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset BIBREF9 and the end-to-end (E2E) text generation dataset BIBREF10. Experimental",
      "on corpus BIBREF10, which have been used in a number of previous works for text generation BIBREF0, BIBREF5, BIBREF11, BIBREF12. PTB consists of more than 40,000 sentences from Wall Street Journal articles whereas the E2E dataset contains over 50,000 sentences of restaurant reviews. The statistics of these two datasets are summarised in Table TABREF11.",
      "the sentences generated by vMF-VAE contain repeated words in quite a few cases, such as `city city area' and `blue spice spice'. In addition, vMF-VAE also tends to generate unnecessary or unrelated words at the end of sentences, making the generated sentences ungrammatical.",
      " In other words, the decoder needs to predict the entire sequence with only the help of the given latent variable $\\mathbf {z}$. In this way, a high-quality representation",
      "L loss as commonly the case in existing works BIBREF0, BIBREF8. The weight between these two terms of our model is simply $1:1$. Experimental Setup ::: Datasets We evaluate our model on two public datasets, namely, Penn Treebank (PTB) BIBREF9 and the end-to-end (E2E) text generation corpus BIBREF10, which have been used in a number of previous works for text generation BIBREF0, BIBREF5, BIBREF11, BIBREF12.",
      "present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. Compared to existing VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality.",
      "abstract\ning the information of the input sentence is much needed for the decoder, and hence enforcing $\\mathbf {z}$ to learn the required information. Overall performance. Table TABREF13 shows the language modelling",
      "VAE also tends to generate unnecessary or unrelated words at the end of sentences, making the generated sentences ungrammatical. The sentences reconstructed by our model, in contrast, are more grammatical and more similar to the corresponding ground truth sentences than vMF-VAE.",
      "cted by vMF-VAE (i.e., the best baseline) and our model in the inputless setting using sentences from the E2E test set as input. As shown in Table TABREF15, the sentences generated by vMF-VAE contain repeated words in quite a few cases, such as `city city area' and `blue spice spice'.",
      "r KL loss vanishing), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling (Bowman et al., 2016).",
      " the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function. Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss BIBREF0, BIBREF8, or resort to designing more sophisticated model structures BIBREF7, BIBREF5, BIBREF6.",
      "ve tasks. Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling (Bowman et al., 2016). In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse.",
      " VAEs to text modelling are still far less successful compared to its application to image and speech BIBREF2, BIBREF3, BIBREF4. When applying VAEs for text modelling, recurrent neural networks (RNNs) are commonly used as the architecture for both encoder and decoder BIBREF0, BIBREF5, BIBREF6."
    ],
    "answer": [
      "Our model evaluates generated text quality based on two public benchmark datasets, namely, the Penn Treebank dataset and the end-to-end (E2E) text generation dataset."
    ]
  },
  {
    "title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances",
    "evidence": [
      "r summarization of sequence at level-1, and tokens are appended to the shorter input sequence before level-2 for joint learning. Note that in this case, using Joint-1 model (jointly training annotated slots & utterance-level intents) for the second level of the hierarchy would not make much sense (without intent keywords).",
      "(i.e., not the ones that are predicted as ‘None/O’) are fed as input to the joint sequence-to-sequence models (described above). See Fig. FIGREF34 (b) for sample network architecture. After the filtering or summarization of sequence at level-1, and tokens are appended to the shorter input sequence before level-2 for joint learning.",
      "ated slots & utterance-level intents) for the second level of the hierarchy would not make much sense (without intent keywords). Hence, Joint-2 model is used for the second level as described below: Hierarchical & Joint-2: Level-1 (Seq2seq Bi-LSTM for intent keywords & slots extraction) + Level-2 (Joint-2 Seq2seq models with slots & intent keywords & utterance-level intents) Table TABREF42 summarizes the",
      "leveraging the backward LSTM output at first time step (i.e., prediction at ) would potentially help for joint seq2seq learning. An overall network architecture can be found in Fig. FIGREF30 for our joint models. We will report the experimental",
      "results\nof various approaches we investigated for utterance-level intent understanding. We achieved 0.91 overall F1-score with our best-performing model, namely Hierarchical & Joint-2. All model",
      "results\non two variations (with and without intent keywords) as follows: Joint-1: Seq2seq Bi-LSTM for utterance-level intent detection (jointly trained with slots) Joint-2: Seq2seq Bi-LSTM for utterance-level intent detection (jointly trained with slots & intent keywords) Proposed hierarchical models are detecting/extracting intent keywords & slots using sequence-to-sequence networks first (i.e., level-1), and then feeding only the words that are predicted as intent keywords & valid slots (i.e., not the ones that are predicted as ‘None/O’) as an input sequence to various separate sequence-to-one models (described above) to recognize final utterance-level intents (i.e., level-2).",
      "results\nwith the initial (Hybrid-0) and currently best performing (H-Joint-2) intent recognizers. With our best model (H-Joint-2), relatively problematic SetDestination and SetRoute intents’ detection performances in baseline model (Hybrid-0) jumped from 0.78 to 0.89 and 0.75 to 0.88, respectively. We compared our intent detection",
      "Context, which incorporates a context/query vector jointly learned during the training process to assist the attention BIBREF7 . All seq2one model variations we experimented with can be summarized as follows: Separate-0: Seq2one LSTM for utterance-level intents Separate-1: Seq2one Bi-LSTM for utterance-level intents Separate-2: Seq2one Bi-LSTM with Attention BIBREF6 for utterance-level intents Separate-3: Seq2one Bi-LSTM with AttentionWithContext BIBREF7 for utterance-level intents Using sequence-to-sequence networks, the approach here is jointly training annotated utterance-level intents and slots/intent keywords by adding / tokens to the beginning/end of each utterance, with utterance-level intent-type as labels of such tokens.",
      "iations of these models that perform best on our dataset to support various natural commands for interacting with the car-agent. The main differences in our proposed models can be summarized as follows: (1) Using the extracted intent keywords in addition to the slots to jointly model them with utterance-level intents (where most of the previous work BIBREF8 , BIBREF9 only join slots and utterance-level intents, ignoring the intent keywords); (2) The 2-level hierarchy we defined by word-level detection/extraction for slots and intent keywords first, and then filtering-out predicted non-slot and non-intent keywords instead of feeding them into the upper levels of the network (i.e., instead of using stacked RNNs with multiple recurrent hidden layers for the full utterance BIBREF9 , BIBREF10 , which are computationally costly for long utterances with many non-slot & non-intent-related words), and finally using only the predicted valid-slots and intent-related keywords as an input to the second level of the hierarchy; (3) Extending joint models BIBREF2 , BIBREF5 to include both beginning-of-utterance and end-of-utterance tokens to leverage Bi-LSTMs (after observing that we achieved better",
      "results\nare obtained via 10-fold cross-validation (10-fold CV) on the same dataset. For our AMIE scenarios, Table TABREF43 shows the intent-wise detection",
      "results\nwith the initial (Hybrid-0) and currently best performing (H-Joint-2) intent recognizers.",
      "h multi-turn dialogues between the passengers and AMIE using a Wizard-of-Oz scheme via a realistic scavenger hunt game activity. After exploring various recent Recurrent Neural Networks (RNN) based techniques, we introduced our own hierarchical joint models to recognize passenger intents along with relevant slots associated with the action to be performed in AV scenarios.",
      "results\nare obtained with Bi-LSTMs and GloVe embeddings (6B tokens, 400K vocabulary size, vector dimension 100). For utterance-level intent detection, we mainly experimented with 5 groups of models: (1) Hybrid: RNN + Rule-based, (2) Separate: Seq2one Bi-LSTM with Attention, (3) Joint: Seq2seq Bi-LSTM for slots/intent keywords & utterance-level intents, (4) Hierarchical & Separate, (5) Hierarchical & Joint."
    ],
    "answer": [
      "The shared information in the joint model is the predicted intent keywords and valid slots, which are fed as an input sequence to the second level of the hierarchy for recognizing final utterance-level intents."
    ]
  },
  {
    "title": "Is there Gender bias and stereotype in Portuguese Word Embeddings?",
    "evidence": [
      "in models composed of predictive methods that try to predict neighboring words with one or more context words, such as Word2Vec. Word embeddings have been used to provide meaningful representations for words in an efficient way. In BIBREF14 , several word embedding models trained in a large Portuguese corpus are evaluated.",
      " terms is still low. Final Remarks This paper presents an analysis of the presence of gender bias in Portuguese word embeddings. Even though it is a work in progress, the proposal showed promising",
      "results\nof a de-bias algorithm application in Portuguese embeddings word2vec model and a short",
      "s different expressions of the language, making it possible to analyze gender bias and stereotype in Portuguese word embeddings. The dataset used was tokenized and normalized by the authors to reduce the corpus vocabulary size, under the premise that vocabulary reduction provides more representative vectors.",
      "the use of this content can represent the insertion of different types of bias in training and may vary with the context worked. This work aims to analyze and remove gender stereotypes from word embedding in Portuguese, analogous to what was done in BIBREF3 for the English language.",
      "ents The purpose of this section is to perform different analysis concerning bias in word2vec models with Portuguese embeddings. The Continuous Bag-of-Words model used was provided by BIBREF14 (described in Section SECREF3 ). For these experiments, we use a model containing 934966 words of dimension 300 per vector representation.",
      "and remove gender stereotypes from word embedding in Portuguese, analogous to what was done in BIBREF3 for the English language. Hence, we propose to employ a public word2vec model pre-trained to analyze gender bias in the Portuguese language, quantifying biases present in the model so that it is possible to reduce the spreading of sexism of such models.",
      "related works\n. Section SECREF3 presents the Portuguese word2vec embeddings model used in this paper and Section SECREF4 proposes our method. Section SECREF5 presents experimental",
      " of the analogies generated through the model, that is, to verify the cases of association matching generated between the words. [!htb] Model Evaluation w2v_evaluate INLINEFORM0 open_model( INLINEFORM1 ) count = 0 INLINEFORM2 in INLINEFORM3 read list of tuples x = model.most_similar(positive=[`ela', male], negative=[`ele']) x = female count += 1 accuracy = count/size(profession_pairs) return accuracy Experiments The purpose of this section is to perform different analysis concerning bias in word2vec models with Portuguese embeddings.",
      "s for words in an efficient way. In BIBREF14 , several word embedding models trained in a large Portuguese corpus are evaluated. Within the Word2Vec model, two training strategies were used. In the first, namely Skip-Gram, the model is given the word and attempts to predict its neighboring words.",
      "the authors, it is possible to perceive the potential use of the method as a means of reducing bias in the use of ML algorithms. In BIBREF3 , the authors propose a method to hardly reduce bias in English word embeddings collected from Google News. Using word2vec, they performed a geometric analysis of gender direction of the bias contained in the data.",
      "4 , the proposal involves the analysis of the most similar analogies generated before and after the application of the BIBREF3 . The work is focused on the analysis of gender bias associated with professions in word embeddings. So therefore into the evaluation of the accuracy of the associations generated, aiming at achieving",
      "t\nIn this work, we propose an analysis of the presence of gender bias associated with professions in Portuguese word embeddings. The objective of this work is to study gender implications related to stereotyped professions for women and men in the context of the Portuguese language."
    ],
    "answer": [
      "Word2Vec."
    ]
  },
  {
    "title": "An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models",
    "evidence": [
      "in training neural language models. Negative examples here are incorrect words in a sentence, such as\"barks\"in\"*The dogs barks\". Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement.",
      "Introduction\nintro Despite not being exposed to explicit syntactic supervision, neural language models (LMs), such as recurrent neural networks, are able to generate fluent and natural sentences, suggesting that they induce syntactic knowledge about the language to some extent.",
      "lities of neural LMs, in particular RNN-LMs, we perform a series of experiments under a different condition from the prior work. Specifically, we extensively analyze the performance of the models that are exposed to explicit negative examples. In this work, negative examples are the sentences or tokens that are grammatically incorrect, such as (UNKREF5) above.",
      "Abstract\nWe explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as\"barks\"in\"*The dogs barks\".",
      "results\nfor RNN language models (RNN-LMs) trained only with raw text are overall negative; prior work has reported low performance on the challenging test cases BIBREF0 even with the massive size of the data and model BIBREF1, or argue the necessity of an architectural change to track the syntactic structure explicitly BIBREF2, BIBREF3.",
      " This result suggests an inherent difficulty to track a syntactic state across an object RC for sequential neural architectures. We finally provide an ablation study to understand the encoded linguistic knowledge in the models learned with the help of our method.",
      "ntences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses. Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.",
      "on task to predict the upcoming verb form (singular or plural) helps models aware of the target syntax (subject-verb agreement). Our experiments basically confirm and strengthen this argument, with even stronger learning signals from negative examples, and we argue this allows to evaluate the true capacity of the current architectures.",
      "oss all tokens and does not inform the models' behaviors on linguistically challenging structures, which are rare in the corpus. This is the main motivation to separately evaluate the models' syntactic robustness by a different task.",
      "ken-level ablation, as knowledge transfer across words appearing in similar contexts is promoted by language modeling objective. We expect that construction-level supervision would be necessary to induce robust syntactic knowledge, as perhaps different phrases, e.g., a PP and a VP, are processed differently. Do models generalize explicit supervision, or just memorize it? :::",
      "abstract\nsyntactic knowledge about the target constructions, and also that the models can generalize the knowledge acquired by negative examples to similar constructions for which negative examples are not explicitly given.",
      "ile not directly exploiting negative examples, trains an LM with additional explicit supervision signals to the evaluation task. They hypothesize that LSTMs do have enough capacity to acquire robust syntactic abilities but the learning signals given by the raw text are weak, and show that multi-task learning with a binary classification task to predict the upcoming verb form (singular or plural) helps models aware of the target syntax (subject-verb agreement).",
      "er learning signals from negative examples, and we argue this allows to evaluate the true capacity of the current architectures. In our experiments (Section exp), we show that our margin loss achieves higher syntactic performance. Another relevant work on the capacity of LSTMs is BIBREF6, which shows that by distilling from syntactic LMs BIBREF7, LSTM-LMs can be robust on syntax."
    ],
    "answer": [
      "\"Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTMs), and other sequential neural architectures.\""
    ]
  },
  {
    "title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset",
    "evidence": [
      ", a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations.",
      "to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages.",
      "el to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design.",
      ",215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken \"Wizard of Oz\" (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is \"self-dialog\" in which crowdsourced workers write the entire dialog themselves.",
      " would openly and candidly express themselves with an automated assistant that provided superior natural language understanding. Perhaps the most relevant work to consider here is the recently released MultiWOZ dataset BIBREF13, since it is similar in size, content and collection methodologies. MultiWOZ has roughly 10,000 dialogs which feature several domains and topics.",
      "ies and lower BLEU scores for our dataset compared to MultiWOZ suggesting that our dataset conversations are difficult to model. Finally, Table TABREF2 also shows that our dataset contains close to 10 times more real-world named entities than MultiWOZ and thus, could potentially serve as a realistic baseline when designing goal oriented dialog systems.",
      "mportant distinction covered is that of human-human vs. human-machine dialog data, each having its advantages and disadvantages. Many of the existing task-based datasets have been generated from deployed dialog systems such as the Let’s Go Bus Information System BIBREF15 and the various Dialog State Tracking Challenges (DSTCs) BIBREF16.",
      "ple annotation schema providing sufficient grounding for the data while making it easy for workers to apply labels consistently. Size: The total of 13,215 dialogs in this corpus is on par with similar, recently released datasets such as MultiWOZ BIBREF13.",
      "shows that our dataset has more unique words, and has almost twice the number of utterances per dialog than the MultiWOZ corpus. Finally, when trained with the Transformer BIBREF21 model, we observe significantly higher perplexities and lower BLEU scores for our dataset compared to MultiWOZ suggesting that our dataset conversations are difficult to model.",
      "l. Our Transformer BIBREF21 model uses 256 dimensions for both input embedding and hidden state, 2 layers and 4 attention heads. For both LSTMs and Transformer, we train the model with ADAM optimizer ($\\beta _{1} = 0.85$, $\\beta _{2} = 0.997$) and dropout probability set to 0.2. GPT-2: Apart from supervised seq2seq models, we also include",
      "s and assistants to stick to detailed scripts and do not restrict them to have conversations surrounding a small knowledge base. Table TABREF2 shows that our dataset has more unique words, and has almost twice the number of utterances per dialog than the MultiWOZ corpus.",
      " quality, goal-oriented dialog data is considered a major hindrance to more significant progress in this area BIBREF9, BIBREF11. To help solve the data problem we present Taskmaster-1, a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures.",
      "ontrasts. Multiple turns: The average number of utterances per dialog is about 23 which ensures context-rich language behaviors. API-based annotation: The dataset uses a simple annotation schema providing sufficient grounding for the data while making it easy for workers to apply labels consistently."
    ],
    "answer": [
      "The six domains covered in the dataset are: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks, and making restaurant reservations."
    ]
  },
  {
    "title": "Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge",
    "evidence": [
      "his limitation by learning execution models for arbitrary language, essentially using a text corpus as a kind of knowledge base. However, all prior approaches to open vocabulary semantic parsing replace a formal KB with textual information, making no use of the KB in their models.",
      "Abstract\nTraditional semantic parsers map language onto compositional, executable queries in a fixed schema. This mapping allows them to effectively leverage the information contained in large, formal knowledge bases (KBs, e.g., Freebase) to answer questions, but it is also fundamentally limiting---these semantic parsers can only assign meaning to language that falls within the KB's manually-produced schema.",
      "ns like “Who are the Democratic front-runners in the US election?”, as Freebase does not encode information about front-runners. Semantic parsers trained for Freebase fail on these kinds of questions. To overcome this limitation, recent work has",
      "ebase, they do not attempt to learn execution models for those predicates, nor can they answer questions using those predicates. Yao and Van Durme yao-2014-info-extraction-freebase-qa and Dong et al. dong-2015-freebase-qa-mccnn proposed question answering models that use similar features to those used in this work.",
      "extit {unknown}$ ; however, it would be better to replace it with $\\textit {honcho\\_N/N}$ , which was seen in the training data. Finally, although using connected entities in Freebase as additional candidates during inference is helpful, it often over- or under-generates candidates. A more tailored, per-query search process could improve performance.",
      "ability of semantic parsing techniques to more complex domains where the assumptions of traditional techniques are too limiting. We are actively exploring applying these techniques to science question answering BIBREF26 , for example, where existing KBs provide only partial coverage of the questions.",
      "ation in a knowledge base, or broad coverage over all of natural language using the information in a large corpus, but not both. In this work, we show how to combine these two approaches by incorporating KB information into open vocabulary semantic parsing models.",
      " language onto a formal query in some fixed schema, which can then be executed against a knowledge base (KB) BIBREF0 , BIBREF1 . For example, the phrase “Who is the president of the United States?” might be mapped onto the query $\\lambda (x).$ $\\textsc {/government/president\\_of}$ ( $x$ , $\\textsc {USA}$ ), which, when executed against Freebase BIBREF2 , returns $\\textsc {Barack Obama}$ .",
      " execution models for the surface logical forms directly, using a weighted combination of Freebase queries as part of the model. None of these prior works can assign meaning to language that is not directly representable in the KB schema.",
      "ing methods, giving them broad coverage, but lacking access to the large, curated KBs available to traditional semantic parsers. Prior work in semantic parsing, then, has either had direct access to the information in a knowledge base, or broad coverage over all of natural language using the information in a large corpus, but not both.",
      "proposed method\ns for open vocabulary semantic parsing overcome this limitation by learning execution models for arbitrary language, essentially using a text corpus as a kind of knowledge base.",
      "extraction system that performs a semantic parse of open-domain text, recognizing when a predicate cannot be mapped to Freebase. However, while they recognize when a predicate is not mappable to Freebase, they do not attempt to learn execution models for those predicates, nor can they answer questions using those predicates.",
      "ore to any entity pair in the KB. This allows our model to considerably improve question answering performance on rare entities. It would be computationally intractable to consider all Freebase entities as answers to queries, and so we use a simple candidate entity generation technique to consider only a small set of likely entities for a given query."
    ],
    "answer": [
      "The proposed method uses a text corpus as a kind of knowledge base to learn execution models for arbitrary language."
    ]
  },
  {
    "title": "SimplerVoice: A Key Message&Visual Description Generator System for Illiteracy",
    "evidence": [
      "s that the generated key message should be able to describe the action of a person using, or interacting with the target object. Moreover, the simple \"S-V-O\" model has been proposed to use in other studies BIBREF11 , BIBREF12 , BIBREF13 since it is able to provide adequate semantics meaning. The detail of generating the S-V-O query is provided in Section SECREF3 .",
      "tured input, the system, then, generates a query string/sequence of text which is the key message describing the object's usage. Due to low-literates' lack of reading capability, the generated text requires not only informativeness, but also simplicity, and clarity.",
      "abase of images. However, the key point is to choose the optimal display for illiteracy which is described in Section SECREF12 . The result of SimplerVoice is provided further in Section SECREF4 . Object2Text This section discusses the process of generating key message from the object's input.",
      "d further in Section SECREF4 . Object2Text This section discusses the process of generating key message from the object's input. Based on the retrieved input, we can easily obtain the object's title through searching in database, or using search engine; hence, we assume that the input of object2text is the object's title. The workflow of object2text is provided in Figure FIGREF4 .",
      "e target input in either of 3 representations: (1) object's title as text, (2) object's shape as image, or (3) other forms, e.g. object's information from scanned barcode, speech from users, etc. Based on the captured input, the system, then, generates a query string/sequence of text which is the key message describing the object's usage.",
      "Conclusion\nIn this work, we introduce SimplerVoice: a key message & visual description generator system for illiteracy.",
      "ms model has been extensively used for various tasks in text mining, and natural language processing field BIBREF14 , BIBREF15 . Here, we use the Google Books n-grams database BIBREF16 , BIBREF17 to generate a set of verbs corresponding to the input object's usage.",
      " presented to the users as the top images result from Google and Bing. We also provided the product title along with the images. SimplerVoice: We shown the generated key messages (Tab. TABREF19 ), and visual description including 2 components: photorealistics images and pictographs (Fig. FIGREF20 ) from SimplerVoice system.",
      "Abstract\nWe introduce SimplerVoice: a key message and visual description generator system to help low-literate adults navigate the information-dense world with confidence, on their own.",
      "step is to generate the S-V-O message (e.g. Table TABREF19 ), and visual description (e.g. Figure FIGREF20 ) of product's usage. Figure FIGREF22 shows an example of the result of SimplerVoice system for product \"H-E-B Bakery Cookies by the Pound\" from a grocery store: (1) the product description, (2) key messages, and (3) visual components.",
      "ge. Based on the built-in ontology tree, SimplerVoice, then, finds the object's category, the parent, and the neighboring nodes. The next step is to generate the S-V-O message (e.g. Table TABREF19 ), and visual description (e.g. Figure FIGREF20 ) of product's usage.",
      "erbs (V). We propose to use 2 methods to generate the suitable verbs for the target object: heuristics-based, and n-grams model. In detail, SimplerVoice has a set of rule-based heuristics for the objects. For instance, if the object belongs to a \"food | drink\" category, the verb is generated as \"eat | drink\".",
      "s. In this work, we propose SimplerVoice: a system that is able to generate key messages, and visual description for illiteracy. SimplerVoice could present easier-to-understand representations of complex objects to low-literate adult users, which helps them gain more confidence in navigating their own daily lives."
    ],
    "answer": [
      "The model used to generate key messages is the S-V-O (Subject-Verb-Object) model."
    ]
  },
  {
    "title": "A Parallel Corpus of Theses and Dissertations Abstracts",
    "evidence": [
      " sets, while the remaining was used for training. For the SMT experiment, we followed the instructions of Moses baseline system. For the NMT experiment, we used the Torch implementation to train a 2-layer LSTM model with 500 hidden units in both encoder and decoder, with 12 epochs.",
      "abstract\ns and other metadata. Our corpus was evaluated through SMT and NMT experiments with Moses and OpenNMT systems, presenting superior performance regarding BLEU score than Google Translate. The NMT model also presented superior",
      "results\nmay be due to two main factors: corpus size, and domain. Our corpus is fairly large for both SMT and NMT approaches, comprised of almost 1.3M sentences, which enables the development of robust models.",
      "trict writing style, with less variation than novels or speeches, for instance, favoring the development of tailored MT systems. Below, we demonstrate some sentences translated by Moses and OpenNMT compared to the suggested human translation. One can notice that in fact NMT model tend to produce more fluent",
      ") and Neural Machine Translation (NMT) models for both language directions, followed by a comparison with Google Translate (GT). Both translation models presented better BLEU scores than GT, with NMT system being the most accurate one. Sentence alignment was also manually evaluated, presenting an average of 82.30% correctly aligned sentences.",
      "r in a packaging and plastic film industry. Human translation: this fact corroborates the difficulty in modeling human behavior. OpenNMT: this fact corroborates the difficulty in modeling human behavior. Moses: this fact corroborated the difficulty in model the human behavior. GT: this fact corroborates the difficulty in modeling human behavior.",
      "tion To evaluate the usefulness of our corpus for SMT purposes, we used it to train an automatic translator with Moses BIBREF8 . We also trained an NMT model using the OpenNMT system BIBREF9 , and used the Google Translate Toolkit to produce state-of-the-art comparison",
      "airly large for both SMT and NMT approaches, comprised of almost 1.3M sentences, which enables the development of robust models. Regarding domain, GT is a generic tool not trained for a specific domain, thus it may produce lower",
      "eriments Prior to the MT experiments, sentences were randomly split in three disjoint datasets: training, development, and test. Approximately 13,000 sentences were allocated in the development and test sets, while the remaining was used for training. For the SMT experiment, we followed the instructions of Moses baseline system.",
      "for both translation directions with English and Portuguese on the development and test partitions for Moses and OpenNMT models. We also included the scores for Google Translate (GT) as a benchmark of a state-of-the-art system which is widely used.",
      "cy. Human translation: this paper presents a study of efficiency and power management in a packaging industry and plastic films. OpenNMT: this work presents a study of efficiency and electricity management in a packaging industry and plastic films. Moses: in this work presents a study of efficiency and power management in a packaging industry and plastic films.",
      " models. We also included the scores for Google Translate (GT) as a benchmark of a state-of-the-art system which is widely used. NMT model achieved better performance than the SMT one for EN INLINEFORM0 PT direction, with approximately 2.17 percentage points (pp) higher, while presenting almost the same score for PT INLINEFORM1 EN.",
      "tion), keywords in both languages, knowledge areas and subareas according to CAPES, and URL for the full-text PDF in Portuguese. An excerpt of the corpus is shown in Table TABREF13 Translation experiments Prior to the MT experiments, sentences were randomly split in three disjoint datasets: training, development, and test."
    ],
    "answer": [
      "Torch implementation to train a 2-layer LSTM model with 500 hidden units in both encoder and decoder, with 12 epochs."
    ]
  },
  {
    "title": "Data Innovation for International Development: An overview of natural language processing for qualitative data analysis",
    "evidence": [
      "s, in turn, limits the ability of timely data driven decision-making which is essential in fast evolving complex social systems. In this paper, we discuss the potential of using natural language processing to systematize analysis of qualitative data, and to inform quick decision-making in the development context.",
      " language processing (NLP) and machine learning (ML) can make feasible the speedy analysis of qualitative data on a large scale. We start with a brief description of the main approaches to NLP and how they can be augmented by human coding. We then move on to the issue of working with multiple languages and different document formats.",
      "ng it in a systematic way to inform quick decision-making poses challenges, in terms of high costs, time, or expertise required. The application of natural language processing (NLP) and machine learning (ML) can make feasible the speedy analysis of qualitative data on a large scale.",
      "language processing to systematize analysis of qualitative data, and to inform quick decision-making in the development context. We illustrate this with interview data generated in a format of micro-narratives for the UNDP Fragments of Impact project.",
      "es on texts in the form of open-ended questions, social networks can be used and their coverage and topicality can be leveraged. Natural language processing has the potential to unlock large quantities of untapped knowledge that could enhance our understanding of micro-level processes and enable us to make better context-tailored decisions.",
      "itative data, as well as its limitations, often make qualitative data the resource upon which development programs heavily rely. Both traditional interview data and social media analysis can provide rich contextual information and are essential for research, appraisal, monitoring and evaluation. These data may be difficult to process and analyze both systematically and at scale.",
      "tional system that use a similar language or that describe similar issues, like small-island states prioritizing climate change. Identifying such groups is often referred to as `dimension reduction' of data. Validation of unsupervised learning",
      "r the supervision of Prof Slava Mikhaylov (University of Essex) and research coordination of Dr Anna Hanchar (The Data Atelier). The objective of this work was to explore how to systematize the analysis of country-level qualitative data, visualize the data, and inform quick decision-making and timely experiment design. The",
      " for collecting qualitative and quantitative data. The micro-narratives were individual responses to context-tailored questions. An example of such a question is: “Share a recent example of an event that made it easier or harder to support how your family lives.” While the analysis and visualization of quantitative data was not problematic, systematic analysis and visualization of qualitative data, collected in a format of micro-narratives, would have been impossible.",
      "discussion\nabove, topic models can be used to analyze social media data.",
      "econciliation, local and rural development, value chain, female entrepreneurship and empowerment, and youth unemployment issues. The micro-narratives were collected using SenseMaker(r), a commercial tool for collecting qualitative and quantitative data. The micro-narratives were individual responses to context-tailored questions.",
      "arch, appraisal, monitoring and evaluation. These data may be difficult to process and analyze both systematically and at scale. This, in turn, limits the ability of timely data driven decision-making which is essential in fast evolving complex social systems.",
      "governments. In the next section we discuss an application of natural language processing in international development research. UNDP Fragments of Impact Initiative In 2015, the United Nations Development Programme (UNDP) Regional Hub for Europe and CIS launched a Fragments of Impact Initiative (FoI) that helped to collect qualitative (micro-narratives) and quantitative data from multiple countries."
    ],
    "answer": [
      "Natural language processing (NLP) and machine learning (ML) can make feasible the speedy analysis of qualitative data on a large scale."
    ]
  },
  {
    "title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages",
    "evidence": [
      "h, German, Spanish, and French and show that its quality is comparable to manually or semi-manually created sentiment resources. With the publication of this paper, we release UniSent lexica as well as Adapted Sentiment Pivot related codes. method.",
      "e to evaluate UniSent. In the adaptation step, we compute the shift between the vocabularies in the Bible and Wikipedia corpora. To show that our adaptation method also works well on domains like Twitter, we propose a second evaluation in which we use Adapted Sentiment Pivot to predict the sentiment of emoticons in Twitter.",
      "F17 . We use the (manually created) English sentiment lexicon (WKWSCI) in BIBREF18 as a resource to be projected over languages. For the projection step (Section SECREF1 ) we use the massively parallel Bible corpus in BIBREF8 . We then propagate the projected sentiment polarities to all words in the Wikipedia corpus.",
      "timent lexica for 1000 languages created using an English sentiment lexicon and a massively parallel corpus in the Bible domain. To the best of our knowledge, UniSent is the largest sentiment resource to date in terms of number of covered languages, including many low resource languages.",
      "REF13 , Macedonian BIBREF14 , and Spanish BIBREF15 . These lexica contain general domain words (as opposed to Twitter or Bible). As gold standard for twitter domain we use emoticon dataset and perform emoticon sentiment prediction BIBREF16 , BIBREF17 . We use the (manually created) English sentiment lexicon (WKWSCI) in BIBREF18 as a resource to be projected over languages.",
      "niSent is the largest sentiment resource to date in terms of number of covered languages, including many low resource languages. To create UniSent, we propose Adapted Sentiment Pivot, a novel method that combines annotation projection, vocabulary expansion, and unsupervised domain adaptation.",
      "d Sentiment Pivot, a novel method that combines annotation projection, vocabulary expansion, and unsupervised domain adaptation. We evaluate the quality of UniSent for Macedonian, Czech, German, Spanish, and French and show that its quality is comparable to manually or semi-manually created sentiment resources.",
      "ly parallel Bible corpus in BIBREF8 . We then propagate the projected sentiment polarities to all words in the Wikipedia corpus. We chose Wikipedia here because its domain is closest to the manually annotated sentiment lexica we use to evaluate UniSent. In the adaptation step, we compute the shift between the vocabularies in the Bible and Wikipedia corpora.",
      "t method, we created UniSent, a sentiment lexicon covering over 1000 (including many low-resource) languages in several domains. The only necessary resources to create UniSent are a sentiment lexicon in any language and a massively parallel corpus that can be small and domain specific. Our evaluation showed that the quality of UniSent is closed to manually annotated resources.",
      "created sentiment lexica in different domains to ensure its quality for the low resource languages. We do this in several steps. As the gold standard sentiment lexica, we chose manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15 . These lexica contain general domain words (as opposed to Twitter or Bible).",
      "eighting method (Section SECREF3 ) mainly models a sense shift between words which is not always equivalent to a polarity shift. In Table TABREF14 we compare the quality of UniSent with the gold standard emoticon lexicon in the Twitter domain. The",
      "ment lexicon in one language (e.g. English) as well as a massively parallel corpus. Following steps are performed on this input. Experimental Setup Our goal is to evaluate the quality of UniSent against several manually created sentiment lexica in different domains to ensure its quality for the low resource languages. We do this in several steps.",
      "Abstract\nIn this paper, we introduce UniSent a universal sentiment lexica for 1000 languages created using an English sentiment lexicon and a massively parallel corpus in the Bible domain."
    ],
    "answer": [
      "UniSent is compared with manually or semi-manually created sentiment resources."
    ]
  },
  {
    "title": "TutorialVQA: Question Answering Dataset for Tutorial Videos",
    "evidence": [
      " of this paper is structured as follows. Section SECREF3 introduces TutorialVQA dataset as a case study of our proposed problem. The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software.",
      "ur task aims to identify a span of a video segment as an answer which contains instructional details with various granularities. This work focuses on screencast tutorial videos pertaining to an image editing program. We introduce a dataset, TutorialVQA, consisting of about 6,000manually collected triples of (video, question, answer span). We also provide experimental",
      "results\nwith several baselines algorithms using the video transcripts. The",
      " Wikipedia, TriviaQA BIBREF7 constructed from trivia questions with answer evidence from Wikipedia, or those from Hermann et al. based on CNN and Daily Mail articles BIBREF8 are factoid-based, meaning the answers typically involve a single entity.",
      "Results\nTables TABREF20, TABREF21, TABREF22 show the",
      " data sources, namely paragraphs from Wikipedia and news sources, are typically straightforward since they are meant to be read. In contrast, video transcripts originate from spoken dialogue, which can verbose, unstructured, and disconnected.",
      "gly used as main data sources in many question answering problems BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF2, BIBREF5. These previous studies have mostly focused on factoid questions, each of which can be answered in a few words or phrases generated by understanding multimodal contents in a short video clip.",
      "nn et al. based on CNN and Daily Mail articles BIBREF8 are factoid-based, meaning the answers typically involve a single entity. Differing from video transcripts, the structures of these data sources, namely paragraphs from Wikipedia and news sources, are typically straightforward since they are meant to be read.",
      "he questions generated by the first task and asked them to write the questions differently while keeping the semantics the same. In this way, we expanded our question dataset. After filtering out the questions with low quality, we collected a total of 6,195 questions.",
      "ram . Each video is pre-processed to provide the transcripts and the time-stamp information for each sentence in the transcript. We then used Amazon Mechanical Turk to collect the question-answer pairs . One naive way of collecting the data is to prepare a question list and then, for each question, ask the workers to find the relevant parts in the video.",
      "sent the segment text and a question. The model first encodes the two inputs. $h^s$ is then re-weighted using attention weights. where $\\odot $ denotes the element-wise multiplication operation. The final score is computed using a one-layer feed-forward network. During training, the model requires negative samples.",
      "mn corresponds to an AMT generated question, while the second column corresponds to the video ID where the segment can be found. As can be seen in the first two rows, multiple types of questions can be answered within the same video (but different segments).",
      "results\nof TutorialVQA, a new type of dataset used to find answer spans in tutorial videos."
    ],
    "answer": [
      "The source of the triples is the TutorialVQA dataset, which includes about 6,000 manually collected triples of (video, question, answer span) from screencast tutorial videos with spoken narratives for a photo-editing software."
    ]
  },
  {
    "title": "Transductive Learning with String Kernels for Cross-Domain Text Classification",
    "evidence": [
      "ic models BIBREF3 , BIBREF4 , knowledge-based models BIBREF14 , BIBREF31 , BIBREF11 and joint optimization frameworks BIBREF12 . The transfer learning methods from the literature show promising",
      "of machine learning methods is usually lower in the cross-domain setting, due to the distribution gap between different domains. However, researchers proposed several domain adaptation techniques by using the unlabeled test data to obtain better performance BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , BIBREF7 .",
      " Algorithm description. In steps 8-17, we compute the domain-adapted string kernel matrix, as described in the previous section. In the first learning iteration (when INLINEFORM0 ), we train several classifiers to distinguish each individual class from the rest, according to the one-versus-all (OVA) scheme.",
      "results\nin various text classification tasks such as native language identification or automatic essay scoring. Moreover, classifiers based on string kernels have been found to be robust to the distribution gap between different domains.",
      "er learning framework can bring significant improvements without having to fine-tune the parameters for each individual setting. Although the framework described in this paper can be generally applied to any kernel method, we focused our work only on string kernel approaches used in text classification.",
      "can be generally applied to any kernel method, we focused our work only on string kernel approaches used in text classification. In future work, we aim to combine the proposed transductive transfer learning framework with different kinds of kernels and classifiers, and employ it for other cross-domain tasks.",
      "results\nin the cross-domain setting without any domain adaptation. In fact, methods based on string kernels have demonstrated impressive",
      "results\non cross-domain classification proposing the structural correspondence learning (SCL) method, and its variant based on mutual information (SCL-MI). Pan et al.",
      "ployed to solve the maximum likelihood problem and to obtain a latent emotion distribution of the author. Franco-Salvador et al. BIBREF11 combined various recent and knowledge-based approaches using a meta-learning scheme (KE-Meta). They performed cross-domain polarity classification without employing any domain adaptation technique. More recently, Fernández et al.",
      "g. Moreover, classifiers based on string kernels have been found to be robust to the distribution gap between different domains. In this paper, we formally describe an algorithm composed of two simple yet effective transductive learning approaches to further improve the",
      " proposing the structural correspondence learning (SCL) method, and its variant based on mutual information (SCL-MI). Pan et al. BIBREF1 proposed a spectral feature alignment (SFA) algorithm to align domain-specific words from different domains into unified clusters, using domain-independent words as a bridge. Bollegala et al.",
      "stinct concepts in different domains can be learned by the Expectation-Maximization process which optimizes the data likelihood. In BIBREF33 , an algorithm to adapt a classification model by iteratively learning domain-specific features from the unlabeled test data is described. Cross-domain polarity classification.",
      "ct identification BIBREF23 , BIBREF17 , BIBREF24 , sentiment analysis BIBREF10 , BIBREF25 and automatic essay scoring BIBREF26 . As long as a labeled training set is available, string kernels can reach state-of-the-art"
    ],
    "answer": [
      "string kernels, structural correspondence learning (SCL) method, mutual information (SCL-MI), spectral feature alignment (SFA) algorithm, Expectation-Maximization process, iterative learning of domain-specific features."
    ]
  },
  {
    "title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches",
    "evidence": [
      "ctively trained on a much larger set, in the sense that they measure a loss and gradient for every possible pair of data points. Our experiments suggest that the models could still be improved with additional layers.",
      "results\nare obtained with deep LSTM RNNs with a combination of several stacked layers and several fully connected layers, optimized with a contrastive Siamese loss.",
      "for 15 epochs. The learning rate is initialized to 0.001 and dropped every 3 epochs until no improvement is seen on the dev set. The final model is taken from the epoch with the highest dev set AP. All models were implemented in Torch BIBREF37 and used the rnn library of BIBREF38 .",
      "lls, for classifier-based embeddings. The best performance in this experiment is achieved by the LSTM network with INLINEFORM2 . However, performance still seems to be improving with additional layers, suggesting that we may be able to further improve performance by adding even more layers of either type.",
      " Siamese networks, the training data consists of all of the same-word pairs in the full training set (approximately 100k pairs). For each such training pair, we randomly sample a third example belonging to a different word type, as required for the INLINEFORM0 loss.",
      "IBREF13 for a study of varying the dimensionality in such a classifier-based embedding model by introducing a bottleneck layer). This model is trained end-to-end and is optimized with a cross entropy loss.",
      "RM3 are learned weight matrices of the appropriate size, and INLINEFORM4 , INLINEFORM5 and INLINEFORM6 are learned bias vectors. All of the above equations refer to single-layer networks.",
      "rom BIBREF13 ) and the best prior result using DTW, obtained with frame features learned with correlated autoencoders BIBREF21 . Both classifier and Siamese LSTM embedding models outperform all prior",
      "such training pair, we randomly sample a third example belonging to a different word type, as required for the INLINEFORM0 loss. Classification network details Our classifier-based embeddings use LSTM or GRU networks with 2–4 stacked layers and 1–3 fully connected layers.",
      "results\nare given in Table TABREF7 . We include a comparison with the best prior",
      " that has begun to be explored is the use of acoustic word embeddings (AWEs), or vector representations of spoken word segments. AWEs are representations that can be learned from data, ideally such that the embeddings of two segments corresponding to the same word are close, while embeddings of segments corresponding to different words are far apart.",
      "ts, while dev and test each contain approximately 11k segments (corresponding to about 60M pairs for computing the dev/test AP). As in BIBREF13 , when training the classification-based embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments.",
      "ypes. There have now been a number of approaches compared on this same task and data BIBREF11 , BIBREF20 , BIBREF21 , BIBREF22 . For a direct comparison with this prior work, in this paper we use the same task and some of the same training losses as Kamper et al., but develop new embedding models based on RNNs."
    ],
    "answer": [
      "The dataset used is not explicitly stated in the text, but it is mentioned that the training data consists of approximately 100k pairs of same-word pairs, and the dev and test sets each contain approximately 11k segments, corresponding to about 60M pairs for computing the dev/test AP."
    ]
  },
  {
    "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
    "evidence": [
      "individual dimensions of the gating signals, we aggregate them to consider their norm, both for the signal and for its saliency. Note that ESIM models have two LSTM layers, the first (input) LSTM performs the input encoding and the second (inference) LSTM generates the representation for inference. In Fig.",
      "lassifier that includes a hidden layer with $m$7 activation and $m$8 output layer. The model is trained in an end-to-end manner. Attention Study Here we provide more examples on the NLI task which intend to examine specific behavior in this model. Such examples indicate interesting observation that we can analyze them in the future works. Table 1 shows the list of all example.",
      "asks. While these models are widely explored, they are hard to interpret and it is often unclear how and why they actually work. In this paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for NLI.",
      "$ (Eq. ) $$\\hat{u} = \\textit {BiLSTM}(u)$$ (Eq. 16) where $m$1 and $m$2 are the reading sequences of $m$3 and $m$4 respectively. Finally the concatenation max and average pooling of $m$5 and $m$6 are pass through a multilayer perceptron (MLP) classifier that includes a hidden layer with $m$7 activation and $m$8 output layer. The model is trained in an end-to-end manner.",
      " paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for NLI. In particular, we propose to interpret the intermediate layers of NLI models by visualizing the saliency of attention and LSTM gating signals.",
      "ariants of ESIM, with dimensionality 50 and 300 respectively, referred to as ESIM-50 and ESIM-300 in the remainder of the paper. Visualization of Attention and Gating In this work, we are primarily interested in the internal workings of the NLI model.",
      "or of deep models in their intermediate layers, specifically, by examining the saliency of the attention and the gating signals. Second, we provide an extensive analysis of the state-of-the-art model for the NLI task and show that our methods reveal interesting insights not available from traditional methods of inspecting attention and word saliency.",
      "r work in NLI have attempted to visualize the attention layer to provide some understanding of their models BIBREF5 , BIBREF15 . Such visualizations generate a heatmap representing the similarity between the hidden states of the premise and the hypothesis (Eq. 19 of Appendix). Unfortunately the similarities are often the same regardless of the decision.",
      " we propose to interpret the intermediate layers of NLI models by visualizing the saliency of attention and LSTM gating signals. We present several examples for which our methods are able to reveal interesting insights and identify the critical information contributing to the model decisions.",
      "ame model on different inputs. Now we use the attention saliency to compare the two different ESIM models: ESIM-50 and ESIM-300. Consider two examples with a shared hypothesis of “A man ordered a book” and premise: John ordered a book from amazon Mary ordered a book from amazon Here ESIM-50 fails to capture the gender connections of the two different names and predicts Neutral for both inputs, whereas ESIM-300 correctly predicts Entailment for the first case and Contradiction for the second.",
      "the sentences. Another round of LSTM reading then produces the final representations, which are compared to make the prediction. Detailed description of ESIM can be found in the Appendix. Using the SNLI BIBREF4 data, we train two variants of ESIM, with dimensionality 50 and 300 respectively, referred to as ESIM-50 and ESIM-300 in the remainder of the paper.",
      "models to understand how and why they work. We demonstrate the effectiveness of the proposed strategies on a complex task (NLI). Our strategies are able to provide interesting insights not achievable by previous explanation techniques.",
      "_1, \\cdots , v_m]$9 by attending to $n$0 while $n$1 represents the extracted relevant information of $n$2 by attending to $n$3 . Next, it passes the enriched information through a projector layer which produce the final output of attention stage. Equations 22 and 23 formally represent this process. $$p$$ (Eq. ) $$q$$ (Eq."
    ],
    "answer": [
      "There are two LSTM layers in the ESIM model."
    ]
  },
  {
    "title": "\"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach",
    "evidence": [
      "results\ndemonstrate qualitatively that the CNN model produces better explanations than the BoW/SVM classifier.",
      "iform or TFIDF weighting, this indicates that the explanations produced by LRP are semantically more meaningful than the latter. Finally, we confirm quantitatively the observations made before, namely that (1) the LRP decomposition method provides better explanations than SA and that (2) the CNN model outperforms the BoW/SVM classifier in terms of explanatory power.",
      "se the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.",
      "results\ndemonstrate qualitatively that the CNN model produces better explanations than the BoW/SVM classifier. After that we move to the evaluation of the document summary vectors, where we show that a 2D PCA projection of the document vectors computed from the LRP scores groups documents according to their topics (without requiring the true labels). Since worse",
      "rvations made in the last section, namely that the neural network outperforms the BoW/SVM classifier in terms of explainability. Figure 3 furthermore suggests that LRP provides semantically more meaningful semantic extraction than the baseline methods. In the next section we will confirm these observations quantitatively.",
      "el, as well as the corresponding TFIDF/uniform weighting baselines (for CNN1 and CNN3 we obtained a similar layout as for CNN2). One can further see from Figure 5 that (1) (element-wise) LRP provides consistently better semantic extraction than all baseline methods and that (2) the CNN2 model has a higher explanatory power than the BoW/SVM classifier since it produces semantically more meaningful summary vectors for KNN classification.",
      "rovides better explanations than SA and that (2) the CNN model outperforms the BoW/SVM classifier in terms of explanatory power. Experimental Setup For our experiments we consider a topic categorization task, and employ the freely available 20Newsgroups dataset consisting of newsgroup posts evenly distributed among twenty fine-grained categories.",
      " that these document vectors present semantic regularities within their original feature space akin word vector representations. px (iv) A measure for model explanatory power is proposed and it is shown that two ML models, a neural network and a BoW/SVM classifier, although presenting similar classification performance may largely differ in terms of explainability.",
      "hot word vectors is significantly better than the corresponding TFIDF document representation with a significance level of 0.05. Lastly, the best CNN2 explanatory power index is significantly higher than the best SVM based explanation at a significance level of 0.10.",
      "e best CNN2 explanatory power index is significantly higher than the best SVM based explanation at a significance level of 0.10. In Figure 5 we plot the mean accuracy of KNN (averaged over ten random test data splits) as a function of the number of neighbors $K$ , for the CNN2 resp.",
      "K$ (corresponding to our EPI metric of explanatory power) is reported for several models and explanation techniques in Table 2 . When pairwise comparing the best CNN based weighting schemes with the corresponding TFIDF baseline result from Table 2 , we find that all LRP element-wise weighted combinations of word2vec vectors are statistical significantly better than the TFIDF weighting of word embeddings at a significance level of 0.05 (using a corrected resampled t-test BIBREF35 ).",
      "easure of model explanatory power which does not depend on a classification performance change, but only on the word relevances. Hereby we consider ML model A as being more explainable than ML model B if its word relevances are more “semantic extractive”, i.e. more helpful for solving a semantic related task such as the classification of document summary vectors.",
      "results\ndiffer a lot from the CNN1 model in the first deletion experiment. In order to quantitatively evaluate and compare the ML models in combination with a relevance decomposition or explanation technique, we apply the evaluation method described in section \"Measuring Model Explanatory Power through Extrinsic Validation\" ."
    ],
    "answer": [
      "The CNN model exhibits a higher level of explainability because it produces semantically more meaningful explanations than the BoW/SVM classifier."
    ]
  },
  {
    "title": "Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks",
    "evidence": [
      "eneric task-invariant representations, which can be considered as off-the-shelf knowledge and then be used for unseen new tasks. To test the transferability of our learned shared representation, we also design an experiment shown in Table TABREF46 . The multi-task learning",
      "c one. Experiment In this section, we investigate the empirical performances of our proposed architectures on three experiments. Exp I: Sentiment Classification We first conduct a multi-task experiment on sentiment classification. We use 16 different datasets from several popular review corpora used in BIBREF20 .",
      " We conduct extensive experiments on 16 different sentiment classification tasks, which demonstrates the benefits of our models. Moreover, the shared sentence encoding model can be transferred to other tasks, which can be further boosted by introducing auxiliary tasks.",
      "task experiment on sentiment classification. We use 16 different datasets from several popular review corpora used in BIBREF20 . These datasets consist of 14 product review datasets and two movie review datasets. All the datasets in each task are partitioned randomly into training set, development set and testing set with the proportion of 70%, 10% and 20% respectively.",
      "results\nare derived by training the first 6 tasks in general multi-task learning.",
      "ntation and each task can select the task-specific information from the shared sentence representation with attention mechanism. The query vector of each task's attention could be either static parameters or generated dynamically. We conduct extensive experiments on 16 different text classification tasks, which demonstrate the benefits of our architecture.",
      "lti-task learning, which uses attention mechanism to select the task-specific information from a shared sentence encoding layer. We conduct extensive experiments on 16 different sentiment classification tasks, which demonstrates the benefits of our models.",
      "results\nto different representations for the same sentence in different tasks.",
      "results\nare shown in Table TABREF51 . We use the same hyperparameters and training procedure as the former experiments. The result shows that by leveraging auxiliary tasks, the performances of SA-MTL and DA-MTL achieve more improvement than PSP-MTL and SSP-MTL.",
      "nd Chunking tasks. There are 8774 sentences in training data, 500 sentences in development data and 1512 sentences in test data. The average sentence length is 24 and has a total vocabulary size as 17k. The experiment",
      "ul to improve the final performance, but the improvement is not guaranteed since it does not directly optimize the desired task. Another approach is multi-task learning BIBREF5 , which is an effective approach to improve the performance of a single task with the help of other related tasks.",
      "results\nare derived by training the first 6 tasks in general multi-task learning. For transfer learning, we choose the last 10 tasks to train our model with multi-task learning, then the learned shared sentence encoding layer are kept frozen and transferred to train the first 6 tasks.",
      "pecific features from different tasks. We initialize word embeddings with the 200d GloVe vectors (840B token version, BIBREF4 ). The other parameters are initialized by randomly sampling from uniform distribution in [-0.1, 0.1]. The mini-batch size is set to 32. For each task, we take hyperparameters which achieve the best performance on the development set via a small grid search."
    ],
    "answer": [
      "The tasks they experimented with include sentiment classification, part-of-speech tagging, named entity recognition, and chunking."
    ]
  },
  {
    "title": "Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation",
    "evidence": [
      "results\nin detail. First, we can observe that the final model “Ours with Mask and Ordered Triplets” outperforms the Baseline and Ablation models on all metrics in previously seen environments.",
      "ion of the environment. We evaluate our model's performance on a new dataset containing 10,050 pairs of navigation instructions. Our model significantly outperforms baseline approaches. Furthermore, our",
      " the proposed model to reduce overfitting. Best performance was achieved with a dropout rate of 0.5 and batch size equal to 256. We also used scheduled sampling BIBREF29 at training time for all models except the baseline.",
      "urs with Mask and Ordered Triplets” outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively. These",
      "results\nin Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments.",
      "work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set.",
      " effort, we contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments. Our model achieved the best performance in this dataset in comparison to a two-step baseline approach for interpreting navigation instructions, and a sequence-to-sequence model that does not consider the behavioral graph. Our quantitative and qualitative",
      " boost of INLINEFORM0 in EM and GM performance. The rearrangement of the graph triplets also helps to reduce ED and increase F1. Lastly, it is worth noting that our proposed model (last row of Table TABREF28 ) outperforms all other models in previously seen environments. In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models.",
      "ute matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path. To test the impact of using the behavioral graphs as an extra input to our translation model, we implemented a version of our approach that only takes natural language instructions as input.",
      "curacy. For the proposed model (with or without reordering the graph triplets), the increase in accuracy is around INLINEFORM1 . Note that the impact of the masking function is less evident in terms of the F1 score because this metric considers if a predicted behavior exists in the ground truth navigation plan, irrespective of its specific position in the output sequence. The",
      "r INLINEFORM1 from the prior cell, and a one-hot embedding of the previous behavior INLINEFORM2 that was predicted by the model. Based on these inputs, the GRU cell outputs a new hidden state INLINEFORM3 to compute likelihoods for the next behavior.",
      "o 128 in all the experiments. In general, we used 12.5% of the training set as validation for choosing models' hyper-parameters. In particular, we used dropout after the encoder and the fully-connected layers of the proposed model to reduce overfitting. Best performance was achieved with a dropout rate of 0.5 and batch size equal to 256.",
      "en environments. In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time."
    ],
    "answer": [
      "The model outperformed the baseline by 35% and 25% in Exact Match and Goal Match metrics, respectively."
    ]
  },
  {
    "title": "Sentiment Analysis of Citations Using Word2vec",
    "evidence": [
      "ighted average of the precision and recall. In the multi-class case, this is the weighted average of the F1 score of each class. There are several types of averaging performed on the data: Micro-F calculates metrics globally by counting the total true positives, false negatives and false positives. Macro-F calculates metrics for each label, and find their unweighted mean.",
      " true positives, false negatives and false positives. Macro-F calculates metrics for each label, and find their unweighted mean. Macro-F does not take label imbalance into account. Weighted-F calculates metrics for each label, and find their average, weighted by support (the number of true instances for each label). Weighted-F alters macro-F to account for label imbalance.",
      "CL100, but there is no difference between 300 and 100 dimensional ACL-embeddings when look at the macro-F and weighted-F scores. Table TABREF26 showed the sent2vec performance on classifying implicit citations with four categories: objective, negative, positive and excluded.",
      "results\nbased on word embeddings. The binary classification",
      "results\n: the best one (based on features n-gram + dependencies + negation) and the baseline (based on 1-3 grams).",
      "hted-F scores indicated that this method may achieve better performances if the evaluations are conducted on a balanced dataset. Among the embeddings, ACL-Embeddings performed better than Brown corpus in terms of macro-F and weighted-F measurements .",
      " polarity are shown in Table TABREF2 . Sentiment analysis of citations plays an important role in plotting scientific idea flow. I can see from Table TABREF2 , one of the ideas introduced in paper A0 is Hidden Markov Model (HMM) based part-of-speech (POS) tagging, which has been referenced positively in paper A1.",
      "results\nusing the combination of features, it required a lot of engineering work and big amount of annotated data to obtain the features.",
      "negative) performance using sent2vec; (2) Comparing the sentiment classification ability of PS-Embeddings with other embeddings. Evaluation Strategy One-Vs-The-Rest strategy was adopted for the task of multi-class classification and I reported F-score, micro-F, macro-F and weighted-F scores using 10-fold cross-validation.",
      "BREF2 performed far better than word embeddings, in terms of macro-F (their best macro-F is 0.90, the one in this work is 0.33). However, the higher micro-F score (The highest micro-F in this work is 0.88, theirs is 0.78) and the weighted-F scores indicated that this method may achieve better performances if the evaluations are conducted on a balanced dataset.",
      "results\nof classifying positive and negative citations using different word embeddings. The macro-F score 0.85 and the weighted-F score 0.86 proved that word2vec is effective on classifying positive and negative citations.",
      "results\n: the best one (based on features n-gram + dependencies + negation) and the baseline (based on 1-3 grams). From Table TABREF25 I can see that the features extracted by BIBREF2 performed far better than word embeddings, in terms of macro-F (their best macro-F is 0.90, the one in this work is 0.33).",
      "tively in paper A1. In paper A2, however, a better approach was brought up making the idea (HMM based POS) in paper A0 negative. This citation sentiment analysis could lead to future-works in such a way that new approaches (mentioned in paper A2) are recommended to other papers which cited A0 positively . Analyzing citation sentences during literature review is time consuming."
    ],
    "answer": [
      "F1 score, micro-F, macro-F, and weighted-F."
    ]
  },
  {
    "title": "Neural Factor Graph Models for Cross-lingual Morphological Tagging",
    "evidence": [
      "experiments on multilingual morphological tagging. They proposed an exponential model and the use of a morphological dictionary. BIBREF17 , BIBREF28 proposed a model that used tag projection of type and token constraints from a resource-rich language to a low-resource language for tagging.",
      "om a different family: Danish/Swedish (da/sv), Russian/Bulgarian (ru/bg), Finnish/Hungarian (fi/hu), Spanish/Portuguese (es/pt). Picking languages from different families would ensure that we obtain",
      "ging and parsing. BIBREF24 , BIBREF25 proposed the use of a higher-order CRF that is approximated using coarse-to-fine decoding. BIBREF26 proposed joint lemmatization and tagging using this framework. BIBREF27 was the first work that performed experiments on multilingual morphological tagging. They proposed an exponential model and the use of a morphological dictionary.",
      "y to generate any tag set, including those not observed in the training data. The strongest improvements are observed for FI/HU. This is likely because the number of unique tags is the highest in this language pair and our method scales well with the number of tags due to its ability to make use of correlations between the tags in different tag sets.",
      "ajor advantages of our model is the ability to interpret what the model has learned by looking at the trained parameter weights. We investigated both language-generic and language-specific patterns learned by our parameters: Language-Generic: We found evidence for several syntactic properties learned by the model parameters.",
      " work has experimented with the use of a linear chain CRF with factors from a neural network BIBREF8 for sequence tagging tasks. We hypothesize that modeling transition factors in a similar manner can allow the model to utilize information about neighboring tags and capture word order features of the language.",
      " for cross-lingual morphological tagging that aims to improve information sharing between languages by relaxing this assumption. The proposed model uses factorial conditional random fields with neural network potentials, making it possible to (1) utilize the expressive power of neural network representations to smooth over superficial differences in the surface forms, (2) model pairwise and transitive relationships between tags, and (3) accurately generate tag sets that are unseen or rare in the training data.",
      "del that used tag projection of type and token constraints from a resource-rich language to a low-resource language for tagging. Most recent work has focused on character-based neural models BIBREF29 , that can handle rare words and are hence more useful to model morphology than word-based models.",
      "rwise factors, we experiment with training general and language-specific parameters for the transition and the pairwise weights. We define the weight matrix INLINEFORM0 to learn the general trends that hold across both languages, and the weights INLINEFORM1 to learn the exceptions to these trends.",
      " for useful comments about this work. We would also like to thank the reviewers who gave valuable feedback to improve the paper. This project was supported in part by an Amazon Academic Research Award and Google Faculty Award.",
      "results\nthat are on average consistent across languages. The sizes of the training and evaluation sets are specified in Table TABREF31 . In order to simulate low-resource settings, we follow the experimental procedure from BIBREF5 .",
      "Abstract\nMorphological analysis involves predicting the syntactic traits of a word (e.g. {POS: Noun, Case: Acc, Gender: Fem}). Previous work in morphological tagging improves performance for low-resource languages (LRLs) through cross-lingual training with a high-resource language (HRL) from the same family, but is limited by the strict, often false, assumption that tag sets exactly overlap between the HRL and LRL.",
      " as translation BIBREF1 , BIBREF2 and parsing BIBREF3 , and errors in the upstream analysis may cascade to the downstream tasks. One difficulty, however, in creating these taggers is that only a limited amount of annotated data is available for a majority of the world's languages to learn these morphological taggers."
    ],
    "answer": [
      "Danish/Swedish (da/sv), Russian/Bulgarian (ru/bg), Finnish/Hungarian (fi/hu), Spanish/Portuguese (es/pt)."
    ]
  },
  {
    "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
    "evidence": [
      ") providing different models in a unified API to prevent overfitting to a specific architecture (and set of pretrained weights). Moreover, the unified front-end of the library makes it easy to compare the performances of several architectures on a common language understanding benchmark.",
      " the architecture to use can be automatically guessed from the shortcut name of the pretrained weights (e.g. `bert-base-cased`). A set of Auto classes provides a unified API that enable very fast switching between different models/configs/tokenizers. There are a total of four high-level",
      "ining-computation-cost-sharing so that low-resource users can reuse pretrained models without having to train them from scratch. These models are accessed through a simple and unified API that follows a classic NLP pipeline: setting up configuration, processing data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference.",
      "abstract\nion enabling a unified API in the library.",
      "ys. In particular, they provide neither a unified API across models nor standardized ways to access the internals of the models. Targeting a more general machine-learning community, these Hubs lack the NLP-specific user-facing features provided by Transformers like tokenizers, dedicated processing scripts for common downstream tasks and sensible default hyper-parameters for high performance on a range of language understanding and generation tasks.",
      "nder a unified API together with an ecosystem of libraries, examples, tutorials and scripts targeting many downstream NLP tasks. HuggingFace's Transformers library features carefully crafted model implementations and high-performance pretrained weights for two main deep learning frameworks, PyTorch and TensorFlow, while supporting all the necessary tools to analyze, evaluate and use these models in downstream tasks such as text/token classification, questions answering and language generation among others.",
      "ains an expensive and time-consuming process restricting the use of these methods to a small sub-set of the wider NLP community. In this paper, we present HuggingFace's Transformers library, a library for state-of-the-art NLP, making these developments available to the community by gathering state-of-the-art general-purpose pretrained models under a unified API together with an ecosystem of libraries, examples, tutorials and scripts targeting many downstream NLP tasks.",
      "abstract\nions to learn was strongly limited and reduced to just three standard classes: configuration, models and tokenizers, which all can be initialized in a simple and unified way by using a common `from_pretrained()` instantiation method.",
      "as been designed around a unified frontend for all the models: parameters and configurations, tokenization, and model inference. These steps reflect the recurring questions that arise when building an NLP pipeline: defining the model architecture, processing the text data and finally, training the model and performing inference in production.",
      "braries, an increased availability of published research code and strong incentives to share state-of-the-art pretrained models. The combination of these factors has lead researchers to reproduce previous",
      "onding pretraining heads (language modeling, next sentence prediction for BERT) for adaptation using the pretraining objectives. Some models fine-tuned on downstream tasks such as SQuAD1.1 are also available. Overall, more than 30 pretrained weights are provided through the library including more than 10 models pretrained in languages other than English.",
      "point for training a model on a downstream task moving from a blank specific model to a general-purpose pretrained architecture. Still, creating these general-purpose models remains an expensive and time-consuming process restricting the use of these methods to a small sub-set of the wider NLP community.",
      "f text using 1024 32GB V100. On Amazon-Web-Services cloud computing (AWS), such a pretraining would cost approximately 100K USD. Contrary to this trend, the booming research in Machine Learning in general and Natural Language Processing in particular is arguably explained significantly by a strong focus on knowledge sharing and large-scale community efforts resulting in the development of standard libraries, an increased availability of published research code and strong incentives to share state-of-the-art pretrained models."
    ],
    "answer": [
      "HuggingFace's Transformers library provides the following state-of-the-art general-purpose pretrained models under a unified API: BERT, RoBERTa, DistilBERT, ALBERT, and XLNet."
    ]
  },
  {
    "title": "Multichannel Variable-Size Convolution for Sentence Classification",
    "evidence": [
      "Conclusion\nThis work presented MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization – diverse versions of pretrained word embeddings are used – and variable-size filters – features of multigranular phrases are extracted with variable-size convolution filters.",
      "Abstract\nWe propose MVCNN, a convolution neural network (CNN) architecture for sentence classification.",
      "ifferent embedding versions to extract higher quality sentence features and thereby improve sentence classification performance. The letters “M” and “V” in the name “MVCNN” of our architecture denote the multichannel and variable-size convolution filters, respectively. “Multichannel” employs language from computer vision where a color image has red, green and blue channels.",
      "Conclusion\nThis work presented MVCNN, a novel CNN architecture for sentence classification.",
      "Abstract\nWe propose MVCNN, a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance.",
      "rsions we can use and they leverage not only the diversity of corpora, but also the different principles of learning algorithms. Model Description We now describe the architecture of our model MVCNN, illustrated in Figure 1 . Multichannel Input.",
      "tigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classification.",
      "em by pretraining MVCNN on unlabeled data. These pretrained weights can then be fine-tuned for the specific classification task. In sum, we attribute the success of MVCNN to: (i) designing variable-size convolution filters to extract variable-range features of sentences and (ii) exploring the combination of multiple public embedding versions to initialize words in sentences.",
      " description by an embedding version. For many sentence classification tasks, only relatively small training sets are available. MVCNN has a large number of parameters, so that overfitting is a danger when they are trained on small training sets. We address this problem by pretraining MVCNN on unlabeled data.",
      "algorithms. Model Description We now describe the architecture of our model MVCNN, illustrated in Figure 1 . Multichannel Input. The input of MVCNN includes multichannel feature maps of a considered sentence, each is a matrix initialized by a different embedding version.",
      "VCNN includes multichannel feature maps of a considered sentence, each is a matrix initialized by a different embedding version. Let $s$ be sentence length, $d$ dimension of word embeddings and $c$ the total number of different embedding versions (i.e., channels). Hence, the whole initialized input is a three-dimensional array of size $c\\times d\\times s$ .",
      "results\non test of MVCNN and its variants with other baselines in the four sentence classification tasks. Row 34, “MVCNN (overall)”, shows performance of the best configuration of MVCNN, optimized on dev.",
      "itting is a danger when they are trained on small training sets. We address this problem by pretraining MVCNN on unlabeled data. These pretrained weights can then be fine-tuned for the specific classification task."
    ],
    "answer": [
      "MVCNN is a novel CNN architecture for sentence classification that combines multichannel initialization and variable-size filters to improve sentence classification performance."
    ]
  },
  {
    "title": "A Lightweight Front-end Tool for Interactive Entity Population",
    "evidence": [
      "interface for users to give feedbacks to the tool intuitively. Furthermore, we also consider the end-to-end user cost reduction. We adhere to the concept of developing installation-free software to distribute the tool among a wide variety of users, including nontechnical clusters.",
      "not covered by existing knowledge bases. We develop a lightweight front-end tool for facilitating interactive entity population. We implement key components necessary for effective interactive entity population: 1) GUI-based dashboards to quickly modify an entity dictionary, and 2) entity highlighting on documents for quickly viewing the current progress.",
      " viewing the current progress. We aim to reduce user cost from beginning to end, including package installation and maintenance. The implementation enables users to use this tool on their web browsers without any additional packages -users can focus on their missions to create entity dictionaries. Moreover, an entity expansion module is implemented as external APIs.",
      "d user feedback assignment. We have implemented LUWAK in pure JavaScript with LocalStorage to make it an installation-free tool. We believe that LUWAK plays an important role in delivering the values of existing entity expansion techniques to potential users including nontechnical people without supposing a large amount of human cost.",
      "the table at any time. (Step 4) the user can also easily see how these entities stored in the Entity table appear in a document. (Step 5) After repeating the same procedure (Steps 2–4) for a sufficient time, the user can publish the Entity table as an output. Implementation LUWAK is implemented in pure JavaScript code, and it uses the LocalStorage of a web browser.",
      "itial entity set, (b) generating entity candidates, (c) obtaining user feedback, and (d) publishing populated entity dictionary. We aim to reduce the user’s total workload as a key metric of an entity population tool. That is, an entity population tool should provide the easiest and fastest solution to collecting entities of a particular entity type.",
      "boards to quickly modify an entity dictionary, and 2) entity highlighting on documents for quickly viewing the current progress. We aim to reduce user cost from beginning to end, including package installation and maintenance.",
      "s, an entity population tool should provide the easiest and fastest solution to collecting entities of a particular entity type. User interaction cost is a dominant factor in the entire workload of an interactive tool. Thus, we carefully design the user interface for users to give feedbacks to the tool intuitively. Furthermore, we also consider the end-to-end user cost reduction.",
      " of developing installation-free software to distribute the tool among a wide variety of users, including nontechnical clusters. This lightweight design of LUWAK might speed up the procedure of the whole interactive entity population workflow. Furthermore, this advantage might be beneficial to continuously improve the whole pipeline of interactive entity population system.",
      "or interactive entity population. LUWAK provides a set of basic functions such as entity expansion and user feedback assignment. We have implemented LUWAK in pure JavaScript with LocalStorage to make it an installation-free tool.",
      "ulation task. LUWAK has numerous features, such as the multiple entity expansion model choices, that are not implemented in IKE. Moreover, LUWAK is a corpus-free tool that does not require a document collection for entity population. Thus, we differentiate LUWAK from IKE, considering it a more lightweight entity population tool.",
      "llection for entity population. Thus, we differentiate LUWAK from IKE, considering it a more lightweight entity population tool. Summary This paper has presented LUWAK, a lightweight front-end tool for interactive entity population. LUWAK provides a set of basic functions such as entity expansion and user feedback assignment.",
      "amount of potential users. This philosophy follows our empirical feeling of the curse of installing required packages/libraries. Moreover, LUWAK does not need users to consider the usage and maintenance of these additional packages. That is why we are deeply committed to making LUWAK a pure client-side tool in the off-the-shelf style."
    ],
    "answer": [
      "entity expansion",
      "user feedback assignment",
      "multiple entity expansion model choices",
      "corpus-free",
      "installation-free",
      "pure client-side tool"
    ]
  },
  {
    "title": "Modeling Trolling in Social Media Conversations",
    "evidence": [
      "terested in, so we give the annotators the option to discard instances for which they couldn't determine the labels confidently. The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF19 in the column “Size”.",
      "e feature set is 549. This result is what makes this dataset interesting: there is still lots of room for research on this task. Again, the primary goal of this experiment is to help identify the difficult-to-classify instances for analysis in the next section.",
      "elease a dataset containing excerpts of Reddit conversations involving suspected trolls and their interactions with other users. Finally, we identify the difficult-to-classify cases in our corpus and suggest potential solutions for them.",
      "ces and 88047 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF19 in the column “Size”. Due to the subjective nature of the task we did not expect perfect agreement.",
      "ost powerful feature set, accuracy-wise, even better than the experiments with all features for all tasks except interpretation. The overall Total Accuracy score reported in table TABREF19 using the entire feature set is 549. This result is what makes this dataset interesting: there is still lots of room for research on this task.",
      " shown in the table TABREF19 in the column “Size”. Due to the subjective nature of the task we did not expect perfect agreement. However, on the 100 doubly-annotated snippets, we obtained substantial inter-annotator agreement according to Cohen's kappa statistic BIBREF4 for each of the four aspects: Intention: 0.788, Intention Disclosure: 0.780, Interpretation: 0.797 and Response 0.776.",
      "ed intensions, and reactions. Perhaps most importantly, we create an annotated dataset that we believe is the first of its sort. We intend to make publicly available with the hope of stimulating research on trolling.",
      "tion and his intention disclosure, as well as the responder's interpretation of the troll's intention and her response strategy. Using this categorization, we annotate and release a dataset containing excerpts of Reddit conversations involving suspected trolls and their interactions with other users.",
      "condary goal of estimating the state of the art on this new task using only shallow (i.e., lexical and wordlist-based) features. Feature Sets For prediction we define two sets of features: (1) a basic feature set taken from Van Hee's van2015detection paper on cyberbullying prediction, and (2) an extended feature set that we designed using primarily information extracted from wordlists and dictionaries.",
      "vents, our datasets will be composed of suspected trolling attempts (i.e., comments that are suspected to be trolling attempts). In other words, some of these suspected trolling attempts will be real trolling attempts, and some of them won't. So, if a suspected trolling attempt is in fact not a trolling attempt, then its author will not be a troll.",
      "Results\nUsing the features described in the previous subsection, we train four independent classifiers using logistic regression, one per each of the four prediction tasks. All the",
      "cess this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index from the comments and queried it for comments containing the word “troll” with an edit distance of 1 in order to include close variations of this word, hypothesizing that such comments would be reasonable candidates of real trolling attempts.",
      " trolling categorization seeks to model not only the troll's behavior but also the impact on the recipients, as described below. Since one of our goals is to identify trolling events, our datasets will be composed of suspected trolling attempts (i.e., comments that are suspected to be trolling attempts)."
    ],
    "answer": [
      "The size of the dataset is 1000 conversations composed of 6833 sentences and 88047 tokens."
    ]
  },
  {
    "title": "Rotations and Interpretability of Word Embeddings: the Case of the Russian Language",
    "evidence": [
      "positive and negative values of coordinates should be considered. Let all word vectors be normalized and $W$ be the word matrix. Inspired by BIBREF21 , where vector space models are used for evaluating topic coherence, we suggest to estimate the interpretability of $k$ th component as $ \\operatorname{interp}_k W = \\sum _{i,j=1}^N W_{i,k} W_{j,k} \\left(W_i \\cdot W_j \\right).",
      "_2$2 , where $M_2$3 . Here $M_2$4 can be chosen as $M_2$5 where $M_2$6 are matrices of right singular vectors in SVD of $M_2$7 . Interpretability measures One of traditional measures of interpretability in topic modeling looks as follows BIBREF29 , BIBREF19 . For each component, $n$ most probable words are selected.",
      "results\nare applicable for any language as well. Related Work Interpretability of the components have been extensively studied for topic models.",
      "um _{k=1}^d \\operatorname{interp}_k W, $ because $\\operatorname{tr}Q^T X Q = \\operatorname{tr}Q^{-1} X Q = \\operatorname{tr}X$ . It turns out that in average the interpretability is constant under any orthogonal transformation. But it is possible to make the first components more interpretable due to the other components.",
      "icable for any language as well. Related Work Interpretability of the components have been extensively studied for topic models. In BIBREF18 , BIBREF19 two methods for estimating the coherence of topic models with manual tagging have been proposed: namely, word intrusion and topic intrusion.",
      "l components between different models (such as CBOW versus Skip-Gram, different train corpora and different languages BIBREF33 . It is also worth to compare our intrinsic interpretability measure with human judgements. Acknowledgements The author is grateful to Mikhail Dektyarev, Mikhail Nokel, Anna Potapenko and Daniil Tararukhin for valuable and fruitful",
      " of $k$ th components of $i$ th and $j$ th words. The dot product $\\left(W_i \\cdot W_j\\right)$ reflects the similarity of words. Thus, this measure will be high if similar words have similar values of $k$ th coordinates. What orthogonal transformation $Q$ maximizes this interpretability (for some, or all components) of $WQ$ ?",
      " principal components indeed are good interpretable. Also, we show that these components are almost invariant under re-learning. It will be interesting to explore the regularities in canonical components between different models (such as CBOW versus Skip-Gram, different train corpora and different languages BIBREF33 .",
      " Other principal components have very similar singular values, and thus they cannot be determined uniquely with high confidence. Normalized interpretibility measures for different components (calculated for 50 top/bottom words) for the source and the rotated models are shown in Fig. 4.",
      " words were proposed in BIBREF20 , BIBREF21 . But unlike topic models, these methods cannot be applied directly to word vectors. There are lots of new models where interpretability is either taken into account by design BIBREF13 (modified skip-gram that produces non-negative entries), or is obtained automagically BIBREF15 (sparse autoencoding).",
      "erpretability in topic modeling looks as follows BIBREF29 , BIBREF19 . For each component, $n$ most probable words are selected. Then for each pair of selected words some co-occurrence measure such as PMI is calculated. These values are averaged over all pairs of selected words and all components. The other approaches use human markup.",
      " W =(W^T W W^T W)_{k, k}, $ and $ \\operatorname{interp}_k WQ = \\left(Q^T W^T W W^T W Q\\right)_{k,k} $ because $Q$ is orthogonal. The total interpretability over all components is $ \\sum _{k=1}^d \\operatorname{interp}_k WQ = \\sum _{k=1}^d \\left(Q^T W^T W W^T W Q \\right)_{k,k} = \\\\ = \\operatorname{tr}Q^T W^T W W^T W Q = \\operatorname{tr}\\left(W^T W W^T W\\right) = \\sum _{k=1}^d \\operatorname{interp}_k W, $ because $\\operatorname{tr}Q^T X Q = \\operatorname{tr}Q^{-1} X Q = \\operatorname{tr}X$ .",
      "om SVD, it is possible both to increase the meaning of some components and to make the components more stable under re-learning. We study the interpretability of components for publicly available models for the Russian language (RusVectores, fastText, RDT)."
    ],
    "answer": [
      "$\\operatorname{interp}_k W = \\sum _{i,j=1}^N W_{i,k} W_{j,k} \\left(W_i \\cdot W_j \\right)$"
    ]
  },
  {
    "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study",
    "evidence": [
      "results\n. In this paper, we re-assess the validity of these",
      "results\n, arguing that they are the result of lack of system adaptation to low-resource settings. We discuss some pitfalls to be aware of when training low-resource NMT systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource NMT.",
      "results\n, arguing that they are the result of lack of system adaptation to low-resource settings.",
      "results\n, arguing that they are the result of lack of system adaptation to low-resource settings.",
      "results\n, arguing that they are the result of lack of system adaptation to low-resource settings. Our main contributions are as follows: Low-Resource Translation Quality Compared Across Systems Figure FIGREF4 reproduces a plot by BIBREF3 which shows that their NMT system only outperforms their PBSMT system when more than 100 million words (approx.",
      "Acknowledgment\ns Rico Sennrich has received funding from the Swiss National Science Foundation in the project CoNTra (grant number 105212_169888). Biao Zhang acknowledges the support of the Baidu Scholarship.",
      "results\nfar outperform the RNN-based",
      "Conclusion\ns Our",
      "of research still accepts the premise that NMT models are data-inefficient and require large amounts of auxiliary data to train. In this work, we want to re-visit this point, and will focus on techniques to make more efficient use of small amounts of parallel training data.",
      "results\non this dataset. Table TABREF21 shows",
      "presented as atomic units at training time, and the ability to learn good high-dimensional representations of these is doubtful. BIBREF33 propose a minimum frequency threshold for subword units, and splitting any less frequent subword into smaller units or characters.",
      "wn by BIBREF4 are similar, showing that unsupervised NMT outperforms supervised systems if few parallel resources are available. In both papers, NMT systems are trained with hyperparameters that are typical for high-resource settings, and the authors did not tune hyperparameters, or change network architectures, to optimize NMT for low-resource conditions.",
      "rperform phrase-based statistical machine translation (PBSMT) or unsupervised methods in low-data conditions BIBREF3 , BIBREF4 . In this paper, we re-assess the validity of these"
    ],
    "answer": [
      "Pitfalls to be aware of when training low-resource NMT systems include:\n- Lack of system adaptation to low-resource settings\n- Using hyperparameters typical for high-resource settings\n- Not tuning hyperparameters or changing network architectures to optimize NMT for low-resource conditions."
    ]
  },
  {
    "title": "Attention Optimization for Abstractive Document Summarization",
    "evidence": [
      " its effectiveness. Experiments ::: Human Evaluation and Case Study We also conduct human evaluation on the generated summaries. Similar to the previous work BIBREF3, BIBREF20, we randomly select 100 samples from the test set of CNN/Daily Mail dataset and ask 3 human testers to measure relevance and readability of each summary.",
      "se the full-length ROUGE F1 and METEOR as our main evaluation metrics. Experiments ::: Preliminaries ::: Implementation Details. The data preprocessing is the same as PGN BIBREF11, and we randomly initialize the word embeddings. The hidden states of the encoder and the decoder are both 256-dimensional and the embedding size is also 256.",
      "amples from the test set of CNN/Daily Mail dataset and ask 3 human testers to measure relevance and readability of each summary. Relevance is based on how much salient information does the summary contain, and readability is based on how fluent and grammatical the summary is.",
      "cripts provided by BIBREF11 to obtain the non-anonymized version of the dataset without preprocessing to replace named entities. The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics. Experiments ::: Preliminaries ::: Implementation Details.",
      "hat they are the gold standard, we give both articles and reference summaries to the annotator to score the generated summaries. In other words, we compare the generated summaries against the reference ones and the original article to obtain the (relative) scores in Table 3. Each perspective is assessed with a score from 1 (worst) to 5 (best).",
      "ding step, and a global variance loss to optimize the attention distributions of all decoding steps from the global perspective. The performances on the CNN/Daily Mail dataset verify the effectiveness of our methods.",
      "results\nhave not outperformed the state-of-the-art models, our model has a much simpler structure with fewer parameters. Besides, these simple methods do yield a boost in performance compared with PGN baseline and may be applied on other models with attention mechanism. We further evaluate how these optimization approaches work. The",
      "results\non CNN/Daily Mail. Although our experimental",
      "s the PGN baseline by 3.85, 2.1 and 3.37 in terms of R-1, R-2 and R-L respectively and receives over 3.23 point boost on METEOR. FastAbs BIBREF3 regards ROUGE scores as reward signals with reinforcement learning, which brings a great performance gain. DCA BIBREF4 proposes deep communicating agents with reinforcement setting and achieves the best",
      "results\non ROUGE BIBREF12 and METEOR BIBREF13 with the state-of-the-art models. Our model surpasses the strong pointer-generator baseline (w/o coverage) BIBREF11 on all ROUGE metrics by a large margin.",
      "iginal article to obtain the (relative) scores in Table 3. Each perspective is assessed with a score from 1 (worst) to 5 (best). The result in Table TABREF21 demonstrate that our model performs better under both criteria w.r.t. BIBREF11. Additionally, we show the example of summaries generated by our model and baseline model in Table TABREF23.",
      "results\nat the bottom of Table TABREF13 verify the effectiveness of our",
      "marization datasets contain the pairs of article with a single reference summary due to the cost of annotating multi-references. Since we use the reference summaries as target sequences to train the model and assume that they are the gold standard, we give both articles and reference summaries to the annotator to score the generated summaries."
    ],
    "answer": [
      "The evaluation metrics used are ROUGE F1 and METEOR."
    ]
  },
  {
    "title": "Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling",
    "evidence": [
      "lti-lingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental",
      "so show their flexibility in incorporating it in attention and CTC based seq2seq model without compromising loss in performance. Future work We could use better architectures such as VGG-BLSTM as a multilingual prior model before transferring them to a new target language by performing stage-2 retraining.",
      "Introduction\nThe sequence-to-sequence (seq2seq) model proposed in BIBREF0 , BIBREF1 , BIBREF2 is a neural architecture for performing sequence classification and later adopted to perform speech recognition in BIBREF3 , BIBREF4 , BIBREF5 .",
      "Abstract\nSequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments.",
      "e confusions. Also, investigation of multilingual bottleneck features BIBREF30 for seq2seq model can provide better performance. Apart from using the character level language model as in this work, a word level RNNLM can be connected during decoding to further improve %WER.",
      "l allows to integrate the main blocks of ASR such as acoustic model, alignment model and language model into a single framework. The recent ASR advancements in connectionist temporal classification (CTC) BIBREF5 , BIBREF4 and attention BIBREF3 , BIBREF6 based approaches has created larger interest in speech community to use seq2seq models.",
      "ain. This trend is also shown in general including hybrid ASR systems and neural network-based sequence-to-sequence ASR systems. The following experiments show a benefit of using a language model in decoding with the previous stage-2 transferred models.",
      "ingual training approaches BIBREF8 , BIBREF9 , BIBREF10 used in hybrid DNN/RNN-HMMs to incorporate them into the seq2seq models. In a context of applications of multilingual approaches towards seq2seq model, CTC is mainly used instead of the attention models. A multilingual CTC is proposed in BIBREF11 , which uses a universal phoneset, FST decoder and language model.",
      "proach, the final layer of the seq2seq model which is mainly responsible for classification is retrained to the target language. In previous works BIBREF10 , BIBREF29 related to hybrid DNN/RNN models and CTC based models BIBREF11 , BIBREF14 the softmax layer is only adapted. However in our case, the attention decoder and CTC decoder both have to be retrained to the target language.",
      "ferent data sizes. As explained already, language models were trained separately and used to decode jointly with seq2seq models. The intuition behind it is to use the separately trained language model as a complementary component that works with a implicit language model within a seq2seq decoder.",
      " BIBREF4 and attention BIBREF3 , BIBREF6 based approaches has created larger interest in speech community to use seq2seq models. To leverage performance gains from this model as similar or better to conventional hybrid RNN/DNN-HMM models requires a huge amount of data BIBREF7 .",
      "h as VGG-BLSTM as a multilingual prior model before transferring them to a new target language by performing stage-2 retraining. The naive multilingual approach can be improved by including language vectors as input or target during training to reduce the confusions. Also, investigation of multilingual bottleneck features BIBREF30 for seq2seq model can provide better performance.",
      "ole of the model in performing alignment and language modeling along with acoustic to character label mapping at each iteration. In this paper, we explore the multilingual training approaches BIBREF8 , BIBREF9 , BIBREF10 used in hybrid DNN/RNN-HMMs to incorporate them into the seq2seq models."
    ],
    "answer": [
      "The architectures explored to improve the seq2seq model include VGG-BLSTM as a multilingual prior model, integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding, and using multilingual bottleneck features for the seq2seq model."
    ]
  },
  {
    "title": "Leveraging Discourse Information Effectively for Authorship Attribution",
    "evidence": [
      "Results\nBaseline-dataset experiments. The",
      "se features in a Convolutional Neural Network text classifier, which achieves a state-of-the-art result by a substantial margin. We empirically investigate several featurization methods to understand the conditions under which discourse features contribute non-trivial performance gains, and analyze discourse embeddings.",
      "information, and (ii) effectively integrate discourse features into the state-of-the-art character-bigram CNN classifier for AA. Beyond confirming the overall superiority of RST features over GR features in larger and more difficult datasets, we present a discourse embedding technique that is unavailable for previously proposed discourse-enhanced models.",
      "works (CNNs) have demonstrated considerable success on AA relying only on character-level INLINEFORM0 -grams BIBREF3 , BIBREF4 . The strength of these models is evidenced by findings that traditional stylometric features such as word INLINEFORM1 -grams and POS-tags do not improve, and can sometimes even hurt performance BIBREF3 , BIBREF5 .",
      "N at the input end yields better performance than concatenating them to the output layer as a feature vector (Section SECREF3 ). The global featurization is more effective than the local one.",
      "ged in the RST tree (i.e., left-to-right), and evaluated this model on novel-50 and IMDB62 with the same hyperparameter setting. The F1 scores turned out to be very close to the CNN2-DE (local) model, at 97.5 and 90.9.",
      "nce ceiling by a large margin. Admittedly, in using the RST features with entity-grids, we lose the valuable RST tree structure. In future work, we intend to adopt more sophisticated methods such as RecNN, as per Ji:17, to retain more information from the RST trees while reducing the parameter size.",
      "cult datasets, we present a discourse embedding technique that is unavailable for previously proposed discourse-enhanced models. The new technique enabled us to push the envelope of the current performance ceiling by a large margin. Admittedly, in using the RST features with entity-grids, we lose the valuable RST tree structure.",
      "Conclusion\nWe have conducted an in-depth investigation of techniques that (i) featurize discourse information, and (ii) effectively integrate discourse features into the state-of-the-art character-bigram CNN classifier for AA.",
      "class condition): 50 for char-bigrams; 20 for discourse features. The learning rate is 0.001 using the Adam Optimizer BIBREF18 . For all models, we apply dropout regularization of 0.75 BIBREF19 , and run 50 epochs (batch size 32). The SVMs in the baseline-dataset experiments use default settings, following F15.",
      "class classification with macro-averaged F1 evaluation. Model configurations. Following F15, we perform 5-fold cross-validation. The embedding sizes are tuned on novel-9 (multi-class condition): 50 for char-bigrams; 20 for discourse features. The learning rate is 0.001 using the Adam Optimizer BIBREF18 .",
      "n can help. However, they achieve limited performance gains and lack an in-depth analysis of discourse featurization techniques. More recently, convolutional neural networks (CNNs) have demonstrated considerable success on AA relying only on character-level INLINEFORM0 -grams BIBREF3 , BIBREF4 .",
      " study, however, works with only one small dataset and their models produce overall unremarkable performance ( INLINEFORM0 85%). BIBREF8 propose an advanced Recursive Neural Network (RecNN) architecture to work with RST in the more general area of text categorization and present impressive"
    ],
    "answer": [
      "The previous state-of-the-art was achieved by a character-bigram CNN classifier."
    ]
  },
  {
    "title": "Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents",
    "evidence": [
      "me. Typically, sequence labeling tasks are evaluated in terms of precision, recall, and F$_1$ at the entity level, per sentence. However, such an evaluation is inappropriate for our task because the content elements represent properties of the entire document as a whole, not individual sentences.",
      " inference time, documents from the test set are segmented into paragraphs and fed into the fine-tuned BERT model one at a time. Typically, sequence labeling tasks are evaluated in terms of precision, recall, and F$_1$ at the entity level, per sentence.",
      "ore fixed forms and are quite close to entities that the model may have encountered during pre-training (e.g., names and dates). In contrast, “difficult” elements are often domain-specific and widely vary in their forms. How data efficient is BERT when fine tuning on annotated data?",
      "iate for our task because the content elements represent properties of the entire document as a whole, not individual sentences. Instead, we adopted the following evaluation procedure: For each content element type (e.g., “tenant”), we extract all tagged spans from the document, and after deduplication, treat the entities as a set that we then measure against the ground truth in terms of precision, recall, and F$_1$.",
      "Results\nOur main",
      "ecause there may be multiple ground truth entities and BERT may mark multiple spans in a document with a particular entity type. Note that the metrics are based on exact matches—this means that, for example, if the extracted entity has an extraneous token compared to a ground truth entity, the system receives no credit.",
      "RT BIBREF1 and their ability to handle sequence labeling tasks, adopting such an approach seemed like an obvious starting point. In this context, we are primarily interested in two questions: First, how data efficient is BERT for fine-tuning to new specialized domains? Specifically, how much annotated data do we need to achieve some (reasonable) level of accuracy?",
      "es, we further applied a date parser based on rules and regular expressions to normalize and canonicalize the extracted outputs. In the regulatory filings, we tried to normalize numbers that were written in a mixture of Arabic numerals and Chinese units (e.g., “UTF8gbsn亿”, the unit for $10^8$) and discarded partial",
      "e of a stock exchange in China. We observe that most of these announcements are fairly formulaic, likely generated by templates. However, we treated them all as natural language text and did not exploit this observation; for example, we made no explicit attempt to induce template structure or apply clustering—although such techniques would likely improve extraction accuracy.",
      "results\n, we see that although there is some degradation in effectiveness between all documents and only unseen templates, it appears that BERT is able to generalize to previously-unseen expressions of the content elements.",
      "results\non all documents (left) and only over those with unseen templates (right). Examining these",
      "ty lease agreements, the contract length, if not directly extracted by BERT, is computed from the extracted start and end dates. Note that these post processing steps were not applied in the evaluation presented in the previous section, and so the figures reported in Tables TABREF6 and TABREF7 actually under-report the accuracy of our models in a real-world setting.",
      "e contribution of our work is twofold: From the scientific perspective, we begin to provide some answers to the above questions. With two case studies, we find that a modest amount of domain-specific annotated data (less than 100 documents) is sufficient to fine-tune BERT to achieve reasonable accuracy in extracting a set of content elements."
    ],
    "answer": [
      "The evaluation metric used for presenting results is precision, recall, and F1 at the entity level."
    ]
  },
  {
    "title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled Data",
    "evidence": [
      "scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. On the sub-population of Twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U.S. in 2018.",
      "e examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U.S. in 2018. We begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data;",
      "itive\" sample (labeled as 1 in the data), and a tweet denying climate change as a “negative\" sample (labeled as -1 in the data). All data were downloaded from Twitter in two separate batches using the “twint\" scraping tool BIBREF5 to sample historical tweets for several different search terms; queries always included either “climate change\" or “global warming\", and further included disaster-specific search terms (e.g., “bomb cyclone,\" “blizzard,\" “snowstorm,\" etc.).",
      " Bomb Cyclone (34.7%), Mendocino Wildfire (80.4%), Hurricane Florence (57.2%), Hurricane Michael (57.6%), and Camp Fire (70.1%). As sanity checks, we examine the predicted sentiments on a subset with geographic user information and compare",
      "ential tweeters that we identified on Twitter (87 of whom accept climate change), yielding a total of 16,360 influential tweets. The second data batch consists of event-related tweets for five natural disasters occurring in the U.S. in 2018. These are: the East Coast Bomb Cyclone (Jan. 2 - 6); the Mendocino, California wildfires (Jul. 27 - Sept. 18); Hurricane Florence (Aug.",
      " or “global warming\", and further included disaster-specific search terms (e.g., “bomb cyclone,\" “blizzard,\" “snowstorm,\" etc.). We refer to the first data batch as “influential\" tweets, and the second data batch as “event-related\" tweets.",
      "lysis Our second goal is to compare the mean values of users' binary sentiments both pre- and post- each natural disaster event. Applying our highest-performing RNN to event-related tweets yields the following breakdown of positive tweets: Bomb Cyclone (34.7%), Mendocino Wildfire (80.4%), Hurricane Florence (57.2%), Hurricane Michael (57.6%), and Camp Fire (70.1%).",
      "fires studied, implying that Twitter users' opinions on climate change are fairly ingrained on this subset of natural disasters. Background Much prior work has been done at the intersection of climate change and Twitter, such as tracking climate change sentiment over time BIBREF2 , finding correlations between Twitter climate change sentiment and seasonal effects BIBREF3 , and clustering Twitter users based on climate mentalities using network analysis BIBREF4 .",
      "ral U.S. population BIBREF9 . Third, we do not take into account the aggregate effects of continued natural disasters over time. Going forward, there is clear demand in discovering whether social networks can indicate environmental metrics in a “nowcasting\" fashion.",
      "owing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change. However, this effect does not hold for the 2018 blizzard and wildfires studied, implying that Twitter users' opinions on climate change are fairly ingrained on this subset of natural disasters.",
      " with a 90%/10% split. The training set contains 49.2% positive samples, and the validation set contains 49.0% positive samples. We form our test set by manually labeling a subset of 500 tweets from the the event-related tweets (randomly chosen across all five natural disasters), of which 50.0% are positive samples.",
      "eets from the the event-related tweets (randomly chosen across all five natural disasters), of which 50.0% are positive samples. Labeling Methodology Our first goal is to train a sentiment analysis model (on training and validation datasets) in order to perform classification inference on event-based tweets.",
      "results\nin line with prior research. We then apply RNNs to conduct a cohort-level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change."
    ],
    "answer": [
      "Bomb Cyclone, Mendocino Wildfire, Hurricane Florence, Hurricane Michael, and Camp Fire."
    ]
  },
  {
    "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding",
    "evidence": [
      "ion Network (LAN), which addresses the properties by aggregating neighbors with both rules- and network-based attention weights. By comparing with conventional aggregators on two knowledge graph completion tasks, we experimentally validate LAN's superiority in terms of the desired properties.",
      "Introduction\nKnowledge graphs (KGs) such as Freebase BIBREF0 , DBpedia BIBREF1 , and YAGO BIBREF2 play a critical role in various NLP tasks, including question answering BIBREF3 , information retrieval BIBREF4 , and personalized recommendation BIBREF5 .",
      " for KGs should possess. (2) We propose a novel aggregator, i.e., Logic Attention Network, to facilitate inductive KG embedding. (3) We conduct extensive comparisons with conventional aggregators on two KG completions tasks. The",
      "hould be seen during training, which is unpractical for real-world knowledge graphs with new entities emerging on a daily basis. Recent efforts on this issue suggest training a neighborhood aggregator in conjunction with the conventional entity and relation embeddings, which may help embed new entities inductively via their existing neighbors.",
      "to an entity's neighbors in a permutation invariant manner, considering both the redundancy of neighbors and the query relation. The weights are estimated from data with logic rules at a coarse relation level, and neural attention network at a fine neighbor level. Experiments show that LAN outperforms baseline models significantly on two typical KG completion tasks.",
      "Abstract\nKnowledge graph embedding aims at modeling entities and relations with low-dimensional vectors.",
      " the first scoring function. Then a similar margin-based ranking loss INLINEFORM2 as Eq. ( EQREF32 ) is defined for the subtask. Finally, we combine the subtask with the main task, and reformulate the overall training objective of LAN as DISPLAYFORM0 Experimental Configurations We evaluate the effectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification.",
      "t a fine neighbor level. Experiments show that LAN outperforms baseline models significantly on two typical KG completion tasks. Acknowledgements We thank the three anonymous authors for their constructive comments. This work is supported by the National Natural Science Foundation of China (61472453, U1401256, U1501252, U1611264, U1711261, U1711262).",
      "s, various KG embedding models are proposed to facilitate KG completion tasks, e.g., link prediction and triplet classification. After vectorizing entities and relations in a low-dimensional space, those models predict missing facts by manipulating the involved entity and relation embeddings.",
      "Abstract\nKnowledge graph embedding aims at modeling entities and relations with low-dimensional vectors. Most previous methods require that all entities should be seen during training, which is unpractical for real-world knowledge graphs with new entities emerging on a daily basis.",
      "ng BIBREF16 xie2016representation, BIBREF6 shi2018open and BIBREF17 xie2016image which use description text or images as inputs. Although the resultant embeddings may be utilized for KG completion, it is not clear whether the embeddings are powerful enough to infer implicit or new facts beyond those expressed in the text/image.",
      "ffectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification. We compare our LAN with two baseline aggregators, MEAN and LSTM, as described in the Encoder section. MEAN is used on behalf of pooling functions since it leads to the best performance in BIBREF9 ijcai2017-250.",
      "aim at embedding nodes for node classification given the entire graph and thus are inapplicable for inductive KG-specific tasks. BIBREF20 schlichtkrull2017modeling and BIBREF21 xiong2018one also rely on neighborhood structures to embed entities, but they either work transductively or focus on emerging relations."
    ],
    "answer": [
      "Knowledge graph completion tasks, i.e., link prediction and triplet classification."
    ]
  },
  {
    "title": "The First Evaluation of Chinese Human-Computer Dialogue Technology",
    "evidence": [
      "st present the two tasks of the evaluation as well as the evaluation metrics. We then describe the released data for evaluation. Finally, we also show the evaluation",
      "en evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. Task 2: Online Testing of Task-oriented Dialogue For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue.",
      "ntent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively.",
      "uccessfully returns the information which the user inquires or the number of dialogue turns is larger than 30 for a single task. For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following.",
      "Results\nThere are 74 participants who are signing up the evaluation. The final number of participants is 28 and the number of submitted systems is 43. Table TABREF14 and TABREF15 show the evaluation",
      "so allow to collect external data for training and developing. To considering that, the task 1 is indeed includes two sub tasks. One is a closed evaluation, in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric.",
      "ogy. We detail the evaluation scheme, tasks, metrics and how to collect and annotate the data for training, developing and test. The evaluation includes two tasks, namely user intent classification and online testing of task-oriented dialogue. To consider the different sources of the data for training and developing, the first task can also be divided into two sub tasks.",
      "Conclusion\nIn this paper, we introduce the first evaluation of Chinese human-computer dialogue technology. In detail, we first present the two tasks of the evaluation as well as the evaluation metrics. We then describe the released data for evaluation.",
      "onstruct a gold standard (usually a reference set) to evaluate a response which is generated by an open domain chit-chat system. For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc., there is no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue.",
      "o consider the different sources of the data for training and developing, the first task can also be divided into two sub tasks. Both the two tasks are coming from the real problems when using the applications developed by industry. The evaluation data is provided by the iFLYTEK Corporation. Meanwhile, in this paper, we publish the evaluation",
      "results\nof task 1. We will add the complete lists of the evaluation",
      "to 30. Evaluation Data In the evaluation, all the data for training, developing and test is provided by the iFLYTEK Corporation. For task 1, as the descriptions in Section SECREF10 , the two top categories are chit-chat (chat in Table TABREF13 ) and task-oriented dialogue. Meanwhile, the task-oriented dialogue also includes 30 sub categories.",
      "y. In this paper, we will present the evaluation scheme and the released corpus in detail. The rest of this paper is as follows. In Section 2, we will briefly introduce the first evaluation of Chinese human-computer dialogue technology, which includes the descriptions and the evaluation metrics of the two tasks. We then present the evaluation data and final"
    ],
    "answer": [
      "F1-score, Task completion ratio, User satisfaction degree, Number of completed tasks, Number of total tasks, Number of dialogue turns."
    ]
  },
  {
    "title": "BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization",
    "evidence": [
      "ur model improves constantly with the improvement of template quality (larger ranges lead to better chances for good templates). Even with randomly-selected templates, our model still works with stable performance, demonstrating its robustness. Training The Retrieve module involves an unsupervised process with traditional indexing and retrieval techniques.",
      "results\nshow that our model can quickly pick out high-quality templates from the training corpus, laying key foundation for effective article representations and summary generations. The",
      "fted templates, we further propose a multi-stage process for automatic retrieval of high-quality templates from training corpus. Extensive experiments were conducted on the Gigaword dataset BIBREF0 , a public dataset widely used for",
      "late discovered from training data to softly select key information from each source article to guide its summarization process. Extensive experiments on a standard summarization dataset were conducted and the",
      "alistic to create all the templates manually since this work requires considerable domain knowledge and is also labor-intensive. Fortunately, the summaries of some specific training articles can provide similar guidance to the summarization as hard templates. Accordingly, these summaries are referred to as soft templates, or templates for simplicity, in this paper.",
      "ith two other types of templates: randomly-selected templates and best templates identified by Fast Rank under different ranges. As shown in Table TABREF47 , the performance of our model improves constantly with the improvement of template quality (larger ranges lead to better chances for good templates).",
      "ing data, one promising direction is to make better use of the available training data by applying filters during summarization. In this paper, we propose a novel Bi-directional Selective Encoding with Template (BiSET) model, which leverages template discovered from training data to softly select key information from each source article to guide its summarization process.",
      "lly created by domain experts, and key snippets are then extracted and populated into the templates to form the final summaries. The advantage of such approach is it can guarantee concise and coherent summaries in no need of any training data.",
      "tive layer with two gates to mutually select key information from an article and its template to assist with summary generation. Due to the limitations in obtaining handcrafted templates, we further propose a multi-stage process for automatic retrieval of high-quality templates from training corpus.",
      "ve, Fast Rerank, and BiSET. For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates. Finally, BiSET mutually selects important information from the source article and the template to generate an enhanced article representation for summarization.",
      "al summaries. The advantage of such approach is it can guarantee concise and coherent summaries in no need of any training data. However, it is unrealistic to create all the templates manually since this work requires considerable domain knowledge and is also labor-intensive.",
      "rocess is essentially based on superficial word matching and cannot measure the deep semantic relationship between two articles. Therefore, the Fast Rerank module is developed to identify a best template from the candidates based on their deep semantic relevance with the source article. We regard the candidate with highest relevance as the template.",
      "results\nappear to be quite promising. Merely using the templates selected by our approach as the final summaries, our model can already achieve superior performance to some baseline models, demonstrating the effect of our templates."
    ],
    "answer": [
      "The model discovers templates from training data through an unsupervised process using traditional indexing and retrieval techniques, and a multi-stage process for automatic retrieval of high-quality templates from the training corpus."
    ]
  },
  {
    "title": "AttSum: Joint Learning of Focusing and Summarization with Neural Attention",
    "evidence": [
      "g features. Although these models achieved the state-of-the-art performance, they still heavily relied on hand-crafted features. A few researches explored to directly measure similarity based on distributed representations. BIBREF5 trained a language model based on convolutional neural networks to project sentences onto distributed representations.",
      "olated model, weights for neither query-dependent nor query-independent features could be learned well from reference summaries. In addition, when measuring the query relevance, most summarization systems merely make use of surface features like the TF-IDF cosine similarity between a sentence and the query BIBREF2 . However, relevance is not similarity.",
      "nting the overlapping units such as the n-grams, word sequences and word pairs between the peer summary and reference summaries. We take ROUGE-2 as the main measures due to its high capability of evaluating automatic summarization systems BIBREF19 . During the training data of pairwise ranking, we also rank the sentences according to ROUGE-2 scores.",
      "tion, the control of marijuana use, and the economic effectiveness, etc. All these aspects are mentioned in reference summaries. The sentences with the high TF-IDF similarity, however, are usually short and simply repeat the key words in the query. The advantage of AttSum over TF-IDF similarity is apparent in query relevance ranking.",
      "face features like the TF-IDF cosine similarity between a sentence and the query BIBREF2 . However, relevance is not similarity. Take the document cluster “d360f” in DUC 2005 as an example. It has the following query: What are the benefits of drug legalization? Here, “Drug legalization” are the key words with high TF-IDF scores.",
      "lly scaled. According to BIBREF13 , cosine similarity is the best metrics to measure the embedding similarity for summarization. In the training process, we apply the pairwise ranking strategy BIBREF10 to tune model parameters. Specifically, we calculate the ROUGE-2 scores BIBREF14 of all the sentences in the training dataset.",
      "models played a leading role in the extractive summarization area, due to its ability to reflect various sentence relationships. For example, BIBREF2 adopted manifold ranking to make use of the within-document sentence relationships, the cross-document sentence relationships and the sentence-to-query relationships.",
      "As can be seen, AttSum always enjoys a reasonable increase over ISOLATION, indicating that the joint model indeed takes effects. With respect to other methods, AttSum largely outperforms two baselines (LEAD and QUERY_SIM) and the unsupervised neural network model DocEmb.",
      ". The other system is called “QUERY_SIM”, which directly ranks sentences according to its TF-IDF cosine similarity to the query. In addition, we implement two popular extractive query-focused summarization methods, called MultiMR BIBREF2 and SVR BIBREF20 .",
      " to represent both manual and system summaries for the task of summary evaluation. Their method, however, did not surpass ROUGE. Recently, some works BIBREF25 , BIBREF4 have tried to use neural networks to complement sentence ranking features. Although these models achieved the state-of-the-art performance, they still heavily relied on hand-crafted features.",
      "M}} {\\bf {v}}(q)^T, {\\bf {M}} \\in \\mathbb {R}^{l \\times l}$ is a tensor function, and $\\sigma $ stands for the sigmoid function. The tensor function has the power to measure the interaction between any two elements of sentence and query embeddings. Therefore, two identical embeddings will have a low score. This characteristic is exactly what we need.",
      "face features are inadequate to measure the query relevance, which further augments the error of the whole summarization system. This drawback partially explains why it might achieve acceptable performance to adopt generic summarization models in the query-focused summarization task (e.g., BIBREF3 ). Intuitively, the isolation problem can be solved with a joint model.",
      "ation methods. Above all, we introduce two common baselines. The first one just selects the leading sentences to form a summary. It is often used as an official baseline of DUC, and we name it “LEAD”. The other system is called “QUERY_SIM”, which directly ranks sentences according to its TF-IDF cosine similarity to the query."
    ],
    "answer": [
      "AttSum compares to ISOLATION, LEAD, QUERY_SIM, MultiMR, SVR, and other generic summarization models."
    ]
  },
  {
    "title": "A Simple Approach to Multilingual Polarity Classification in Twitter",
    "evidence": [
      "c language given the complexity of having a number of lexical variations and errors introduced by the people generating content. In this contribution, our aim is to provide a simple to implement and easy to use multilingual framework, that can serve as a baseline for sentiment analysis contests, and as starting point to build new sentiment analysis systems.",
      "Conclusion\ns We presented a simple to implement multilingual framework for polarity classification whose main contributions are in two aspects. On one hand, our approach can serve as a baseline to compare other classification systems.",
      "discussion\nare presented on Section SECREF4 . Finally, Section SECREF5 concludes. Our Approach: Multilingual Polarity Classification We propose a method for multilingual polarity classification that can serve as a baseline as well as a framework to build more complex sentiment analysis systems due to its simplicity and availability as an open source software.",
      "ic; in fact, we use such property to adapt to several languages. Table TABREF2 summarizes the parameters and their valid values. The search space contains more than 331 thousand configurations when limited to multilingual and language independent parameters; while the search space reaches close to 4 million configurations when we add our three language-dependent parameters.",
      " a framework to build more complex sentiment analysis systems due to its simplicity and availability as an open source software. As we mentioned, this baseline algorithm for multilingual Sentiment Analysis (B4MSA) was designed with the purpose of being multilingual and easy to implement.",
      "ransforming techniques that optimize some performance measures where the chosen techniques are robust to typical writing errors. In this context, we propose a robust multilingual sentiment analysis method, tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish.",
      " sentiment analysis contests Figure FIGREF17 shows the performance on four contests, corresponding to three different languages. The performance corresponds to the multilingual set of features, i.e., we do not used language-dependent techniques. Figures UID18 - UID21 illustrates the",
      "ntests, namely, TASS'15 (Spanish) BIBREF17 , SemEval'15-16 (English) BIBREF18 , BIBREF19 , and SENTIPOLC'14 (Italian) BIBREF20 . In addition, our approach was tested with other languages (Arabic, German, Portuguese, Russian, and Swedish) to show that is feasible to use our framework as basis for building more complex sentiment analysis systems. From these languages, datasets and",
      "ransformations considered are either simple to implement or there is a well-known library (e.g. BIBREF6 , BIBREF7 ) to use them. It is important to note that to maintain the cross-language property, we limit ourselves to not use additional knowledge, this include knowledge from affective lexicons or models based on distributional semantics.",
      " practitioners or researchers looking for a bootstrapping sentiment classifier method in order to build more elaborated systems. Besides the text-transformations, the proposed framework uses a SVM classifier (with linear kernel), and, hyper-parameter optimization using random search and H+M over the space of text-transformations. The experimental",
      "epresentation such as spelling features, emoticons, word-based n-grams, character-based q-grams and language dependent features. On the other hand, our approach is a framework for practitioners or researchers looking for a bootstrapping sentiment classifier method in order to build more elaborated systems.",
      " are categorized in cross-language features (see Subsection SECREF3 ) and language dependent features (see Subsection SECREF9 ). It is important to note that, all the text-transformations considered are either simple to implement or there is a well-known library (e.g. BIBREF6 , BIBREF7 ) to use them.",
      "guages, three of them have important international contests, namely, SemEval (English), TASS (Spanish), and SENTIPOLC (Italian). Within the competitions our approach reaches from medium to high positions in the rankings; whereas in the remaining languages our approach outperforms the reported"
    ],
    "answer": [
      "The components of the multilingual framework include text-transformations such as spelling features, emoticons, word-based n-grams, character-based q-grams, and language-dependent features, as well as a SVM classifier with linear kernel and hyper-parameter optimization using random search and H+M over the space of text-transformations."
    ]
  },
  {
    "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing",
    "evidence": [
      "une the models on held-out data consisting of 30% training questions, while for final testing we use the complete training data. We use average precision (avg P.), average recall (avg R.) and average F $_1$ (avg F $_1$ ) proposed by berantsemantic2013 as evaluation metrics. Baselines We use GraphParser without paraphrases as our baseline.",
      "Results\nand",
      "results\nfrom five different systems for our question-answering experiments: original, mt, naive, ppdb and bilayered. First two are baseline systems. Other three systems use paraphrases generated from an L-PCFG grammar.",
      "F $_1$ ) proposed by berantsemantic2013 as evaluation metrics. Baselines We use GraphParser without paraphrases as our baseline. This gives an idea about the impact of using paraphrases. We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation BIBREF24 , BIBREF36 .",
      "naive, ppdb and bilayered. First two are baseline systems. Other three systems use paraphrases generated from an L-PCFG grammar. naive uses a word lattice with a single start-to-end path representing the input question itself, ppdb uses a word lattice constructed using the PPDB rules, and bilayered uses bi-layered L-PCFG to build word lattices.",
      "h with a very large beam. Experimental Setup Below, we give details on the evaluation dataset and baselines used for comparison. We also describe the model features and provide implementation details. Evaluation Data and Metric We evaluate our approach on the WebQuestions dataset BIBREF5 .",
      " . Note that these two graphs are non-isomorphic making it impossible to derive the correct grounding from the ungrounded graph. In fact, at least 15% of the examples in our development set fail to satisfy isomorphic assumption.",
      "results\nin correct answer is shown in Figure 3 . Note that these two graphs are non-isomorphic making it impossible to derive the correct grounding from the ungrounded graph.",
      ". For a given input question, first we build ungrounded graphs from its paraphrases. We convert these graphs to Freebase graphs. To learn this mapping, we rely on manually assembled question-answer pairs.",
      "phrases. We convert these graphs to Freebase graphs. To learn this mapping, we rely on manually assembled question-answer pairs. For each training question, we first find the set of oracle grounded graphs—Freebase subgraphs which when executed yield the correct answer—derivable from the question's ungrounded graphs.",
      "dylargescale2014 and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG.",
      "rge monolingual parallel corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for open-domain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser BIBREF25 .",
      "estions represents real Google search queries. We use the standard train/test splits, with 3,778 train and 2,032 test questions. For our development experiments we tune the models on held-out data consisting of 30% training questions, while for final testing we use the complete training data."
    ],
    "answer": [
      "GraphParser without paraphrases, naive, mt, ppdb, and bilayered."
    ]
  },
  {
    "title": "Story Ending Generation with Incremental Encoding and Commonsense Knowledge",
    "evidence": [
      "raph representation techniques, and therefore, be used to facilitate understanding story context and inferring coherent endings. Integrating the context clues and commonsense knowledge, the model can generate more reasonable endings than state-of-the-art baselines.",
      "ing the context clues and commonsense knowledge, the model can generate more reasonable endings than state-of-the-art baselines. Our contributions are as follows: Related Work The corpus we used in this paper was first designed for Story Cloze Test (SCT) BIBREF10 , which requires to select a correct ending from two candidates given a story context.",
      "ng a set of criteria BIBREF0 . Previous studies can be roughly categorized into two lines: rule-based methods and neural models. Most of the traditional rule-based methods for story generation BIBREF0 , BIBREF1 retrieve events from a knowledge base with some pre-specified semantic relations.",
      "ge, the model is able to produce reasonable story endings. context clues implied in the post and make the inference based on it. Automatic and manual evaluation shows that our model can generate more reasonable story endings than state-of-the-art baselines.",
      " Science Foundation of China (Grant No.61876096/61332007), and the National Key R&D Program of China (Grant No. 2018YFC0830200). We would like to thank Prof. Xiaoyan Zhu for her generous support. Appendix A: Annotation Statistics We presented the statistics of annotation agreement in Table 4 .",
      "ts in $X_3$ to right in $X_2$ , lay/down in $X_4$ to perfect/everything in $X_3$ , get/back in $Y$ to lay/down in $X_4$ , etc.). 2) The right column: for the use of commonsense knowledge, each sentence has attention weights ( $\\alpha _{x_{k,j}}^{(i)}$ ) to the knowledge graphs of the preceding sentence (e.g. eat in $X_2$ to meal in $X_1$ , dinner in $X_3$ to eat in $X_2$ , etc.).",
      "ding $Y=y_1y_2...y_l$ which is reasonable in logic, formally as $${Y^*} = \\mathop {argmax}\\limits _{Y} \\mathcal {P}(Y|X).$$ (Eq. 9) As aforementioned, context clue and commonsense knowledge is important for modeling the logic and casual information in story ending generation.",
      " generation. The model adopts an incremental encoding scheme to represent context clues which are spanning in the story context. In addition, commonsense knowledge is applied through multi-source attention to facilitate story comprehension, and thus to help generate coherent and reasonable endings.",
      "e, the chronological order or causal relationship between entities (or events) in adjacent sentences can be captured implicitly. To leverage commonsense knowledge which is important for generating a reasonable ending, a one-hop knowledge graph for each word in a sentence is retrieved from ConceptNet, and each graph can be represented by a vector in two ways.",
      "ot only to the words in the preceding sentence, but also the knowledge graphs which are retrieved from ConceptNet for each word. In this manner, commonsense knowledge can be encoded in the model through graph representation techniques, and therefore, be used to facilitate understanding story context and inferring coherent endings.",
      "d case shows that our model predicted a wrong entity at the last position where car is obviously more appropriate than daughter. It happens when the attention focuses on the wrong position, but in more cases it happens due to the noise of the commonsense knowledge base.",
      " the largest weight to some of everything to be just right (everything $\\rightarrow $ everything, perfect $\\rightarrow $ right). The example illustrates how the connection between context clues are built through incremental encoding and use of commonsense knowledge.",
      "to the knowledge graphs of the preceding sentence (e.g. eat in $X_2$ to meal in $X_1$ , dinner in $X_3$ to eat in $X_2$ , etc.). In this manner, the knowledge information is added into the encoding process of each sentence, which helps story comprehension for better ending generation (e.g., kitchen in $Y$ to oven in $X_2$ , etc.)."
    ],
    "answer": [
      "State-of-the-art baselines."
    ]
  },
  {
    "title": "Multi-Source Syntactic Neural Machine Translation",
    "evidence": [
      "l data as input to both encoders, while seq + null uses null input for the parsed encoder, where every source sentence is “( )”. The parse2seq system fails when given only sequential source data. On the other hand, both multi-source systems perform reasonably well without parsed data, although the BLEU scores are worse than multi-source with parsed data.",
      "urce systems perform reasonably well without parsed data, although the BLEU scores are worse than multi-source with parsed data. BLEU by Sentence Length For models that use source-side linearized parses (multi-source and parse2seq), the source sequences are significantly longer than for the seq2seq baseline.",
      "se2seq and multi-source systems require parsed source data at inference time. However, the parser may fail on an input sentence. Therefore, we examine how well these systems do when given only unparsed source sentences at test time. Table TABREF13 displays the",
      " are advantageous because they can inject syntactic information into the models without significant changes to the architecture. However, using linearized parses in a sequence-to-sequence (seq2seq) framework creates some challenges, particularly when using source parses.",
      "ences, since they contain node labels as well as words. Second, these systems often fail when the source sentence is not parsed. This can be a problem for inference, since the external parser may fail on an input sentence at test time.",
      "compared to multi-source; thus, while adding syntax to NMT can be helpful, some ways of doing so are more effective than others. Inference Without Parsed Sentences The parse2seq and multi-source systems require parsed source data at inference time. However, the parser may fail on an input sentence.",
      "n mechanism. The proposed model improves over both seq2seq and parsed baselines by over 1 BLEU on the WMT17 English-German task. Further analysis shows that our multi-source syntactic model is able to translate successfully without any parsed input, unlike standard parsed methods. In addition, performance does not deteriorate as much on long sentences as for the baselines.",
      "nges by taking both the sequential source sentence and its linearized parse simultaneously as input in a multi-source framework. Thus, the model is able to use the syntactic information encoded in the parse while falling back to the sequential sentence when necessary. Our proposed model improves over both standard and parsed NMT baselines.",
      "results\nof these experiments. For the parse2seq baseline, we use only sequential (seq) data as input. For the lexicalized and unlexicalized multi-source systems, two options are considered: seq + seq uses identical sequential data as input to both encoders, while seq + null uses null input for the parsed encoder, where every source sentence is “( )”.",
      "results\nof these experiments. For the parse2seq baseline, we use only sequential (seq) data as input.",
      "parses into NMT. This model consists of two identical RNN encoders with no shared parameters, as well as a standard RNN decoder. For each target sentence, two versions of the source sentence are used: the sequential (standard) version and the linearized parse (lexicalized or unlexicalized).",
      "other for syntax. However, they combined these representations at the word level, whereas we combine them on the sentence level. Their mixed RNN model is also similar to our parse2seq baseline, although the mixed RNN decoder attended only to words. As the mixed RNN model outperformed the parallel RNN model, we do not attempt to compare our model to parallel RNN.",
      "ntence is not parsed. This can be a problem for inference, since the external parser may fail on an input sentence at test time. We propose a method for incorporating linearized source parses into NMT that addresses these challenges by taking both the sequential source sentence and its linearized parse simultaneously as input in a multi-source framework."
    ],
    "answer": [
      "The performance drop of the model when there is no parsed input is not explicitly stated in the text. However, it is mentioned that the multi-source systems perform reasonably well without parsed data, although the BLEU scores are worse than multi-source with parsed data."
    ]
  },
  {
    "title": "Domain Adaptation for Neural Networks by Parameter Augmentation",
    "evidence": [
      "all, domain adaptation can give a large benefit. We compared the domain adaptation methods by the percentage of correct answers. The source dataset is 40K samples from MS COCO and the target dataset is the TOEIC dataset. We split the TOEIC dataset to 400 samples for training and 210 samples for testing. The percentages of correct answers for each method are summarized in Table 8 .",
      "n $\\mathcal {D}^s$ . Also, we have a target domain dataset $D^t$ , which is sampled from another distribution $\\mathcal {D}^t$ . Since we are considering supervised settings, each element of the datasets has a form of input output pair $(x,y)$ .",
      "xperiments on the following three datasets. The first experiment focuses on the situation where the domain adaptation is useful. The second experiment show the benefit of domain adaptation for both directions: from source to target and target to source. The third experiment shows an improvement in another metric.",
      " each image has captions. Suppose also that we would like to generate captions for exotic cuisine, which are rare in the corpus. It is usually very costly to make a new corpus for the target domain, i.e., taking and captioning those images. The research question here is how we can leverage the source domain dataset to improve the performance on the target domain.",
      "ins are quite limited due to the annotation cost. We use domain adaptation methods to improve the captions of the target domain. To simulate the scenario, we split the Microsoft COCO dataset into food and non-food domain datasets. The MS COCO dataset contains approximately 80K images for training and 40K images for validation; each image has 5 captions BIBREF12 .",
      "in adaptation of a binary classification problem. Suppose that we train SVM models for the source and target domains separately. The objective functions have the form of $ \\frac{1}{n_s} \\sum _{(x,y) \\in \\mathcal {D}_s} \\max (0, 1 - y(w_s^T \\Phi (x))) + \\lambda \\Vert w_s \\Vert ^2 \\\\ \\frac{1}{n_t} \\sum _{(x,y) \\in \\mathcal {D}_t} \\max (0, 1 - y(w_t^T \\Phi (x))) + \\lambda \\Vert w_t \\Vert ^2 , $ where $\\Phi (x)$ is the feature vector and $w_s, w_t$ are the SVM parameters.",
      " aims at improving the generalization performance of a new (target) domain by using a dataset from the original (source) domain. Suppose that, as the source domain dataset, we have a captioning corpus, consisting of images of daily lives and each image has captions. Suppose also that we would like to generate captions for exotic cuisine, which are rare in the corpus.",
      "e and Dual are at almost the same level. The second result is the domain adaptation from Flickr30K to MS COCO shown in Table 6 . This may not be a typical situation because the number of samples in the target domain is larger than that of the source domain. The SrcOnly model is trained only with Flickr30K and tested on the MS COCO dataset.",
      "es. The research question here is how we can leverage the source domain dataset to improve the performance on the target domain. As described by Daumé daume:07, there are mainly two settings of domain adaptation: fully supervised and semi-supervised. Our focus is the supervised setting, where both of the source and target domain datasets are labeled.",
      "ut of the network is “dualized”. In other words, we use different parameters $W$ in Eq. ( 1 ) for the source and target domains. For the source dataset, the model is trained with the first output and the second for the target dataset. The rest of the parameters are shared among the domains. This type of network design is often used for multi-task learning.",
      "networks. Their idea is to minimize the domain shift by aligning the second-order statistics of source and target distributions. In our setting, it is not necessarily true that there is a correspondence between the source and target input distributions, and therefore we cannot expect their method to work well.",
      "ic notations and formalization for domain adaptation. Let $\\mathcal {X}$ be the set of inputs and $\\mathcal {Y}$ be the outputs. We have a source domain dataset $D^s$ , which is sampled from some distribution $\\mathcal {D}^s$ . Also, we have a target domain dataset $D^t$ , which is sampled from another distribution $\\mathcal {D}^t$ .",
      "ion of the parameters, and therefore the latter training gives a good generalization even if the target domain dataset is small. The downside of this approach is the lack of the optimization objective. The other method is to design the neural network so that it has two outputs."
    ],
    "answer": [
      "The number of examples in the target domain is 210."
    ]
  },
  {
    "title": "Interactive Machine Comprehension with Information Seeking Agents",
    "evidence": [
      "set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information.",
      "how how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making. We believe that this setting can contribute in scaling models to web-level QA scenarios.",
      "scale pretrained language model). Our proposed setup and baseline agent presently use only a single word with the query command. However, a host of other options should be considered in future work. For example, multi-word queries with fuzzy matching are more realistic. It would also be interesting for an agent to generate a vector representation of the query in some latent space.",
      "lude\" the majority of a document's text and add context-sensitive commands that reveal \"glimpses\" of the hidden text to a model. We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making.",
      " representations $M_t$. Using $M_t$, the action generator outputs commands (defined in previous sections) to interact with iMRC. If the generated command is stop or the agent is forced to stop, the question answerer takes the current information at game step $t$ to generate head and tail pointers for answering the question; otherwise, the information gathering procedure continues.",
      "he encoder reads observation string $o_t$ and question string $q$ to generate attention aggregated hidden representations $M_t$. Using $M_t$, the action generator outputs commands (defined in previous sections) to interact with iMRC.",
      "t one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents.",
      "er a question about information contained therein. The supporting document is, more often than not, static and fully observable. This raises concerns, since models may find answers simply through shallow pattern matching; e.g., syntactic similarity between the words in questions and documents.",
      "unities to work toward solving this task. For our baseline, we adopted an off-the-shelf, top-performing MRC model and RL method. Either component can be replaced straightforwardly with other methods (e.g., to utilize a large-scale pretrained language model). Our proposed setup and baseline agent presently use only a single word with the query command.",
      "uestion $q$, rather than showing the entire paragraph $p$, we only show an agent the first sentence $s_1$ and withhold the rest. The agent must issue commands to reveal the hidden sentences progressively and thereby gather the information needed to answer question $q$. An agent decides when to stop interacting and output an answer, but the number of interaction steps is limited.",
      "y find answers simply through shallow pattern matching; e.g., syntactic similarity between the words in questions and documents. As pointed out by BIBREF5, for questions starting with when, models tend to predict the only date/time answer in the supporting document.",
      "we limit the agent to the Ctrl+F and stop commands. In this setting, an agent is forced to explore by means of search a queries. Baseline Agent ::: Training Strategy In this section, we describe our training strategy. We split the training pipeline into two parts for easy comprehension.",
      "more realistic. It would also be interesting for an agent to generate a vector representation of the query in some latent space. This vector could then be compared with precomputed document representations (e.g., in an open domain QA dataset) to determine what text to observe next, with such behavior tantamount to learning to do IR."
    ],
    "answer": [
      "The setup provides the following commands to models seeking information: Ctrl+F (search for token) and stop for searching through partially observed documents."
    ]
  },
  {
    "title": "Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models",
    "evidence": [
      " LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text.",
      " HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data.",
      "ributions obtained by the HMM's forward pass, and then add this information to the architecture in parallel with a 1-layer LSTM. The linear layer between the LSTM and the prediction layer is augmented with an extra column for each HMM state.",
      "on the state vectors (Figures 3 , 3 ). We explore several methods for building interpretable models by combining LSTMs and HMMs. The existing body of literature mostly focuses on methods that specifically train the RNN to predict HMM states BIBREF5 or posteriors BIBREF6 , referred to as hybrid or tandem methods respectively.",
      "s with the predictive power of RNNs. Sometimes, a small hybrid model can perform better than a standalone LSTM of the same size. We use visualizations to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
      "ch to increasing interpretability is by combining an RNN with a hidden Markov model (HMM), a simpler and more transparent model. We explore various combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model.",
      ". We then take the reverse approach where the HMM state probabilities are added to the output layer of the LSTM (see Figure 1 ). The LSTM model can then make use of the information from the HMM, and fill in the gaps when the HMM is not performing well, resulting in an LSTM with a smaller number of hidden state dimensions that could be interpreted individually (Figures 3 , 3 ).",
      " In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data).",
      " needs to fill in the gaps in the HMM's predictions. The HMM is written in Python, and the rest of the architecture is in Torch. We also build a joint hybrid model, where the LSTM and HMM are simultaneously trained in Torch. We implemented an HMM Torch module, optimized using stochastic gradient descent rather than FFBS.",
      "a 1-layer LSTM. The linear layer between the LSTM and the prediction layer is augmented with an extra column for each HMM state. The LSTM component of this architecture can be smaller than a standalone LSTM, since it only needs to fill in the gaps in the HMM's predictions. The HMM is written in Python, and the rest of the architecture is in Torch.",
      "t each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data.",
      "ically train the RNN to predict HMM states BIBREF5 or posteriors BIBREF6 , referred to as hybrid or tandem methods respectively. We first investigate an approach that does not require the RNN to be modified in order to make it understandable, as the interpretation happens after the fact.",
      ". On all text data sets, the hybrid algorithm performs a bit better than the standalone LSTM with the same LSTM state dimension. This effect gets smaller as we increase the LSTM size and the HMM makes less difference to the prediction (though it can still make a difference in terms of interpretability)."
    ],
    "answer": [
      "The authors use the following methods to reach the conclusion that LSTMs and HMMs learn complementary information: \n1. Training an LSTM to fill in gaps in the HMM's performance.\n2. Jointly training a hybrid model with both LSTM and HMM.\n3. Adding HMM state probabilities to the output layer of the LSTM.\n4. Using visualizations to show how the LSTM and HMM components complement each other in terms of features learned in the data.\n5. Applying k-means clustering to the LSTM state vectors and color-coding the training data with the clusters."
    ]
  },
  {
    "title": "BERT-Based Arabic Social Media Author Profiling",
    "evidence": [
      "ugmentation. To further improve the performance of our models, we introduce in-house labeled data that we use to fine-tune BERT. For the gender classification task, we manually label an in-house dataset of 1,100 users with gender tags, including 550 female users, 550 male users. We obtain 162,829 tweets by crawling the 1,100 users' timelines.",
      "ta. Then we augment shared task data with in-house data for gender and dialect, showing the utility of augmenting training data. Our best models on the shared task test data are acquired with a majority voting of various BERT models trained under different data conditions.",
      "alect, and gender using BERT models under various data conditions, showing the utility of additional, in-house data on the task. We also showed that a majority vote of our models trained under different conditions outperforms single models on the official evaluation.",
      "results\n. We then provide a literature review and conclude. Data For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers.",
      " experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test.",
      " has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}.",
      "he level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users.",
      "weet level and then port these predictions at the user level. For our core modelling, we fine-tune BERT on the shared task data. We also introduce an additional in-house dataset labeled with dialect and gender tags to the task as we will explain below. As a baseline, we use a small gated recurrent units (GRU) model. We now introduce our tweet-level models.",
      "likes what on Facebook), network (e.g., who follows a given user on Instagram), and content (e.g., what people post to Twitter). Availability of such data have made it possible to make discoveries about individuals and communities, mobilizing social and psychological research and employing natural language processing methods.",
      "Introduction\nThe proliferation of social media has made it possible to collect user data in unprecedented ways.",
      "results\non the official test data. We acquire 54.72% accuracy for age, 81.67% accuracy for gender, 93.75% accuracy for dialect, and 40.97% joint accuracy.",
      "ender dataset with the gender TRAIN data (from shared task) to obtain an extended dataset, to which we refer as EXTENDED_Gender. For the dialect identification task, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task. In this way, we obtain 298,929 tweets (Sudan only has 18,929 tweets).",
      "re, we will investigate automatically extending training data for these tasks as well as better representation learning methods. Acknowledgement We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), the Social Sciences Research Council of Canada (SSHRC), and Compute Canada (www.computecanada.ca)."
    ],
    "answer": [
      "In-house data employed: \n- 1,100 users labeled with gender tags (550 female, 550 male)\n- 162,829 tweets crawled from the 1,100 users' timelines\n- 298,929 tweets from an in-house dataset gold labeled with 15 dialect classes\n- EXTENDED_Gender dataset created by combining the in-house gender dataset with the shared task gender TRAIN data"
    ]
  },
  {
    "title": "WiSeBE: Window-based Sentence Boundary Evaluation",
    "evidence": [
      "valuated and compared the performance of two different SBD systems over a set of YouTube videos in a multi-reference enviroment. The first system (S1) employs a Convolutional Neural Network to determine if the middle word of a sliding window corresponds to a SU boundary or not BIBREF30 .",
      "ber of windows INLINEFORM7 with at least one boundary INLINEFORM8 divided by the number of windows INLINEFORM9 in INLINEFORM10 . Evaluating with WiSeBEWiSeBE To exemplify the INLINEFORM0 score we evaluated and compared the performance of two different SBD systems over a set of YouTube videos in a multi-reference enviroment.",
      "ranscripts into “something that is similar to sentences”. They used a syntatic analyzer to identify the phrases within the text. A wide study focused in unbalanced data for the SBD task was performed by Liu et al. BIBREF16 .",
      "discussed in Section SECREF1 , further NLP tasks rely on the result of SBD, meaning that is crucial to have a good segmentation. But comparing the output of a system against a unique reference will provide a reliable score to decide if the system is good or bad? Bohac et al. BIBREF24 compared the human ability to punctuate recognized spontaneous speech.",
      " explored BIBREF24 , BIBREF25 , BIBREF19 , BIBREF26 , which is advantageous when both audio signal and transcript are available. With respect to the methods used for SBD, they mostly rely on statistical/neural machine translation BIBREF18 , BIBREF27 , language models BIBREF9 , BIBREF16 , BIBREF22 , BIBREF6 , conditional random fields BIBREF21 , BIBREF28 , BIBREF23 and deep neural networks BIBREF29 , BIBREF20 , BIBREF13 .",
      "Sentence Boundary Detection (SBD) should be performed over ASR transcripts to reach a minimal syntactic information in the text. To measure the performance of a SBD system, the automatically segmented transcript is evaluated against a single reference normally done by a human. But given a transcript, does it exist a unique reference?",
      "fficulties that are present when working with spoken language and the possible disagreements that a task like SBD could provoke. INLINEFORM0 shows to be correlated with standard SBD metrics, however we want to measure its correlation with extrinsic evaluations techniques like automatic summarization and machine translation.",
      "h not only evaluates the performance of a system against all references, but also takes into account the agreement between them. According to your point of view, this inclusivity is very important given the difficulties that are present when working with spoken language and the possible disagreements that a task like SBD could provoke.",
      "of ASR systems to obtain the correct sequence of words with almost no concern of the overall structure of the document BIBREF9 . Similar to SBD is the Punctuation Marks Disambiguation (PMD) or Sentence Boundary Disambiguation. This task aims to segment a formal written text into well formed sentences based on the existent punctuation marks BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 .",
      " two approaches. The fist one was to calculate the SER for each of one the two available references and then compute their mean. They found this approach to be very strict because for those boundaries where no agreement between references existed, the system was going to be partially wrong even the fact that it has correctly predicted the boundary.",
      "sed metric for evaluating SBD systems based on multi-reference (dis)agreement. The rest of this article is organized as follows. In Section SECREF2 we set the frame of SBD and how it is normally evaluated. WiSeBE is formally described in Section SECREF3 , followed by a multi-reference evaluation in Section SECREF4 . Further analysis of WiSeBE and",
      "ify the phrases within the text. A wide study focused in unbalanced data for the SBD task was performed by Liu et al. BIBREF16 . During this study they followed the segmentation scheme proposed by the Linguistic Data Consortium on the Simple Metadata Annotation Specification V5.0 guideline (SimpleMDE_V5.0) BIBREF14 , dividing the transcripts in Semantic Units.",
      "Language Processing tasks like Part of Speech Tagging, Question Answering or Automatic Summarization. But what about evaluation? Do standard evaluation metrics like precision, recall, F-score or classification error; and more important, evaluating an automatic system against a unique reference is enough to conclude how well a SBD system is performing given the final application of the transcript?"
    ],
    "answer": [
      "The two SBD systems compared were S1, which employs a Convolutional Neural Network to determine if the middle word of a sliding window corresponds to a sentence boundary or not, and another system that uses a syntactic analyzer to identify phrases within the text."
    ]
  },
  {
    "title": "Recognizing Musical Entities in User-generated Content",
    "evidence": [
      "instantiate several machine learning algorithms to perform entity recognition combining task-specific and corpus-based features. We also show how to improve recognition",
      " recently several contributions by Machine Learning scientists have helped in integrating probabilistic models into NER systems. In particular, new developments in neural architectures have become an important resource for this task.",
      " state-of-the-art NLP libraries and SimpleBrainz, an RDF knowledge base created from MusicBrainz after a simplification process. Furthermore, Twitter has also been at the center of many studies done by the MIR community. As example, for building a music recommender system BIBREF17 analyzes tweets containing keywords like nowplaying or listeningto.",
      "terature and partly specifically designed for this task, for building statistical models able to recognize the musical entities. To that aim, we perform several experiments with a supervised learning model, Support Vector Machine (SVM), and a recurrent neural network architecture, a bidirectional LSTM with a CRF layer (biLSTM-CRF).",
      "uch as Decision Trees and Naive Bayes, obtaining the best accuracy when detecting named entities from the user-generated tweets. However, recent advances in Deep Learning techniques have shown that the NER task can benefit from the use of neural architectures, such as biLSTM-networks BIBREF3 , BIBREF4 .",
      "results\nshown that SVM outperforms other machine learning models, such as Decision Trees and Naive Bayes, obtaining the best accuracy when detecting named entities from the user-generated tweets.",
      "results\n, the criteria is to give priority to the entities recognized from the machine learning techniques. If they do not return any entities, the entities predicted by the schedule matching are considered. Our strategy is justified by the poorer",
      "ecognizing the entities. From the experiments we have seen that generally the biLSTM-CRF architecture outperforms the SVM model. The benefit of using the whole set of features is evident in the training part, but while testing the inclusion of the features not always leads to better",
      " our experiments are tailored to the case of classical music, hence they might not be representative if applied to other fields. We do not exclude that our method can be adapted for detecting other kinds of entity, but it might be needed to redefine the features according to the case considered.",
      "chine learning, psychology and many others, for extracting knowledge from musical objects (be them audio, texts, etc.) BIBREF6 . In the last decade, several MIR tasks have benefited from NLP, such as sound and music recommendation BIBREF7 , automatic summary of song review BIBREF8 , artist similarity BIBREF9 and genre classification BIBREF10 .",
      "o a specific class. It starts to be defined in the early '80, and over the years several approaches have been proposed BIBREF2 . Early systems were based on handcrafted rule-based algorithms, while recently several contributions by Machine Learning scientists have helped in integrating probabilistic models into NER systems.",
      ", Support Vector Machine (SVM), and a recurrent neural network architecture, a bidirectional LSTM with a CRF layer (biLSTM-CRF). The contributions in this work are summarized as follows: The paper is structured as follows. In Section 2, we present a review of the previous works related to Named Entity Recognition, focusing on its application on UGC and MIR.",
      " of the Recall score is inverted. Finally, we test the impact of using the schedule matching together with a biLSTM-CRF network. In this experiment, we consider the network trained using all the features proposed, and the embeddings not pre-trained. Table 8 reports the"
    ],
    "answer": [
      "Support Vector Machine (SVM), and a recurrent neural network architecture, a bidirectional LSTM with a CRF layer (biLSTM-CRF)."
    ]
  },
  {
    "title": "Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER",
    "evidence": [
      "results\n. There are many recent approaches to zero-resource cross-lingual classification and NER, including adversarial learning BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , using a model pretrained on parallel text BIBREF8 , BIBREF9 , BIBREF10 and self-training BIBREF11 .",
      "NEFORM2 for the discriminator loss. In Table TABREF13 , we report the classification accuracy for all of the languages in MLDoc. Generally, adversarial training improves the accuracy across all languages, and the improvement is sometimes dramatic versus the BERT non-adversarial baseline.",
      " very well in zero-shot and zero-resource cross-lingual settings, where only labeled English data is used to finetune the model. We improve upon multilingual BERT's zero-resource cross-lingual performance via adversarial learning. We report the magnitude of the improvement on the multilingual MLDoc text classification and CoNLL 2002/2003 named entity recognition tasks.",
      "on (without the labels) for the adversarial training. We formulate the adversarial task as a binary classification problem (i.e. English versus non-English.) We add a language discriminator module which uses the BERT embeddings to classify whether the input sentence was written in English or the non-English language.",
      "L NER tasks. Language-adversarial training was generally effective, though the size of the effect appears to depend on the task. We observed that adversarial training moves the embeddings of English text and their non-English translations closer together, which may explain why it improves cross-lingual performance.",
      "ined, we were able to improve on BERT's zero-resource cross-lingual performance on the MLDoc classification and CoNLL NER tasks. Language-adversarial training was generally effective, though the size of the effect appears to depend on the task.",
      "e magnitude of the improvement on the multilingual MLDoc text classification and CoNLL 2002/2003 named entity recognition tasks. Furthermore, we show that language-adversarial training encourages BERT to align the embeddings of English documents and their translations, which may be the cause of the observed performance gains.",
      "adversarial training, which suggests that the cross-lingual performance is more consistent when adversarial training is applied. (We note that the batch size and learning rates are the same for all the languages in MLDoc, so the variation seen in Figure FIGREF15 are not affected by those factors.) CoNLL NER",
      "even though cross-lingual resources (e.g. parallel text, dictionaries, etc.) are not used during BERT pretraining or finetuning. We apply adversarial learning to further improve upon this baseline, achieving state-of-the-art zero-resource",
      " For language-adversarial training, we used the text portion of german.train.10000, french.train.10000, etc. without the labels. We used a learning rate of INLINEFORM0 for the task loss, INLINEFORM1 for the generator loss and INLINEFORM2 for the discriminator loss. In Table TABREF13 , we report the classification accuracy for all of the languages in MLDoc.",
      "loss and INLINEFORM2 for the discriminator loss. In Table TABREF19 , we report the F1 scores for all of the CoNLL NER languages. When combined with adversarial learning, the BERT cross-lingual F1 scores increased for German over the non-adversarial baseline, and the scores remained largely the same for Spanish and Dutch. Regardless, the BERT zero-resource performance far exceeds the",
      "sarial Learning Language-adversarial training BIBREF12 was proposed for generating bilingual dictionaries without parallel data. This idea was extended to zero-resource cross-lingual tasks in NER BIBREF5 , BIBREF6 and text classification BIBREF4 , where we would expect that language-adversarial techniques induce features that are language-independent.",
      "results\nbased on the average of 4 training runs. [ht] Pseudocode for adversarial training on the multilingual text classification task. The batch size is set at 1 for clarity. The parameter subsets are INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 ."
    ],
    "answer": [
      "Adversarial learning has stronger performance gains for text classification than for NER."
    ]
  }
]